[
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "Below is a list of contributors to this blog.\n\n\n  \n\n\nJinseob Kim\nJinhwan Kim\nChangwoo Lim\nWon Kim\nSeoyoon Kim\nJunhyuk Ko\nYujin Lee\nJihee Han\nBeomsu Park\nYumin Kim\nSiyeol Jung\nJisoo Kim\nHyunjun Ko\nChaehee Lee\nHyunki Lee\n\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Contributors},\n  url = {https://blog.zarathu.com/jp/contributors.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Contributors.” n.d. https://blog.zarathu.com/jp/contributors.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "차라투 블로그",
    "section": "",
    "text": "rhub와 Github action를 활용한 OS별 R 패키지 검증\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ngithub\n\n\ngithub action\n\n\nrhub\n\n\nrcmdcheck\n\n\n\nrhub 패키지와 Github action을 사용해 R 패키지를 다양한 OS에서 정상적으로 설치, 실행할 수 있도록 확인 하는 R CMD CHECK 방법을 소개합니다. \n\n\n\n\n\n2024/05/13\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nbslib: input_task_button 소개\n\n\n\n\n\n\nR\n\n\nshiny\n\n\nbslib\n\n\nux\n\n\n\nbslib의 0.7.0에서 새롭게 추가된 input_task_button에 대해 소개합니다. \n\n\n\n\n\n2024/03/30\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nProcess macro 소개\n\n\n\n\n\n\nR\n\n\n\n조절효과, 매개효과 분석을 위한 도구인 process macro를 소개합니다.\n\n\n\n\n\n2024/03/14\n\n\nHeeseok Choi\n\n\n\n\n\n\n\n\n\n\n\n\nshinylive 를 활용한 quarto 블로그에 shiny 추가 방법\n\n\n\n\n\n\nR\n\n\nshiny\n\n\ngithubpage\n\n\ndocumentation\n\n\nwebsite\n\n\nquarto\n\n\n\nwebR의 개선 버전인 shinylive 패키지를 사용하여 정적 페이지에 shiny application 추가하기 \n\n\n\n\n\n2024/03/05\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR&D 시험인증 후기 및 개발, 행정 관련 느낀 점\n\n\n\n\n\n\nR&D\n\n\nreview\n\n\nIITP\n\n\n시험인증\n\n\n행정\n\n\n\nR&D 시험인증 및 R&D 관련 느낀점입니다. \n\n\n\n\n\n2024/01/23\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio Server에 2FA(OTP) 도입하기\n\n\n\n\n\n\nR\n\n\nrstudio server\n\n\nsecurity\n\n\n\nRStudio Server에 2차 인증(OTP)를 도입한 후기입니다. \n\n\n\n\n\n2024/01/05\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Dashboard를 이용해 대시보드 개발하기\n\n\n\n\n\n\nR\n\n\npython\n\n\nquarto\n\n\nquarto dashboard\n\n\ndashboard\n\n\n\nQuarto를 사용하여 R과 Python, Julia로 인터랙티브한 대시보드를 만드는 방법을 소개합니다. \n\n\n\n\n\n2023/12/11\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRstudio에서 Copilot을 활용해 AI로 코딩하기\n\n\n\n\n\n\nR\n\n\ngithub\n\n\nrstudio\n\n\ncopilot\n\n\n\nGithub 의 AI 서비스 Copilot을 Rstudio에 연동하여 자동 완성으로 코딩하는 방법을 소개합니다. \n\n\n\n\n\n2023/11/21\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nDiscourse 기반 커뮤니티 구축\n\n\n\n\n\n\ndocker\n\n\nwebsite\n\n\ndiscourse\n\n\ncommunity\n\n\n\n커뮤니티를 만들기 위한 오픈소스 무료 플랫폼인 Discourse와 만드는 과정을 소개합니다. \n\n\n\n\n\n2023/11/10\n\n\nJisoo Kim\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto Manuscripts를 이용해 학술 논문 작성하기\n\n\n\n\n\n\nR\n\n\nquarto\n\n\nquarto manuscript\n\n\narticle\n\n\n\n코드 블록을 포함한 학술 논문을 웹에 발행하고, PDF, docx등 다양한 형식으로 다운받을 수 있는 Quarto Manuscript를 소개합니다. \n\n\n\n\n\n2023/10/18\n\n\nJihee Han\n\n\n\n\n\n\n\n\n\n\n\n\nR로 만든 PPT 슬라이드 고해상도로 저장하기\n\n\n\n\n\n\nR\n\n\npowerpoint\n\n\nvisualization\n\n\n\nofficer 패키지를 활용해 파워포인트로 저장한 이미지를 300DPI의 고해상도로 내보내기 (해상도 설정 변경 방법은 운영체제마다 다르며, 본 게시글은 Windows OS를 기준으로 합니다.) \n\n\n\n\n\n2023/09/27\n\n\nJihee Han\n\n\n\n\n\n\n\n\n\n\n\n\nelectron forge를 활용하여 Standalone Shiny Application 제작하기\n\n\n\n\n\n\nR\n\n\nelectron forge\n\n\nshiny\n\n\nquarto\n\n\nstandalone\n\n\nexe\n\n\nwindows\n\n\n\nelectron forge라는 기술을 활용하여 사용자의 PC에서 R과 Rstudio를 설치하지 않고도 Shiny App을 사용할 수 있게 하는 (exe) 프로그램을 만드는 방법을 소개합니다. 단, 개발 과정은 OS에 따라 조금씩 다르며, windows를 기준으로 합니다. \n\n\n\n\n\n2023/09/18\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nweb assembly를 이용하여 웹페이지에서 Shiny App 활용하기\n\n\n\n\n\n\nR\n\n\nwebR\n\n\nshiny\n\n\nwasm\n\n\ngithub page\n\n\nquarto\n\n\n\nwebR이라는 기술을 활용하여 별도의 웹 서버를 사용하지 않고도 유저의 웹 브라우저(크롬)에서 Shiny App을 사용할 수 있게 하는 방법을 소개합니다. \n\n\n\n\n\n2023/09/10\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nweb assembly를 이용하여 웹페이지에서 R 활용하기\n\n\n\n\n\n\nR\n\n\nwebR\n\n\nwasm\n\n\ngithub page\n\n\nquarto\n\n\n\nwebR이라는 기술을 활용하여 별도의 웹 서버를 사용하지 않고도 유저의 웹 브라우저(크롬)에서 R을 사용할 수 있게 하는 방법을 소개합니다. \n\n\n\n\n\n2023/09/09\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기\n\n\n\n\n\n\nR\n\n\nofficer\n\n\nvectorgraphics\n\n\nggplot2\n\n\npowerpoint\n\n\nvisualization\n\n\n\nR을 활용하여 만든 이미지를 PowerPoint에서 편집할 수 있도록, 벡터 그래픽을 만드는 법을 소개합니다. \n\n\n\n\n\n2023/07/01\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny 기반 방역관리 위험도 평가 대시보드\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\nRSQLite, shinyauthr 등 여러 R package를 이용한 웹 기반 방역관리 위험도 평가 대시보드 제작 과정 \n\n\n\n\n\n2023/06/20\n\n\nYeongho Kim\n\n\n\n\n\n\n\n\n\n\n\n\nADaM in CDISC and tidyCDISC\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\n\nAnalysis Data Model in CDISC, tidyCDISC 오픈소스 프로그램에 대해서 알아보자 \n\n\n\n\n\n2023/05/02\n\n\nSeoyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nsunburstr 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\n\nsunburstr 패키지의 이용한 계층적 데이터 표시 방법에 대해 알아보자 \n\n\n\n\n\n2023/04/21\n\n\nSeoyoon Kim\n\n\n\n\n\n\n\n\n\n\n\n\npkgdown을 활용한 R 패키지 문서화\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\npkgdown\n\n\ngithub page\n\n\ndocumentation\n\n\nwebsite\n\n\n\nR 패키지를 다른 사람들도 잘 활용할 수 있게 설명해주는 웹사이트를 pkgdown을 사용하여 만들어보자 \n\n\n\n\n\n2023/03/17\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nshiny.likert 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\nplotly\n\n\nshinyapps\n\n\nlikert\n\n\ndatavis\n\n\nnocode\n\n\n\nlikert 패키지를 사용하는 shiny apps 개발/ 배포 과정 \n\n\n\n\n\n2023/02/15\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython을 이용한 검색포털 API 활용\n\n\n\n\n\n\npython\n\n\nAPI\n\n\n\npython을 이용한 크롤링 검색결과 저장하기 \n\n\n\n\n\n2023/02/15\n\n\nWon Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython과 Slack API를 이용한 대나무숲 앱 제작\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\n\nSlack API를 이용하여 Python 기반 대나무숲 앱 만들기 \n\n\n\n\n\n2023/02/07\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nlikert 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\nshiny\n\n\nplotly\n\n\nquarto\n\n\n\n문항별 만족/불만족을 한번에 표현하는 likert chart를 그려보자 \n\n\n\n\n\n2023/02/05\n\n\nJinhwan Kim\n\n\n\n\n\n\n\n\n\n\n\n\nPython Streamlit 패키지를 이용한 대시보드 만들기\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\n\nstreamlit 패키지의 사용방법을 알아보자 \n\n\n\n\n\n2023/02/01\n\n\nWon Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의료데이터분석가 성장기\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의료데이터분석가 성장기를 다시 정리했습니다. 본 내용은 보건산업진흥원이 지원하고 성균관대학교 의과대학에서 주관하는 “융합형 의사과학자 심포지엄” 에서 발표 예정입니다. \n\n\n\n\n\n2022/10/17\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGAM(Generalized Additive Model) 소개\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n비선형모델인 GAM(Generalized Additive Model) 을 소개합니다. 본 강의는 성균관대 바이오헬스 규제과학과 강의자료로 쓰일 예정입니다. \n\n\n\n\n\n2022/09/30\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny for Python\n\n\n\n\n\n\npython\n\n\nshiny\n\n\n\nPython으로 반응형 웹 어플리케이션을 만들수 있는 Shiny for Python에 대해 소개합니다.\n\n\n\n\n\n2022/08/19\n\n\nChaehee Lee\n\n\n\n\n\n\n\n\n\n\n\n\ncollapse 패키지 소개\n\n\n\n\n\n\nR\n\n\ncollapse\n\n\ndata.table\n\n\n\ncollapse 패키지를 소개하고 data.table 패키지와 비교하여 파악해보겠습니다.\n\n\n\n\n\n2022/07/25\n\n\nBeomsu Park\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table 패키지 기초\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\nlecture\n\n\n\n데이터를 빠르게 가공할 수 있는 data.table에 대하여 패키지 설치부터, 기본 구조 및 데이터를 가공하여 재구조화 하는 방법에 대해서 소개합니다.\n\n\n\n\n\n2022/07/13\n\n\nJunhyuk Ko\n\n\n\n\n\n\n\n\n\n\n\n\n데이터과학자가 갖춰야할 기술\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nstatistics\n\n\n\n의료분야 데이터과학자에 필요한 역량을 정리하였습니다. 본 내용은 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 발표할 예정입니다. \n\n\n\n\n\n2022/06/27\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n22년 지원사업 후기\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n22년 각종 지원사업 선정, 탈락 후기를 공유합니다. \n\n\n\n\n\n2022/04/12\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR로 논문용 그래프 그리기\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\n\nR 기본 함수, ggplot2 패키지, ggpubr 패키지를 활용해 의학논문에 필요한 그래프를 만들어보자. \n\n\n\n\n\n2022/03/25\n\n\nYumin Kim\n\n\n\n\n\n\n\n\n\n\n\n\nReviewer들을 위한 의학통계\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 로 발표할 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2022/03/19\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\ndata.table 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ndata.table\n\n\n\n대용량의 데이터를 분산 처리 시스템 없이 처리할 수 있는 data.table 데이터 구조와 이를 조작, 관리하는데 사용하는 data.table 패키지에 대해서 소개합니다. \n\n\n\n\n\n2022/02/11\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n\ndocker\n\n\nlecture\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n2022/02/08\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\ntableone 패키지 소개\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n효율적으로 의학 연구 논문에 들어갈 table1을 만들 수 있는 tableone 패키지에 대해 소개합니다.\n\n\n\n\n\n2022/02/07\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\ngtsummary 패키지 소개\n\n\n\n\n\n\nR\n\n\nrpackage\n\n\ngtsummary\n\n\n\n데이터 셋의 변수를 하나의 테이블로 요약하여 효율적으로 논문에 들어갈 table1을 만들 수 있는 gtsummary 패키지에 대해 소개합니다.\n\n\n\n\n\n2022/02/04\n\n\nYujin Lee\n\n\n\n\n\n\n\n\n\n\n\n\n창업 경험 공유\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 창업 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2022/01/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용\n\n\n\n\n\n\ndocker\n\n\ngithub\n\n\n\nMySQL 기반 DockerContainer에서 GitHub-Repository로의 주기적인 DB 백업 구현\n\n\n\n\n\n2022/01/20\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\n인턴십 - Django로 게시판 만들고 기능 추가하기\n\n\n\n\n\n\ndjango\n\n\n\n숭실대학교 인턴십 프로그램을 통해 인턴으로 활동하게 된 차라투에서 1주차 동안 학습한 내용에 대해 공유합니다.\n\n\n\n\n\n2022/01/05\n\n\nSiyeol Jung\n\n\n\n\n\n\n\n\n\n\n\n\nNotion으로 홈페이지 제작후 oopy로 배포한 후기\n\n\n\n\n\n\nreview\n\n\n\nNotion과 Oopy를 사용해 개편한 당사 홈페이지의 구축 논의 사항과 장·단점을 설명하였습니다.\n\n\n\n\n\n2021/10/01\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 환자데이터 입력웹 개발(2)\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n4월에 이어 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이터 입력웹을 소개합니다. 본 내용은 차라투가 후원하는 Shinykorea 10월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2021/09/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Login\n\n\n\n\n\n\nreview\n\n\n\nZarathu 앱에 적용할 예정인 구글 로그인에 대해 소개합니다.\n\n\n\n\n\n2021/09/11\n\n\nHyunki Lee\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 공공빅데이터 분석지원\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\nR 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유합니다. 본 내용은 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에서 발표할 예정입니다.\n\n\n\n\n\n2021/08/21\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 의학연구지원\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\nR 활용 의학연구지원경험을 공유합니다. 본 내용은 “Be a data scientist - major actor in the future research” 라는 제목으로 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에서 발표할 예정입니다. \n\n\n\n\n\n2021/08/19\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\ngoogleAuth\n\n\n\n\n\n\nlecture\n\n\n\nFollow up assignment of ShinyProxy lecture\n\n\n\n\n\n2021/07/25\n\n\nHyunjun Ko\n\n\n\n\n\n\n\n\n\n\n\n\n창업지원사업 도전기\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n지금까지 창업지원사업 도전했던 경험을 공유합니다. 본 내용은 차라투가 후원하는 Shinykorea 7월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2021/07/11\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n채널톡(channel.io) 설치 후기\n\n\n\n\n\n\nreview\n\n\n\n최근 Zarathu 공식 홈페이지에 추가한 channel.io 서비스 관련 적용 후기입니다. \n\n\n\n\n\n2021/07/05\n\n\nChangwoo Lim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 환자데이터 입력웹 개발\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 4월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2021/04/02\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n코로나 수리모델링: 서울시 감염병연구센터 자문\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 정리했습니다. 본 내용은 2월 Shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2021/01/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n생존분석 실습\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\nKaplan-meier curve, 비례위험가정 확인, Time-dependent analysis 그리고 모수적 생존분석을 중심으로 R 코드를 정리했습니다. 본 내용은 성균관의대 사회의학교실 특강에서 실습할 예정입니다.\n\n\n\n\n\n2020/10/31\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학통계지원 소개\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n차라투 업무 소개입니다. 본 내용은 영남대학교 “의사과학자 역량 배가 프로젝트” 에서 발표할 예정입니다.\n\n\n\n\n\n2020/10/05\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio & Shiny Docker 소개\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앴습니다. 본 내용은 Shinykorea 10월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2020/10/05\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n메타분석 웹 개발 후기\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\n메타분석 ShinyApps 만든 후기를 정리하였습니다. 본 내용은 Shinykorea 8월 밋업에서 발표할 예정입니다..\n\n\n\n\n\n2020/08/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n회귀분석 in 의학연구\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의학 연구에서 사용하는 선형/로지스틱 회귀분석과 Cox 비례위험모형을 소개합니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n2020/07/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학 연구에서의 기술통계\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n의학 연구에서 Table 1 에 활용되는 기술통계를 정리하였습니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n2020/07/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n2020년 만들었던 ShinyApps\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\nR\n\n\n\n올해 만들었던 ShinyApps 를 간단히 정리하였습니다. 본 내용은 6월 shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2020/06/20\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\n %&gt;% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다.\n\n\n\n\n\n2020/04/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트: 기초\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\nR 기본 문법과, 보험공단 샘플 데이터를 이용한 데이터 매니지먼트 방법을 정리하였습니다. 본 내용은 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다. \n\n\n\n\n\n2020/03/10\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nTokyoR 81회 리뷰\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n일본 R 밋업 중 하나인 TokyoR 중 shiny 특집을 리뷰하였습니다. 본 내용은 차라투가 후원하는 Shinykorea 3월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2020/02/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR로 만드는 웹 애플리케이션\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\nshiny\n\n\n\nR과 shiny로 웹 애플리케이션을 만든 경험을 소개합니다. 본 내용은 디시인사이드가 후원하는 프로그래밍 갤러리 컨퍼런스 2020 에서 발표할 예정입니다.\n\n\n\n\n\n2020/01/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRSelenium 이용 팁\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\nRSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 1월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n2019/11/30\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n의료데이터 분석가가 되기까지의 경험을 슬라이드로 공유합니다. 본 내용은 동국대학교 의생명공학과 세미나에서 발표할 예정으로, 초청해주신 김진식 교수님께 감사드립니다.\n\n\n\n\n\n2019/11/04\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 워크샵: 서울IT직업전문학교 국비교육\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nshiny\n\n\nR\n\n\nworkshop\n\n\n\nShiny 기초학습을 위한 강의 슬라이드와 실습파일입니다. 본 내용은 “서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정” 에서 쓰일 예정입니다.\n\n\n\n\n\n2019/10/27\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRUCK2019 발표: From ShinyApps to CRAN\n\n\n\n\n\n\npresentation\n\n\nrpackage\n\n\nR\n\n\nshiny\n\n\n\n맞춤형 의학연구 앱을 만들고, 그것을 패키지로 만들어 CRAN에 배포한 경험을 슬라이드로 정리하였습니다. 본 내용은 R User Conference in Korea 2019(RUCK 2019)에서 발표하였습니다. \n\n\n\n\n\n2019/10/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 이용 공공빅데이터 분석 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nR\n\n\n\nR을 이용, 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의대 학술원 특강에서 발표할 예정입니다. \n\n\n\n\n\n2019/09/20\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShinyApps 에 로그인 기능 넣기\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\nShiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명하였습니다. 본 내용은 Zarathu가 후원하는 Shinykorea 9월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/08/25\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장(2): 허수축 도입\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nrpackage\n\n\n\n이전 글 “선형모형의 다차원공간으로의 확장” 의 추가 제안으로, 선형모형의 무대를 허수축(Imaginary Axis)을 포함한 휘어진 다차원공간으로 확장, Inverted U-shape 관계를 선형관계로 재해석하였습니다. \n\n\n\n\n\n2019/08/14\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n괴델의 불완전성 정리\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nmathematics\n\n\n\n괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명하였습니다. 본 내용은 “제주대학교 경영정보학과 산업·직무 특화 전문가 특강” 에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/22\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 맞춤형 통계지원 소개\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n의학연구를 지원하면서 다양하게 R을 활용했던 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/13\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShiny 활용 의학연구지원 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\nShiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 슬라이드로 정리하였습니다. 본 내용은 차라투(주)가 후원하는 Shinykorea 5월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/05/11\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n세무기장마법사 머니핀(MoneyPin) 리뷰\n\n\n\n\n\n\nreview\n\n\nfinance\n\n\n\n법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다. \n\n\n\n\n\n2019/04/04\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nShinyApps를 R 패키지로 만들기\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nrpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록, RStudio Addins을 포함한 R 패키지를 만들어 CRAN에 배포신청했으나 실패한 경험을 정리하였습니다. 본 내용은 Anpanman이 후원하는 Shinykorea 2월 밋업에서 발표할 예정입니다. \n\n\n\n\n\n2019/02/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nSelf-controlled case series\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nsccs\n\n\n\n성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2019/02/06\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown 기초\n\n\n\n\n\n\nlecture\n\n\nmarkdown\n\n\n\nYAML Header, 마크다운(Markdown) 텍스트, R 코드 청크(chunk) 그리고 그림과 테이블을 중심으로, R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 정리하였습니다. \n\n\n\n\n\n2019/01/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n진료실 밖 의사로서의 경험\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다. \n\n\n\n\n\n2019/01/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nR 데이터 매니지먼트: tidyverse\n\n\n\n\n\n\nlecture\n\n\ntidyverse\n\n\ndata.table\n\n\npurrr\n\n\nR\n\n\n\n파일을 읽는 readr, 읽기 쉬운 코드를 만드는 %&gt;% 연산자, 데이터를 다루는 dplyr 그리고 반복문을 다루는 purrr 패키지를 중심으로 tidyverse 생태계에서 데이터를 다루는 방법을 정리하였습니다. \n\n\n\n\n\n2019/01/23\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n의학 연구에서의 기술 통계 with R\n\n\n\n\n\n\nlecture\n\n\nstatistics\n\n\nR\n\n\nshiny\n\n\n\n중앙보훈병원 정신건강의학과에서 강의한 내용으로, 의학 연구에 필요한 기술 통계(descriptive statistics)를 정리하고 웹 애플리케이션과 Rstudio Addins을 이용하여 실습하였습니다. \n\n\n\n\n\n2018/11/28\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\ndevOps\n\n\nR\n\n\ndocker\n\n\nshiny\n\n\n\nR User Conference in Korea 2018(RUCK 2018)에서 발표했던 내용입니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nNew Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\n유전율(heritability)은 어떤 형질의 유전적인 측면을 정량적으로 설명하는 유용한 지표이나 이분형 형질의 경우 해석이 어렵습니다. 이 때는 Sibling recurrence risk가 직관적인 지표이나, 유병률 정보가 필요하고 다른 변수의 보정이 어려운 문제가 있습니다. 본 연구에서는 흔히 쓰는 OR scale을 이용, 이분형 변수에서 직관적이고 다른 변수의 보정도 가능한 유전율 지표를 제안합니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nRedefine Null Hypothesis\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\nP value의 가장 큰 문제는 샘플 숫자만 늘리면 아무리 작은 차이라도 유의미한 결과로 만들 수 있다는 것입니다. 이는 대부분의 연구에서 차이가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 생기는 문제인데, 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못합니다. 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였습니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nrpackage\n\n\n\n아인슈타인의 일반상대성이론은 태양 근처에서 빛이 휘어지는 현상을 빛이 아닌 시공간이 휘어지는 것으로 해석합니다. 비슷한 아이디어를 통계학에 적용하여 U-shape 관계를 휘어진 다차원 공간에서의 선형모형으로 재해석하였습니다. \n\n\n\n\n\n2018/11/08\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\nradix\n\n\n\nWelcome to Anpanman! \n\n\n\n\n\n2018/09/25\n\n\nJinseob Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html",
    "href": "posts/2019-01-03-rmarkdown/index.html",
    "title": "R Markdown 기초",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, R Markdown으로 R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 강의할 예정입니다. 강의 내용을 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "title": "R Markdown 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR Markdown은 R 코드와 분석을 포함한 컨텐츠를 만드는 툴이며 크게 3가지 활용법이 있다.\n\n문서(pdf, html, docx): 글쓰기, 분석 결과, 참고문헌 등 논문의 모든 작업을 R Markdown으로 수행한다.\n프리젠테이션(pdf, html, pptx): R 코드나 분석결과가 포함된 프리젠테이션을 만든다. 기본 템플릿1 외에 xaringan2 패키지가 최근 인기를 끌고 있다.\n웹(html): 웹사이트나 블로그를 만든다. blogdown3 이나 distill4 패키지가 대표적이다. 이 글의 블로그도 distill로 만들었으며, 과거 차라투 홈페이지는 blogdown을 이용하였다.\n\n본 강의는 1의 가장 기초에 해당하는 강의로 간단한 문서를 작성하는 것을 목표로 한다. pdf 문서를 만들기 위해서는 추가로 LaTeX 문서작성 프로그램인 Tex Live를 설치해야 하며 본 강의에서는 생략한다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "href": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "title": "R Markdown 기초",
    "section": "Rmd 문서 시작하기",
    "text": "Rmd 문서 시작하기\nR Markdown은 Rmd 파일로 작성되며 rmarkdown5 패키지를 설치한 후, Rstudio에서 File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R markdown… 의 순서로 클릭하여 시작할 수 있다(Figure @ref(fig:rmdfilemenu), @ref(fig:rmdstart)).\n\n\n\n\nRstudio File 메뉴6\n\n\n\n\n\n\n\nR markdown 시작 메뉴7\n\n\n\n문서의 제목과 저자 이름을 적은 후 파일 형태를 아무거나 고르면(나중에도 쉽게 수정 가능) Figure @ref(fig:rmdfile)처럼 확장자가 Rmd인 문서가 생성될 것이다.\n\n\n\n\nR markdown 기본 문서8\n\n\n\n파일 내용을 보면 맨 먼저 제목을 쓰는 부분이 있고 글과 코드를 작성하는 부분도 있다. 일단 이 파일을 문서로 만들어보자. 문서 이름이 있는 바로 아래의 knit 탭을 누르거나, 그 옆의 아래방향 화살표를 누르고 원하는 파일 형태를 클릭하면 된다(Figure @ref(fig:knittab)). 처음에 언급했듯이 pdf는 Tex Live를 설치한 후 이용할 수 있다.\n\n\n\n\nknit 탭9\n\n\n\n다음은 각각 html, pdf, docx로 생성된 문서이다.\n\n\n\n\nhtml 문서10\n\n\n\n\n\n\n\npdf 문서11\n\n\n\n\n\n\n\nword 문서12\n\n\n\n생각보다 간단하지 않은가? 이제 본격적으로 Rmd 파일의 내용을 살펴보면서 어떻게 글과 R 코드를 작성하는지 알아보자. Rmd는 크게 제목을 적는 YAML Header, 글을 쓰는 Markdown Text와 코드를 적는 Code Chunk로 나눌 수 있다(Figure @ref(fig:rmdcontents)).\n\n\n\n\nRmd 파일 구성13"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "href": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "title": "R Markdown 기초",
    "section": "YAML Header",
    "text": "YAML Header\nYAML은 YAML Ain’t Markup Language의 재귀형식의 이름을 갖고 있는 언어로 가독성에 초점을 두고 개발되었다. R Markdown은 Rmd의 시작 부분에 문서 형식을 설정하는 용도로 이 포맷을 이용한다. 다음은 기초 정보만 포함된 YAML이다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2023-02-11\"\noutput: html_document\n---\nKnit 버튼 오른쪽의 설정() \\(\\rightarrow\\) Output Options…를 클릭하여 html, pdf, word 포맷 각각에 대한 기본 설정을 할 수 있다(Figure @ref(fig:outputoption), @ref(fig:outputhtml)).\n\n\n\n\nOutput Options14\n\n\n\n\n\n\n\nHTML Option15\n\n\n\n설정을 마치면 업데이트 된 YAML을 볼 수 있다. 모든 포맷 공통인 설정값은\n\ntoc(yes or no): 목차 포함 여부\n그림의 높이(fig_height) 와 넓이(fig_width): R 코드로 만든 그림에는 해당되지 않는다. Figures 에서 다시 설명하겠다.\n\n이며, 자동으로 현재 날짜를 입력하려면 아래와 같이 `r format(Sys.Date())`를 이용하면 된다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\" \n---\n아래는 필자가 Rmd 문서를 만들 때 흔히 쓰는 YAML 설정이다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\"\noutput:\n  html_document:\n    fig_height: 6\n    fig_width: 10\n    highlight: textmate\n    theme: cosmo\n    toc: yes\n    toc_depth: 3\n    toc_float: yes\n  pdf_document:\n    fig_height: 6\n    fig_width: 10\n    toc: no\n  word_document:\n    fig_height: 6\n    fig_width: 9\n    toc: no\n---\nhtml은 theme16에서 테마, highlight17에서 글씨 강조 스타일을 설정할 수 있으며, toc_float 옵션으로 움직이는 목차를 만들 수 있다(@ref(fig:tocfloat)).\n\n\n\n\ntoc_float- 움직이는 목차18\n\n\n\ndocx는 미리 설정을 마친 docx 문서를 아래와 같이 YAML에 추가하여 템플릿으로 이용할 수 있다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2023-02-11\"\noutput: \n    word_document:\n      reference_docx: mystyles.docx\n---\ndocx에 대한 자세한 내용은 Rstudio 블로그19를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "href": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "title": "R Markdown 기초",
    "section": "Markdown 글쓰기",
    "text": "Markdown 글쓰기\nR Markdown은 이름에서 알 수 있듯이 마크다운(Markdown) 을 기반으로 만들어졌다. 마크다운은 문법이 매우 간단한 것이 특징으로 깃허브의 README.md가 대표적인 마크다운 문서이다. 아래의 [R markdown reference]20에 흔히 쓰는 문법이 정리되어 있다.\n2 가지만 따로 살펴보겠다.\nInline R code\n문장 안에 분석 결과값을 적을 때, 분석이 바뀔 때마다 바뀐 숫자를 직접 수정해야 한다. 그러나 숫자 대신 `r &lt;코드&gt;` 꼴로 R 코드를 넣는다면 재분석시 그 숫자를 자동으로 업데이트 시킬 수 있다.\nThere were  `r nrow(cars)` cars studied\n\nThere were 50 cars studied\n\n수식\nLaTeX 문법을 사용하며 hwp 문서의 수식 편집과 비슷하다. inline 삽입은 $...$, 새로운 줄은 $$...$$ 안에 식을 적으면 된다.\nThis summation expression $\\sum_{i=1}^n X_i$ appears inline.\n\nThis summation expression \\(\\sum_{i=1}^n X_i\\) appears inline.\n\n$$\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n$$\n\\[\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\]\n수식 전반은 LaTeX math and equations21을 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "href": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "title": "R Markdown 기초",
    "section": "R chunk",
    "text": "R chunk\nRmd 문서에서 R 코드가 들어가는 방식은 4가지이다.\n\n몰래 실행. 코드와 결과는 다 숨긴다 - 최초 설정 때 쓰임.\n실행. 코드와 결과를 모두 보여준다.\n실행. 코드는 숨기고 결과만 보여준다.\n실행하지 않음. 코드 보여주기만 한다.\n\n하나씩 살펴보도록 하자.\n최초 설정\n문서를 처음 생성했을 때 최초로 보이는 R 코드는 다음과 같다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\ninclude=FALSE 옵션으로 문서에는 포함시키지 않고 몰래 실행할 수 있으며, 주로 최초 설정에 이용된다. setup은 이 코드에 해당하는 라벨로 생략 가능하다. knitr::opts_chunk$set 에서 디폴트 옵션을 설정할 수 있으며 echo = TRUE는 코드를 보여준다는 뜻이다. 흔히 쓰는 옵션들은 아래와 같다.\n\n\neval=F - 코드를 실행하지 않는다.\n\necho=F - 코드를 보여주지 않는다.\n\ninclude=F - 실행 결과를 보여주지 않는다.\n\nmessage=F - 실행 때 나오는 메세지를 보여주지 않는다.\n\nwarning=F - 실행 때 나오는 경고를 보여주지 않는다.\n\nerror=T - 에러가 있어도 실행하고 에러코드를 보여준다.\n\nfig.height = 7 - 그림 높이, R로 그린 그림에만 해당한다.\n\nfig.width = 7 - 그림 너비, R로 그린 그림에만 해당한다.\n\nfig.align = 'center' - 그림 위치, R로 그린 그림에만 해당한다.\n\n다음은 필자가 논문을 Rmd로 쓸 때 흔히 쓰는 디폴트 옵션이다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=F, fig.align = \"center\", message=F, warning=F, fig.height = 8, cache=T, dpi = 300, dev = \"jpg\")\n```\n전체 옵션은 knitr::opts_chunk$get 함수로 확인할 수 있다.\nknitr::opts_chunk$get()\n\n\n$eval\n[1] TRUE\n\n$echo\n[1] FALSE\n\n$results\n[1] \"markup\"\n\n$tidy\n[1] FALSE\n\n$tidy.opts\nNULL\n\n$collapse\n[1] FALSE\n\n$prompt\n[1] FALSE\n\n$comment\n[1] NA\n\n$highlight\n[1] TRUE\n\n$size\n[1] \"normalsize\"\n\n$background\n[1] \"#F7F7F7\"\n\n$strip.white\n[1] TRUE\n\n$cache\n[1] FALSE\n\n$cache.path\n[1] \"index_cache/html/\"\n\n$cache.vars\nNULL\n\n$cache.lazy\n[1] TRUE\n\n$dependson\nNULL\n\n$autodep\n[1] FALSE\n\n$cache.rebuild\n[1] FALSE\n\n$fig.keep\n[1] \"high\"\n\n$fig.show\n[1] \"asis\"\n\n$fig.align\n[1] \"center\"\n\n$fig.path\n[1] \"index_files/figure-html/\"\n\n$dev\n[1] \"png\"\n\n$dev.args\nNULL\n\n$dpi\n[1] 96\n\n$fig.ext\nNULL\n\n$fig.width\n[1] 7\n\n$fig.height\n[1] 5\n\n$fig.env\n[1] \"figure\"\n\n$fig.cap\nNULL\n\n$fig.scap\nNULL\n\n$fig.lp\n[1] \"fig:\"\n\n$fig.subcap\nNULL\n\n$fig.pos\n[1] \"\"\n\n$out.width\nNULL\n\n$out.height\nNULL\n\n$out.extra\nNULL\n\n$fig.retina\n[1] 2\n\n$external\n[1] TRUE\n\n$sanitize\n[1] FALSE\n\n$interval\n[1] 1\n\n$aniopts\n[1] \"controls,loop\"\n\n$warning\n[1] TRUE\n\n$error\n[1] FALSE\n\n$message\n[1] TRUE\n\n$render\nNULL\n\n$ref.label\nNULL\n\n$child\nNULL\n\n$engine\n[1] \"R\"\n\n$split\n[1] FALSE\n\n$include\n[1] TRUE\n\n$purl\n[1] TRUE\n\n$fenced.echo\n[1] FALSE\n\n$ft.shadow\n[1] FALSE\n\n\nChunk 별 설정\n최초 설정 이후부터는 아래와 같이 간단하게 코드를 보여주거나 실행할 수 있다.\n```{r}\nhead(mtcars)\n```\nhead(mtcars)\n\n\n\n  \n\n\n\n기본 설정과 다른 옵션을 적용하려면 chunk에 옵션을 따로 적으면 된다. 예를 들어 코드는 숨기고 결과만 보여주려면 echo=F 를 추가하면 된다.\n```{r, echo=F}\nhead(mtcars)\n```\n\n\n\n  \n\n\n\n반대로 실행은 안하고 코드만 보여주려면 eval=F를 추가하면 된다.\n```{r, eval=F}\nhead(mtcars)\n```\nhead(mtcars)"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#figures",
    "href": "posts/2019-01-03-rmarkdown/index.html#figures",
    "title": "R Markdown 기초",
    "section": "Figures",
    "text": "Figures\nRmd 문서에 그림이 들어가는 방법은 2가지가 있다.\n\nR 코드로 생성 : plot 함수, ggplot2 패키지 등\n외부 그림 삽입\n\n앞서도 언급했듯이 주의할 점은 그림이 만들어지는 방법에 따라 서로 다른 옵션이 적용된다는 것이다. 일단 전자부터 살펴보자.\nFigures with R\n\nR 코드에서 자체적으로 만든 그림은 전부 chunk 옵션의 지배를 받아 간단하다.\n```{r, fig.cap = \"scatterplot: cars\", fig.width = 8, fig.height = 6}\nplot(cars, pch = 18)\n```\nplot(cars, pch = 18)\n\n\n\n\nscatterplot - cars\n\n\n\nExternal figures\n외부 그림은 R 코드로도 삽입할 수 있고 마크다운 문법을 쓸 수도 있는데, 어떤 방법을 쓰느냐에 따라 다른 옵션을 적용받는다는 것을 주의해야 한다. R에서는 knitr::include_graphics 함수를 이용하여 그림을 넣을 수 있고 이 때는 chunk 내부의 옵션이 적용된다.\n```{r, fig.cap = \"tidyverse logo\", fig.align = \"center\"}\nlibrary(knitr)\ninclude_graphics(\"https://www.tidyverse.org/images/tidyverse-default.png\")\n```\n\n\n\n\ntidyverse logo\n\n\n\n같은 그림을 chunk없이 바로 마크다운에서 삽입할 수도 있다. 이 때는 YAML의 옵션이 적용된다.\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n\n\ntidyverse logo\n\n{ width=50% } 는 그림의 크기를 조절하는 옵션이며 R chunk에서도 같은 옵션 out.width=\"50%\"이 있다. 위치를 가운데로 조절하려면 &lt;center&gt;...&lt;/center&gt; 를 포함시키자.\n&lt;center&gt;\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n&lt;/center&gt;\n\n\n\ntidyverse logo\n\n\n개인적으로는 외부 이미지도 chunk 내부에서 읽는 것을 추천한다. chunk 내부의 옵션들이 마크다운의 그것보다 훨씬 체계적이고 쉬운 느낌이다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#tables",
    "href": "posts/2019-01-03-rmarkdown/index.html#tables",
    "title": "R Markdown 기초",
    "section": "Tables",
    "text": "Tables\n논문을 쓸 때 가장 귀찮은 부분 중 하나가 분석 결과를 테이블로 만드는 것으로, knitr::kable() 함수를 쓰면 문서 형태에 상관없이 Rmd에서 바로 테이블을 만들 수 있다. 아래는 데이터를 살펴보는 가장 간단한 예시이다.\n```{r}\nkable(iris[1:5, ], caption = \"A caption\")\n```\n\n\n\nA caption\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\nepiDisplay 패키지의 regress.display, logistic.display 함수를 활용하면 회귀분석의 결과를 바로 테이블로 나타낼 수 있다(Table @ref(tab:regtable)).\n```{r}\nmtcars$vs &lt;- as.factor(mtcars$vs)\nmtcars$cyl &lt;- as.factor(mtcars$cyl)\n\nmodel &lt;- glm(mpg ~ disp + vs + cyl, data = mtcars)\nmodel.display &lt;- epiDisplay::regress.display(model, crude = T, crude.p.value = T)\nmodel.table &lt;- model.display$table[rownames(model.display$table)!=\"\", ]\nkable(model.table, caption = model.display$first.line)\n```\n\n\n\nLinear regression predicting mpg\n\n\n\n\n\n\n\n\n\n\ncrude coeff.(95%CI)\ncrude P value\nadj. coeff.(95%CI)\nP(t-test)\nP(F-test)\n\n\n\ndisp (cont. var.)\n-0.04 (-0.05,-0.03)\n&lt; 0.001\n-0.03 (-0.05,0)\n0.019\n&lt; 0.001\n\n\nvs: 1 vs 0\n7.94 (4.6,11.28)\n&lt; 0.001\n0.04 (-3.81,3.89)\n0.984\n0.334\n\n\ncyl: ref.=4\n\n\n\n\n0.041\n\n\n6\n-6.92 (-10.11,-3.73)\n&lt; 0.001\n-4.77 (-8.56,-0.98)\n0.016\n\n\n\n8\n-11.56 (-14.22,-8.91)\n&lt; 0.001\n-4.75 (-12.19,2.7)\n0.202\n\n\n\n\n\n\n테이블을 좀 더 다듬으려면 kableExtra 패키지가 필요하며, 자세한 내용은 cran 설명서22를 참고하기 바란다. html 문서의 경우 kable()외에도 다양한 함수들을 이용할 수 있는데, 대표적인 것이 rmarkdown::paged_table() 함수와 DT 패키지이다. 전자는 아래와 같이 YAML에서 테이블 보기의 기본 옵션으로 설정할 수도 있다.\n---\ntitle: \"Motor Trend Car Road Tests\"\noutput:\n  html_document:\n    df_print: paged\n---\nDT 패키지에 대한 설명은 Rstudio DT 홈페이지23를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "href": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "title": "R Markdown 기초",
    "section": "마치며",
    "text": "마치며\n본 강의를 통해 R Markdown으로 기본적인 문서를 만드는 법을 알아보았다. 본 강의에서는 시간 관계상 참고문헌 다는 법을 언급하지 않았는데 궁금하다면 Bibliographies and Citations24을 참고하자. 이 내용까지 숙지한다면 R Markdown으로 논문을 쓸 준비가 된 것이다. R Markdown에 대한 전반적인 내용은 아래의 R Markdown Cheet Sheet25에 잘 요약되어 있으니 그때그떄 확인하면 좋다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#footnotes",
    "href": "posts/2019-01-03-rmarkdown/index.html#footnotes",
    "title": "R Markdown 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://rmarkdown.rstudio.com/lesson-11.html↩︎\nhttps://github.com/yihui/xaringan↩︎\nhttps://github.com/rstudio/blogdown↩︎\nhttps://rstudio.github.io/distill/↩︎\nhttps://github.com/rstudio/rmarkdown↩︎\nhttps://rachaellappan.github.io/rmarkdown/↩︎\nhttps://rachaellappan.github.io/rmarkdown/↩︎\nhttps://aberdeenstudygroup.github.io/studyGroup/lessons/SG-T5-RMarkdown/Images/New_Markdown_v2.png↩︎\nhttps://rstudioblog.files.wordpress.com/2014/06/r-markdown-formats.png↩︎\nhttps://stackoverflow.com/questions/47317229/rmarkdown-knit-pdf-to-look-exactly-like-html↩︎\nhttps://stackoverflow.com/questions/47317229/rmarkdown-knit-pdf-to-look-exactly-like-html↩︎\nhttps://rmarkdown.rstudio.com/articles_docx.html↩︎\nhttps://rfriend.tistory.com/311↩︎\nhttps://richardlent.github.io/post/rstudio-as-a-research-and-writing-platform/↩︎\nhttps://stackoverflow.com/questions/24934781/rstudio-knitr-themes↩︎\nhttp://www.datadreaming.org/post/r-markdown-theme-gallery/↩︎\nhttps://eranraviv.com/syntax-highlighting-style-in-rmarkdown/↩︎\nhttps://rfriend.tistory.com/311↩︎\nhttps://rmarkdown.rstudio.com/articles_docx.html↩︎\nhttps://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf↩︎\nhttps://www.latex-tutorial.com/tutorials/amsmath/↩︎\nhttps://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html↩︎\nhttps://rstudio.github.io/DT/↩︎\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html↩︎\nhttps://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf↩︎"
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html",
    "href": "posts/2022-04-12-status2022/index.html",
    "title": "22년 지원사업 후기",
    "section": "",
    "text": "김진섭 대표는 4월 20일(수) Shinykorea 밋업에서 “22년 지원사업 후기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#요약",
    "href": "posts/2022-04-12-status2022/index.html#요약",
    "title": "22년 지원사업 후기",
    "section": "요약",
    "text": "요약\n사무실\n\n송파ICT청년창업지원센터 입주, 기업부설연구소 설립\n공개SW 창업기업 6개월 연장(선릉 저스트코타워)\n\nR&D 과제\n\n창업성장기술개발사업(디딤돌 첫걸음) 서류탈락\n정보통신·방송 기술개발사업 앤틀러과 같이 지원, 6:1 경쟁률 결과 기다리는중\n\n창업지원사업\n\n혁신분야 창업패키지(BIG3), SW고성장기업 서류탈락\n창업도약패키지 심사중, 클라우드서비스 이용지원, Datastars 심사중\n\n특허지원사업\n\n국제 지재권분쟁 대응전략 지원사업(특허) 심사중\n\n멘토링\n\nSW마에스트로 기술멘토 선정, 한이음멘토링\n오픈소스 컨트리뷰톤 심사중\n\n고용지원, 인턴십, 기타\n\n`21 청년디지털일자리, 미래청년인재육성사업 TO 5명\n’22년 일경험프로그램 TO 1명, 송파구 중소기업 청년취업인턴제 TO 1명\n숭실대 스타트업 인턴십 2명(6주), ICT 학점연계 프로젝트 인턴십 TO 1명\n인공지능고성능컴퓨팅지원 3년 연속 선정"
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#slide",
    "href": "posts/2022-04-12-status2022/index.html#slide",
    "title": "22년 지원사업 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/status2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html",
    "href": "posts/2019-09-19-nhiswithr/index.html",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "",
    "text": "김진섭 대표는 9월 25일 을지의대 학술원 특강에 참석, R 을 이용하여 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#요약",
    "href": "posts/2019-09-19-nhiswithr/index.html#요약",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "요약",
    "text": "요약\nR 로 심평원, 공단 빅데이터의 데이터 정리와 통계분석을 수행하였다.\n\nhaven 패키지로 SAS 파일을 직접 읽을 수 있다.\ndplyr, data.table 을 활용, 기존 R 문법보다 빠르게 데이터 를 정리할 수 있다.\ndbplyr 를 활용, R 코드를 PROC SQL 문으로 바꿔 복잡한 SAS 작업을 수행할 수 있다.\n자체 개발한 jsmodule 패키지를 이용, GUI 환경에서 기술통계와 회귀/생존분석을 수행하고 테이블과 그림을 바로 다운받는다.\n\njsmodule 의 표본조사 데이터 분석 기능을 활용, GUI 환경에서 국건영 데이터 분석을 수행하였다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#slide",
    "href": "posts/2019-09-19-nhiswithr/index.html#slide",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/NHIS_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html",
    "href": "posts/2024-03-30-input-task-button/index.html",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslibとはbootstrapのcssをRで使えるようにしたパッケージです。\n正確な原文の説明はTools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. で、ShinyとRmarkdown(もちろんQuartoを含む)で色んなテーマを活用することができます。\n\nこの記事ではbslibの活用方法の中でShinyに集中して説明します。\n実際、Shinyは基本的にデザインのためbootstrapを使います。 しかし、Shinyはパッケージを構成してるコンポーネントと、関係が複雑に絡まっていて、かなり重いパッケージになってしまい、このため、アップデートで影響を受ける部分が多く、機能中心のアップデートをすることが知られています。\nつまり、UIを主に扱うbootstrapの部分は5年前のバージョンである3.4.1バージョンを使っていて、別のテーマ設定をしない場合、特有のブルー/グレーのテーマを基本的に使うことになります。(最近のバージョンは5.3.3)\n\nそれでshinyでは停滞したUIをアップデートするため、UIを扱う別のRパッケージを作って上書きするように最近bootstrapの機能を提供するようになりました。\n\n\n\n\n\n\nこの記事ではbslibの主な使い方は説明しません。"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#bslib",
    "href": "posts/2024-03-30-input-task-button/index.html#bslib",
    "title": "bslib: input_task_button 소개",
    "section": "",
    "text": "bslibとはbootstrapのcssをRで使えるようにしたパッケージです。\n正確な原文の説明はTools for theming Shiny and R Markdown via Bootstrap 3, 4, or 5. で、ShinyとRmarkdown(もちろんQuartoを含む)で色んなテーマを活用することができます。\n\nこの記事ではbslibの活用方法の中でShinyに集中して説明します。\n実際、Shinyは基本的にデザインのためbootstrapを使います。 しかし、Shinyはパッケージを構成してるコンポーネントと、関係が複雑に絡まっていて、かなり重いパッケージになってしまい、このため、アップデートで影響を受ける部分が多く、機能中心のアップデートをすることが知られています。\nつまり、UIを主に扱うbootstrapの部分は5年前のバージョンである3.4.1バージョンを使っていて、別のテーマ設定をしない場合、特有のブルー/グレーのテーマを基本的に使うことになります。(最近のバージョンは5.3.3)\n\nそれでshinyでは停滞したUIをアップデートするため、UIを扱う別のRパッケージを作って上書きするように最近bootstrapの機能を提供するようになりました。\n\n\n\n\n\n\nこの記事ではbslibの主な使い方は説明しません。"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "actionButton",
    "text": "actionButton\nShinyが提供する機能は本当に多様ですが、核心的な機能を挙げるなら、actionButtonを挙げることができます。\nactionButton`とはユーザがボタンを押すとserverで予め宣言した特定の動作を実行する機能で、普通はユーザがデータをアップロードしたら、このデータを活用して計算結果を生成するために使います。\nactionButtonの使用例としては下記のコードのように(?shiny::actionButtonで確認することができます)ユーザーが選択した観測数に合うヒストグラムを描くことができます。\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "href": "posts/2024-03-30-input-task-button/index.html#long-actionbutton",
    "title": "bslib: input_task_button 소개",
    "section": "long actionButton",
    "text": "long actionButton\nところで、この actionButton の問題点の一つは、もし演算に時間がかかる場合、ユーザーはボタンを押して結果を待っている間は何もできないということです。\nさらに、単純に何もできないだけでなく、ボタンがクリックされなかったと勘違いしてボタンを何度もクリックすることもあります。\nもし、大容量の誘電体データを活用した計算のためのShinyであれば、1回の計算に分単位の時間が必要な場合もあり、これは様々な問題を引き起こす可能性があります。\n特に、何度もクリックした場合、長い時間をかけて演算を終えた直後にまた同じ演算をまたやって、また待って、…という悪循環に陥ることもあります。"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "href": "posts/2024-03-30-input-task-button/index.html#shiny-with-loading",
    "title": "bslib: input_task_button 소개",
    "section": "shiny with loading",
    "text": "shiny with loading\nactionButton`のこの問題を解決するため、色んな方法がありました。\n\n\n**progress indicatorを使う方法\n\nこの方法はshinyで基本的に提供するProgress Indicator UIを活用する方法で、演算の過程/段階が進むにつれて進行度をユーザーに見せることができます。\n\nserver &lt;- function(input, output) {\n  output$plot &lt;- renderPlot({\n    input$goPlot \n\n    dat &lt;- data.frame(x = numeric(0), y = numeric(0))\n\n    withProgress(message = 'Making plot', value = 0, {\n      n &lt;- 10\n\n      for (i in 1:n) {\n        dat &lt;- rbind(dat, data.frame(x = rnorm(1), y = rnorm(1)))\n        incProgress(1/n, detail = paste(\"Doing part\", i))\n        Sys.sleep(0.1)\n      }\n    })\n\n    plot(dat$x, dat$y)\n  })\n}\n\nui &lt;- shinyUI(basicPage(\n  plotOutput('plot', width = \"300px\", height = \"300px\"),\n  actionButton('goPlot', 'Go plot')\n))\n\nshinyApp(ui = ui, server = server)\n\n\nしかし、これは withProgress, incProgress または Progress などの関数やオブジェクトを時間がかかる演算に追加でコードを書かなければならないというデメリットがあります。\n2.別のRパッケージを使う\nRのエコシステムには解決しようとする色んな問題があり、その問題毎のRパッケージがあると考えても過言ではないですが、actionButtonと演算結果の間の長い空白をUIに表記するための機能も同じです。\n以前のprogress indicatorと似て追加コードを書いて解決する必要がありもう少しデザインや詳細設定をすることができるカスタム機能があると思ってください。\n以下はいくつかの例示パッケージと事例です(アルファベット順)。\n\nshinybusy\n\n\n\nshinycssloaders\n\n\n\nshinycustomloader\n\n\n\nwaiter"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "href": "posts/2024-03-30-input-task-button/index.html#input_task_button",
    "title": "bslib: input_task_button 소개",
    "section": "input_task_button",
    "text": "input_task_button\ninput_task_buttonは上の方法とは違ってactionButtonを拡張した機能で、actionButton`を押すと演算が進行中であることを知らせてボタンが無効になり、演算が終わったら再びボタンが有効になる機能を提供します。\n何より一番大きな違いはactionButtonを置き換えることができるので追加コードを使う必要がないことです。\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  sliderInput(\"obs\", \"Number of observations\", 0, 1000, 500),\n  # actionButton(\"goButton\", \"Go!\", class = \"btn-success\"),\n  input_task_button(\"goButton\", \"Go!\", type = \"success\"),\n  plotOutput(\"distPlot\")\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    input$goButton\n    Sys.sleep(5)\n    dist &lt;- isolate(rnorm(input$obs))\n    hist(dist)\n  })\n}\n\nshinyApp(ui, server)\n\n\ninput_task_button の使用のため、5行目の actionButton を6行目の input_task_button に置き換え、さらに、意図的に長くかかる演算を作るため、13行目の Sys.sleep() コードを活用して5秒を遅延させました。\ninput_task_buttonの使い方は次のようになります。\n\n\n\n\n\n\ninput_task_buttonはactionButton` を無理なく置き換えることができますが、若干のパラメータ修正が必要です。\n\n\n\n\n\nactionButton\ninput_task_button\n役割\n\n\n\n\ninputId\nid\nボタンID\n\n\n\nlabel\nlabel\nボタンラベル\n\n\n\nicon\nicon\nボタンアイコン\n\n\n\n\nlabel_busy\nボタン非アクティブ時のラベル\n\n\n\n\nicon_busy\nボタン非アクティブ時のアイコン\n\n\n\nclass\ntype\nボタンテーマ/色\n\n\n\n\n\nリリースノート原文\ninput_task_buttonマニュアル"
  },
  {
    "objectID": "posts/2024-03-30-input-task-button/index.html#まとめ",
    "href": "posts/2024-03-30-input-task-button/index.html#まとめ",
    "title": "bslib: input_task_button 소개",
    "section": "まとめ",
    "text": "まとめ\n今回はbslibの最新機能であるinput_task_buttonと簡単な使用事例を紹介しました。\nこれは潜在的にユーザーエクスペリエンスを向上させる機能であり、既存のshinyでは追加コードを書かなければならない手間を解決してくれる機能だと思います。\n特に、他のウェブアプリケーションとは違ってShinyでは大容量データ演算のため時間が長くかかる場合が多いですが、比較的貧弱なUI/UXを持っていて、既存のactionButtonをinput_task_buttonに置き換える場合、これを補完するのに大きな助けになると思います。\n今回の記事が参考になったかと思いますが、次の記事でまたお会いしましょう！"
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html",
    "href": "posts/2020-07-08-table1inmed/index.html",
    "title": "의학 연구에서의 기술통계",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 1주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#요약",
    "href": "posts/2020-07-08-table1inmed/index.html#요약",
    "title": "의학 연구에서의 기술통계",
    "section": "요약",
    "text": "요약\n\n연속변수의 2그룹 비교: 정규분포 인정하면 t-test, 아니면 Wilcox-test\n연속변수의 3그룹 이상 비교: 정규분포 인정하면 one-way ANOVA, 아니면 Kruskal–Wallis one-way ANOVA\n범주형 변수의 그룹 비교: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n본사가 개발한 웹 과 R 패키지 에서 바로 Table 1 을 얻을 수 있다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#slide",
    "href": "posts/2020-07-08-table1inmed/index.html#slide",
    "title": "의학 연구에서의 기술통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/table1 를 클릭하면 볼 수 있다. 강의록은 과거 내용인 https://blog.zarathu.com/posts/2018-11-24-basic-biostatistics 를 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html",
    "href": "posts/2020-07-22-regressionbasic/index.html",
    "title": "회귀분석 in 의학연구",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 2주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#요약",
    "href": "posts/2020-07-22-regressionbasic/index.html#요약",
    "title": "회귀분석 in 의학연구",
    "section": "요약",
    "text": "요약\n\n\n\n\n\n\n\n\n\nDafault\nRepeated measure\nSurvey\n\n\n\nContinuous\nlinear regression\nGEE\nSurvey GLM\n\n\nEvent\nGLM (logistic)\nGEE\nSurvey GLM\n\n\nTime & Event\nCox\nmarginal Cox\nSurvey Cox\n\n\n0,1,2,3 (rare event)\nGLM (poisson)\nGEE\nSurvey GLM"
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#slide",
    "href": "posts/2020-07-22-regressionbasic/index.html#slide",
    "title": "회귀분석 in 의학연구",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/regression 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2024-01-23-RND-review/index.html",
    "href": "posts/2024-01-23-RND-review/index.html",
    "title": "R&D 시험인증 후기 및 개발, 행정 관련 느낀 점",
    "section": "",
    "text": "공개SW 기반의 클라우드 통계 패키지SW 과제를 수행하면서, 외부 공인 기관에 시험인증을 의뢰해야 했습니다. 이와 관련된 후기, 또 전반적인 과제 관련 정보를 남깁니다. 당사로서는 처음 수행하는 R&D였습니다. 저희 팀원들과 공동연구기관에서 다 같이 열심히 참여하였기에 잘 완수할 수 있었습니다. 그러나 그 과정에서 어쩔 수 없이 겪은 몇가지 어려움이 있었기에 이를 공유합니다.\n\n개발\n최초에 R&D 연구개발계획서를 작성하면서, 계획서 표지(요약 페이지)와 하단에, 높은 TRL단계를 달성하겠다고 기재하였습니다. 또, 하단에는 외부 기관으로부터 인증을 받겠다고도 기재하였습니다.\n이를 인지하지 못하고 있다, 2차년도 10/26에 최초로 위와 같이 기재한 것을 발견하였습니다. 관련하여 주관 기관에, 수행하지 않아도 될지에 대해 문의하였으나 외부 시험이 필요할 것이라는 회신을 받았습니다(사유: 최초 연구개발계획서에 기재되어 있고, TRL이 높은 단계인 점 등 고려)\n그 후, 12/18에 현장 테스트가 완료되었으며, 12/27경 최종 보고서 초안를 받았습니다. 시험 준비부터 완료까지 거의 2달이 소요됩니다. 만약 이러한 준비가 되어 있지 않은 경우 마감이 아주 촉박해 제때 마무리 하지 못할 우려가 있습니다. 우리만 준비되면 되는 것이 아니고, 시험인증 업체측 일정도 고려해야 합니다. SW인증업체가 전국에 10개소 정도밖에 되지 않는 것으로 알고 있습니다. SW R&D는 최소 100건은 될 것입니다. 모두가 시험인증을 거치는 것은 아니지만, 미리 준비해야 합니다. 제가 10월 말경 문의하였을때 이미 12월 말까지 일정이 있어 의뢰받지 못한다고 회신한 업체도 2~3개정도 있었습니다.\n시험인증은 상당히 시간이 많이 소요되는 작업입니다. 가능하다면 피하는 것이 좋으나, 현재 우리 회사의 기술 수준을 볼 때 대부분 상용화/성숙 단계 R&D일 것이므로 피하기 어려울 것입니다. 미리 준비해야 합니다.\n\n연구개발계획서 내용을 R&D 참가 인원과 잘 공유하고, 우리가 해야 할 것을 정리\n처음 연구개발계획서를 가장 잘 쓰는 것이 중요하고, 그 R&D 전 기간에 거쳐 참가할 사람과 공유하여 향후 어떠한 검증을 외부 업체에서 받아야 하는지를 사전에 알고 있는 것이 중요합니다. 그렇게 해야만 해당 부분의 유지보수를 조금 더 잘 할 수 있으리라 생각합니다.\n현재는 연구개발계획서를 다 같이 보는 것이 아니라, 대표님이 작성하시고 그것을 개개인이 읽어 보는 형태로 진행되고 있습니다. 하지만 앞으로는 한 자리에 모여 다같이 연구개발계획서를 검토하고, 비현실적인 부분이 있는지, 또는 미리 준비해야 하는 부분이 있는지를 검토하고 가능하면 쉽게 수행할 수 있도록 향후 우려되는 요소를 미리 제거한 연구개발계획서를 작성하는 것이 중요하겠습니다.\n\n\nRFP에 기반한 목표치를 알맞게 설정\nRFP의 ‘국내 기업 최고치’, ‘해외 기업 최고치’를 준용하지 않는 경우(즉, 해당 값을 변경하거나 새로운 기준을 만드는 경우)에는 ’국내 기업 최고치’, ’해외 기업 최고치’가 해당 값인 것도 우리가 입증해야 함에 주의하여야 합니다(간사 문의사항). 다행히도, 이번 과제의 경우 RFP상에서 국내/해외 기업 최고치를 제시하였기에 우리가 입증 할 필요가 없었습니다. 시험인증이 불가하여 외부에 위탁하지는 않았지만, 예를 들어 ’분석 속도’에 정확한 기준을 적지 않은 채, 해외 기업을 100%라고 한 경우 매우 곤란한 경우가 생길 수도 있으리라 생각됩니다. 연구개발계획서를 작성 할 때에는 상대적인 수치가 아닌, 절대치로 기재하는것이 좋겠으며 달성이 어렵더라도, 입증 자체는 쉽게 할 수 있는 항목을 기재하는 것이 최종 결과보고서 작성에 유리합니다.\n\n\n다른 업체와의 업무 분담 명확화\n이번 시험인증에서는 다른 공동연구개발기관와 함께 시험인증을 진행했습니다. 그 공동연구개발기관 역시 시험인증에 대한 준비가 되지 않은 상황이었으며, 상대측 실무자 또한 경험이 많이 없어 미숙한 면이 있었다고 보입니다. 마찬가지로 연구개발계획서를 작성 할 때에 언제쯤, 어떻게 시험을 진행할 지, 각자 서류작업(페이퍼워크)의 분담은 어떻게 할 지 등을 미리 검토 할 필요가 있습니다. 그렇지 않으면 서로 소통이 되지 않아 일을 이중으로 하게 됩니다.\n이번 시험인증의 경우 다른 공동연구개발기관에 요청한 서류 대부분을 사용하지 못하고 우리 회사측에서 새로 작성하였습니다. 주관기관으로 참여할 경우 대부분의 페이퍼워크를 우리가 부담해야 하니 미리 알고 있을 필요가 있습니다.\n\n\n사전에 검증 가능한 항목인지, 공인인증 업체에 미리 확인\n통계분석 시간 단축률 항목의 경우, 다른 상용 프로그램과의 비교가 불가능하다는 시험인증 업체의 회신을 받았습니다, 이는 최종 평가에서 분명히 마이너스 요소가 될 수 있는 사안입니다. 다음 연구개발계획서 작성 시에는, 평가 항목을 사전에 업체와 조율하여 평가가 가능한지를 파악한 후 이를 기재하는것이 바람직하리라 생각됩니다.\n\n\n개발 시 시험항목과 테스트 항목에 맞추어 개발\n기능을 개발 할 때에, 시험항목에 포함되는 개발 항목과, 시험 항목에 포함되지는 않지만 연구개발계획서에는 존재하는 항목이 있습니다. 시험인증을 하지 않아도 되는 항목의 경우 일단 구현만 하면 연구개발계획서를 어기지는 않은 반면, 시험인증을 해야 하는 항목의 경우 여러 번/여러 플랫폼에서 테스트를 할 필요가 있고, 기능 명세도 작성해야 합니다. 또 이러한 항목의 경우 코드에 주석 작업을 철저히 하여 추후 오류가 발생했을 때 쉽게 수정할 수 있어야 할 것입니다. 미리미리 해 두지 않으면 추후 유지보수가 굉장히 어려워, 시간이 2~3배 더 소요될 수 있습니다.\n\n\n양식 작성 시 스크린샷은 최대한 자세하게\n이번 시험인증을 진행한 업체를 포함해, 약 4개정도의 업체에서 견적을 받거나, 최소한 문의하여 양식을 작성했습니다. 공통적인(비슷한) 양식이 있는 것을 보니 정형화된 서류가 존재하는 듯 합니다.\n이번에 수행한 업체의 경우, 해당 서류를 작성해서 제출했음에도 불구하고, 그 이후에 해당 서류의 내용을 바탕으로 전체적인 테스트를 수행한 후 보내달라고 하였습니다. 최초에 테스트 할 항목을 작성할 때, 미리 테스트를 겸하여 어떻게 작동되는지를 스크린샷으로 촬영 해 두면 간편하게 시험인증을 마무리 할 수 있습니다.\n\n\n테스트 도구의 도입\n아직 한번도 이런 도구를 사용해보진 못했지만, 이러한 도구의 필요성을 절실히 느꼈습니다. 예를 들어, statgarten의 ML 중 lightGBM 패키지가 업데이트 되어, 특정 버전을 지정하여 설치하도록 한 경우에는 다른 ML에 영향이 없는지 확인이 필요합니다. 어떤 dependency가 업데이트 되었을 때, 테스트 도구가 없다면 매번 모든 항목을 수동으로 테스트 해야 합니다. 이런 번거로움을 막고자 테스트 도구 도입을 검토 할 필요가 있습니다.\n\n\n\n행정\n\n연구비는 인건비와 연구활동비 위주로 편성\n연구비로 회사에 필요한 비품을 구매하거나, 용역을 주는 것은 쉽지 않습니다, 이번 IITP에서 회계를 제가 담당할 때는 대부분 인건비와 식비였기에 예산 집행에 큰 어려움은 없었습니다. 하지만 그 후에, NIPA나 창업도약패키지 연구비 집행을 옆에서 지켜 본 결과, 사실상 인건비와 식사 이외에는 사용하지 말라는 의도라고 생각하게 되었습니다. 실물 재료가 필요한 다른 연구와는 달리, 우리는 SW개발업이기에 인건비가 가장 많은 비중을 차지하고, 최대한 인건비를 연구비로 계상하는 것이 중요할 것 같습니다. 만약 다른 항목(예: 장비 구매 등)이 꼭 필요하다면, “협약 초기”에 끝내는 것이 좋겠습니다.\n2024년 1월 추가: 2023년 12월 28일부터 회의비 사용이 어려워졌습니다. 대부분 인건비로 처리가 필요합니다.\n\n\n간사와의 연락은 메일로\n출장 등으로 전화가 잘 되지 않을 뿐더러, 상대가 전화를 선호하지 않습니다. 한 명의 간사가 10개 이상의 사업을 관리하는 경우도 있기에 메일이 좋습니다. 또 추후 문제가 생겼을 때 우리가 대응하기 굉장히 껄끄러운 상대방입니다. 따라서 가능한 한 기록이 남는 메일로 연락을 주고받는것이 좋겠습니다.\n\n\n협약 변경은 한번에\n비교적 자유로웠던 IITP과제와는 달리, 현재 수행중인 타 과제는 협약 변경이 매우 번거롭습니다. 그 기관의 특성으로 보이나, 연구 분야를 보았을 때 우리가 추가로 R&D를 수행한다면 그 기관 과제일 확률이 매우 높습니다. 앞에 언급 한 것처럼 최대한 최초 연구개발계획서에 구체적으로, 우리가 유리한 방향으로 작성하는것이 좋습니다.\n또 위 기관의 경우 한번 협약 변경에 오랜 시일이 걸립니다. 지난 10월경 협약변경한 건은 약 1개월 걸렸으며, 그 과정에서 반려도 최소 5회 이상 당했습니다. 또, 과제 종료 시기(11월 이후?)가 되면, 원래는 협약변경 가능한 시기임에도 불구하고 협약변경을 잘 해주지 않으려 합니다. 이 점 감안하셔서, 하반기 진입 시 과제 상황을 한 번 체크하시는 것이 좋습니다.\n\n\n전문연구사업자\n전문연구사업자 자격이 있으므로 기존인력 인건비 계상이 가능합니다. 현재 수행하고 있는 R&D의 주관기관은 거부하였는데 해당 법령 소관기관인 과기부 문의 결과 부처 상관없이 기존인력 인건비 계상이 가능하여야 합니다. 현재는 기존인력인건비 계상이 꼭 필요하지는 않았지만, 앞으로 필요시 과기부 문의 결과를 바탕으로 요청해보아야 할 것 같습니다.\n\n\nSlack을 통한 계획서 공유는 지양\nSlack으로 계획서를 공유하면 간편하지만, 버전 관리 문제가 발생합니다. 같은 파일에 각기 다른 내용이 추가되고, 이들을 합칠 수 없는 문제가 생깁니다. 그리 길지 않은 문서라면 대조하며 확인이 가능하지만, 백 페이지가 넘는 문서들은 그렇게 대조하는 것이 매우 고통스러운 작업일 것입니다. 가급적이면 버전 관리가 가능한 회사 드라이브를 통해 하는 것이 좋습니다.\n\n\n주관기관이 대부분의 일을 합니다\n만약 우리가 주관기관으로 참여하게 되면 대부분의 서류 작업을 우리가 해야 합니다. 각 기관별 예산/성과등은 각 기관에게 요청할 수 있지만, 대부분의 페이퍼워크를 우리가 해야 하므로 미리 준비해야 합니다.\n\n\n\n결론 요약\n\n과제 지원/시작 단계에서 다같이 모여 연구개발계획서와 (있는 경우) 과업지시서를 읽고 논의하자\n개발 단계부터 체계를 갖추고, 테스트를 잘 수행하자\n협약 관련 사항은 여름까지는 완료하고, 인건비 이외 지출항목은 가능하면 연초에 소진하자\n\n\n\n\n\nReuseCC BY-NC-SA 4.0CitationBibTeX citation:@online{lim2024,\n  author = {Lim, Changwoo},\n  title = {R\\&D {시험인증} {후기} {및} {개발,} {행정} {관련} {느낀}\n    {점}},\n  date = {2024-01-23},\n  url = {https://blog.zarathu.com/jp/posts/2024-01-23-RND-review},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLim, Changwoo. 2024. “R&D 시험인증 후기 및 개발, 행정 관련\n느낀 점.” January 23, 2024. https://blog.zarathu.com/jp/posts/2024-01-23-RND-review."
  },
  {
    "objectID": "posts/2023-02-07-bambooforest-with-slack-api/index.html",
    "href": "posts/2023-02-07-bambooforest-with-slack-api/index.html",
    "title": "Python과 Slack API를 이용한 대나무숲 앱 제작",
    "section": "",
    "text": "직장 동료가 추천해준 이 글을 보고, 우리 회사 Slack에도 대나무숲을 만들어 보았습니다. 원 글은 Node.js를 이용했지만, Javascript에 익숙하지 않아 Slack에서 제공하는 Python용 Bolt API를 사용했습니다.\n본 글을 읽기에 앞서, Bolt Documentation을 읽고 오시면 이해해 큰 도움이 됩니다.\n\n\n\n먼저, Slack API에 접속하여, Create New App을 선택합니다. From Scratch를 선택한 후, App Name과 워크스페이스를 선택합니다. 저는 대나무숲 봇, Zarathu 워크스페이스를 선택했습니다.\n\n먼저, Incoming Webhooks에서, Incoming Webhooks를 활성화합니다.\n다음으로, Interactivity & Shortcuts에 들어가 Interactivity를 활성화시키고, Request URL을 본인의 ngrok주소 또는 실제 주소 + “/slack/events”(바뀌면 작동하지 않습니다.)로 설정합니다.\n아래의 Shortcuts에서, 두 가지를 추가합니다. 한 가지는 Global로(게시글 포스팅), 다른 한 가지는 On Messages로(댓글 포스팅) 합니다. Name, Short Description, Callback ID는 적당히 작성합니다.\n다음으로, OAuth & Permission에 들어가 아래의 Scopes에서, Bot Token Scopes에 필요한 권한을 추가합니다.\n\nchannels:history, read\nchat: write, write.customize, write.public\ncommands\nincoming-webhook\n\n그 후, 같은 페이지 상단에 보이는 Bot User OAuth Token이 자신의 Bot Token입니다.\n\n\n\n파일 구조는 아래와 같습니다.\nproject\n│   dockerfile\n│   requirements.txt    \n│   .env\n│   .gitignore\n│ \n└───src\n│   │   app.py\n│   │   name.py\n│   \n└───.venv\n    │   ...\n먼저, Python 가상 환경을 만들기 위해 다음 명령어를 실행합니다. python -m venv .venv\n그리고, venv를 활성화 시킵니다. source .venv/bin/activate (macOS)\n이제 Python 가상 환경에 접속했습니다.\n먼저, Slack API 및 dotenv를 사용하기 위해 다음 명령어로 bolt와 dotenv를 설치합니다.\npip install python-dotenv\npip freeze로 현재 설치된 pip 패키지를 조회 해 보면,\npython-dotenv==0.21.0\nslack-bolt==1.16.1\nslack-sdk==3.19.5\n위와 같습니다. 버전은 설치 한 시점에 따라 다를 수 있습니다.\n시작하기에 앞서, 환경변수를 설정하도록 하겠습니다.\n프로젝트 루트의 .env 파일에\nSLACK_SIGNING_SECRET=&lt;자신의 Signing Secret&gt;\nSLACK_BOT_TOKEN=&lt;자신의 Bot Token&gt;\nCHANNEL_NAME=대나무숲\n를 추가합니다.\nSigning Secret은 api.slack.com에서 자신의 앱을 선택한 후 Basic Information에 있는 Signing Secret 입니다.\n이제 실제로 코드를 만들어 보겠습니다. src/app,py를 생성합니다.\n\nif False:\n    import os\n    from dotenv import load_dotenv\n    load_dotenv(verbose=True)\n\n    SLACK_SIGNING_SECRET = os.getenv('SLACK_SIGNING_SECRET')\n    SLACK_BOT_TOKEN = os.getenv('SLACK_BOT_TOKEN')\n    CHANNEL_NAME=os.getenv(\"CHANNEL_NAME\")\n\n로 dotenv 환경을 불러오고, 변수에 .env의 내용을 불러옵니다.\n다음으로,\n\nif False:\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n\n    from slack_bolt import App\n    from slack_sdk.errors import SlackApiError\n\n로 logging 기능과 Slack API를 불러옵니다. SlackApiError는 추후 Error Handling에 사용 될 에정입니다.\n\nif False:\n    app = App(\n        token=SLACK_BOT_TOKEN,\n        signing_secret=SLACK_SIGNING_SECRET\n    )\n\n위 코드로, Slack API를 초기화합니다.\nSlack API는 Decorator를 통해 각각의 Action에 따른 기능을 수행합니다. FastAPI를 사용 한 경험에 비추어 설명 해 보면, 예를 들어 @app.post(\"/\")의 경우 “/”로 post 요청을 보낸 경우에 작동하는 것 처럼, @app.shortcut(\"post\")는 post라는 ID의 shortcut을 실행 한 경우에 작동합니다.\n우리는 Modal을 이용해 게시글과 답글을 작성할 수 있도록 할 것입니다. Slack에서는 Modal을 쉽게 생성할 수 있는 도구를 제공하고 있습니다. 그 도구를 통해 Modal을 생성한 후, action_id와 text, callback_id, private_metadata 등을 수정하면 쉽게 우리가 원하는 Modal 창을 얻을 수 있습니다.\n저는 아래와 같이 했습니다.\n\n우리는 - “reply” action - “port” shortcut - “reply” shortcut 을 받은 경우 modal이 필요하므로 아래와 같은 Decorator를 작성합니다.\n\nif False:\n    @app.action(\"reply\")\n    @app.shortcut(\"post\")\n    @app.shortcut(\"reply\")\n    def open_modal(ack, body, client, logger):\n        ...\n\nSlack API는 요청을 받는 경우 ack()를 실행하도록 요청하고 있습니다. 그러므로, open_modal(…)이 호출되는 경우 {python} ack()가 실행되어야 합니다.\n\nif False:\n    def open_modal(ack, body, client, logger):\n        ack()\n\n다음으로, open_modal이 끝난 경우, 즉 유저가 Modal에 무엇인가를 입력하여 반환 된 경우를 보겠습니다. Modal이 닫히면, 그 내용을 대나무숲 봇이 #bambooforest 채널에 포스팅/댓글 작성 해야합니다. 이를 위해, Modal에 callback_id를 설정해야 합니다. 우리는 “view_post”라고 하겠습니다.\n\nif False:\n    def open_modal(ack, body, shortcut, client, logger, block_actions):\n        ack()\n        try:\n            try:\n                try:\n                    message_ts=body['message_ts']\n                except:\n                    message_ts=body['container']['message_ts']\n            except:\n                message_ts=\"\" # Global의 경우 message_ts가 없음\n            result = client.views_open(\n                trigger_id=body[\"trigger_id\"],\n                view={\n                    # View 위 내용 생략\n                    \"callback_id\": \"view_post\",\n                    \"private_metadata\": message_ts\n                }\n            )\n            logger.info(result)\n        except SlackApiError as e:\n            logger.error(\"Error creating conversation: {}\".format(e))\n\n그러면 위 “view_post” 콜백를 처리할 수 있는 함수를 만들어 보겠습니다. 마찬가지로 Decorator을 사용합니다.\n\nif False:\n    @app.view(\"view_post\")\n    def handle_submission(ack, body, client, view, logger):\n        ack()\n        logger.info(body)\n        message_ts=view['private_metadata']\n        content=view['state']['values']['post_input_block']['post_content_input']['value']\n        username=view[\"state\"][\"values\"][\"name_input_block\"][\"name_input\"]['value']\n\n“view_post” 콜백이 실행된 경우, ack() 하고 body를 logging 합니다. 이후, 복잡한 경로에 있는 값을 접근하기 쉽도록 값을 변수에 대입합니다. 우리는 message_ts를 private_metadata로 전달했습니다. 또, content와 username의 값은 view[“state”][“values”]의 [‘post_input_block’][‘post_content_input’][‘value’] 및 [“name_input_block”][“name_input”][‘value’]에 존재합니다.\n익명 게시판이므로, Username을 작성하지 않은 경우가 많을 것입니다. 이 경우, 아래에서 만들 username() 함수를 통해 랜덤한 username을 생성합니다. 또한, 게시글의 경우 message_ts 값이 없습니다(위에서 ““를 대입합니다). 경우에 따라 type=”게시글” 또는 type=“댓글”을 저장합니다.\n\nif False:\n    if username is None:\n            username=randname()\n\n    if message_ts == \"\":\n        type=\"게시글\"\n    else:\n        type=\"댓글\"\n\n위에서 정리된 내용을 바탕으로 메시지를 발송합니다. message_ts가 있으면(즉 댓글이면) 메시지를 해당 thread에, 아니면 .env에서 지정한 채널에 발송합니다.\n\nif False:\n    send_message=[\n                    {\n                        \"type\": \"header\",\n                        \"text\": {\n                            \"type\": \"plain_text\",\n                            \"text\": \":bamboo: 익명 메시지 :bamboo:\"\n                        }\n                    },\n                    {\n                        \"type\": \"divider\"\n                    },\n                    {\n                        \"type\": \"section\",\n                        \"text\": {\n                            \"type\": \"mrkdwn\",\n                            \"text\": f\"*{username}님의 {type}입니다.*\"\n                        }\n                    },\n                    {\n                        \"type\": \"section\",\n                        \"text\": {\n                            \"type\": \"plain_text\",\n                            \"text\": f\"{content}\"\n                        },\n                        \"accessory\": {\n                            \"type\": \"button\",\n                            \"action_id\": \"reply\",\n                            \"text\": {\n                                \"type\": \"plain_text\",\n                                \"text\": \"댓글 달기\",\n                                \"emoji\": True\n                            }\n                        }\n                    }\n                ]\n    \n    if message_ts == \"\":\n        client.chat_postMessage(channel=CHANNEL_NAME, blocks=send_message)\n    else: \n        client.chat_postMessage(channel=CHANNEL_NAME, thread_ts=message_ts, blocks=send_message)\n\nsrc/name.py는 다음과 같습니다.\n\nimport random\n\nfirstNames = [\n  \"깔끔한\",\n  \"근면한\",\n#  ...\n]\n\nlastNames = [\n  \"토끼\",\n  \"오리\",\n#  ...\n]\n\ndef randname():\n    return firstNames[random.randint(0, len(firstNames)-1)] + \" \" + lastNames[random.randint(0, len(lastNames)-1)]\n\nrandname()은 firstNames와 lastNames의 원소를 각각 랜덤으로 추출하여 합쳐 반환하는 함수입니다.\n\n위 firstNames, lastNames는 이 글의 GitHub 저장소를 참조하였습니다.\n\nsrc/app.py 상단에\n\nif False:\n    from name import randname\n\n을 추가합니다.\n마지막으로, src/app.py 마지막에\n\nif False:\n    if __name__ == \"__main__\":\n        app.start(3000)\n\n을 추가하고, 서버에서 python3 src/app.py를 실행하면 정상적으로 작동하는 것을 알 수 있습니다.\n\n\n\n생각보다 Slack API Documentation이 충실하고, 회사에서 서버도 자유롭게 사용하도록 허용해주고 있어 쉽게 구현할 수 있었습니다. 실제로는 Docker를 사용하여 회사 서버에서 사용하고 있습니다. 테스트 목적으로는 https://ngrok.com을 사용하면 쉽게 테스트 할 수 있습니다.\n다만, 한 달 정도 대나무숲을 사용 해 본 결과 실질적으로 수십 명 정도의 조직이어야 제대로 사용할 수 있을 것이라는 생각이 들었습니다.\n전체 코드는 링크에서 확인하실 수 있습니다.(글과 일부 바뀌었을 수 있음.)"
  },
  {
    "objectID": "posts/2023-02-07-bambooforest-with-slack-api/index.html#slack-apibolt를-사용해-대나무숲-앱을-제작한-후기",
    "href": "posts/2023-02-07-bambooforest-with-slack-api/index.html#slack-apibolt를-사용해-대나무숲-앱을-제작한-후기",
    "title": "Python과 Slack API를 이용한 대나무숲 앱 제작",
    "section": "",
    "text": "직장 동료가 추천해준 이 글을 보고, 우리 회사 Slack에도 대나무숲을 만들어 보았습니다. 원 글은 Node.js를 이용했지만, Javascript에 익숙하지 않아 Slack에서 제공하는 Python용 Bolt API를 사용했습니다.\n본 글을 읽기에 앞서, Bolt Documentation을 읽고 오시면 이해해 큰 도움이 됩니다.\n\n\n\n먼저, Slack API에 접속하여, Create New App을 선택합니다. From Scratch를 선택한 후, App Name과 워크스페이스를 선택합니다. 저는 대나무숲 봇, Zarathu 워크스페이스를 선택했습니다.\n\n먼저, Incoming Webhooks에서, Incoming Webhooks를 활성화합니다.\n다음으로, Interactivity & Shortcuts에 들어가 Interactivity를 활성화시키고, Request URL을 본인의 ngrok주소 또는 실제 주소 + “/slack/events”(바뀌면 작동하지 않습니다.)로 설정합니다.\n아래의 Shortcuts에서, 두 가지를 추가합니다. 한 가지는 Global로(게시글 포스팅), 다른 한 가지는 On Messages로(댓글 포스팅) 합니다. Name, Short Description, Callback ID는 적당히 작성합니다.\n다음으로, OAuth & Permission에 들어가 아래의 Scopes에서, Bot Token Scopes에 필요한 권한을 추가합니다.\n\nchannels:history, read\nchat: write, write.customize, write.public\ncommands\nincoming-webhook\n\n그 후, 같은 페이지 상단에 보이는 Bot User OAuth Token이 자신의 Bot Token입니다.\n\n\n\n파일 구조는 아래와 같습니다.\nproject\n│   dockerfile\n│   requirements.txt    \n│   .env\n│   .gitignore\n│ \n└───src\n│   │   app.py\n│   │   name.py\n│   \n└───.venv\n    │   ...\n먼저, Python 가상 환경을 만들기 위해 다음 명령어를 실행합니다. python -m venv .venv\n그리고, venv를 활성화 시킵니다. source .venv/bin/activate (macOS)\n이제 Python 가상 환경에 접속했습니다.\n먼저, Slack API 및 dotenv를 사용하기 위해 다음 명령어로 bolt와 dotenv를 설치합니다.\npip install python-dotenv\npip freeze로 현재 설치된 pip 패키지를 조회 해 보면,\npython-dotenv==0.21.0\nslack-bolt==1.16.1\nslack-sdk==3.19.5\n위와 같습니다. 버전은 설치 한 시점에 따라 다를 수 있습니다.\n시작하기에 앞서, 환경변수를 설정하도록 하겠습니다.\n프로젝트 루트의 .env 파일에\nSLACK_SIGNING_SECRET=&lt;자신의 Signing Secret&gt;\nSLACK_BOT_TOKEN=&lt;자신의 Bot Token&gt;\nCHANNEL_NAME=대나무숲\n를 추가합니다.\nSigning Secret은 api.slack.com에서 자신의 앱을 선택한 후 Basic Information에 있는 Signing Secret 입니다.\n이제 실제로 코드를 만들어 보겠습니다. src/app,py를 생성합니다.\n\nif False:\n    import os\n    from dotenv import load_dotenv\n    load_dotenv(verbose=True)\n\n    SLACK_SIGNING_SECRET = os.getenv('SLACK_SIGNING_SECRET')\n    SLACK_BOT_TOKEN = os.getenv('SLACK_BOT_TOKEN')\n    CHANNEL_NAME=os.getenv(\"CHANNEL_NAME\")\n\n로 dotenv 환경을 불러오고, 변수에 .env의 내용을 불러옵니다.\n다음으로,\n\nif False:\n    import logging\n    logging.basicConfig(level=logging.DEBUG)\n\n    from slack_bolt import App\n    from slack_sdk.errors import SlackApiError\n\n로 logging 기능과 Slack API를 불러옵니다. SlackApiError는 추후 Error Handling에 사용 될 에정입니다.\n\nif False:\n    app = App(\n        token=SLACK_BOT_TOKEN,\n        signing_secret=SLACK_SIGNING_SECRET\n    )\n\n위 코드로, Slack API를 초기화합니다.\nSlack API는 Decorator를 통해 각각의 Action에 따른 기능을 수행합니다. FastAPI를 사용 한 경험에 비추어 설명 해 보면, 예를 들어 @app.post(\"/\")의 경우 “/”로 post 요청을 보낸 경우에 작동하는 것 처럼, @app.shortcut(\"post\")는 post라는 ID의 shortcut을 실행 한 경우에 작동합니다.\n우리는 Modal을 이용해 게시글과 답글을 작성할 수 있도록 할 것입니다. Slack에서는 Modal을 쉽게 생성할 수 있는 도구를 제공하고 있습니다. 그 도구를 통해 Modal을 생성한 후, action_id와 text, callback_id, private_metadata 등을 수정하면 쉽게 우리가 원하는 Modal 창을 얻을 수 있습니다.\n저는 아래와 같이 했습니다.\n\n우리는 - “reply” action - “port” shortcut - “reply” shortcut 을 받은 경우 modal이 필요하므로 아래와 같은 Decorator를 작성합니다.\n\nif False:\n    @app.action(\"reply\")\n    @app.shortcut(\"post\")\n    @app.shortcut(\"reply\")\n    def open_modal(ack, body, client, logger):\n        ...\n\nSlack API는 요청을 받는 경우 ack()를 실행하도록 요청하고 있습니다. 그러므로, open_modal(…)이 호출되는 경우 {python} ack()가 실행되어야 합니다.\n\nif False:\n    def open_modal(ack, body, client, logger):\n        ack()\n\n다음으로, open_modal이 끝난 경우, 즉 유저가 Modal에 무엇인가를 입력하여 반환 된 경우를 보겠습니다. Modal이 닫히면, 그 내용을 대나무숲 봇이 #bambooforest 채널에 포스팅/댓글 작성 해야합니다. 이를 위해, Modal에 callback_id를 설정해야 합니다. 우리는 “view_post”라고 하겠습니다.\n\nif False:\n    def open_modal(ack, body, shortcut, client, logger, block_actions):\n        ack()\n        try:\n            try:\n                try:\n                    message_ts=body['message_ts']\n                except:\n                    message_ts=body['container']['message_ts']\n            except:\n                message_ts=\"\" # Global의 경우 message_ts가 없음\n            result = client.views_open(\n                trigger_id=body[\"trigger_id\"],\n                view={\n                    # View 위 내용 생략\n                    \"callback_id\": \"view_post\",\n                    \"private_metadata\": message_ts\n                }\n            )\n            logger.info(result)\n        except SlackApiError as e:\n            logger.error(\"Error creating conversation: {}\".format(e))\n\n그러면 위 “view_post” 콜백를 처리할 수 있는 함수를 만들어 보겠습니다. 마찬가지로 Decorator을 사용합니다.\n\nif False:\n    @app.view(\"view_post\")\n    def handle_submission(ack, body, client, view, logger):\n        ack()\n        logger.info(body)\n        message_ts=view['private_metadata']\n        content=view['state']['values']['post_input_block']['post_content_input']['value']\n        username=view[\"state\"][\"values\"][\"name_input_block\"][\"name_input\"]['value']\n\n“view_post” 콜백이 실행된 경우, ack() 하고 body를 logging 합니다. 이후, 복잡한 경로에 있는 값을 접근하기 쉽도록 값을 변수에 대입합니다. 우리는 message_ts를 private_metadata로 전달했습니다. 또, content와 username의 값은 view[“state”][“values”]의 [‘post_input_block’][‘post_content_input’][‘value’] 및 [“name_input_block”][“name_input”][‘value’]에 존재합니다.\n익명 게시판이므로, Username을 작성하지 않은 경우가 많을 것입니다. 이 경우, 아래에서 만들 username() 함수를 통해 랜덤한 username을 생성합니다. 또한, 게시글의 경우 message_ts 값이 없습니다(위에서 ““를 대입합니다). 경우에 따라 type=”게시글” 또는 type=“댓글”을 저장합니다.\n\nif False:\n    if username is None:\n            username=randname()\n\n    if message_ts == \"\":\n        type=\"게시글\"\n    else:\n        type=\"댓글\"\n\n위에서 정리된 내용을 바탕으로 메시지를 발송합니다. message_ts가 있으면(즉 댓글이면) 메시지를 해당 thread에, 아니면 .env에서 지정한 채널에 발송합니다.\n\nif False:\n    send_message=[\n                    {\n                        \"type\": \"header\",\n                        \"text\": {\n                            \"type\": \"plain_text\",\n                            \"text\": \":bamboo: 익명 메시지 :bamboo:\"\n                        }\n                    },\n                    {\n                        \"type\": \"divider\"\n                    },\n                    {\n                        \"type\": \"section\",\n                        \"text\": {\n                            \"type\": \"mrkdwn\",\n                            \"text\": f\"*{username}님의 {type}입니다.*\"\n                        }\n                    },\n                    {\n                        \"type\": \"section\",\n                        \"text\": {\n                            \"type\": \"plain_text\",\n                            \"text\": f\"{content}\"\n                        },\n                        \"accessory\": {\n                            \"type\": \"button\",\n                            \"action_id\": \"reply\",\n                            \"text\": {\n                                \"type\": \"plain_text\",\n                                \"text\": \"댓글 달기\",\n                                \"emoji\": True\n                            }\n                        }\n                    }\n                ]\n    \n    if message_ts == \"\":\n        client.chat_postMessage(channel=CHANNEL_NAME, blocks=send_message)\n    else: \n        client.chat_postMessage(channel=CHANNEL_NAME, thread_ts=message_ts, blocks=send_message)\n\nsrc/name.py는 다음과 같습니다.\n\nimport random\n\nfirstNames = [\n  \"깔끔한\",\n  \"근면한\",\n#  ...\n]\n\nlastNames = [\n  \"토끼\",\n  \"오리\",\n#  ...\n]\n\ndef randname():\n    return firstNames[random.randint(0, len(firstNames)-1)] + \" \" + lastNames[random.randint(0, len(lastNames)-1)]\n\nrandname()은 firstNames와 lastNames의 원소를 각각 랜덤으로 추출하여 합쳐 반환하는 함수입니다.\n\n위 firstNames, lastNames는 이 글의 GitHub 저장소를 참조하였습니다.\n\nsrc/app.py 상단에\n\nif False:\n    from name import randname\n\n을 추가합니다.\n마지막으로, src/app.py 마지막에\n\nif False:\n    if __name__ == \"__main__\":\n        app.start(3000)\n\n을 추가하고, 서버에서 python3 src/app.py를 실행하면 정상적으로 작동하는 것을 알 수 있습니다.\n\n\n\n생각보다 Slack API Documentation이 충실하고, 회사에서 서버도 자유롭게 사용하도록 허용해주고 있어 쉽게 구현할 수 있었습니다. 실제로는 Docker를 사용하여 회사 서버에서 사용하고 있습니다. 테스트 목적으로는 https://ngrok.com을 사용하면 쉽게 테스트 할 수 있습니다.\n다만, 한 달 정도 대나무숲을 사용 해 본 결과 실질적으로 수십 명 정도의 조직이어야 제대로 사용할 수 있을 것이라는 생각이 들었습니다.\n전체 코드는 링크에서 확인하실 수 있습니다.(글과 일부 바뀌었을 수 있음.)"
  },
  {
    "objectID": "posts/2018-09-25-welcome/index.html",
    "href": "posts/2018-09-25-welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Anpanman의 블로그를 만들었습니다.\nDiamond Prices\nCross reference 연습입니다.\nFootnote 연습입니다.1"
  },
  {
    "objectID": "posts/2018-09-25-welcome/index.html#footnotes",
    "href": "posts/2018-09-25-welcome/index.html#footnotes",
    "title": "Welcome",
    "section": "Footnotes",
    "text": "Footnotes\n\nThis will become a hover-able footnote↩︎"
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html",
    "href": "posts/2023-12-11-quarto-dashboard/index.html",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "",
    "text": "Quarto는 R Markdown을 기반으로 하는 문서 작성 도구입니다.\nR Markdown은 RStudio에서 제공하는 문서 작성 도구로, R 코드와 문서를 한 번에 작성할 수 있습니다.\nQuarto는 R Markdown의 flexdashboard의 역할을 이어가는 기능으로써, R Markdown의 장점을 그대로 가져오면서, R 뿐만 아니라 Python, Julia 등 다양한 언어를 지원합니다.\n본 게시글은 Quarto 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#개요",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#개요",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "",
    "text": "Quarto는 R Markdown을 기반으로 하는 문서 작성 도구입니다.\nR Markdown은 RStudio에서 제공하는 문서 작성 도구로, R 코드와 문서를 한 번에 작성할 수 있습니다.\nQuarto는 R Markdown의 flexdashboard의 역할을 이어가는 기능으로써, R Markdown의 장점을 그대로 가져오면서, R 뿐만 아니라 Python, Julia 등 다양한 언어를 지원합니다.\n본 게시글은 Quarto 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#quarto-dashboard-소개",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#quarto-dashboard-소개",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "Quarto Dashboard 소개",
    "text": "Quarto Dashboard 소개\n\n\n\n\n\n\nQuarto Versione\n\n\n\nQuarto Dashboard는 현재 개발 중인 기능으로, 1.4 버전 이상의 Quarto를 사용해야 합니다.\n\n\n이번 글에서는 Quarto를 사용하여 Markdown과 R, Python, Julia등을 활용해 아래 이미지 같은 인터랙티브한 대시보드를 만드는 방법을 소개합니다.\n\n\n더 많은 예시는 링크에서 확인 할 수 있습니다.\nQuarto dashboard는 다양한 언어를 활용할 수 있기 때문에 이들로 부터 파생되는 Plotly, Leaflet, Jupyter Widgets, Htmlwidgets를 포함한 다양한 커스텀 위젯을 사용할 수 있습니다.\n\n\n\n\n\n\nPrerequisite\n\n\n\nQuarto와 Rmarkdown에 대한 설명은 이전의 글과 자료를 참고하시길 바랍니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드의-구성",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드의-구성",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드의 구성",
    "text": "대시보드의 구성\n보통 대시보드의 구성은 아래 그림과 같이 5개의 영역으로 구분합니다.\n\n\n\nMain: 대시보드에서 주요 지표를 포함한 내용을 표현하는 공간입니다.\nHeader / Footer : 대시보드에 대한 일반적인 메타 정보를 소개합니다.\nNavigator: 대시보드가 여러 개의 내용을 담고 있어 main을 구분해야하는 경우 각 페이지를 구분짓는 역할을 합니다.\nSide: 대시보드의 내용을 조절하는 역할을 합니다.\n\n즉, 위의 예시에서 보여진 Labor and Delivery Dashboard는 아래와 같이 영역을 구분해볼 수 있습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-만들기",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-만들기",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드 만들기",
    "text": "대시보드 만들기\nqmd 파일에서 (이후 배포를 고려하면 파일명은 index.qmd를 권장합니다.)\n---\nformat: dashboard\n---\n를 추가하여 quarto dashboard를 만들 수 있습니다.\nQuarto dashboard에서 반드시 알아야 하는 컨셉은 3가지이며 하나씩 소개하면더 대시보드를 만들어가겠습니다.\n\n카드\n대시보드 내부 요소 배치 (레이아웃)\n대시보드 구성 (페이지)\n\n카드\n카드는 대시보드의 Main을 구성하는 그래프나 테이블, 값등을 포함하는 하나의 단위입니다.\nquarto에서는 다음과 같이 사용합니다.\n::: {.card}\nThis text will be displayed within a card\n:::\n추가로\n```{r}\n...\n```\n를 사용하는 기본 코드 블록은 Dashboard format에서 자동으로 카드로 변경됩니다.\n이때 카드에 사용할 수 있는 주요 옵션은 아래와 같습니다.\n\n\ntitle: 카드의 제목\n\nexpandable: 카드를 접을 수 있을지의 여부\n\noutput: 결과를 출력할지 여부\n\nlayout-ncol: 카드 내용을 구분할 column의 개수 (layout-nrow)\n\n이 외에 기본 코드블록을 사용하기 때문에 코드 블록의 옵션들을 사용 가능합니다.\n이제 카드를 qmd에 2개 추가해보겠습니다.\n```{r}\n#| echo: false\nlibrary(ggplot2)\n```\n\n```{r}\n#| title: \"Card 1\"\n#| layout-ncol: 2\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = wt)) + \n  geom_point()\n\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = qsec)) + \n  geom_point()\n```\n\n```{r}\n#| title: \"Card 2\"\n#| output: false\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = vs)) + \n  geom_point()\n```\n\n```{r}\n#| title: \"Card 3\"\n#| expandable: false\nmtcars |&gt; \n  ggplot(aes(x = mpg, y = vs)) + \n  geom_point()\n```\n위 코드의 실행결과는 아래 그림처럼 2개의 카드를 만들어냅니다. (2번째는 output:false)\n\n\n레이아웃\n대시보드의 레이아웃은 특별한 설정을 하지 않으면 (위 예시처럼) 1개의 카드가 1개의 행으로 배치됩니다.\n그런데 ## Row 태그를 사용하면 ## Row 태그 아래에 있는 카드들을 1개의 행에 배치할 수 있습니다.\n(마찬가지로 별도의 설정을 하지 않으면 Column에 같은 크기로 배치됩니다)\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n:::{.card}\nCard 2-1\n:::\n\n:::{.card}\nCard 2-2\n:::\n\n\n추가로, Row로 먼저 행을 구분 한뒤, Column을 사용해 디테일한 배치도 가능합니다.\n이때 Column은 ### Column으로 ## Row안에만 사용할 수 있습니다.\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n### Column {width=40%}\n:::{.card}\nCard 2-1\n:::\n\n### Column {width=60%}\n:::{.card}\nCard 2-2\n:::\n\n\n\n\n\n\n\n\nScroll\n\n\n\n별 다른 설정을 하지 않으면 각 Row에 배치된 요소의 높이들의 합이 100%에 맞추어 크기가 일괄적으로 조절되지만, Scrolling 옵션을 사용하여 요소의 원래 크기를 유지하며 화면이 스크롤 되게 변경할 수 있습니다.\nformat: \n  dashboard:\n    scrolling: true \n\n\nTabset\n카드를 행과 열로 배치하는 것 외에도, Tabset을 사용하여 카드를 탭으로 구분할 수 있습니다.\n## Row {.tabset}\n\n\n페이지\n대시보드에 여러 요소들을 담아야 한다면 별도의 페이지로 구분하여 만들 수 있습니다.\n이때 페이지는 # 태그를 사용하고, yaml에 “title”을 설정해야합니다.\n---\ntitle: \"dashboard\"\nformat: dashboard\n---\n# Page A\n\n## Row {height=70%}\n\n:::{.card}\nCard 1\n:::\n\n## Row {height=30%}\n\n### Column {width=40%}\n:::{.card}\nCard 2-1\n:::\n\n### Column {width=60%}\n:::{.card}\nCard 2-2\n:::\n\n# Page B\n\n:::{.card}\nCard 3\n:::\n\n\n이처럼 title을 설정하면 Header가 생성되며 Page 가 존재한다면 Navigation 역할도 같이 수행합니다.\nHeader\nHeader에서는 title 외에 author를 사용해 일종의 “subtitle” 역할을 할 수 있습니다. 추가로 logo를 사용하여 대시보드의 로고를 설정하거나, 외부 링크나 자료로 연동할 수 있는 nav-buttons도 사용가능합니다.\n네비게이션 버튼을 커스텀으로 제작하는 것에 대해서는 설명하지 않고 공식문서의 링크로 대체합니다.\n\n\n\n\n\n\nQuarto Versione\n\n\n\nQuarto 1.4.455 버전을 기준으로, 네비게이션 버튼은 버그로 아직 작동하지 않습니다.\n\n\nSidebar\nSidebar는 sidebar 태그를 사용하여 만들 수 있으며 특정 페이지에 종속되지 않습니다.\n\n# {.sidebar}\n\nSidebar content \n\n로고와 sidebar를 설정하고 난 대시보드의 결과는 다음과 같습니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-채우기",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#대시보드-채우기",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "대시보드 채우기",
    "text": "대시보드 채우기\n대시보드에 들어갈 수 있는 내용은 주로 r로 만든 결과물이지만, 다른 형태의 요소들도 활용할 수 있습니다.\n테이블\n여러 옵션이 있으며, kable, DT, reactable 정도가 사용됩니다.\n\nknitr::kable(mtcars) # kable\n\nDT::datatable(mtcars) # DT\n\nreactable::reactable(mtcars) # reactable\n\n\n\nValue Box\nQuarto 대시보드에서만 사용할 수 있는 “지표를 표현하기 위한” 특별한 방법입니다.\n아래 예시처럼 {.valuebox} 코드로 사용할 수 있습니다.\n옵션으로 색상은 primary, secondary, success, danger, warning, info, light, dark를 사용할 수 있으며, 사용 가능한 (bootstrap) 아이콘의 종류는 링크에서 확인 가능합니다.\n\n::: {.valuebox icon=\"pencil\" color=primary}\nArticles per day\n\n`r articles`\n:::\n\n\n\nTheme\n다른 Quarto 기능들과 마찬가지로 Quarto Dashboard도 yaml에서 사용되어 색상과 스타일을 꾸밀 수 있는 Theme 기능을 제공합니다.\n기본 값은 cosmo이고, 가능한 값의 목록은 링크에서 확인 가능합니다.\n---\nformat: \n  dashboard:\n    logo: Zarathu.png\n    theme: sandstone\n---\n\n\n\n\n\n\n\n\n대시보드 게시하기\n\n\n\nQuarto로 만든 Dashboard는 Quarto pub, Github page, Posit connect, Netlify 등의 서비스를 활용하여 공유할 수 있습니다.\n이 글에서는 다루지 않으며, 차라투의 Quarto 교육자료 17페이지를 참고하시길 바랍니다."
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#다른-대시보드-툴과-quarto-dashboard의-차이",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#다른-대시보드-툴과-quarto-dashboard의-차이",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "다른 대시보드 툴과 Quarto Dashboard의 차이",
    "text": "다른 대시보드 툴과 Quarto Dashboard의 차이\n\n\nQuarto Dashboard는 Tableau, Power BI, Shiny나 Streamlit과 같은 대시보드를 만들 수 있는 도구와 비교될 수 있습니다.\n제일 먼저 대시보드 사용에 필요한 비용입니다. 즉 Tableau, Power BI등 상용 서비스는 클릭으로 쉽게 만들 수 있지만 사용하기 위해 비용이 발생합니다.\n한편 Quarto Dashboard는 별도의 비용은 없지만, 개발을 통해 대시보드를 만들어야 합니다.\n두번째로는 대시보드의 내용이 변하는 가의 여부입니다.\n즉 사용자의 선택 값에 따라 값이 동적으로 바뀌어야 한다면 Shiny나 Streamlit을 사용하는 것이 좋습니다.\n반면 사용자의 선택은 없고 값이 정적으로 고정되어 있다면 Quarto Dashboard를 사용하는 것이 좋습니다.\n이 둘의 차이는 동적 대시보드는 사용자의 입력을 처리하고 결과를 호스팅할 서버가 필요하고 이를 한 비용이 필요합니다. (정적은 비용이 들지 않습니다)\n\n\n\n\n\n\n정적/동적\n\n\n\n예를 들어, 어제의 매출 데이터를 계산하여 보여주는 대시보드는 날마다 값이 변하긴 하지만, 사용자의 입력이 없기 때문에 정적으로 이루어져 Quarto Dashboard로도 충분합니다.\n그런데 동일한 매출 데이터이지만, 사용자로부터 날짜를 입력 받고, 그에 따라 매출 데이터를 계산하여 보여주는 대시보드는 데이터가 변해야하기 때문에 Shiny나 Streamlit 과 같은 동적 대시보드가 필요합니다.\n\n\n(Quarto Dashboard에서의 페이지나 탭셋은 모든 결과를 만들어두고 필요에 따라 보여주는 내용을 다르게 한다의 관점으로 데이터가 변하지 않는 정적 기능입니다.)"
  },
  {
    "objectID": "posts/2023-12-11-quarto-dashboard/index.html#마치며",
    "href": "posts/2023-12-11-quarto-dashboard/index.html#마치며",
    "title": "Quarto Dashboard를 이용해 대시보드 개발하기",
    "section": "마치며",
    "text": "마치며\n이번 아티클에서는 새롭게 공개된 Quarto Dashboard를 통해 대시보드를 구성하는 방법에 대해 알아보았습니다.\n꼭 헬스케어, 메디컬 IT 업계에서뿐 아니라 다양한 산업과 역할에서 데이터를 활용하기 위해 대시보드를 활용하고 있습니다.\nQuarto와 사용 가능한 R이나 Python을 활용할 수 있다면 Quarto Dashboard를 사용해 정적 대시보드 구성과 공유가 더욱 편리하게 이루어지기를 기대하며 글을 마칩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html",
    "href": "posts/2023-02-01-streamlit/index.html",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "",
    "text": "https://docs.streamlit.io/library/api-reference 를 바탕으로 정리한 글입니다.\n2023년 2월 기준) streamlit version 1.17.0 을 기준으로 작성하였습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "1.1 streamlit과 shiny 예제 비교",
    "text": "1.1 streamlit과 shiny 예제 비교\n\nstreamlit 활용 사례\n\nhttps://streamlit.io/gallery\n\nshiny 활용 사례\n\nhttps://shiny.rstudio.com/gallery/"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.1 스트림릿 설치",
    "text": "2.1 스트림릿 설치\n파이썬, 가상환경 설치와 관한 내용은 다른 게시물을 참고하시기 바랍니다.\npip install streamlit \n파이썬 가상환경에 streamlit패키지를 설치합니다\napp.py를 생성한 후 다음 과 같이 수정하여 저장합니다.\n\nimport streamlit as st\n\nst.title('Hello Streamlit')\n\n이후 터미널에서\nstreamlit run app.py \nstreamlit run app.py 명렁어를 실행하면, 로컬서버로 페이지가 만들어지게 됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.2 스트림릿 기능 소개",
    "text": "2.2 스트림릿 기능 소개\n더 자세한 내용은 https://docs.streamlit.io/library/api-reference에서 확인하실 수 있습니다.\n\n2.2.1 강조 문구\n\ntitle\nheader\nsubheader\n\n\nimport streamlit as st\n\nst.title('this is title')\nst.header('this is header')\nst.subheader('this is subheader')\n\n\n제목과 헤더,서브헤더를 구현할 수 있다.\n\netc\n\n추가적으로 Markdown 문법을 st.markdown으로, caption, Latex Code block을 활용가능합니다\n\n\n\n2.2.2 layout 짜기\n페이지의 공간을 레이아웃을 통해 웹페이지를 분할할 수 있다.\n\ncolumn\n\n\nimport streamlit as st\n\ncol1,col2 = st.columns([2,3])\n# 공간을 2:3 으로 분할하여 col1과 col2라는 이름을 가진 컬럼을 생성합니다.  \n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# with 구문 말고 다르게 사용 가능 \ncol1.subheader(' i am column1  subheader !! ')\ncol2.checkbox('this is checkbox2 in col2 ') \n#=&gt;위에 with col2: 안의 내용과 같은 기능을합니다\n\n\n결과물\n\n\ntab\n\n\nimport streamlit as st\n\n# 탭 생성 : 첫번째 탭의 이름은 Tab A 로, Tab B로 표시합니다. \ntab1, tab2= st.tabs(['Tab A' , 'Tab B'])\n\nwith tab1:\n  #tab A 를 누르면 표시될 내용\n  st.write('hello')\n    \nwith tab2:\n  #tab B를 누르면 표시될 내용 \n  st.write('hi')\n\n 다음을 실행하면 tab A 를 눌렀을 경우 hello, tab B를 눌렀을 경우 hi가 나오게 됩니다.\n탭의 특징으로는, 탭을 클릭과 동시에 데이터가 만들어지는 것이 아니라,탭에 표시될 데이터가 이미 만들어져 았는 것이 특징입니다. 장점이 될 수도 있고, 단점이 될 수도 있습니다.\n \n\nsidebar\n\n\nimport streamlit as st\n\n#st.sidebar는 \n\nst.sidebar.title('this is sidebar')\nst.sidebar.checkbox('체크박스에 표시될 문구')\n# 사이드바에 체크박스, 버튼등 추가할 수 있습니다! \n\n \n\netc\n\n추가적으로 Expander, Container ,Empty가 있습니다\n\n\n2.2.3 이미지 불러오기\n\nimport streamlit as st\nfrom PIL import Image\n\n#PIL 패키지에 이미지 모듈을 통해 이미지 열기 \n# Image.open('이미지 경로')\nzarathu_img = Image.open('zarathu.png')\n\ncol1,col2 = st.columns([2,3])\n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# 컬럼2에 불러온 사진 표시하기\ncol2.image(zarathu_img)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "href": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.3 웹사용자로부터 input 받기",
    "text": "2.3 웹사용자로부터 input 받기\n이제 UI적인 부분을 전반적으로 살펴봤기 때문에, 사용자로 하여금 input을 받아 interactive하게 데이터를 보여주는 페이지를 만들어보겠습니다\n데이터는 사이킷런의 아이리스 데이터를 가져와 사용하도록 하겠습니다. \n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.datasets import load_iris \nimport matplotlib.pyplot as plt\nimport streamlit as st\n\niris_dataset = load_iris()\n\ndf= pd.DataFrame(data=iris_dataset.data,columns= iris_dataset.feature_names)\ndf.columns= [ col_name.split(' (cm)')[0] for col_name in df.columns] # 컬럼명을 뒤에 cm 제거하였습니다\ndf['species']= iris_dataset.target \n\n\nspecies_dict = {0 :'setosa', 1 :'versicolor', 2 :'virginica'} \n\ndef mapp_species(x):\n  return species_dict[x]\n\ndf['species'] = df['species'].apply(mapp_species)\nprint(df)\n\n\nstreamlit 에서 데이터 프레임을 보여주는 방식은 table과 dataframe 두가지를 사용할 수 있습니다.\ndataframe의 head함수를 이용하여 첫 5행의 데이터에 대해 table과 dataframe으로 출력한 경우입니다.\n\nst.subheader('this is table')\nst.table(df.head())\n\nst.subheader('this is data frame')\nst.dataframe(df.head())\n\n\n이제 버튼을 동작시키는 방법을 배워보도록 하겠습니다.\n 1. Select Box\n\n# 사이드바에 select box를 활용하여 종을 선택한 다음 그에 해당하는 행만 추출하여 데이터프레임을 만들고자합니다.\nst.sidebar.title('Iris Species🌸')\n\n# select_species 변수에 사용자가 선택한 값이 지정됩니다\nselect_species = st.sidebar.selectbox(\n    '확인하고 싶은 종을 선택하세요',\n    ['setosa','versicolor','virginica']\n)\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species']== select_species]\n# 선택한 종의 맨 처음 5행을 보여줍니다 \nst.table(tmp_df.head())\n\n\n사용자가 sidebar에서 종을 바꿀 때마다 자동으로 해당하는 종의 테이블 정보가 불러와지게 됩니다.\n\n\n\nmulti select\n\n\n# 여러개 선택할 수 있을 때는 multiselect를 이용하실 수 있습니다 \n# return : list\nselect_multi_species = st.sidebar.multiselect(\n    '확인하고자 하는 종을 선택해 주세요. 복수선택가능',\n    ['setosa','versicolor','virginica']\n\n)\n\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species'].isin(select_multi_species)]\n# 선택한 종들의 결과표를 나타냅니다.  \nst.table(tmp_df)\n\n\n\n\nRadio / Slider\n\n\n# 라디오에 선택한 내용을 radio select변수에 담습니다\nradio_select =st.sidebar.radio(\n    \"what is key column?\",\n    ['sepal length', 'sepal width', 'petal length','petal width'],\n    horizontal=True\n    )\n# 선택한 컬럼의 값의 범위를 지정할 수 있는 slider를 만듭니다. \nslider_range = st.sidebar.slider(\n    \"choose range of key column\",\n     0.0, #시작 값 \n     10.0, #끝 값  \n    (2.5, 7.5) # 기본값, 앞 뒤로 2개 설정 /  하나만 하는 경우 value=2.5 이런 식으로 설정가능\n)\n\n# 필터 적용버튼 생성 \nstart_button = st.sidebar.button(\n    \"filter apply 📊 \"#\"버튼에 표시될 내용\"\n)\n\n# button이 눌리는 경우 start_button의 값이 true로 바뀌게 된다.\n# 이를 이용해서 if문으로 버튼이 눌렸을 때를 구현 \nif start_button:\n    tmp_df = df[df['species'].isin(select_multi_species)]\n    #slider input으로 받은 값에 해당하는 값을 기준으로 데이터를 필터링합니다.\n    tmp_df= tmp_df[ (tmp_df[radio_select] &gt;= slider_range[0]) & (tmp_df[radio_select] &lt;= slider_range[1])]\n    st.table(tmp_df)\n    # 성공문구 + 풍선이 날리는 특수효과 \n    st.sidebar.success(\"Filter Applied!\")\n    st.balloons()\n\nslider_range : list 형식으로 2개의 값이 저장됩니다. 양쪽 앞뒤로 두개의 값을 저장합니다.\nslider_range[0] : 최솟값 \nslider_range[1] : 최댓값  \n\npetal_width 컬럼값이 0에서 1.38사이인 값들로 정보를 filtering한 결과물이 표시됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#plotly",
    "href": "posts/2023-02-01-streamlit/index.html#plotly",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.1 plotly",
    "text": "3.1 plotly\npython의 시각화로 자주 이용되는 패키지는 matplotlib,seaborn,bokeh,plotly 등이 있습니다.\n스트림릿에서는 bokeh, plotly, matplotlib 등의 패키지를 통해 생성한 그림(figure)를 streamlit을 통해 웹에서 표시하는 기능을 제공합니다. \n\n3.1.1 st.plotly_chart\n\n#st.plotly_chart(figure_or_data, use_container_width=False, theme=\"streamlit\", **kwargs)\n\n 인자로 줄 수 있는 옵션들에 대해서 하나씩 설명해드리도록 하겠습니다.\n\nfigure_or_data : 첫번째 인자로 plotly로 생성한 그림의 이름이 들어가는 위치입니다.\nuse_container_width : 레이아웃으로 지정한 사이즈에 그림이 해상도를 조절해서 들어갈 것인지 (-&gt;‘True’)  아니면 원래그림 크기대로 표시될 것인지 (-&gt; ‘False’) 선택하는 옵션입니다.\ntheme : 스트림릿 웹에 어떻게 표시될지 테마를 설정합니다. 인자로는 “streamlit” 과 None(입력하지 않음)을 선택할 수 있습니다.\n\n\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\n\nfig = px.scatter(\n    df.query(\"year==2007\"),\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=60,\n)\nfig.show()\n\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\nwith tab1:\n    # Use the Streamlit theme.\n    # This is the default. So you can also omit the theme argument.\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\nwith tab2:\n    # Use the native Plotly theme.\n    st.plotly_chart(fig, theme=None, use_container_width=True)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#지도표시",
    "href": "posts/2023-02-01-streamlit/index.html#지도표시",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.2 지도표시",
    "text": "3.2 지도표시\n\nimport numpy as np\nimport pandas as pd \n\n#지도 위에 표시될 점 좌표 값을 위도경도에 담습니다 .\nbase_position =  [37.5073423, 127.0572734]\n#중심점의 위도, 경도 좌표를 리스트에 담습니다.\n\n# base_position에, 랜덤으로 생성한 값을 더하여 5개의 좌표를 데이터 프레임으로 생성하였고,\n# 컬럼명은 위도 :lat  경도 lon으로 지정하였습니다. \n\n\nmap_data = pd.DataFrame(\n    np.random.randn(5, 1) / [20, 20] + base_position , \n    columns=['lat', 'lon'])\n# map data 생성 : 위치와 경도 \n\nprint(map_data)\n\n이어서 이 위도 경도 데이터를 스트림릿 웹 페이지에 지도로 나타내는 과정은 다음과 같습니다. \n\nst.code('st.map(map_data)')\n# 웹사이트에 어떤 코드인지 표시해주기 \nst.subheader('Map of Data ')\n# 제목 생성 \nst.map(map_data)\n# 지도 생성 \n\n결과물"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.1 st.session_state 세션 스테이트란?",
    "text": "4.1 st.session_state 세션 스테이트란?\n상태가 자꾸 변하는 것들을 세션스테이트에 관리해두면 바뀌는 값에 따라 내용이 바뀌는 것들을 기록할 수 있다.\n** 아주 중요한 역할을 하는 기능입니다. **"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.2 st.session_state 활용",
    "text": "4.2 st.session_state 활용\n\n\n\nwhy we use sessionstate\n\n\n세션스테이트에 key값의 초기값이 없으면, 초기값을 생성하여 놓는 작업을 if문을 통해 진행합니다.\n세션스테이트에 사용자가 입력한 인풋에 따라서 dataframe이 재가공 되는데 이 값이 interactive하게 지정되게 하기 위해 st.session_state값으로 사용합니다.\n\n# 예시코드 \n# import streamlit as st\n\n\n\n# if 'final_dataframe' not in st.session_state:\n#   # session state 에 final 이라는 값이 없으면, \n#   st.session_state['final_dataframe']= df\n#   # 초기 값 설정 : session_state에 final_dataframe키 값에 초기값 데이터를 집어넣습니다 .\n\n# #아래 코드는 df의 테이블 값이 바뀌더라도 interactive하게 연동되서 바뀌지 않습니다\n# st.table(df)\n\n# #  아래 코드는  이제 dataframe가 조작될 때 마다 session_state객체 안에 final_dataframe값을 변경하면, \n# #  수정 될 때  계속 바뀌어서 보여줍니다. \n# st.table(st.session_state.final_dataframe)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "href": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.3 cache 란 무엇인가?",
    "text": "4.3 cache 란 무엇인가?\n\n\n\n캐싱기능의 필요성\n\n\n캐싱에 관한 간단한 개념은 주문이 들어왔을 때 우리가 만들기 시작하면 코드가 결과물을 만들어내는데 시간이 오래 걸리는 경우 유저가 결과물을 오랜시간 기달려야하는 경우가 발생합니다.\n따라서 만들어내는데 오래걸리는 결과물을 미리 만들어두고, 보이지 않는 곳에 캐싱하여 필요할때 찾아 꺼내는 것을 cache기능이라고 간단히 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.4 streamlit 에서 cache 기능 사용하기",
    "text": "4.4 streamlit 에서 cache 기능 사용하기\nstreamlit에서는 시간이 오래걸리는 작업: 데이터 로드 등을 할때 위에 @st.cache를 추가하여 캐싱할 수 있게 할 수 있다.\n\nimport streamlit as st\nimport pandas as pd \n\nfile_path = '~~~filepath'\n@st.cache\ndef load_data():\n  data = pd.read_csv(file_path)\n  return data\n\n큰 데이터를 로드하거나, 실행이 오래걸리는 복잡한 연산을 해야할 때 cache기능을 이용하면 용이하다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "href": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.5 로딩상태 구현",
    "text": "4.5 로딩상태 구현\n\n4.5.1 st.progress\n\nimport time \n\n# 방법 1 progress bar \nlatest_iteration = st.empty()\nbar = st.progress(0)\n\nfor i in range(100):\n  # Update the progress bar with each iteration.\n  latest_iteration.text(f'Iteration {i+1}')\n  bar.progress(i + 1)\n  time.sleep(0.05)\n  # 0.05 초 마다 1씩증가\n\nst.balloons()\n# 시간 다 되면 풍선 이펙트 보여주기 \n\n코드 수행 소요시간이 긴 경우, progress bar와 streamlit의 spinner를 통해서 로딩페이지를 만들 수 있다!\n \n\n\n4.5.2 st. spinner 사용\n\n#방법2  st.spinner 사용 \nimport time \n\nwith st.spinner('Wait for it...'):\n  time.sleep(5)\n  st.success('Done!')"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "href": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.1 근본적 한계",
    "text": "5.1 근본적 한계\nstreamlit 에서 제공하는 UI들이 대부분 예쁘지만, 디테일적으로 맘에 들지 않을 수가 있다.\n\n예를 들어 4.87로 표시된 부분의 폰트가 맘에들지 않는다고 하자. 그렇다면 streamlit를 markdown을 통해서 style을 직접 수정해주어야한다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "href": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.2 극복방안 : CSS hack",
    "text": "5.2 극복방안 : CSS hack\n이 작업을 streamlit CSS hack이라고 한다\n\n브라우저에 검사기능을 이용하여 css해당하는 부분을 마크다운으로 직접 style tag를 넣어서 수정하면 된다.\n아래 유튜브를 통해서 더욱 자세한 내용을 배울 수 있습니다.\nhttps://www.youtube.com/watch?v=gr_KyGfO_eU"
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html",
    "href": "posts/2019-01-03-rdatamanagement/index.html",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, tidyverse 생태계에서의 데이터 매니지먼트 방법을 강의할 예정입니다. 강의 내용을 미리 정리하였습니다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 두 번째의 기초에 해당하며\n\nreadr 패키지의 read_csv 함수로 데이터를 빠르게 읽은 후\nmagrittr 패키지의 %&gt;% 연산자와 dplyr 패키지의 select, mutate, filter, group_by, summarize 함수로 직관적인 코드를 작성하고\npurrr 패키지의 map 함수로 쉽게 반복문을 처리하는 것을 목표로 한다.\n\n각각의 패키지를 따로 설치할 수도 있고 install.packages(\"tidyverse\")로 tidyverse 생태계의 패키지를 모두 설치할 수도 있다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 읽기: readr\n",
    "text": "데이터 읽기: readr\n\nreadr 패키지에서 csv 파일을 읽는 함수는 read_csv이며, 구분자(ex: 공백, 탭)가 다를 때는 read_delim 함수를 이용하여 구분자를 설정할 수 있다. 예제 데이터를 읽어보자.\n\nlibrary(readr)\na &lt;- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\")\n\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Sex, STRESS_EXIST\ndbl (12): Age, Height, Weight, BMI, DM, HTN, Smoking, MACCE, Death, MACCE_da...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n데이터를 읽으면 각 변수들을 어떤 형태(숫자형, 문자형)로 읽었는지 표현되는데, 바꾸고 싶은 것이 있으면 아래와 같이 col_types 옵션을 이용하면 된다.\n\n## Character: col_character() or \"c\"\na &lt;- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\",col_types = cols(HTN = \"c\"))\n\n이제 데이터를 살펴보면 HTN 변수가 문자형이 된것을 볼 수 있다.\n\na\n\n\n\n\n  \n\n\n\n기본 함수인 read.csv 와 비교했을 때 아주 작은 데이터에서는 장점이 없으나, 그 크기가 커질수록 read_csv가 더 좋은 성능을 보임이 알려져 있다(Figure @ref(fig:readfunc)).\n\n\n\n\n파일 읽기 함수 비교3\n\n\n\nread_csv로 읽은 데이터는 tibble이라는 새로운 클래스로 저장된다. 직접 확인해보자.\n\nclass(a)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n기존의 data.frame 외에도 tbl_df, tbl 와 같은 것들이 추가되어 있다. tibble은 data.frame보다 좀 더 날 것(?)의 정보를 보여주는데, 범주형 변수를 factor가 아닌 character 그대로 저장하고 변수명을 그대로 유지하는 것(data.frame에서 변수명의 특수문자나 공백은 .으로 바뀜)이 가장 큰 특징이다. tibble의 추가 내용은 jumpingrivers 블로그4를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "href": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "직관적인 코드: %>% 연산자",
    "text": "직관적인 코드: %&gt;% 연산자\ntidyverse 생태계에서 가장 중요한 것을 하나만 고르라면 magrittr 패키지의 %&gt;% 연산자를 선택하겠다. %&gt;%은 Rstudio에서 단축키 Ctrl + Shift + M (OS X: Cmd + Shift + M)로 입력할 수 있는데, 이것을 이용하면 의식의 흐름대로 코딩을 할 수 있어 직관적인 코드를 작성할 수 있다. head함수를 통해 %&gt;%의 장점을 알아보자.\n\nlibrary(magrittr)\n\n## head(a)\na %&gt;% head\n\n\n\n\n  \n\n\n\n\na %&gt;% head(n = 10)\n\n\n\n\n  \n\n\n\nhead(a)와 a %&gt;% head는 같은 명령어 “a의 head를 보여줘”로, a %&gt;% head가 생각의 흐름이 반영된 코드임을 알 수 있다. 일반적으로 %&gt;% 연산자는 함수의 맨 처음 인자를 앞으로 빼오는 역할을 하고 f(x, y)는 x %&gt;% f(y)로 바꿀 수 있다. 맨 처음 인자가 아닐 때에는 y %&gt;% f(x, .)과 같이 앞으로 빼온 변수의 자리에 .를 넣으면 된다. 예를 들면 아래에 나오는 세 명령어는 모두 같다.\n\na %&gt;% head(n = 10)\n10 %&gt;% head(a, .)   \n10 %&gt;% head(a, n = .)\n\n%&gt;%의 진가는 여러 함수를 한 번에 사용할 때 나타나는데, head와 subset을 동시에 쓰는 경우를 예로 살펴보겠다.\n\n## head(subset(a, Sex == \"M\"))\na %&gt;% subset(Sex == \"M\") %&gt;% head\n\n\n\n\n  \n\n\n\n이 명령어는 a에서 남자만 뽑아서 head를 보여줘로 기존의 head(subset(a, Sex == \"M\"))보다 훨씬 직관적이며 함수가 3개, 4개로 늘어날수록 비교가 안될 정도의 가독성을 보여준다. 남자만 뽑아 회귀분석을 수행하고 그 계수와 p-value를 구하는 예를 살펴보자. 먼저 기존의 코딩 스타일대로 명령을 수행하면 아래와 같다.\n\nb &lt;- subset(a, Sex == \"M\")\nmodel &lt;- glm(DM ~ Age + Weight + BMI, data = b, family = binomial)\nsumm.model &lt;- summary(model)\nsumm.model$coefficients\n\n\n\n\n\n\n\n다음은 %&gt;% 를 이용한 코딩이다.\n\na %&gt;% \n  subset(Sex == \"M\") %&gt;% \n  glm(DM ~ Age + Weight + BMI, data = ., family = binomial) %&gt;% \n  summary %&gt;% \n  .$coefficients\n\n\n\n\n\n\n\n%&gt;% 연산자로 쓴 코드는 읽기 쉬운 것은 물론 불필요한 중간변수(b, model, summ.model)가 없어 깔끔하고 메모리의 낭비도 없다. 딱 하나 주의할 점은 코드를 내려쓸 때 각 줄은 반드시 %&gt;%로 끝나야 한다는 점이다. 예를 들어 아래의 코드를 실행하면 맨 윗줄만 실행되는 것을 확인할 수 있다.\n\na %&gt;% subset(Sex == \"M\")\n  %&gt;% head \n\n본 강의 후 다른 것은 까먹어도 %&gt;% 만 익숙해지면 성공이라고 생각한다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 정리: dplyr\n",
    "text": "데이터 정리: dplyr\n\ndplyr 는 데이터를 효과적으로 다룰 수 있는 일련의 함수들을 제공한다. 이중 group_by와 summarize는 쉽게 그룹 별 요약통계량을 보여줌으로서 기존 R 문법과 차별화된 가치를 제공하므로 꼭 익혀두자.\nfilter\nfilter는 subset 함수와 같은 기능으로 특정 조건으로 데이터를 필터링하는 데 이용된다. 아래는 데이터에서 남자만 추출하는 예시이다.\n\nlibrary(dplyr)\na %&gt;% filter(Sex == \"M\") \n\n\n\n\n  \n\n\n\nfilter에서는 & 외에 ,으로도 AND 조건을 쓸 수 있어 가독성이 좋다. between 함수를 이용하면 연속변수의 특정 범위를 선택할 수도 있는데, 이것 역시 기존의 &를 활용하는 것보다 직관적이다. 50~60세 사이를 필터링하는 예시를 살펴보자.\n\n## Age between 50 and 60.\na %&gt;% filter(between(Age, 50, 60))\n\n\n\n\n  \n\n\n\n아래의 &나 ,로 표현한 조건도 between을 이용한 것과 같은 결과를 보여준다.\n\na %&gt;% filter(Age &gt;= 50 & Age &lt;= 60)\na %&gt;% filter(Age &gt;= 50, Age &lt;= 60)\n\n\narrange: 정렬\narrange는 특정 순서에 따라 데이터를 정렬하는 함수로, 정렬 순서만 알려주는 order 함수와는 달리 정렬된 데이터를 보여주는 것이 특징이다.\n\n## a[order(a$Age), ]\na %&gt;% arrange(Age)\n\n\n\n\n  \n\n\n\n정렬 조건이 2개 이상이면 ,로 같이 적을 수 있으며 내림차순 정렬은 desc 명령어를 이용한다. 아래는 Age에 대해 오름차순, BMI에 대해 내림차순 정렬을 수행하는 예시이다.\n\n## a[order(a$Age, -a$BMI), ]\na %&gt;% arrange(Age, desc(BMI))\n\n\n\n\n  \n\n\n\n조건에 Age 대신 \"Age\"와 같이 문자열을 넣을 때는 언더바(_)가 붙은 arrange_ 함수를 이용하며, 이것은 나머지 함수들에서도 마찬가지이다.\n\na %&gt;% arrange_(\"Age\")\n\n\nselect: 변수 선택\nselect는 데이터에서 특정 변수들을 선택하는 함수로 기본 R 에는 없는 유용한 기능들을 제공한다.\n\n## a[, c(\"Sex\", \"Age\")]\na %&gt;% select(Sex, Age, Height)\n\n\n\n\n  \n\n\n\n변수명 대신에 열 번호를 넣어도 되며 Sex:Height을 이용해서 Sex와 Height 사이의 모든 변수를 선택할 수도 있다. arrange 함수와는 달리 \"Sex\"와 같은 문자열도 그대로 입력 가능하며, 아래의 방법들은 모두 a %&gt;% select(Sex, Age, Height)와 같은 코드이다.\n\na %&gt;% select(Sex:Height)\na %&gt;% select(\"Sex\":\"Height\")\na %&gt;% select(2, 3, 4)\na %&gt;% select(c(2, 3, 4))\na %&gt;% select(2:4)\n\n특정 변수들을 빼려면 -Sex나 -(Sex:Height)와 같이 적으면 된다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %&gt;% select(-Sex, -Age, -Height)\n\n\n\n\n  \n\n\n\n아래의 코드도 a %&gt;% select(-Sex, -Age, -Height)와 같은 결과를 준다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %&gt;% select(-2, -3, -4)\na %&gt;% select(-(2:4))\na %&gt;% select(-c(2, 3, 4))\n\na %&gt;% select(-(Sex:Height))\na %&gt;% select(-\"Sex\", -\"Age\", -\"Height\")\na %&gt;% select(-(\"Sex\":\"Height\"))\n\n만약 MACCE_date, Death_date와 같이 _date로 끝나는 변수들을 선택하고 싶다면 end_with 함수를 이용하면 된다.\n\n## a[, grep(\"_date\", names(a))]\na %&gt;% select(ends_with(\"date\"))\n\n\n\n\n  \n\n\n\n이외에 select와 함께 쓸 수 있는 유용한 함수들을 정리하면 아래와 같다.\n\nstart_with(\"abc\"): ’abc’로 시작하는 이름\nend_with(\"xyz\"): ’xyz’로 끝나는 이름\ncontains(\"ijk\"): ’ijk’를 포함하는 이름\none_of(c(\"a\", \"b\", \"c\")): 변수명이 a, b, c 중 하나\nmatches(\"(.)\\\\1\"): 정규표현식 조건\nnum_range(\"x\", 1:3): x1, x2, x3\n\nmutate: 변수 생성\nmutate는 새로운 변수를 만드는 함수이다. Age와 BMI 변수에서 고령과 비만을 뜻하는 Old, Overweight 변수를 만들어 보자.\n\n## a$old &lt;- as.integer(a$Age &gt;= 65); a$overweight &lt;- as.integer(a$BMI &gt;= 27)\na %&gt;% mutate(Old = as.integer(Age &gt;= 65),\n             Overweight = as.integer(BMI &gt;= 27)\n             )\n\n\n\n\n  \n\n\n\n새로운 변수만 보여주려면 mutate 대신 transmute를 사용한다.\n\na %&gt;% transmute(Old = as.integer(Age &gt;= 65),\n             Overweight = as.integer(BMI &gt;= 27)\n             )\n\n\n\n\n  \n\n\n\n\ngroup_by와 summarize\n\ngroup_by, summarize를 이용하여 원하는대로 그룹을 나누고 각 그룹의 요약통계량을 간단히 구할 수 있다. 기본 R에서는 aggregate함수가 같은 기능을 수행한다.\n\na %&gt;% \n  group_by(Sex, Smoking) %&gt;% \n  summarize(count = n(),\n            meanBMI = mean(BMI),\n            sdBMI = sd(BMI))\n\n\n\n\n  \n\n\n\ngroup_by에 \"Age\" 같은 문자열을 넣으려면 언더바(_)가 붙은 group_by_ 함수를 이용해야 한다.\n모든 변수에 같은 요약방법을 적용하려면 summarize_all 함수를 사용한다. 아래는 50세 이상을 대상으로 모든 변수에 그룹별 평균을 적용한 예시이다.\n\na %&gt;% \n  filter(Age &gt;= 50) %&gt;% \n  group_by(Sex, Smoking) %&gt;% \n  summarize_all(mean) \n\n\n\n\n  \n\n\n\n평균값을 계산할 수 없는 범주형 변수는 NA를, 그 외 변수들은 평균값을 보여줌을 확인할 수 있다. 여러 요약값을 동시에 보여주려면 funs 명령어로 여러 함수를 같이 적어주면 된다.\n\na %&gt;% \n  filter(Age &gt;= 50) %&gt;% \n  select(-Patient_ID, -STRESS_EXIST) %&gt;%       ## Except categorical variable\n  group_by(Sex, Smoking) %&gt;% \n  summarize_all(funs(mean = mean, sd = sd)) \n\n\n\n\n  \n\n\n\n\n%&gt;%와 기본함수만으로 똑같이 구현하기.\n마지막에 수행했던 작업을 %&gt;%와 R 기본함수만으로 구현하면서 본 단원을 마무리하겠다. filter 대신 subset, select 대신 [], group_by와 summarize_all 대신에 aggregate를 이용하면 된다.\n\na %&gt;% \n  subset(Age &gt;= 50) %&gt;%\n  .[, -c(1, 14)] %&gt;% \n  aggregate(list(Sex = .$Sex, Smoking = .$Smoking), \n            FUN = function(x){c(mean = mean(x), sd = sd(x))})\n\n위와 같이 기본 함수로도 %&gt;% 연산자를 이용하여 그럴듯하게 코드를 작성할 수 있으나, dplyr 와 비교했을 때 아무래도 가독성이 떨어진다는 점에서 dplyr가 유용한 패키지임을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "반복문 처리: purrr\n",
    "text": "반복문 처리: purrr\n\n본 내용에서는 for문이나 멀티코어의 사용은 언급하지 않는다. 해당 내용은 과거 강의5를 참고하기 바란다.\nR에서 반복문을 처리하는데 가장 많이 이용되는 함수는 lapply(또는 sapply)일 것이다. purrr의 대표 함수인 map은 이 lapply를 tidyverse 철학으로 구현한 것이다. 이제부터 차이점을 알아보자.\nmap\n데이터의 모든 변수들의 형태를 살펴보는 lapply 구문은 아래와 같다.\n\nlapply(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\n이것을 map으로 구현하면 lapply와 정확히 같은 형태가 된다.\n\nlibrary(purrr)\nmap(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\nmap은 list 형태로 결과를 반환하며 특정 형태를 지정하려면 아래와 같은 함수들을 이용한다.\n\nmap: 리스트\nmap_lgl: 논리값(T, F)\nmap_int: 정수\nmap_dbl: 실수\nmap_chr: 문자열\nmap_dfr: rbind된 data.frame\nmap_dfc: cbind된 data.frame\n\n앞의 예를 map_chr을 이용해 다시 실행하자.\n\nmap_chr(a, class)\n\n         Sex          Age       Height       Weight          BMI           DM \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \nSTRESS_EXIST Number_stent \n \"character\"    \"numeric\" \n\n\n이것은 sapply함수의 tidyverse 버전으로 기억하면 좋다. 아래의 명령어들은 모두 같은 결과를 나타낸다.\n\nmap_chr(a, class)\na %&gt;% map_chr(class)\nsapply(a, class)\nunlist(map(a, class))\nunlist(lapply(a, class))\n\n각 변수에서 첫 번째 값을 뽑는 아래의 코드들도 sapply와 map_*의 차이는 없다.\n\na %&gt;% sapply(function(x){x[1]})\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %&gt;% sapply(`[`, 1)\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %&gt;% map_chr(`[`, 1)\n\n          Sex           Age        Height        Weight           BMI \n          \"M\"   \"52.000000\"  \"160.000000\"   \"63.000000\"   \"24.609375\" \n           DM           HTN       Smoking         MACCE         Death \n   \"0.000000\"           \"1\"    \"1.000000\"    \"0.000000\"    \"0.000000\" \n   MACCE_date    Death_date  STRESS_EXIST  Number_stent \n\"1056.000000\" \"1056.000000\"     \"No test\"    \"3.000000\" \n\n\nclass나 mean, 그리고 `[` 등 간단한 함수들은 lapply를 쓰나 map을 쓰나 차이가 없다. map의 진가는 반복할 함수가 복잡할 때 드러난다. 성별로 같은 회귀분석을 반복하는 코드를 예로 들어 보자. 먼저 lapply를 이용한 코드는 아래와 같다.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  lapply(function(x){lm(Death ~ Age, data = x, family = binomial)})\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\n위 예시에서 알 수 있듯이 lapply에서 복잡한 함수를 반복하려면 function(x) 문이 꼭 필요하고 가독성을 저해하는 원인이 된다. 그러나 map에서는 ~로 간단히 function(x)를 대체할 수 있다. 아래 코드를 살펴보자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map(~lm(Death ~ Age, data = ., family = binomial))\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\nmap함수를 이용하여 function(x)를 ~로, 성별 데이터에 해당하는 x를 .으로 바꾸니 훨씬 읽기가 쉬워졌다.\n이번엔 같은 회귀분석을 수행한 후 Age의 p-value만 뽑는다고 하자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  sapply(function(x){\n    lm(Death ~ Age, data = x, family = binomial) %&gt;% \n      summary %&gt;% \n      .$coefficients %&gt;% \n      .[8]     ## p-value: 8th value\n    }) \n\n[1] 9.016998e-01 2.094342e-07\n\n\n%&gt;% 연산자를 이용해 나름대로 읽기 쉬운 코드가 되었다. 이것을 map_dbl로 다시 표현하면 아래와 같다.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map_dbl(~lm(Death ~ Age, data = ., family = binomial) %&gt;% \n        summary %&gt;% \n        .$coefficients %&gt;% \n        .[8]\n  )\n\n[1] 9.016998e-01 2.094342e-07\n\n\nfunction(x) 가 없어 더 잘 보이기는 하나 고작 이 정도의 장점이라면 map을 쓸 필요가 없을 것 같다. map을 좀 더 적극적으로 사용해 보자.\n\na %&gt;% \n  group_split(Sex) %&gt;% \n  map(~lm(Death ~ Age, data = ., family = binomial)) %&gt;% \n  map(summary) %&gt;% \n  map(\"coefficients\") %&gt;% \n  map_dbl(8)\n\n[1] 9.016998e-01 2.094342e-07\n\n\n기존 코드는 .$coefficient와 .[8] 같은 부분이 거슬렸는데, map(\"coefficients\"), map_dbl(8)으로 바꾸니 보기에 훨씬 깔끔하다. 참고로 map(\"coefficients\") 은 map(`[[`, \"coefficients\")의, map_dbl(8)은 map_dbl(`[`, 8)의 축약형 표현이다. 사실 아까 다루었던 첫 번째 값 뽑기의 경우도 아래와 같이 더 간단하게 쓸 수 있다.\n\na %&gt;% map_chr(`[`, 1)\na %&gt;% map_chr(1) \n\nmap을 통해 함수의 모든 중간과정을 따로따로 구현한 것도 장점이다. 에러가 발생했을 때 드래그로 중간과정까지만 실행할 수 있어 함수의 어느 부분에서 에러가 발생했는지를 쉽게 알아낼 수 있다.\n\nmap2, pmap: 입력 변수 2개 이상\n2개 이상의 입력값에 대해 반복문을 수행하는 함수로는 mapply가 있다. 이것의 tidyverse 버전이 map2와 pmap인데 전자는 2개의 조건에, 후자는 리스트 형태로 입력값의 갯수에 상관없이 반복문을 구현할 수 있다. 본 강의에서는 간단한 예만 다루어 보겠다. 먼저 mapply를 이용해서 여러 입력값의 합을 구하는 코드를 살펴보자.\n\nmapply(sum, 1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %&gt;% mapply(1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %&gt;% mapply(1:5, 1:5, 1:5)\n\n[1]  3  6  9 12 15\n\n\nmapply는 첫번째 인수에 함수를, 그 다음부터는 입력값들을 2개, 3개… 계속 받을 수 있다. 반면 map2와 pmap은 입력값을 먼저 받는데, 이 때문에 pmap에서는 입력값들을 리스트 형태로 받는다.\n\nmap2(1:5, 1:5, sum)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\npmap(list(1:5, 1:5, 1:5), sum)\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 6\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 12\n\n[[5]]\n[1] 15\n\n\n리턴 형태를 지정하려면 map과 마찬가지로 map2_*나 pmap_* 꼴의 함수를 이용하면 되며 아래의 코드들은 같은 결과를 나타낸다.\n\npmap_int(list(1:5, 1:5, 1:5), sum)\n\n[1]  3  6  9 12 15\n\nlist(1:5, 1:5, 1:5) %&gt;% pmap_int(sum)\n\n[1]  3  6  9 12 15\n\n\n마지막으로 paste함수로 문자열을 합치는 예를 살펴보겠다. 먼저 map2_chr로 두 문자열을 합쳐보자.\n\nname &lt;- c(\"Alice\", \"Bob\")\nplace &lt;- c(\"LA\", \"New york\")\nmap2_chr(name, place, ~paste(.x, \"was born at\", .y))\n\n[1] \"Alice was born at LA\"     \"Bob was born at New york\"\n\n\n첫 번째 입력값은 함수에서 .x로 두 번째 입력값은 .y로 표현할 수 있다. pmap 함수를 이용할 때는 ..1, ..2, ..3으로 바꿔 표현하면 된다.\n\nlife &lt;- c(\"born\", \"died\")\nlist(name, life, place) %&gt;% pmap_chr(~paste(..1, \"was\", ..2, \"at\", ..3))\n\n[1] \"Alice was born at LA\"     \"Bob was died at New york\""
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "href": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "마치며",
    "text": "마치며\n지금까지 tidyverse 생태계에서 몇 가지 패키지를 이용해 데이터를 다루는 방법을 살펴보았다. 앞서 말했듯이 이 생태계에서 가장 중요한 것은 %&gt;% 연산자를 이용하여 의식의 흐름대로 코딩을 수행하는 것이며, 이후 나머지 내용을 하나씩 적용해나가면 어느 순간 tidyverse 없이 살 수 없는(?) 자신을 발견하게 될 것이다. 본 글에서 다루지 않은 내용인 long, short 포맷을 다루는 tidyr, 문자열을 다루는 stringr, factor를 다루는 forcats, 날짜를 다루는 lubridate 그리고 모델을 다루는 modelr과 broom 등은 R for Data Science[^8]와 Rstudio cheetsheet[^9]를 참고하기 바란다. 다음번에는 빠른 속도가 장점인 data.table를 다시 한번 정리해 볼 생각이다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#footnotes",
    "href": "posts/2019-01-03-rdatamanagement/index.html#footnotes",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://jinseob2kim.github.io/rbasic.html↩︎\nhttps://jinseob2kim.github.io/radv1.html↩︎\nhttps://csgillespie.github.io/efficientR/5-3-importing-data.html↩︎\nhttps://www.jumpingrivers.com/blog/the-trouble-with-tibbles/↩︎\nhttps://jinseob2kim.github.io/radv1.html#faster_for-loop↩︎"
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html",
    "href": "posts/2021-07-11-kstartup/index.html",
    "title": "창업지원사업 도전기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 7월 Shinykorea 밋업에 참석, 창업지원사업 도전경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#요약",
    "href": "posts/2021-07-11-kstartup/index.html#요약",
    "title": "창업지원사업 도전기",
    "section": "요약",
    "text": "요약\n4전 5기만에 창업지원사업(비대면스타트업육성사업) 선정(1.4억)\n\n비대면 의료 분야: 블록체인 기반 의료데이터 입력, 관리, 분석 플랫폼\n\n18년초 창업전, 지도교수, 대학원동료와 함께 창업선도대학 선정(6천만)\n\n서울대학교: 블록체인 기반 유전체 빅데이터 플랫폼\n\n심평원 공모전 선정: 맞춤형 의학연구웹\n창업 후 주요사업 광탈, 작은 사업 선정\n\n20년 초기창업패키지, 추경(비대면), 추경2차, 21년 초기창업패키지 4연속 서류탈락\n비대면바우처(400만원), 클라우드 지원사업(720만원) 선정\n벤처기업인증: 혁신성장유형\n\n21년 공개소프트웨어 기반 창업기업 선정, 선릉역 오피스 6개월 지원."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#slide",
    "href": "posts/2021-07-11-kstartup/index.html#slide",
    "title": "창업지원사업 도전기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/kstartup 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html",
    "href": "posts/2019-05-13-rmedicalresearch/index.html",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에 참석, 의학연구를 지원하면서 다양하게 R을 활용했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공하는 것을 업으로 삼고 있다.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포한다.\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 통계지원을 맡고 있다.\n심평원/보험공단 빅데이터 연구도 개별적으로 지원중."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/CRScube 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html",
    "href": "posts/2021-10-01-notionoopy/index.html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "",
    "text": "기존 Hugo로 작성하고 Netlify로 배포하던 홈페이지의 한계점로, 새롭게 홈페이지를 구축하기로 결정.\n당사 홈페이지가 정적 웹이고, 최근 트렌드를 고려하여, Notion으로 홈페이지를 구축하기로 결정.\nNotion을 홈페이지화 해 주는 배포 서비스를 oopy로 결정.\n구축 후 oopy를 통해 커스텀 URL, 추가 JS, CSS 등 설정."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "href": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Jekyll 프레임워크와 GitHub Pages",
    "text": "Jekyll 프레임워크와 GitHub Pages\n정적 웹 프레임워크에서 가장 유명한 두 개의 프레임워크 중 하나는 Jekyll입니다. 당사는 소스코드를 GitHub로 관리하기에, GitHub Pages로 바로 배포가 가능한 Jekyll도 고려하였으나, 근본적으로 Hugo와 큰 차이가 없어 기존의 문제를 다 해결할 수는 없을 것이라고 생각해 포기하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "href": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "새롭게 웹 작성",
    "text": "새롭게 웹 작성\n새롭게 홈페이지을 작성하고 당사 서버의 남은 자원으로 호스팅 하는 방법도 있었습니다. 다만 빠른 제작이 필요했던 탓에 단기간에 만들기 힘든 부분이 있어 배제하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "href": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포",
    "text": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포\n최근 많은 기업체에서 채용 등 일부 페이지를 Notion으로 구축하는 사례가 늘고 있습니다. 예를 들어 왓챠나 클라썸같은 유명 스타트업이 있습니다. 다만 이들 또한 주 홈페이지까지 Notion을 사용하지는 않았습니다. 각 회사에서 서비스하는 제품이 정적 웹으로는 해결되지 않았기 때문입니다. 그러나 당사의 경우 주 홈페이지는 정적 웹으로 충분하고, 실제 서비스는 Zarathu App을 사용하기 때문에 큰 문제가 되지 않습니다.\n또한 노션의 기본 기능 뿐 아니라, oopy에서 간단한 SEO, 스타일(테마), HTML(CSS, JS포함), 클린 URL, Google Analytics나 channel.io 등 플러그인까지 지원하기에 자유도와 편리함이 적절하다 판단했습니다. oopy와 비슷한 서비스 - 대표적으로 Super - 도 존재하나, 지원의 편리성과 가격을 고려하여 Notion과 oopy를 사용하기로 최종 결정하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML",
    "text": "커스텀 HTML\n저희가 Notion과 oopy조합을 사용하게 된 가장 큰 이유는 커스텀 HTML(이하 CSS, JS 포함)입니다. 당사 홈페이지 하단의 ‘연구지원 신청하기’ 버튼은 oopy의 페이지별 HTML기능을 사용한 것입니다. oopy는 모든 페이지에 커스텀 HTML을 적용시킬 수도 있고, 페이지마다 개별적으로 적용할 수도 있습니다.  차라투는 위 사진처럼 media쿼리를 사용해 사용자의 환경에 따라 컨텐츠를 다르게 적용하기도 하였으며,\n\n\n\n전역적으로 적용하는 HTML\n\n\n위 사진처럼 모든 페이지 하단에 연구지원 신청하기 버튼을 만들기도 하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "href": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "쉽게 적용하는 플러그인",
    "text": "쉽게 적용하는 플러그인\n\n\n\n플러그인\n\n\n대부분 JS를 통해 직접 설치하는 플러그인(Google Analytics나 channel.io)또한 위 사진처럼 쉽게 추가가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "href": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "클린 URL",
    "text": "클린 URL\n 노션 주소를 공유할 때에는 위 사진의 복잡한 부분처럼 uuid형식으로 된 값을 사용합니다. 이는 효율적인 방법일 수 있으나, 심미적인 관점에서는 그렇지 않습니다. 또 링크를 누군가에게 공유할 때에도 링크만을 보고 대략적 내용을 유추할 수 없어 비 효율적인 면이 있습니다. oopy는 이러한 Notion의 URL을 쉽게 바꿀 수 있게 해 줍니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML의 편의성 부재",
    "text": "커스텀 HTML의 편의성 부재\nMicrosoft의 Visual Studio Code(VSCode)나 JetBrains사의 WebStorm같은 전문 프로그램은 물론이고, 최근에는 가벼원 편집기들 또한 자동 완성이나 괄호 자동 닫음 기능을 지원합니다. oopy의 커스텀 html 편집의 경우 아래의 사진과 같은 창에서 수행하게 되는데, 외부 편집기를 이용할 수 없어 Intellisence같은 기능을 사용할 수 없는 등의 단점은 있습니다. 다행히, 기초적인 오류는 경고 표시로 탐지가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "href": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "부족한 SEO",
    "text": "부족한 SEO\n우피는 자체적으로 SEO를 지원한다고 밝혔으나, 실제로는 Favicon과 og:image 설정, 메타태그 설정 등만이 가능하고, robots.txt는 일괄 변경만이 가능해 보여 해당 기능들의 추가가 필요합니다."
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html",
    "href": "posts/2022-01-25-doctorskku2022/index.html",
    "title": "창업 경험 공유",
    "section": "",
    "text": "김진섭 대표는 1월 28일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정으로, 발표 슬라이드를 미리 공유합니다. 자세한 내용은 메디게이트뉴스 http://medigatenews.com/news/3852895813 참고 부탁드립니다."
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "href": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "title": "창업 경험 공유",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n데이터 입력/관리/분석 통합서비스\n공동연구에 코인 인센티브 = 공동연구플랫폼\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n연구지원전문가 새로운 직업 창출"
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "href": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "title": "창업 경험 공유",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/doctorskku2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html",
    "href": "posts/2023-02-05-likert/index.html",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "사용자로 부터 수집되는 데이터중 많은 부분은 설문조사를 통해 얻을 수 있으며, 제시된 문장에 얼마나 동의하는지의 단계를 나타내는 표현하는 방법으로 리커트 척도 라는 방법이 쓰이기도 합니다.\n하나의 예시를 들면 다음과 같습니다.\n\n\n\n\n  \n  매우 그렇다\n  \n\n  \n  그런 편이다\n  \n  \n  \n  보통이다\n  \n  \n  \n  그렇지 않다\n  \n  \n  \n  전혀 그렇지 않다\n  \n\n\n이 설문을 10명에게 진행했고, 그 결과가 다음 테이블과 같다고 가정해보겠습니다.\n\n\nLoading required package: xtable\n\n\n\n\n\n\n이렇게 순서가 있는, 여러 카테고리의 데이터를 어떻게 시각화 할 수 있을까요? (Likert 외에도 NPS도 해당합니다.)\n\n제일 먼저 해볼 수 있는 것은, 문항별로 개수를 보여주는 것입니다.\n\n\n\n\n\n\n\n\n이 방법 자체는 나쁘지는 않지만, 1개가 아닌 여러개 문항의 결과를 보여줘야 한다면 아쉬운 문제점이 생깁니다.\n\n다음과 같이 데이터를 세줄 더 추가해보겠습니다.\n\n\n\n\n\n\n이 여러개의 데이터를 표기 하는 방법은 문항을 하나의 축 (x)에, 그리고 문항의 결과들을 나머지 축(y)을 활용하여 그려야 하기 때문에 “stacked bar”를 활용하는 방법이 있습니다.\n\nImage from The Data Visualisation Catalogue\n\n\n\n\n\n\n\n\n문항별로 응답자 수는 같기 때문에 (NA는 고려하지 않습니다) 전부 높이가 동일한 결과가 나오게 됩니다.\n그런데 주의해야할 점으로는, 당연하게도 문항에 따라 분포가 다를 수 있습니다. “한쪽으로 몰리는” 답이 나오는 경우가 있을 수 있다는 의미입니다.\n위의 그림에서 보통이다는 동일하게 각각 1개씩 답변이 있음에도 불구하고 도움이됨의 위치는 높게 되어있어 시각적 비교에 혼선을 줄 수 있습니다.\n이를 위해서 아래 이미지처럼 Neutral(회색)의 위치를 고정시키고, 이를 기준으로 위 아래로 Positive / Negative를 붙여 표현하는 방법을 고려할 수 있습니다.\n\nImage from daydreamingnumbers\n이러한 차트의 정확한 이름은 모르겠지만, 이 글에서는 편의상 Likert chart라 표현하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#barplot",
    "href": "posts/2023-02-05-likert/index.html#barplot",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "제일 먼저 해볼 수 있는 것은, 문항별로 개수를 보여주는 것입니다.\n\n\n\n\n\n\n\n\n이 방법 자체는 나쁘지는 않지만, 1개가 아닌 여러개 문항의 결과를 보여줘야 한다면 아쉬운 문제점이 생깁니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#stacked-bar-plot",
    "href": "posts/2023-02-05-likert/index.html#stacked-bar-plot",
    "title": "likert 패키지 소개",
    "section": "",
    "text": "다음과 같이 데이터를 세줄 더 추가해보겠습니다.\n\n\n\n\n\n\n이 여러개의 데이터를 표기 하는 방법은 문항을 하나의 축 (x)에, 그리고 문항의 결과들을 나머지 축(y)을 활용하여 그려야 하기 때문에 “stacked bar”를 활용하는 방법이 있습니다.\n\nImage from The Data Visualisation Catalogue\n\n\n\n\n\n\n\n\n문항별로 응답자 수는 같기 때문에 (NA는 고려하지 않습니다) 전부 높이가 동일한 결과가 나오게 됩니다.\n그런데 주의해야할 점으로는, 당연하게도 문항에 따라 분포가 다를 수 있습니다. “한쪽으로 몰리는” 답이 나오는 경우가 있을 수 있다는 의미입니다.\n위의 그림에서 보통이다는 동일하게 각각 1개씩 답변이 있음에도 불구하고 도움이됨의 위치는 높게 되어있어 시각적 비교에 혼선을 줄 수 있습니다.\n이를 위해서 아래 이미지처럼 Neutral(회색)의 위치를 고정시키고, 이를 기준으로 위 아래로 Positive / Negative를 붙여 표현하는 방법을 고려할 수 있습니다.\n\nImage from daydreamingnumbers\n이러한 차트의 정확한 이름은 모르겠지만, 이 글에서는 편의상 Likert chart라 표현하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#예시-데이터",
    "href": "posts/2023-02-05-likert/index.html#예시-데이터",
    "title": "likert 패키지 소개",
    "section": "예시 데이터",
    "text": "예시 데이터\n시각화 패키지의 특성상 사용자의 데이터의 형태와 패키지가 요구하는 형태가 다를 수 있어, likert 패키지에서는 활용할 수 있는 예시데이터를 제공합니다.\n\n# install.package('likert')\nlibrary(likert)\ndata(pisaitems)\nhead(pisaitems[,1:3])\n\n         CNT           ST24Q01           ST24Q02\n68038 Canada          Disagree    Strongly agree\n68039 Canada             Agree Strongly disagree\n68040 Canada    Strongly agree Strongly disagree\n68041 Canada          Disagree          Disagree\n68042 Canada Strongly disagree          Disagree\n68043 Canada             Agree Strongly disagree\n\n# str(pisaitems)\n# View(pisaitems)\n\npisaitems 데이터의 경우, - rownames를 가지고 있으며 - 첫 CNT 이후로는 전부 scale을 갖는 factor의 형태로 이루어져 있습니다. - 개수는 각 column 마다 다르며, NA를 포함하고 있기도 합니다. (ex: ST36Q01)\n이후 이 데이터를 likert 오브젝트로 변환해야 합니다. 다만 모든 데이터를 다 사용하진 않고, CNT 이후의 처음 3개만 사용하겠습니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#likert-print",
    "href": "posts/2023-02-05-likert/index.html#likert-print",
    "title": "likert 패키지 소개",
    "section": "likert: print",
    "text": "likert: print\n\nsummary(pisaitems[, 2:4])\n\n              ST24Q01                   ST24Q02                   ST24Q03     \n Strongly disagree:14947   Strongly disagree:13323   Strongly disagree:13900  \n Disagree         :23515   Disagree         :23811   Disagree         :22072  \n Agree            :20000   Agree            :20935   Agree            :23525  \n Strongly agree   : 7029   Strongly agree   : 7487   Strongly agree   : 5917  \n NA's             : 1199   NA's             : 1134   NA's             : 1276  \n\n100 * 14947 / (14947 + 23515 + 20000 + 7029) # ST24Q01: Strongly disagree \n\n[1] 22.82298\n\nll &lt;- likert(pisaitems[, 2:4])\nprint(ll)\n\n     Item Strongly disagree Disagree    Agree Strongly agree\n1 ST24Q01          22.82298 35.90570 30.53855      10.732772\n2 ST24Q02          20.32308 36.32162 31.93453      11.420770\n3 ST24Q03          21.24927 33.74201 35.96325       9.045464\n\n\n위의 summary와 바로 비교해보면 알 수 있듯, likert 를 출력했을때는 NA를 제외한 각 문항별 factor의 백분율을 출력합니다."
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#likert-summary",
    "href": "posts/2023-02-05-likert/index.html#likert-summary",
    "title": "likert 패키지 소개",
    "section": "likert: summary",
    "text": "likert: summary\n한편 summary 함수를 사용하면 low와 high를 표기해주는데 이는 각각 Neutral보다 낮은 / 높은 값의 백분율 합을 표기합니다. (예시의 경우 Strongly disagree + Disagree, Strongly agree + agree)\n\nsummary(ll)\n\n     Item      low neutral     high     mean        sd\n3 ST24Q03 54.99129       0 45.00871 2.328049 0.9090326\n2 ST24Q02 56.64470       0 43.35530 2.344530 0.9277495\n1 ST24Q01 58.72868       0 41.27132 2.291811 0.9369023\n\n22.82298 + 35.90570 # ST24Q01's Low\n\n[1] 58.72868\n\n\n\n물론 문항이 4-5개가 아닌, 여러개도 가능하기 때문에 Neutral은 가운데를 기준으로 설정하지만, 사용자가 center를 통해 지정할 수도 있습니다.\n\nmean과 sd는 크게 신경쓰지 않아도 괜찮습니다.\n\n\nsummary(ll, center = 1.5)\n\n     Item      low neutral     high     mean        sd\n2 ST24Q02 20.32308       0 79.67692 2.344530 0.9277495\n3 ST24Q03 21.24927       0 78.75073 2.328049 0.9090326\n1 ST24Q01 22.82298       0 77.17702 2.291811 0.9369023"
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#liert-plot",
    "href": "posts/2023-02-05-likert/index.html#liert-plot",
    "title": "likert 패키지 소개",
    "section": "liert: Plot",
    "text": "liert: Plot\nlikert object는 그냥 plot에 넣는 것으로도 결과를 바로 만들어 낼 수 있습니다.\n\nplot(ll)\n\n\n\n\n\n\n\n기본 center는 가운데, 이 경우 2와 3사이인 2.5를 기준으로 그려지고, 문항별 High, Low를 차트 양옆에 추가로 표기하게 됩니다.\ninformation\n차트에서의 factor 별 %, high, low 는 plot.percents, plot.percent.low, plot.percent.high를 통해 설정할 수 있습니다.\n\nplot(ll, plot.percents = TRUE, plot.percent.low = FALSE, plot.percent.high = FALSE)\n\n\n\n\n\n\n\ncolors\n기본 색상외에 colors를 사용하여 색상을 커스텀 할 수 있습니다. 단, 개수는 factor의 수와 동일해야합니다.\n\nplot(ll, colors=c('orange','darkorange','darkblue','blue'))\n\n\n\n\n\n\n\ncenter\n만약 위의 summary 처럼 Neutral을 바꾸고 싶다면, center를 통해 값을 지정하여 그릴 수 있습니다.\n여기서 center가 정수라면 해당하는 factor의 색상이 자동으로 회색으로 바뀌는 것을 확인 할 수 있습니다.\n\nplot(ll, center = 2)\n\n\n\n\n\n\n\ninclude.center\n차트에서 Neutral을 제외하고 그리고 싶다면 include.center = FALSE를 사용하여 제거할 수 있습니다.\n\nplot(ll, center = 2, include.center = FALSE)\n\n\n\n\n\n\n\ncentered\n단순히 stacked bar chart를 그리고 싶다면 centered = FALSE를 사용하면 됩니다.\n\nplot(ll, centered = FALSE)\n\n\n\n\n\n\n\nNA info (histogram)\n문항에서 결측치 (NA) 정보를 같이 표현하고 싶은 경우, include.histogram = TRUE를 설정하여 그릴 수 있습니다.\n\nsummary(pisaitems[,2])\n\nStrongly disagree          Disagree             Agree    Strongly agree \n            14947             23515             20000              7029 \n             NA's \n             1199 \n\n100 * 1199 / 66690 # 1.79% NA\n\n[1] 1.797871\n\nplot(ll, include.histogram = TRUE)\n\n\n\n\n\n\n\ndensity plot\nlikert 오브젝트는 type = 'density'를 사용하여 bar chart의 형태가 아닌 density plot으로도 표현할 수 있습니다.\n\nplot(ll, type = 'density')\n\n\n\n\n\n\n\n여러개의 density plot을 facet을 사용하여 하나로 겹치게 보여줄 수 있으며, 추가로 legend를 지정하는 예시입니다. (구분은 잘 안가지만, fill의 색상은 미세하게 다르게 표현되어 있습니다)\n\nplot(ll, type='density', facet=FALSE) + \n  guides(\n    color = guide_legend(title=\"Legend with Color\"),\n    fill = guide_legend(title=\"Legend with Fill\")\n  )\n\n\n\n\n\n\n\nHeat map\nlikert 오브젝트는 type = 'heat'를 사용해 heatmap으로 도 표현할 수 있습니다.\n\nplot(ll, type='heat')"
  },
  {
    "objectID": "posts/2023-02-05-likert/index.html#group-likert",
    "href": "posts/2023-02-05-likert/index.html#group-likert",
    "title": "likert 패키지 소개",
    "section": "Group Likert",
    "text": "Group Likert\n한편 likert 오브젝트를 만들때, 사용자의 그룹을 설정해준다면 (예시데이터의 CNT와 같이) dplyr의 group by를 사용한 것과 유사한 결과를 낼 수 있습니다.\n사용 가능한 parameter는 위와 동일합니다.\n\nllg &lt;- likert(pisaitems[,2:4], grouping=pisaitems$CNT)\nprint(llg) \n\n          Group    Item Strongly disagree Disagree    Agree Strongly agree\n1        Canada ST24Q01          25.69810 35.12856 24.88383      14.289507\n2        Canada ST24Q02          26.77758 35.18871 24.63608      13.397637\n3        Canada ST24Q03          25.22917 31.68150 33.47062       9.618706\n4        Mexico ST24Q01          21.87500 36.76845 33.45526       7.901293\n5        Mexico ST24Q02          15.26451 36.42523 37.79077      10.519491\n6        Mexico ST24Q03          18.44410 34.78607 37.89150       8.878331\n7 United States ST24Q01          17.16996 33.00426 33.97213      15.853659\n8 United States ST24Q02          29.08282 40.51858 21.03328       9.365325\n9 United States ST24Q03          24.31646 35.13671 32.79038       7.756448\n\n\n\nplot(llg, group.order=c('Mexico', 'Canada', 'United States'))\n\n\n\n\n\n\n\nGroup horizontal Plot\n그룹을 사용하면 차트를 여러개의 그룹에 따라 구분지어 그려야 하는 만큼 ggplot의 facet처럼 차트를 나누어야 하는데, 이 과정에서 위아래가 아닌, 좌우로도 설정 할 수 있습니다.\n\nplot(llg, panel.arrange='h', wrap=20)"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html",
    "href": "posts/2024-03-04-shinylive/index.html",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "href": "posts/2024-03-04-shinylive/index.html#review-wasm",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "",
    "text": "이전에 작성한 글에서 별도의 서버 없이 작동하는 정적 페이지에서 어떻게 shiny application을 사용할 수 있는 방법을 소개한 적 있다.\n이 방법의 핵심은 wasm이라는 기술로, 웹 브라우저에서 사용할 수 있게 변환 된 R과 Shiny 관련 라이브러리, 파일들을 불러오고 이를 활용하는 방법이었는데, wasm의 가장 큰 문제는 R 개발자들에게도 환경 설정 자체가 어렵다는 것이었다.\n다행히 몇개월 정도의 시간이 지나고 이 환경 설정을 해결해주는 R 패키지가 나왔고, 이를 활용하여 정적 페이지에 shiny application을 추가하는 방법을 소개하고자 한다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive",
    "text": "shinylive\nshinylive는 python 버전과 r 버전이 있으며, 이 글에서는 r 버전을 기준으로 소개한다.\nshinylive는 웹 페이지 생성에 필요한 HTML, Javscript, CSS 등의 요소와 shiny 를 사용하기 위한 wasm 관련 파일들을 생성하는 일을 한다.\nshinylive로 만든 예시는 이 링크에서 확인할 수 있다.\n\nshinylive 설치\nshinylive는 CRAN에 올라가 있기도 하지만 최근 릴리즈 된 버전이 0.1.1인만큼 수시로 업데이트 될 수 있어 github의 최신 버전을 사용하는 것을 권장한다. 추가로 pak는 최근 posit에서 R 패키지를 설치하기 위해 권장하는 R 패키지로, 기존의 install.packages(), remotes::install_github() 등의 함수를 대체할 수 있다.\n\n# install.packages(\"pak\")\npak::pak(\"posit-dev/r-shinylive\")\n\n\n\n\n\n\n\nVersion\n\n\n\n통상적으로 1.0 이전의 버전은 아직 개발 중인 버전이라고 생각해도 좋다.\n\n\nshinylive 사용방법\nshinylive는 기존에 만든 shiny application에 wasm을 추가하는 것으로 생각할 수 있다. 즉, 먼저 shiny application을 만들어야 한다.\n예시 실습을 위해 shiny에서 기본으로 제공하는 코드를 사용한다.(이는 Rstudio 콘솔에서 shiny::runExample(\"01_hello\")를 입력해서 확인할 수도 있다.)\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n      breaks = bins, col = \"#75AADB\", border = \"white\",\n      xlab = \"Waiting time to next eruption (in mins)\",\n      main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n이 코드는 아래 그림과 같이 사용자의 입력에 반응하여 갯수만큼 histogram을 만드는 간단한 shiny application을 만들어 낸다.\n\n이 코드를 shinylive를 사용해 정적인 페이지를 만드는 방법은 2가지가 있는데 하나는 별도의 웹페이지로 만들어내는 것이고, 다른 하나는 이 기술 블로그 같은 quarto 블로그 페이지에 내부 콘텐츠로 심는 것이다.\n먼저 별도의 웹페이지를 만드는 방법은 다음과 같다."
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-via-web-page",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive via Web page",
    "text": "shinylive via Web page\n별도의 정적 웹페이지에서 shiny를 제공하려면, 이전에 설치했던 shinylive 패키지를 사용하여 app.R을 웹페이지로 변환하는 과정이 필요하다.\n내 문서(Documents)의 shinylive라는 폴더를 만들고 이 안에 app.R을 저장했을 때를 기준으로, export 함수의 사용 예시는 다음과 같다.\n\n# library(shinylive)\nshinylive::export('~/Documents/shinylive', '~/Documents/shinylive_out')\n\n이 코드를 실행하면 shinylive와 동일한 위치, 즉 내 문서(Documents)에 shinylive_out이라는 폴더를 새롭게 만들고 그 안에 shinylive 패키지를 사용해 변환된 wasm 버전의 shiny 코드를 생성한다.\n이 shinylive_out 폴더의 내용물을 확인해보면 다음과 같으며 이전 글에서 언급했던 webr, serviceworker 등이 포함되어 있는 것을 확인할 수 있다.\n\n조금 더 구체적으로 export 함수는 현재 R studio를 실행하고 있는 로컬 PC에서 shinylive 패키지의 파일들, 즉 shiny와 관련된 라이브러리 파일들을 out 디렉토리에 추가하는 역할을 한다.\n\n이제 이 폴더의 내용물을 기준으로 github page등을 만들면 shiny 를 제공하는 정적인 웹페이지를 제공할 수 있으며 그 결과는 아래의 명령어를 통해 미리 확인해 볼 수 있다.\n\n\n\n\n\n\ngithub page\n\n\n\n깃허브 페이지 배포를 위해서는 이전에 작성했던  pkgdown의 글 을 참고하길 권장하며, 이를 위해 shinylive_out 대신 docs 폴더로 결과를 내보내길 권장한다.\n\n\n\nhttpuv::runStaticServer(\"~/Documents/shinylive_out\")"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "href": "posts/2024-03-04-shinylive/index.html#shinylive-in-quarto",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "shinylive in Quarto",
    "text": "shinylive in Quarto\nquarto 블로그에 shiny application을 추가하기 위해서는 별도의 extension을 사용해야한다. quarto extension은 quarto의 기능을 확장하는 별도의 패키지로, 기본 R에 R 패키지를 사용해 기능을 추가하는 것과 유사하다.\n먼저 Rstudio의 터미널에서 다음 코드를 실행하여 quarto extenstion을 추가해야 한다.\nquarto add quarto-ext/shinylive\nquarto 블로그에 shiny 를 심기 위해서 별도의 파일을 만들 필요는 없으며, {shinylive-r}이라는 코드 블록을 사용한다. 추가로 index.qmd의 yaml에 shinylive 를 설정해야만 한다.\nfilters: \n  - shinylive\n이후 shinylive-r 블록에 앞서 만든 app.R 의 내용을 작성한다.\n```{shinylive-r}\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n아래는 실제 코드 블록이 어플리케이션으로 실행되는 결과이며 slider를 움직일때 반응하여 histogram을 그리는 것을 확인할 수 있다.\n#| standalone: true\n#| viewerHeight: 800\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        inputId = \"bins\",\n        label = \"Number of bins:\",\n        min = 1,\n        max = 50,\n        value = 30\n      )\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x,\n         breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\"\n    )\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "posts/2024-03-04-shinylive/index.html#정리",
    "href": "posts/2024-03-04-shinylive/index.html#정리",
    "title": "shinylive 를 활용한 quarto 블로그에 shiny 추가 방법",
    "section": "정리",
    "text": "정리\nshinylive는 wasm을 활용해 깃허브 페이지 또는 quarto 블로그 같은 정적 페이지에서 shiny를 실행할 수 있게 하는 기능으로 각각 R 패키지와 quarto extension을 통해 사용할 수 있다.\n물론 아직 나온지 1년이 되지 않은 기능인만큼 모든 기능이 제공되는 것은 아니며 정적 페이지를 사용하는 만큼 별도의 shiny server를 활용하는 것에 비하면 단점이 있기도 하다.\n그러나 shiny 사용법이나 간단한 통계 분석을 소개하고, 이를 웹사이트에서 별도의 R 설치 없이도 바로 실습할 수 있다는 점에서 많이 사용되고 있으며 앞으로도 더 많은 기능이 추가될 것으로 기대된다.\n이 블로그에 사용한 코드는 링크에서 확인할 수 있다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html",
    "href": "posts/2023-09-10-wasm2/index.html",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "",
    "text": "이번 글은 이전 글 에이어 정적 페이지에서 shiny application을 webR로 제공하는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#개요",
    "href": "posts/2023-09-10-wasm2/index.html#개요",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "",
    "text": "이번 글은 이전 글 에이어 정적 페이지에서 shiny application을 webR로 제공하는 방법에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#webr-r과-shiny의-차이",
    "href": "posts/2023-09-10-wasm2/index.html#webr-r과-shiny의-차이",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "webR (R)과 shiny의 차이",
    "text": "webR (R)과 shiny의 차이\nwebR을 사용하기 위해 R과 Shiny의 차이점중 한가지를 짚고 넘어가는 것이 좋습니다.\nR은 한번에 하나의 코드만 실행 가능합니다. 즉, 새로운 코드를 실행하기 위해서는 이전의 연산이 종료된 상태여야 합니다.\n그런데 Shiny는 (app.R이라는) 코드를 실행한 상태에서 사용자로부터 입력을 받고, 이를 통해 새로운 계산을 진행합니다.\n\n\n\n이는 webR과 유사하게 shiny에서 ( 로컬에 ) 별도의 server를 만들고, 브라우저의 입력값과 계산된 출력 값을 서버와 주고 받으면서 ui로 보내주는 과정을 거칩니다.\n그래서 webR과 Shiny를 이어주려면 아래 그림처럼 또 하나의 브라우저 (service worker)를 만든다고 이해하는 것이 편합니다.\n\n\n\nShiny를 사용자의 브라우저에서 실행시키면 얻을 수 있는 장점과 단점은 webR의 장단점과 대체로 유사합니다.\n그러나 service worker라는 추가 개념이 등장했고, 이로 인한 Cross-Origin Isolation이라는 새로운 문제점이 발생합니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#cross-origin-isolation",
    "href": "posts/2023-09-10-wasm2/index.html#cross-origin-isolation",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "Cross-Origin Isolation",
    "text": "Cross-Origin Isolation\nShiny를 정적 페이지에서 제공하기 위해 Cross-Origin Isolation의 정확한 개념을 이해할 필요는 없지만, 설명을 하면 다음과 같습니다.\nservice worker는 별도의 프로세스를 위해 외부에서 (자체 실행을 위한) 파일을 다운로드 받고 이를 사용합니다.\n그런데 최근의 웹페이지에서는 보안 이슈로 인해 (검증되지 않은) 외부의 파일을 실행하는 것을 정책적으로 막고 있습니다.\n이를 위해 정적 페이지를 제공하는 서비스 (github, netlify, wordpress 등)에서 외부 파일의 실행을 허용하는 설정을 해야합니다.\n차라투 블로그가 사용하는 github page를 위해서는  enable-thread.js 라는 스크립트를 실행해야합니다.\n마찬가지로 Quarto를 활용해 웹페이지를 만들고, Shiny에서 기본으로 제공하는 01_hello 심도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#quarto-페이지의-구성",
    "href": "posts/2023-09-10-wasm2/index.html#quarto-페이지의-구성",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "Quarto 페이지의 구성",
    "text": "Quarto 페이지의 구성\nShiny를 정적페이지로 제공하는 quarto page에는 3가지가 필요합니다.\n\nJavascript 파일:\n\n\nservice worker를 만들기 위한 httpuv-serviceworker.js (from Inspired from Here)\n(github blog 기준) Cross origin isolation을 풀기 위한 enable-threads.js\n\n\nButton & iframe: shiny는 webR에 비해 조금 더 로딩이 필요하기 때문에 진행 상태를 보여줄 버튼과, shiny app을 심을 iframe이 필요합니다.\nJavascript 파일: webR을 준비하고, iframe에 Shiny를 심는 loadshiny.js\n\n&lt;!-- 1. scripts --&gt;\n&lt;script src='httpuv-serviceworker.js'&gt;&lt;/script&gt;\n&lt;script src='enable-threads.js'&gt;&lt;/script&gt;\n\n&lt;!-- 2. button & iframe --&gt;\n&lt;button class=\"btn btn-success btn-sm\" type=\"button\" style=\"background-color: dodgerblue\" id=\"statusButton\"&gt;\n  &lt;i class=\"fas fa-spinner fa-spin\"&gt;&lt;/i&gt;Loading webR...\n&lt;/button&gt;\n&lt;div id=\"iframeContainer\"&gt;&lt;/div&gt;\n\n&lt;!-- 3. webR & shiny--&gt;\n&lt;script type=\"module\" src='loadshiny.js'&gt;&lt;/script&gt;\n추가적으로 loadshiny.js의 두 부분을 커스텀 해야합니다. (TODO로 표기)\n\n임베드할 shiny app을 담고 있는 url 코드 (app.R): 이 글의 경우 차라투 블로그의 github에 (공개 된) 있는 코드를 사용합니다.\nservice worker를 적용할 범위: httpuv-serviceworker.js를 등록하고, index.html을 담고 있는 디렉토리를 scope에 저장합니다.\n\n마지막으로 디렉토리 구조는 다음과 같습니다.\n\n2023-09-10-wasm2\n\nenable-threads.js\nindex.qmd\nindex.html\nloadshiny.js\nhttpuv-serviceworker.js"
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#실제-webr-shiny-결과",
    "href": "posts/2023-09-10-wasm2/index.html#실제-webr-shiny-결과",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "실제 webR + shiny 결과",
    "text": "실제 webR + shiny 결과\n\n\n\n\n\n  Loading webR..."
  },
  {
    "objectID": "posts/2023-09-10-wasm2/index.html#정리",
    "href": "posts/2023-09-10-wasm2/index.html#정리",
    "title": "web assembly를 이용하여 웹페이지에서 Shiny App 활용하기",
    "section": "정리",
    "text": "정리\n단순히 webR을 심는 것보다 shiny를 심는 것은 3가지 이유에서 조금 더 복잡합니다.\n\nservice worker 의 구성 및 연결\nshiny app의 실행을 위한 shiny 패키지를 webR에 설치\n실제 정적 페이지에 배포하기 전까지 로컬 PC 에서는 shiny 결과를 확인할 수 없음\n\n또한 이전의 webR과 마찬가지로 단순한 shiny app만 webR로 구현이 가능합니다.\n가벼운 (특히 shiny 교육) 용도로는 shiny app을 별도의 서버를 구성하지 않고도 정적 페이지로 사용자에게 제공할 수 있다는 점은 꽤 흥미롭고, (수초의 시간이 더 필요하긴 하지만) 여전히 개선할 수 있는 부분이 존재합니다.\n다만 R과 quarto를 넘어서 javascript를 포함한 웹 개발 지식과 service worker 구성을 위한 네트워크와 인프라 구성에 대한 경험이 없다면 많은 시행착오가 필요하기도 합니다.\n이 webR에 대해 appsilon은 초창기 기술인 만큼 아쉬운 점도 있지만 동적 페이지에서 제공되는 shiny app을 대체할 기술이라기보단, 상호간에 보조할 수 있는 방법이라고 평가했습니다.\n\n이전 글과 이번 글을 통해서 webR과 shiny를 정적페이지로 사용자에게 제공하는 방법을 알아보았습니다.\n배경 지식이 없다면 한번에 이해하긴 어렵고, 이렇게 사용할 수 도 있다는 것만 기억하셔도 충분합니다.\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html",
    "href": "posts/2023-09-09-wasm/index.html",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "",
    "text": "이번 글에서는 R 패키지 webR를 사용하여 별도의 서버 기능을 제공하지 않는 정적 웹페이지(이 블로그 같은!) 에서 R을 사용할 수 있게 하는 과정에 대해 소개합니다.\n단 이 글에서는 wasm에 대한 이론적 배경 내용보다는 wasm을 활용하는 방법을 위주로 소개합니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#개요",
    "href": "posts/2023-09-09-wasm/index.html#개요",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "",
    "text": "이번 글에서는 R 패키지 webR를 사용하여 별도의 서버 기능을 제공하지 않는 정적 웹페이지(이 블로그 같은!) 에서 R을 사용할 수 있게 하는 과정에 대해 소개합니다.\n단 이 글에서는 wasm에 대한 이론적 배경 내용보다는 wasm을 활용하는 방법을 위주로 소개합니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#정적-페이지",
    "href": "posts/2023-09-09-wasm/index.html#정적-페이지",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "정적 페이지",
    "text": "정적 페이지\nwebR의 이해를 돕기 위해서 먼저 정적 페이지 (Static Page)를 설명하겠습니다.\n위키피디아의 설명을 인용하면, 정적 페이지는 모든 상황에서 모든 사용자에게 동일한 정보를 표시하는 페이지로, 처음 만들어 놓은 콘텐츠만이 사용자에게 전달되는 페이지(HTML 문서)라고 볼 수 있습니다.\n서버에 저장되어 있는 데이터가 변경되지 않는한, 사용자와의 상호작용이나 요청에 관계 없이 동일한 정보를 보여주기 때문에 회사의 홈페이지나 개인의  이력서 페이지, 기술 문서나 설명서, 블로그 등이 이에 해당됩니다.\n\n\n반대로 동적 페이지는 서버에서 사용자의 입력을 기반으로 추가 연산을 거쳐 결과물을 만들어내는 페이지로, 댓글이나 날씨, 잔여 재고수 , 연산 프로그램 등 정보의 업데이트가 있어야 하는 곳 등 대부분의 웹페이지에 사용됩니다.\n\n\n정적 페이지의 주요 특징 중 하나는 동적 페이지에 비해 배포가 쉽고, 비용이 거의 들지 않는 다는 것입니다.\n가령 동적 페이지의 예시중 하나인 shiny 어플리케이션을 제공하려면 별도의 서버를 준비하고 shiny server를 설치하여 배포 해야하지만 정적 페이지인 블로그의 경우 단순히 markdown을 github 에 올리는 것만으로 배포가 가능합니다.\n(연산 로직이 필요하지 않아 사실상 콘텐츠를 print로 출력하는 것과 큰 차이가 없습니다.)"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#웹과-r",
    "href": "posts/2023-09-09-wasm/index.html#웹과-r",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "웹과 R",
    "text": "웹과 R\n차라투에서는 openstat이라는 사용자의 PC에 R을 설치하거나, 프로그래밍을 하지 않고도 웹에서 R의 기능을 이용할 수 있게 하는 의학통계 앱을 서비스로 제공하고 있습니다.\n\n\n이 서비스는 간단하게 표현하면 다음과 같은 구조로 이루어져 있습니다.\n\n\n즉, openstat은 R의 연산 기능을 웹에서 활용하기 위해 동적 페이지를 구성했기 때문에 다음과 같은 문제점이 발생할 수 있습니다.\n\n사용량이 많아지면 서버가 뻗을 가능성 존재. 특히나 R은 모든 작업들을 메모리에서 하기 때문에 데이터에 따라 사용자 당 GB 단위 이상을 필요로 하기도 합니다.\n서버에 데이터가 왔다갔다하는 과정에서 (네트워크에서의) 보안 문제가 발생할 수도 있습니다.\n모든 연산 결과를 네트워크를 통해 전달해야하기 때문에 사용자의 PC 성능이 아닌, 네트워크 연결 상태와 서버의 PC 성능에 어플리케이션의 퍼포먼스가 영향을 받습니다.\n\n물론 openstat에서 제공하는 (의학 연구용) 분석은, 스케일이나 컴퓨팅 퍼포먼스가 필요하지 않은 간단한 통계 분석 작업이 많기 때문에 별다른 문제가 발생하지는 않습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#wasm-webr",
    "href": "posts/2023-09-09-wasm/index.html#wasm-webr",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "wasm & webR",
    "text": "wasm & webR\nwasm은 Web Assembly의 줄임말로, 웹 브라우저 (크롬)에서 실행할 수 있는 프로그래밍 언어 정도로 이해해도 충분합니다.\n2017년에 처음 등장한 개념으로, 각 프로그래밍 언어를 대상으로 작업되고 있으며 R에서는 webR이라는 이름으로 2022년 1월부터 작업이 진행되고 있습니다.\n이 webR의 정확한 원리는 복잡하지만, 간단하게는 아래 그림과 같이 자바스크립트 코드를 통해 사용자 PC에 백그라운드에서 실행되는 별도의 브라우저(web worker)를 만들어 서버 역할을 하게 한다 정도로 생각하셔도 충분합니다.\n\n\nwebR을 사용하기 위해서는 웹페이지에 web worker를 실행하기 위한 자바스크립트 코드를 추가해야 하는데, quarto를 사용해서 간단한 웹페이지를 만들어 보겠습니다. 이후 배포는 github (github page)를 사용하며, 이 글에서는 이를 다루지 않습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#quarto-page의-구성",
    "href": "posts/2023-09-09-wasm/index.html#quarto-page의-구성",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "Quarto page의 구성",
    "text": "Quarto page의 구성\nwebR을 정적 페이지에서 활용하기 위해 1. 코드를 입력할 공간과 2. 실행 결과를 보여줄 공간 (에디터), 3. 입력된 코드를 실행하게 할 버튼, 4. 마지막으로 앞서 언급했던 자바스크립트 코드를 추가해야합니다. (단, 자바스크립트가 에디터보다 먼저 로드되어야 하므로 앞서 선언해야함)\nquarto에서 html을 실행하게 하기 위해서 아래의 내용을 ```{=html} … ```로 감싸야 합니다.\n&lt;!-- 4. scripts --&gt;\n&lt;link rel=\"stylesheet\" href=\"codemirror.min.css\"&gt;\n&lt;script src=\"codemirror.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"r.js\"&gt;&lt;/script&gt;\n&lt;script src='webr-worker.js'&gt;&lt;/script&gt; \n&lt;script src='webr-serviceworker.js'&gt;&lt;/script&gt; \n&lt;script type=\"module\" src='editor.js'&gt;&lt;/script&gt;\n\n&lt;!-- editor --&gt;\n\n&lt;!-- 1. code editor --&gt;\n&lt;h4&gt;Editor&lt;/h4&gt;\n&lt;div id=\"editor\"&gt;&lt;/div&gt;\n\n&lt;!-- 3. run button --&gt;\n&lt;p style=\"text-align: right;\"&gt;\n  &lt;button class=\"btn btn-success btn-sm\" disabled type=\"button\" id=\"runButton\"&gt;\n    Loading webR...\n  &lt;/button&gt;\n&lt;/p&gt;\n\n&lt;!-- 2. code result --&gt;\n&lt;h4&gt;Result&lt;/h4&gt;\n&lt;pre&gt;&lt;code id=\"out\"&gt;&lt;/code&gt;&lt;/pre&gt;\n먼저 4. scripts는 (웹페이지에 보여지는 내용은 아니지만) 아래와 같이 크게 3종류로 구분할 수 있습니다.\n\n에디터를 위한 script: codemirror.min.css, codemirror.min.js, r.js\n\n백그라운드 프로세스 (사용자의 PC에서의 서버)를 위한 script: webr-worker.js, webr-serviceworker.js\n\n에디터와 백그라운드 프로세스를 연결하는 script\n\n두번째로 1. code editor와 3. run button 부분은 이미지처럼 UI에 코드를 넣는 에디터(껍데기)와 이 코드를 실행하는 버튼을 만드는데 사용됩니다.\n\n\n마지막으로 2. code result 부분은 아래처럼 처음에는 없지만 계산 결과를 보여주는 공간을 만드는데 사용됩니다.\n\n\n추가로 script 파일의 경우는 아래처럼 CDN(웹)에서 불러와도 되지만 해당 파일도 다운로드 받아 미리 정적 페이지에서 제공하게 실행할 수도 있습니다.\n&lt;link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/codemirror/6.65.7/codemirror.min.css\"&gt; (웹에서 사용자가 실시간으로 다운로드)\n&lt;link rel=\"stylesheet\" href=\"codemirror.min.css\"&gt; (페이지에서 미리 다운로드 후 제공)"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#실제-webr-결과",
    "href": "posts/2023-09-09-wasm/index.html#실제-webr-결과",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "실제 webR 결과",
    "text": "실제 webR 결과\n아래는 실제 위 코드를 차라투 블로그 (정적 페이지)에 추가해 빌드한 결과물로 Run code 버튼을 누를때마다 랜덤한 값을 생성하고 평균과 표준편차를 계산하는 webR 예시입니다.\n특별히 설치가 필요한 R 패키지와 함수를 사용하지 않는 이상\n\nprint(head(iris))\n\n위의 내용처럼 코드를 바꿔 작성하면 그 결과도 바뀌는 것을 확인할 수 있습니다.\n\nEditor\n\n\n\n  \n    Loading webR...\n  \n\n\nResult"
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#정리",
    "href": "posts/2023-09-09-wasm/index.html#정리",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 webR을 이용하여 정적 페이지에서 사용자가 R을 실행할 수 있는 방법을 정리했습니다.\n이러한 방법의 장단점은 다음과 같습니다.\n장점\n\nR의 간단한 개념을 설명할때 효과적\n\n\nex) head()를 사용하면 데이터의 처음 6개 행을 보여준다\n\n코드와 실행 결과를 이미지로 첨부할 수도 있지만, head(iris) 같은 코드를 webR로 심어두는 것으로 내용을 더욱 효과적으로 전달할 수 있습니다.\n만약 learnr패키지를 함께 사용한다면 더욱 효과적으로 활용할 수 있습니다.\n\n동적 페이지를 위한 서버 비용을 지출 없음\n\n정적페이지는 무료로 제공할 수 있는 서비스가 많이 있고, 모든 R 연산은 사용자의 PC에서 이루어지기 때문에 별도의 서버 비용을 신경쓰지 않아도 좋습니다. (파일만 미리 서버에 넣어두면)\n\n데이터가 오고 가지 않기 때문에 보안상의 문제가 전혀 발생하지 않습니다. (이론상으로는 페이지의 모든 내용을 사용자가 저장해두면 오프라인 상태에서도 webR 활용 가능)\n단점\n\nwebR은 아직 초기 단계이기 때문에 지원되지 않는 기능이나, 활용할 수 있는 자료가 거의 없습니다.\n파일을 업로드/다운로드 하는 것 같이 단순하지 않은 작업은 webR에서 할 수 없습니다."
  },
  {
    "objectID": "posts/2023-09-09-wasm/index.html#번외",
    "href": "posts/2023-09-09-wasm/index.html#번외",
    "title": "web assembly를 이용하여 웹페이지에서 R 활용하기",
    "section": "번외",
    "text": "번외\n\n\n\n\n\n\nquarto\n\n\n\n추가로, 위처럼 별도의 스크립트 파일 준비 없이 quarto에서 바로 webR을 사용할 수 있게 하는 템플릿도 있어  링크를 첨부합니다.\n\n\n\n\n\n\n\n\nShiny\n\n\n\nshiny application 또한 webR을 이용해서 정적페이지에서 제공할 수 있습니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html",
    "href": "posts/2023-11-21-copilot/index.html",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Github Copilot은 OpenAI의 GPT-3를 기반으로 만들어진 AI 코딩 도우미로, Github에 있는 수많은 “Public Repository”의 코드들을 학습하여, 자동 완성 형태의 제안을 통해 사용자의 코드 제작을 돕습니다.\nGithub에는 코드도 있지만, README와 같은 여러 종류의 설명 글 또한 있기 때문에 Copilot에서는 아래 이미지처럼 (Quarto로 블로그 작성) 다양한 종류의 자동 완성을 제공합니다.\n\n\n\nMicrosoft의 설명에 따르면 chatGPT는 자연어 처리 기술로, Microsoft 365 Copilot은 코드 생성 기술로 설명하고 있습니다. (Microsoft 365 copilot과 github copilot은 살짝 다르긴합니다.)\n즉, chatGPT는 자연어 처리 기술을 통해 사용자의 질문에 대한 답변을 생성하는 것이 주 역할입니다.\n따라서 chatGPT는 아래 작업 용도로 활용되기도 합니다.\n\n에세이, 이메일 및 커버 레터 작성\n목록 만들기\n예술에 대해 자세히 설명\n코드 작성\n콘텐츠 요약\n시와 노래 가사 만들기\n이력서 작성\n\n한편 Copilot은 Github에서의 학습을 바탕으로 코드 스니펫을 제공하는 것에 더 큰 장점을 가지고 있습니다.\n코드 스니펫이란 아래 이미지처럼 자주 사용되는 조각 코드를 미리 작성해 놓은 것을 말합니다.\n\n\n얼핏보면 큰 차이가 없어보이지만, chatGPT는 최근의 기술은 잘 반영하지 못한다는 단점이 있습니다. 또한 Copilot은 IDE (Rstudio)에서 바로 사용할 수 있다는 장점도 있습니다.\n어느 것이 좋다/나쁘다 라기보단 서로 다른 특징을 가지고 있기에, 두 방법 모두를 필요한 목적에 따라 적절히 활용하는 것을 권장합니다.\n\nCopilot을 사용하기 위해서는 몇가지 준비가 필요합니다.\n\n\n\n\n\n\n이 글에서는 Github 에서의 Copilot 결제를 비롯한 요금제에 대해서는 설명하지 않습니다.\n\n\n\n\nGithub 개인 계정 (무료)\nIDE (코드 에디터) - VS Code, Rstudio 등. 글에서는 Rstudio를 기준으로 설명\nGithub Copilot 가입 (첫 한달은 무료, 이후 월 10달러의 유료,  Pricing  참조)\n\nCopilot의 공식 홈페이지에서 설명이 제공하는 개발 도구는 Azure Data studio, JetBrains IDEs, Vim/NeoVim, Visual Studio, Visual Studio Code가 있지만 다행히 Rstudio에서도 사용가능합니다.\n\n\n\n\n\n\n\n\n현재는 Rstudio Desktop 2023.09.0 이상 버전에서만 사용 가능하며 Rstudio Server나 Posit Workbench에서는 관리자 설정 이후 사용 가능합니다.\n버전을 확인하기 위해서는 Rstudio에서 Help &gt; About Rstudio를 클릭하면 됩니다.\n\n\n\n\n이후 Copilot 설정 과정은 다음과 같습니다.\n\nRstudio에서 Tools &gt; Global Options &gt; copilot을 클릭합니다.\n\n\n\n\n“Enable Github Copilot”을 체크합니다.\n\n\n\n\n“Sign in”을 클릭합니다.\n\n\n\n\n이후 나타나는 Device Activation에 Rstudio에서 보여지는 코드를 입력합니다.\n\n\n\n\n이제 Github Copilot을 사용할 수 있습니다.\n\n\n\n\nCopilot은 \"Ghost text\"라고 불리는 방법으로 사용자의 코드를 자동 완성합니다.\n앞서 본 gif 이미지나, 아래의 예시처럼 코드의 일정 부분을 작성하면 나머지 부분을 회색으로 보여주어 탭 키를 누르는 것으로 완성할 수 있습니다.\n\n\n\nCopilot이 인지할 코드: 이때 꼭 주석으로 하지 않아도 이전 코드를 기반으로 copilot이 자동으로 제안합니다.\nCopilot이 제안하는 코드: 회색으로 보여지는 부분이 제안되는 부분입니다.\nCopilot 상태바: Waiting for Completions(대기), Completion response received(코드 제안 완료), No completions available(제안 없음) 등의 상태를 보여줍니다.\n\n한편 상태바 옆의 언어 설정을 통해 어떤 코드를 자동생성할지 설정할 수 있습니다.\n\n\n추가로 Copilot 옵션에서 Index project files… 를 선택하여 현재 Rstudio 프로젝트의 파일을 코드제안에 반영할 수도 있습니다.\n\nCopilot을 사용하는 가장 기본적인 방법은 코드를 자동완성하는 것입니다.\n\n\n위의 예시처럼, 함수의 기능을 잘 설명하는 이름을 작성하는 것으로 Copilot은 함수의 목적을 이해하고, 함수의 기능에 맞는 코드를 제안합니다.\n\n\n한편 함수 이름에 기능을 명시하지 않고 적절한 한글 주석을 통해서도 함수를 자동으로 완성할 수 있습니다.\n물론 이를 위해서는 (이름으로나 주석으로나) 함수의 목적을 명확하게 알아야만 합니다.\n\nCopilot은 코드를 자동완성하는 것 외에도, 코드를 작성하는데 도움을 주는 질문을 제안합니다.\n\n\n이때 질문을 위해서는 코드와는 다르게 주석에 q:와 a:형식을 맞춰야만 합니다.\n\n# q: QUESTION\n# a: \n\n개인적으로 이러한 방법의 활용을 위해서는 코드를 위주로 학습한 Copilot보다는, chatGPT를 바로 쓰거나 gptStudio, chattr 패키지를 사용해 LLM 모델을 사용하는 것도 좋다고 생각합니다.\n\n\n\n\n\n\n gptStudio 설명글 \n chattr 패키지 웹페이지\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 주석을 작성하는데에도 쓰일 수 있습니다.\n예를 들면, 아래의 표준 편차를 계산하는 함수에 대해 주석을 작성하게 할 수도 있습니다.\n\ncalc_se &lt;- function(x, na.rm = TRUE) {\n  if (!is.numeric(x)) {\n    stop(\"x must be numeric\")\n  }\n  if (na.rm) {\n    x &lt;- x[!is.na(x)]\n  }\n  sqrt(var(x) / length(x))\n}\n\ncalc_se(1:10)\n\n[1] 0.9574271\n\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 코드의 품질을 올리기 위한 목적의 테스트 코드를 작성하기 위해서도(!) 쓰일 수 있습니다.\n\n\n\n당연한 이야기지만, Copilot은 유용한 코드를 생성하는 경우가 많지만 항상 유효하거나 의도한 문제를 정확하게 해결하지 않을 수도 있습니다.\n또한 Github의 다양한 수준의 코드를 학습한 만큼 안전하지 않은 코딩 패턴이나, 버그, 비효율적인 관행등을 포함한 코드를 만들 수 도 있기 때문에 완전히 신뢰할 수는 없습니다.\n그러나 대부분의 R 사용자에게는 크게 체감될만한 문제가 없을 것으로 보이며, 특히 데이터 매니지먼트의 목적으로는 매우 유용하게 사용할 수 있을 것으로 보입니다.\n꼭 Rstudio가 아니더라도 다른 IDE에서 SQL, SASS 등의 다른 언어를 목적으로도 사용할 수 있기에 Copilot은 대체로 코드 작업에 아주 아주 효과적인 방법입니다.\n그러나 개인 기준 월 10달러의 비용이 들기 때문에, 코딩 작업이 많이 필요하지 않은 사람에게는 다소 부담스러울 수도 있으니 무료 기간동안 활용해보고 결정하는 것도 좋을 것 같습니다.\n비교를 위한 넷플릭스의 요금제\n\n\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot이란",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot이란",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Github Copilot은 OpenAI의 GPT-3를 기반으로 만들어진 AI 코딩 도우미로, Github에 있는 수많은 “Public Repository”의 코드들을 학습하여, 자동 완성 형태의 제안을 통해 사용자의 코드 제작을 돕습니다.\nGithub에는 코드도 있지만, README와 같은 여러 종류의 설명 글 또한 있기 때문에 Copilot에서는 아래 이미지처럼 (Quarto로 블로그 작성) 다양한 종류의 자동 완성을 제공합니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#chatgpt와의-차이점",
    "href": "posts/2023-11-21-copilot/index.html#chatgpt와의-차이점",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Microsoft의 설명에 따르면 chatGPT는 자연어 처리 기술로, Microsoft 365 Copilot은 코드 생성 기술로 설명하고 있습니다. (Microsoft 365 copilot과 github copilot은 살짝 다르긴합니다.)\n즉, chatGPT는 자연어 처리 기술을 통해 사용자의 질문에 대한 답변을 생성하는 것이 주 역할입니다.\n따라서 chatGPT는 아래 작업 용도로 활용되기도 합니다.\n\n에세이, 이메일 및 커버 레터 작성\n목록 만들기\n예술에 대해 자세히 설명\n코드 작성\n콘텐츠 요약\n시와 노래 가사 만들기\n이력서 작성\n\n한편 Copilot은 Github에서의 학습을 바탕으로 코드 스니펫을 제공하는 것에 더 큰 장점을 가지고 있습니다.\n코드 스니펫이란 아래 이미지처럼 자주 사용되는 조각 코드를 미리 작성해 놓은 것을 말합니다.\n\n\n얼핏보면 큰 차이가 없어보이지만, chatGPT는 최근의 기술은 잘 반영하지 못한다는 단점이 있습니다. 또한 Copilot은 IDE (Rstudio)에서 바로 사용할 수 있다는 장점도 있습니다.\n어느 것이 좋다/나쁘다 라기보단 서로 다른 특징을 가지고 있기에, 두 방법 모두를 필요한 목적에 따라 적절히 활용하는 것을 권장합니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot을-사용하기-위한-준비",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot을-사용하기-위한-준비",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Copilot을 사용하기 위해서는 몇가지 준비가 필요합니다.\n\n\n\n\n\n\n이 글에서는 Github 에서의 Copilot 결제를 비롯한 요금제에 대해서는 설명하지 않습니다.\n\n\n\n\nGithub 개인 계정 (무료)\nIDE (코드 에디터) - VS Code, Rstudio 등. 글에서는 Rstudio를 기준으로 설명\nGithub Copilot 가입 (첫 한달은 무료, 이후 월 10달러의 유료,  Pricing  참조)\n\nCopilot의 공식 홈페이지에서 설명이 제공하는 개발 도구는 Azure Data studio, JetBrains IDEs, Vim/NeoVim, Visual Studio, Visual Studio Code가 있지만 다행히 Rstudio에서도 사용가능합니다.\n\n\n\n\n\n\n\n\n현재는 Rstudio Desktop 2023.09.0 이상 버전에서만 사용 가능하며 Rstudio Server나 Posit Workbench에서는 관리자 설정 이후 사용 가능합니다.\n버전을 확인하기 위해서는 Rstudio에서 Help &gt; About Rstudio를 클릭하면 됩니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#rstudio에서-github-copilot-사용-설정",
    "href": "posts/2023-11-21-copilot/index.html#rstudio에서-github-copilot-사용-설정",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "이후 Copilot 설정 과정은 다음과 같습니다.\n\nRstudio에서 Tools &gt; Global Options &gt; copilot을 클릭합니다.\n\n\n\n\n“Enable Github Copilot”을 체크합니다.\n\n\n\n\n“Sign in”을 클릭합니다.\n\n\n\n\n이후 나타나는 Device Activation에 Rstudio에서 보여지는 코드를 입력합니다.\n\n\n\n\n이제 Github Copilot을 사용할 수 있습니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#github-copilot-사용하기",
    "href": "posts/2023-11-21-copilot/index.html#github-copilot-사용하기",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "Copilot은 \"Ghost text\"라고 불리는 방법으로 사용자의 코드를 자동 완성합니다.\n앞서 본 gif 이미지나, 아래의 예시처럼 코드의 일정 부분을 작성하면 나머지 부분을 회색으로 보여주어 탭 키를 누르는 것으로 완성할 수 있습니다.\n\n\n\nCopilot이 인지할 코드: 이때 꼭 주석으로 하지 않아도 이전 코드를 기반으로 copilot이 자동으로 제안합니다.\nCopilot이 제안하는 코드: 회색으로 보여지는 부분이 제안되는 부분입니다.\nCopilot 상태바: Waiting for Completions(대기), Completion response received(코드 제안 완료), No completions available(제안 없음) 등의 상태를 보여줍니다.\n\n한편 상태바 옆의 언어 설정을 통해 어떤 코드를 자동생성할지 설정할 수 있습니다.\n\n\n추가로 Copilot 옵션에서 Index project files… 를 선택하여 현재 Rstudio 프로젝트의 파일을 코드제안에 반영할 수도 있습니다.\n\nCopilot을 사용하는 가장 기본적인 방법은 코드를 자동완성하는 것입니다.\n\n\n위의 예시처럼, 함수의 기능을 잘 설명하는 이름을 작성하는 것으로 Copilot은 함수의 목적을 이해하고, 함수의 기능에 맞는 코드를 제안합니다.\n\n\n한편 함수 이름에 기능을 명시하지 않고 적절한 한글 주석을 통해서도 함수를 자동으로 완성할 수 있습니다.\n물론 이를 위해서는 (이름으로나 주석으로나) 함수의 목적을 명확하게 알아야만 합니다.\n\nCopilot은 코드를 자동완성하는 것 외에도, 코드를 작성하는데 도움을 주는 질문을 제안합니다.\n\n\n이때 질문을 위해서는 코드와는 다르게 주석에 q:와 a:형식을 맞춰야만 합니다.\n\n# q: QUESTION\n# a: \n\n개인적으로 이러한 방법의 활용을 위해서는 코드를 위주로 학습한 Copilot보다는, chatGPT를 바로 쓰거나 gptStudio, chattr 패키지를 사용해 LLM 모델을 사용하는 것도 좋다고 생각합니다.\n\n\n\n\n\n\n gptStudio 설명글 \n chattr 패키지 웹페이지\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 주석을 작성하는데에도 쓰일 수 있습니다.\n예를 들면, 아래의 표준 편차를 계산하는 함수에 대해 주석을 작성하게 할 수도 있습니다.\n\ncalc_se &lt;- function(x, na.rm = TRUE) {\n  if (!is.numeric(x)) {\n    stop(\"x must be numeric\")\n  }\n  if (na.rm) {\n    x &lt;- x[!is.na(x)]\n  }\n  sqrt(var(x) / length(x))\n}\n\ncalc_se(1:10)\n\n[1] 0.9574271\n\n\n\n\n\nCopilot은 코드를 작성하는데 도움을 주는 질문 외에도, 코드의 품질을 올리기 위한 목적의 테스트 코드를 작성하기 위해서도(!) 쓰일 수 있습니다."
  },
  {
    "objectID": "posts/2023-11-21-copilot/index.html#정리",
    "href": "posts/2023-11-21-copilot/index.html#정리",
    "title": "Rstudio에서 Copilot을 활용해 AI로 코딩하기",
    "section": "",
    "text": "당연한 이야기지만, Copilot은 유용한 코드를 생성하는 경우가 많지만 항상 유효하거나 의도한 문제를 정확하게 해결하지 않을 수도 있습니다.\n또한 Github의 다양한 수준의 코드를 학습한 만큼 안전하지 않은 코딩 패턴이나, 버그, 비효율적인 관행등을 포함한 코드를 만들 수 도 있기 때문에 완전히 신뢰할 수는 없습니다.\n그러나 대부분의 R 사용자에게는 크게 체감될만한 문제가 없을 것으로 보이며, 특히 데이터 매니지먼트의 목적으로는 매우 유용하게 사용할 수 있을 것으로 보입니다.\n꼭 Rstudio가 아니더라도 다른 IDE에서 SQL, SASS 등의 다른 언어를 목적으로도 사용할 수 있기에 Copilot은 대체로 코드 작업에 아주 아주 효과적인 방법입니다.\n그러나 개인 기준 월 10달러의 비용이 들기 때문에, 코딩 작업이 많이 필요하지 않은 사람에게는 다소 부담스러울 수도 있으니 무료 기간동안 활용해보고 결정하는 것도 좋을 것 같습니다.\n비교를 위한 넷플릭스의 요금제\n\n\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "title": "Redefine Null Hypothesis",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "title": "Redefine Null Hypothesis",
    "section": "Abstract",
    "text": "Abstract\n통계적 가설검정에서 광범위하게 이용되는 \\(p\\)-value는 샘플 숫자만 늘리면, 아무리 작은 차이라도 유의미한 결과로 해석되는 문제가 있다. 이것은 대부분의 연구에서 차이 또는 효과가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 발생하는 문제이며, 실제 차이가 0.0000001이라도 정확히 0만 아니라면 샘플 수만 늘려도 결국 \\(p &lt;0.05\\)인 유의한 결과를 얻을 수 있다. 그러나 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며, 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못한다. 이에 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였고, 이 방법에 따른 새로운 \\(p\\)-value가 기존의 \\(p\\)-value와 effect size를 종합한 지표로서 지나치게 작은 차이를 유의한 결과로 해석하는 문제를 해결할 수 있음을 보였다. 한편 다중비교에서 검정할 가설의 갯수가 증가하는 것을 각 가설의 uncertainty가 증가하는 것으로 재해석할 수 있었고, 이 접근법이 기존의 family-wise error rate(FWER), false discovery rate(FDR) control이 지나치게 큰 sample size를 필요로 하는 단점을 보완할 수 있었다. 이 새로운 가설검정방법이 기존 가설검정의 단점을 극복한 새로운 표준이 될 것으로 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "title": "Redefine Null Hypothesis",
    "section": "Introduction",
    "text": "Introduction\n통계적 가설검정은 물리학, 생물학, 의학 등의 자연과학뿐만 아니라 경제학, 심리학 등의 사회과학에서도 어떤 주장을 하기위한 필수적인 도구로 이용되며 핵심 개념은 가설검정(hypothesis test)이다(Anderson, Burnham, and Thompson 2000). R.A Fisher에 의해 처음으로 사용된 귀무가설(null hypothesis)은 반박하려는 가설로 이용되며, 이 가설 하에서 일어날 확률이 낮은 사건을 제시함으로서 이것이 틀렸음을 설명하는데, 이는 수학의 증명법 중 하나인 귀류법과 비슷하다(Yates 1964). 여기서 일어날 확률이 낮은 사건을 정량적으로 표현하는 것이 \\(p\\)-value이며 흔히 기준이 되는 \\(p &lt;0.05\\)는 연구의 결과보다 더 극단적인 사건이 발생할 확률이 5%미만임을 의미한다(Wasserstein and Lazar 2016).\n\\(p &lt;0.05\\)는 지금까지 과학 연구에서 새로운 발견의 절대적인 기준으로 이용되었는데, 이에 대한 비판도 꾸준히 제시되어 왔으며 최근에는 유의성의 기준을 0.05에서 0.005로 바꿔야한다는 주장까지 나왔다(JA 2018; Anderson, Burnham, and Thompson 2000; Benjamin et al. 2018; Wasserstein and Lazar 2016). 그러나 \\(p\\)-value의 개념은 그대로 두고 유의수준만 낮추는 것은 지금부터 언급할 가설검정법의 근본적인 문제를 해결하지 못한 임시방편에 불과하다.\n본 연구자들은 비현실적인 귀무가설을 가설검정법의 근본적인 문제로 판단한다. 철학자 쇼펜하우어는 논쟁에서 이기는 38가지 비법이라는 책에서 상대방의 주장을 단순화하라고 주장했는데, 가설검정에서도 귀무가설은 명확하게 제시하는 것이 원칙이며(예: \\(\\mu =0\\)), 가설이 명확해야 그것을 반박하기도 쉽다(Goffey 2005). 그러나 명확한 귀무가설은 현실에서는 존재하지 않는다. 예를 들어 어떤 값이 정확히 0이라고 주장하는 사람이 있을까? 대부분은 어느 정도 uncertainty를 마음속에 갖고 있으며 실제 측정값이 0.0000001이기 때문에 값이 0이라는 가설은 틀렸다고 주장한다면 설득력을 얻기 어렵다. 그러나 현재의 가설검정방법은 uncertainty가 전혀 없는 명확한 귀무가설을 사용하기 때문에 실제 측정값과 가정의 차이가 아무리 작아도 샘플수만 커지면 결국은 \\(p &lt; 0.05\\)인 결과를 얻을 수 있고, 이는 유의수준을 아무리 올려도 마찬가지이다. 예를 들어 Two-sided \\(z\\)-test의 검정통계량은 \\(z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\) 꼴인데 \\(\\bar{X}-\\mu\\)가 0만 아니라면 \\(n\\)을 늘려서 \\(p\\)값을 무한히 0에 가깝게 만들 수 있다. 이에 \\(p\\)-value와 별개로 실제 차이값도 살펴봐야 한다는 주장이 제기되었는데, 이 차이를 표준화한 개념이 effect size로 원래도 required sample size를 계산할 때 이용하는 지표이며, 흔히 차이를 표준편차로 나눈 값을 사용한다(Sullivan and Feinn 2012). 이처럼 현실적인 귀무가설을 설정하는 것은 그 자체로 중요한 것 뿐만 아니라 effect size를 고려하는 것과도 연결되어 있지만 이것을 개념화한 이론이 없어 실제 연구설계에 반영되기 어렵다.\n이에 저자는 uncertainty를 포함한 귀무가설과 이것을 이용한 가설검정방법을 제안할 것이며 이를 통해 계산된 새로운 \\(p\\)-value가 기존 \\(p\\)-value와 effect size를 종합적으로 고려할 수 있는 지표임을 보일 것이다. 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 직접적으로 연결되어 있음을 보임으로서 가설검정 시 effect size를 고려해야 하는 이유에 대한 이론적 배경을 탄탄히 할 수 있으며, 새로운 \\(p\\)를 기준으로 유의수준을 설정하면 샘플수의 힘만으로 작은 차이를 유의한 결과로 해석하는 것을 방지할 수 있다. 간단한 Two-sided \\(z\\)-test를 예로 들어 개념을 정리 한 후 다른 test로 확장할 것이며, 마지막으로 이 개념을 다중비교에 활용하여 새로운 다중비교법을 제안하고 FWER, FDR control방법과 비교해 보겠다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "title": "Redefine Null Hypothesis",
    "section": "Basic Concept",
    "text": "Basic Concept\nBrief Review of Hypothesis Test\n평균은 모르고 분산이 \\(\\sigma^2\\)인 모집단에서 \\(n\\)개의 샘플 \\(X_1, X_2, \\cdots, X_n\\)을 뽑아 모평균이 \\(\\mu_0\\)인지 가설검정을 한다고 하자. 기존에는 귀무가설 \\(H_0\\)를 \\(\\mu = \\mu_0\\)로 설정한 후, 이 가정 하에서 \\(\\bar{X}\\sim N(\\mu_0, \\frac{\\sigma^2}{n})\\)임을 이용해서(\\(n\\)이 적당히 클 때 중심극한정리) \\((\\bar{X}-\\mu_0)\\sim N(0,\\frac{\\sigma^2}{n})\\)을 얻을 수 있으며 검정통계량은 아래와 같이 표현된다.\n\\[ z= \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\nTwo-sided test라면 \\(z\\)값이 1.96보다 크거나 -1.96보다 작을 때 \\(p &lt;0.05\\)로 유의한 결과로 판단한다. 한편, \\(\\bar{X}-\\mu_0\\)이 딱 0만 아니라면 \\(n\\)이 무한히 커질 때 \\(z\\)도 무한히 커져 어떠한 \\(p\\)-value 기준으로도 유의한 결과가 되는 문제가 있다.\nNull hypothesis with uncertainty\n위의 귀무가설에 uncertainty 개념을 추가해 보자. \\(H_0: \\mu \\sim N(\\mu_0,\\tau^2)\\)는 모평균이 \\(\\mu_0\\)이라고 생각하지만 \\(\\tau\\)정도의 uncertainty를 갖고 있다는 뜻으로 \\(\\tau=0\\)이면 기존의 귀무가설과 일치한다(Figure @ref(fig:fig1)).\n\n\n\n\nNull hypothesis with uncertainty: \\(\\mu_0=0\\)\n\n\n\n앞에서는 \\(\\bar{X}-\\mu_0\\)의 분산이 \\(\\frac{\\sigma^2}{n}\\)이었지만 이번에는 모평균 \\(\\mu_0\\)이 uncertainty를 갖고 있으므로 \\(\\bar{X}-\\mu_0\\)의 분산은 \\(\\frac{\\sigma^2}{n}\\)이 아니고 \\(\\frac{\\sigma^2}{n}+ \\tau^2\\)이 된다. 이를 토대로 검정통계량을 새로 표현하면\n\\[z_{\\tau}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + \\tau^2}} \\sim N(0,1)\\]\n이고 이 때의 two-sided \\(p\\)-value를 \\(p_{\\tau} = 2\\times P(Z\\ge |z_{\\tau}|)\\)로 정의한다. \\(p_0\\)은 기존의 \\(p\\)-value와 같으며 \\(\\tau\\)가 0이 아니라면 \\(n\\)이 아무리 커져도 \\(|z_{\\tau}| &lt; |\\frac{\\bar{X}-\\mu_0}{\\tau}|\\) 이 되어 \\(1.96\\times \\tau\\) 이하의 차이는 \\(p_{\\tau}&lt;0.05\\)를 얻을 수 없다.\nUncertainty interpretation\nUncertainty \\(\\tau\\)의 의미는 그것을 모표준편차 \\(\\sigma\\)와 비교했을 때 더 잘 드러난다. \\(\\tau = c\\sigma\\)로 치환한 후 이를 \\(z_{\\tau}\\)에 적용하면 아래와 같다.\n\\[z_{\\tau}=z_{c\\sigma}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + c^2\\sigma^2}} = \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{z}{\\sqrt{1+nc^2}}\\]\n이제 통계적으로 유의한결과를 얻으려면 기존 가설검정 대비 \\(\\sqrt{1+nc^2}\\)배의 \\(z\\)값이 필요함을 알 수 있으며, 기존 \\(p\\)-value와 \\(z_{c\\sigma}\\)로 계산한 \\(p_{c\\sigma}\\)의 관계를 Figure @ref(fig:fig2)에 그래프로 나타내었다.\n\n\n\n\n\\(p\\) and \\(p_{c\\sigma}\\)-values according uncertainty and sample size\n\n\n\n한편 \\(|z_{c\\sigma}|\\)를 effect size \\(d=|\\frac{\\bar{X}-\\mu_0}{\\sigma}|\\)에 대해 표현하면\n\\[|z_{c\\sigma}|= |\\dfrac{\\bar{X}-\\mu_0}{\\sigma}| \\cdot \\dfrac{1}{\\sqrt{1/n + c^2}} = \\dfrac{d}{\\sqrt{1/n+c^2}}\\]\n의 관계를 얻는다(Figure @ref(fig:fig3)).\n\n\n\n\n\\(p_{c\\sigma}\\) according to effect size, sample size and uncertainty of null hypothesis\n\n\n\n만약 \\(d\\)가 \\(1.96\\times c\\) 이하라면\n\\[|z_{c\\sigma}| = \\dfrac{d}{\\sqrt{1/n+c^2}} &lt;  \\dfrac{d}{c} \\le \\dfrac{1.96\\times c}{c}  = 1.96\\]\n이 되어 샘플수 \\(n\\)을 아무리 늘리더라도 \\(p_{c\\sigma} &lt;0.05\\)를 얻을 수는 없다. 연구자는 이 사실을 이용하여 역으로 \\(c\\)를 조절함으로서 연구자가 생각하기에 너무 작은 effect size가 유의한 결과가 되는 것을 막을 수 있는데, \\(c=0.01\\) 즉, 모집단 표준편차 대비 1%의 불확실성을 설정한다면 \\(d\\)가 0.0196 이하인 매우 작은 effect가 유의한 결과를 얻는 것을 막을 수 있다. \\(n\\)이 작을 때는 1%의 uncertainty가 기존의 \\(p\\)-value에 거의 영향을 끼치지 않지만, \\(n=10,000\\)일 때는 유의한 \\(p\\)-value 기준을 0.005로 강화한 것과 같은 효과를 갖는다(Figure 2).\nSample Size Calculation\nUncertainty의 개념이 추가되면 Required sample size도 바뀌게 되는데, 먼저 기존 two-sided \\(z\\)-test의 샘플수 구하는 공식을 standard error에 대해 정리하면 아래와 같다(\\(\\alpha\\): Type 1 error rate, \\(\\beta\\): Type 2 error rate)(Noordzij et al. 2011).\n\\[\n\\begin{aligned}\nn &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{(\\mu-\\mu_0)/\\sigma})^2 \\\\\n\\dfrac{n}{\\sigma^2} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{\\sigma^2\\cdot(1/n)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2\n\\end{aligned}\n\\]\n이제 uncertainty parameter \\(c\\)를 추가하려면 좌변의 분모에 \\(c^2\\sigma^2\\)을 더해주면 되고 새로운 샘플수 구하는 공식은 아래와 같이 바뀌게 된다.\n\\[\n\\begin{aligned}\n\\dfrac{1}{\\sigma^2\\cdot(1/n_{c\\sigma}+c^2)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{n_{c\\sigma}} &= (\\dfrac{(\\mu-\\mu_0)/\\sigma}{Z_{1-\\alpha/2} + Z_{1-\\beta}})^2 -c^2 \\\\\n               &= \\dfrac{1}{n}-c^2 \\\\\nn_{c\\sigma} &= \\dfrac{1}{1/n-c^2}\n\\end{aligned}\n\\]\n따라서 \\(\\frac{1}{n} -c^2 \\le 0\\) 즉, 기존 가설검정방법으로도 \\(1/c^2\\) 이상의 샘플수가 필요했었다면, uncertainty를 포함한 가설검정으로는 아무리 샘플수를 늘려도 Type 1,2 error를 컨트롤할 수 없다. Figure @ref(fig:fig4)에 \\(\\alpha = 0.05\\), \\(1-\\beta=0.8\\)인 경우의 effect size와 required sample size의 관계가 그래프로 표현되어 있으며, uncertainty가 존재한다면 아무리 샘플수를 늘려도 어떤 effect size이하는 검정할 수 없음을 확인할 수 있다.\n\n\n\n\nRequired sample size according effect size in some uncertainty scenarios: \\(\\alpha=0.05\\), \\(1-\\beta=0.8\\)"
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "title": "Redefine Null Hypothesis",
    "section": "Extend to Other Statistics",
    "text": "Extend to Other Statistics\n이번엔 \\(Z_{c\\sigma}\\)의 논리를 다른 통계량으로 확장해보자.\n\n\\(t\\)-test: One or paired 2 sample\nOne sample \\(t\\)-test의 귀무가설은 위의 \\(z\\)-test와 일치하며, 모분산 \\(\\sigma^2\\)을 표본분산인 \\(s^2\\)으로만 바꾸면 통계량은 자유도 \\(n-1\\)인 \\(t\\)-distribution을 따르는 것이 알려져 있다(Kim 2015). 이제 귀무가설에 \\(cs\\)만큼의 uncertainty를 추가하면 아래와 같다.\n\\[t_{cs} = \\dfrac{\\bar{X}-\\mu_0}{\\sqrt{s^2/n + c^2s^2}} = \\dfrac{\\bar{X}-\\mu_0}{s/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{t}{\\sqrt{1+nc^2}}\\]\nPaired \\(t\\)-test의 경우도 \\(\\bar{X}\\)를 두 그룹의 차이의 평균인 \\(\\bar{d}\\), \\(s\\)를 차이의 표준편차인인 \\(s_d\\)로 바꾸면 위와 동일하다.\nIndependent 2 sample \\(t\\)-test\n독립된 2집단(equal variance)의 평균이 같은지를 비교할 때 귀무가설은 \\(H_0: \\mu_1-\\mu_2=0\\)의 형태가 되고 통계량은 아래와 같다(Kim 2015).\n\\[t=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\]\n(\\(s_p\\): pooled variance, \\(n_1, n_2\\): sample numbers of group 1, 2 )\n이 \\(t\\)값이 자유도 \\(n_1+n_2-2\\)인 \\(t\\)-distritution을 따른다고 알려져 있으며, 귀무가설 \\(H_0: \\mu_1-\\mu_2=0\\) \\(s_p\\)의 \\(c\\)배만큼 uncertainty를 추가하면 통계량은\n\\[t_{cs_p}=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})+c^2s_p^2)}}= \\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\cdot \\dfrac{1}{\\sqrt{1+\\frac{1}{\\frac{1}{n_1}+\\frac{1}{n_2}}}c^2} = \\dfrac{t}{\\sqrt{1+\\frac{n_1n_2}{n_1+n_2}c^2}}\\]\n이 된다.\nRegression coefficient\n종속변수 \\(Y\\)와 독립변수 \\(X_1\\)의 선형관계를 살펴보는 \\(Y=\\beta_0 + \\beta_1X_1\\)에서 \\(H_0: \\beta_1=0\\)을 검정하는 아래 통계량은 자유도 \\(n-2\\)인 \\(t\\)-distribution을 따른다는 것이 알려져 있다(@ Altman and Krzywinski 2015).\n\\[t= \\dfrac{\\hat{\\beta}_1}{se({\\hat{\\beta}_1})}=\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/ \\sqrt{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}} \\]\n(\\(\\hat\\sigma_{\\varepsilon}\\): Mean squared error(MSE), \\(s_{x_1}\\): standard deviance of \\(X_1\\))\n이제 \\(\\dfrac{\\hat\\sigma_{\\varepsilon}}{{s_{x_1}}}\\)의 \\(c\\)배 만큼 귀무가설에 uncertainty를 추가하면 통계량은 아래와 같다.\n\\[t_{c\\hat\\sigma_{\\varepsilon}/s_{x_1}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n+c^2}} = (\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}}) \\cdot \\dfrac{1}{\\sqrt{1+nc^2}}= \\dfrac{t}{\\sqrt{1+nc^2}}\\]\n일반적으로 \\(Y\\)와 \\(X_1, X_2, \\cdots, X_p\\)들의 선형모형은 행렬을 이용해서 \\(\\mathbf{Y}= \\mathbf{X\\beta}\\)로 표현할 수 있으며, \\(\\beta_i=0\\)을 검정하는 경우 앞서 \\(\\frac{1}{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}\\)대신 \\((\\mathbf{X'X})^{-1}\\)의 \\(i+1\\)번째 diagonal인 \\(g_{ii}\\)을 대입한 통계량을 사용하며 자유도는 \\(n-p-1\\)이다(Alexopoulos 2010). 이제 uncertainty를 추가한 통계량을 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)로 정의하면 simple linear model의 경우를 자연스럽게 확장한 셈이 된다. 선형모형을 \\(Y\\)가 정규분포가 아닐 때로 확장한 Generalized linear model(GLM)의 경우에는 \\(z=\\frac{\\hat{\\beta}_i}{se{(\\hat{\\beta}_i)}}\\)가 asymptotically normal distribution을 따르는 것을 이용하며, 앞서와 마찬가지로 uncertainty를 추가한 통계량을 \\(\\dfrac{z}{\\sqrt{1+nc^2}}\\)로 정의하면 일관성을 유지할 수 있다(Nelder and Wedderburn 1972; Wedderburn 1974). .\nCorrelation coefficients\n두 변수 \\(X\\), \\(Y\\)의 상관관계를 볼 때 흔히 사용되는 pearson correlation coefficient(\\(r\\))은 아래 통계량을 이용한다.\n\\[t= \\dfrac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\]\n이것이 자유도 \\(n-2\\)인 \\(t\\)-distribuition을 따른다는 것이 알려져 있으며 실제로 이 값을 계산해보면 앞서 simple linear model의 \\(\\dfrac{\\hat{\\beta}_1}{se(\\hat{\\beta}_1)}\\)과 정확히 일치한다. 따라서 uncertainty를 추가한 \\(t\\)를 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)으로 정의하면 일관성을 유지할 수 있으며, 이는 \\(H_0: r'=\\dfrac{r}{\\sqrt{1-r^2}}=0\\)을 검정할 때 \\(\\dfrac{n}{n-2}\\)의 \\(c\\)배 만큼 uncertainty를 준 것으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "title": "Redefine Null Hypothesis",
    "section": "New Approach for Multiple Comparison: Uncertainty control",
    "text": "New Approach for Multiple Comparison: Uncertainty control\n마지막으로 귀무가설의 uncertainty를 조절하여 multiple comparison 문제를 다루는 uncertainty control을 제안한다.\nDefinition of uncertainty control\n여러 개의 귀무가설을 동시에 검정할 때 원래의 유의수준을 그대로 적용하면 위양성이 늘어나는 문제가 있는데, 각 가설마다의 유의수준이 \\(\\alpha\\)라면 전체 \\(m\\)개의 가설에서 하나라도 위양성결과를 얻을 확률은 \\(1-(1-\\alpha)^m \\simeq m\\alpha\\) 이 되어 원래 위양성 확률의 \\(m\\)배가 된다. 이를 해결하기 위해 흔히 이용되는 방법은 family-wise error rate(FWER) control로 bonferroni correction이 대표적이다. Bonferroni correction에서는 가설이 \\(m\\)개라면 각 가설의 유의수준을 \\(\\alpha_m=\\frac{\\alpha}{m}\\)로 낮춰 family-wise error rate를 \\(\\alpha\\)이하로 control 할 수 있으며 식으로 표현하면 아래와 같다(Armstrong 2014).\n\\[-\\text{log}\\alpha_m = -\\text{log}\\alpha + \\text{log}m\\]\n한편, 여러 개의 귀무가설을 동시에 검정한다는 것을 각 가설의 uncertainty가 늘어나는 것으로 바라볼 수도 있다. Genome-Wide Association Study(GWAS)가 대표적인 예로 이것은 어떤 유전자가 질병과 관계있는지 아무런 사전정보도 없지만 일단 갖고있는 유전자들을 전부 각각 검정해보는 방법이며, 각 검정의 유의수준은 보통 100만개의 유전자를 동시검정할 때의 bonferroni correction에 해당하는 \\(5\\times 10^{-8}\\)를 사용한다(Jannot, Ehret, and Perneger 2015). 그런데 어떤 유전자가 질병과 관련있는지 사전정보가 없음에도 불구하고 각 효과가 정확히 0이라고 생각하는 사람이 과연 있을까? 검정하려는 귀무가설의 갯수가 증가할수록 가설의 uncertainty도 증가한다고 간주하는 것이 자연스러우며 가설이 \\(m\\)개일 때의 uncertainty \\(c_m\\)을 아래와 같이 정의하겠다.\n\\[\\text{log}c_m = \\text{log}c + b\\cdot\\text{log}m\\]\n이것은 아까의 bonferroni correction과 비슷한 형태로 원래의 \\(c\\)(log scale)에 \\(\\text{log}m\\)에 비례하는 penalty를 추가로 부여하며, \\(1.96\\times c_m\\)보다 작은 effect size는 \\(n\\)이 아무리 커도 통계적으로 유의한 결과를 얻을 수 없다. 연구자는 \\(c\\)를 조절하여 initial uncertainty를, 상수 \\(b\\)를 조정하여 uncertainty control의 정도를 조절할 수 있으며, \\(c=0.01\\), \\(b=0.069\\)으로 정하면 \\(n=10000\\)에서 bonferroni correction과 같은 정도의 control을 줄 수 있다 (Figure @ref(fig:fig5)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and required \\(p\\)-values(\\(p_{\\text{req}}\\)): bonferroni correction vs uncertainty control(\\(b=0.069\\), \\(c=0.01\\)\n\n\n\nFigure @ref(fig:fig5)를 보면 uncertainty control도 bonferroni correction과 마찬가지로 \\(m\\)이 증가함에 따라 \\(p\\)-value의 기준을 강화함을 알 수 있다. 차이점은 샘플 수에 따라 기준을 강화하는 정도가 다르다는 것인데, \\(m=10^6\\) 기준으로 \\(N=1500\\)일 때는 \\(5.46\\times 10^{-3}\\), \\(N=3000\\)일 때는 \\(6.61\\times 10^{-4}\\), \\(N=5000\\)일 때는 \\(4.22\\times 10^{-5}\\)의 \\(p\\)-value 기준을 갖는다. \\(N=10000\\)일 때는 아까 언급했듯이 \\(p\\)-value 기준 \\(5.06\\times 10^{-8}\\)로 bonferroni correction과 비슷한 정도의 control을 주게 된다.\nWe can look at the change in significance levels in terms of the effect size. Unlike the Bonferroni correction, the degree of significance correction can be changed by the effect sizes even if they have the same \\(p_{c_{m\\sigma}}\\)-values of the Uncertainty control (Figure @ref(fig:fig6)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and \\(p_{c_m}\\) by effect size(\\(d\\)): uncertainty control\n\n\n\nIn Figure @ref(fig:fig6), when applying \\(c=0.01\\) and \\(b=0.069\\), the result with an effect size of \\(0.5\\) and \\(p_{c_{m\\sigma}}=5\\times 10^{-5}\\) is hardly affected by the increase in the number of hypotheses; however, the result with an the effect size of \\(0.05\\) cannot gain significance when the number of hypotheses increases to \\(10^6\\) even if the initial \\(p_{c_{m\\sigma}}\\)-value is same as above.\nSome scenarios: compare with bonferroni correction, false discovery rate(FDR)\n몇 가지 시나리오를 설정하여 uncertainty correction 방법을 흔히 이용하는 multiple comparison 방법인 bonferroni correction, FDR control과 비교해보겠다(Benjamini and Hochberg 1995). \\(10^6\\)개의 귀무가설 중 1% 즉, 10000개가 실제로 참인 가설이고 그 effect size는 \\(d\\)라고 가정하자. 통계적 유의성은 \\(d\\)와 sample size \\(N\\), 그리고 multiple comparison 방법에 따라 달라지게 되며 이론상의 결과와 데이터를 생성해서 simulation한 결과를 Table 1,2에 정리하였다. FWER와 FDR control의 기준은 모두 0.05이며 simulation 때 FDR control은 Benjamini-Hochberg Procedure를 이용하였다(Benjamini and Hochberg 1995).\n\n\n\nThe number of false positive/negative results among \\(10^6\\) hypotheses(1% true) with some scenarios\n\n\n\n\n\n\n\n\n\n\n\nSample size\nd\nBonferroni: False (+)\nBonferroni: True (+)\nFDR: False (+)\nFDR: True (+)\nUncertainty: False (+)\nUncertainty: True (+)\n\n\n\n2000\n0.02\n0\n0\n0\n0\n2655\n176\n\n\n2000\n0.04\n0\n1\n0\n2\n2655\n1125\n\n\n2000\n0.06\n0\n28\n41\n788\n2655\n3750\n\n\n2000\n0.08\n0\n305\n245\n4652\n2655\n7176\n\n\n2000\n0.10\n0\n1637\n437\n8311\n2655\n9292\n\n\n3000\n0.02\n0\n0\n0\n0\n654\n104\n\n\n3000\n0.04\n0\n6\n4\n81\n654\n1123\n\n\n3000\n0.06\n0\n152\n167\n3172\n654\n4526\n\n\n3000\n0.08\n0\n1424\n424\n8050\n654\n8355\n\n\n3000\n0.10\n0\n5103\n515\n9776\n654\n9809\n\n\n4000\n0.02\n0\n0\n0\n0\n164\n62\n\n\n4000\n0.04\n0\n17\n24\n450\n164\n1082\n\n\n4000\n0.06\n0\n488\n301\n5727\n164\n5115\n\n\n4000\n0.08\n0\n3477\n496\n9429\n164\n9021\n\n\n4000\n0.10\n0\n8087\n525\n9979\n164\n9947\n\n\n5000\n0.02\n0\n0\n0\n0\n42\n37\n\n\n5000\n0.04\n0\n44\n64\n1217\n42\n1027\n\n\n5000\n0.06\n0\n1134\n400\n7596\n42\n5588\n\n\n5000\n0.08\n0\n5814\n519\n9857\n42\n9409\n\n\n5000\n0.10\n0\n9474\n526\n9998\n42\n9985\n\n\n10000\n0.02\n0\n3\n1\n20\n0\n3\n\n\n10000\n0.04\n0\n733\n350\n6656\n0\n736\n\n\n10000\n0.06\n0\n7084\n523\n9944\n0\n7091\n\n\n10000\n0.08\n0\n9946\n526\n10000\n0\n9946\n\n\n10000\n0.10\n0\n10000\n526\n10000\n0\n10000\n\n\n\n\n\nTable 1을 보면 uncertainty control는 나머지 둘과는 달리 sample size가 작을 때 유의한 결과를 많이 얻을 수 있음을 알 수 있다. 구체적으로 \\(d\\)가 0.02일 때 bonferroni correction, FDR control로는 \\(n\\)이 5000이하일 때 유의한 결과를 하나도 얻을 수 없지만 uncertainty control로는 \\(n=1500,3000,5000\\)일 때 각각 5175, 646, 62개의 유의한 결과를 얻을 수 있다. \\(d=0.05\\)인 경우 \\(n=1500, 3000\\)이면 uncertainty control &gt; FDR control &gt; bonferroni correction, \\(n=5000\\)이면 FDR control &gt; uncertainty control &gt; bonferroni correction 순으로 유의한 결과를 많이 얻을 수 있다. 한편 \\(n=10000\\)인 모든 시나리오에서 uncertainty control은 bonferroni correction보다도 강한 기준으로 나타나고 sample size의 힘만으로 작은 effect size를 유의한 결과로 판단하는 것에 penalty를 주게 된다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "title": "Redefine Null Hypothesis",
    "section": "Discussion",
    "text": "Discussion\n본 연구에서는 uncertainty의 개념을 포함한 현실적인 귀무가설을 만들고 그것을 검정할 수 있는 가설검정법을 제안하였으며, 이를 통해 계산된 \\(p_{c\\sigma}\\)-value가 기존의 \\(p\\)-value와 effect size를 종합적으로 고려한 지표로 작은 effect size가 sample size의 힘만으로 유의한 결과를 얻는 것에 penalty를 줄 수 있음을 보였다. 이는 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 어떻게 연결되는지를 개념화했다는 의의가 있으며, 이제 연구자들은 차이가 정확히 0이라는 비현실적인 귀무가설에서 벗어나 현실적인 가설검정을 수행하면서 effect size를 가설검정의 틀 안에서 고려할 수 있을 것이다.\n얼마만큼의 uncertainty를 고려해야 하는지는 검증가능한 최소 effect size가 어느정도인지에 달려 있는데, 이는 연구분야와 주제에 따라서 달라질 수 있기 때문에 각 분야의 연구자들의 공감대가 형성되어야 할 것으로 생각된다. 앞서 언급한 \\(c=0.01\\)이 기존의 \\(p\\)-value에 큰 영향을 끼치지 않으면서 10000이상의 매우 큰 sample size에 0.005정도의 유의수준을 부여할 수 있는 적절한 기준이라 판단한다. 한편 effect size로 표준화하지 않더라도 차이값 자체를 기준으로 uncertainty를 고려할 수도 있는데, 사실 어떤 지표든지 그것을 측정하는 순간 측정의 최소단위로 인한 uncertainty가 존재할 수 밖에 없다. 예를 들어 수축기 혈압을 측정해서 120이 나왔을 때 그것의 의미는 실제값이 정확히 120이라는 것이 아니라 실제값이 119.5~120.5 사이에 존재한다는 것이다. 따라서 차이가 1이하인 것을 가설검정하는 것은 그 자체로 비현실적이라 할 수 있으며 \\(\\tau= \\frac{1}{1.96}\\)로 두면 \\(|z_{\\tau}| &lt; |\\frac{\\bar{X}-\\mu_0}{\\tau}| \\le \\frac{1}{\\tau} = 1.96\\)이 되어 1이하의 차이를 유의한 결과로 판단하는 것을 방지할 수 있다. 꼭 측정의 최소단위가 아니더라도 연구자가 생각하는 차이의 최소 단위를 \\(\\tau\\)에 반영할 수도 있다.\n베이지안 통계에서도 prior를 통해 사전지식의 uncertainty를 다룰 수 있는데, prior의 정보가 데이터의 정보가 합쳐져서 posterior가 된다는 것에서 본 연구와는 차이가 있다. 예를 들어 분산이 \\(\\sigma\\)인 정규분포 에서 \\(n\\)개의 sample을 뽑아 베이지안 방법으로 모평균을 추정할 때, prior(conjugate prior)의 분산이 \\(\\tau^2\\)이라면 posterior의 분산은 \\(\\dfrac{1}{\\frac{1}{\\sigma^2/n}+\\frac{1}{\\tau^2}}\\)가 된다(Murphy 2007). 이 때 \\(\\tau\\)가 무한히 커지면 분산은 \\(\\sigma^2/n\\)이 되는데, \\(\\tau\\)가 커질수록 prior의 정보가 없어 data의 정보가 posterior로 그대로 반영되는 것으로 볼 수 있다. 반면 본 연구에서는 귀무가설의 uncertainty값인 \\(\\tau\\)가 penalty로 작용하여 이것이 커질수록 data의 정보가 희석된다고 생각할 수 있다.\n3개 이상의 그룹을 비교할 수 있는 ANOVA와 Chi-square test에 대해서는 uncertainty의 개념을 적용하기 힘들었다. ANOVA의 귀무가설은 \\(H_0: \\mu_1= \\mu_2 = \\cdots = \\mu_k\\)꼴로 여러 그룹들을 동시에 비교하기 때문에 본 연구의 uncertainty의 개념을 그대로 적용할 수 없지만 실용적으로 \\(F_c= \\dfrac{F}{1+nc^2}\\)로 정의하면 다른 test들과 일관성을 유지할 수 있을 것이다. 범주형 변수끼리의 연관성을 비교할 때 쓰이는 카이제곱검정의 경우에도 \\(\\chi_c= \\dfrac{\\chi}{1+nc^2}\\)가 실용적인 지표로 활용될 수 있으리라 생각한다.\nUncertainty의 개념은 multiple comparison에서도 활용될 수 있는데, 검정할 귀무가설의 갯수가 늘어나는 것을 각 가설에 대한 uncertainty가 늘어나는 것으로 생각할 수 있기 때문이다. 이 기준을 이용하면 sample size가 적을 때 상대적으로 유의한 결과를 많이 얻을 수 있어 반대의 특징을 갖는 FWER, FDR control의 한계를 보완할 수 있다. 다중비교가 흔히 쓰이는 GWAS의 경우 \\(p&lt;5 \\times 10^{-8}\\)라는 강한 기준이 gold standard로 사용되어 실제 효과가 있는 유전자는 통계적 유의성을 얻지 못하고, 아주 미미한 효과가 sample size의 힘만으로 의미있는 유전자가 되는 문제가 있었는데, uncertainty control을 같이 활용함으로서 이를 극복할 수 있으리라 생각한다.\n귀무가설에 uncertainty를 추가한 본 연구의 가설검정 방법이 과학연구에서 기존 가설 검정방법을 포괄하는 새로운 표준이 될 수 있으리라 확신한다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#개요",
    "href": "posts/2023-11-10-discourse1/index.html#개요",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "개요",
    "text": "개요\n이번 글에서는 차라투에서 사용하는 커뮤니티 플랫폼인 Discourse를 사용해서 커뮤니티를 구축하는 과정에 대해 소개합니다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#discourse",
    "href": "posts/2023-11-10-discourse1/index.html#discourse",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "Discourse",
    "text": "Discourse\nDiscourse는 쉽게 모던한 커뮤니티를 만들 수 있는 오픈소스 커뮤니티 플랫폼이며 사이트 개발과 배포를 완벽히 조절할 수 있도록 다양한 커스텀 옵션과 설치 방식을 제공합니다. 또한 10년이 넘게 수많은 테스트를 거쳐왔으며 카카오, Zoom, Jetbrain 등의 다양한 회사에서 사용 되고 있고 지속적으로 업데이트 되고 있습니다.\n또한 적절히 분리된 게시판에 Markdown 게시글을 작성하는 기능부터 채팅 기능까지 사용할 수 있고, 다양한 테마를 다운받아 사용하거나 직접 코드를 수정해서 커뮤니티를 꾸밀 수 있습니다. 마지막으로 AI를 통한 챗봇이나 Data Explorer를 사용한 SQL 분석 기능 등 다양한 범위의 플러그인을 제공하며 이를 직접 개발할 수도 있습니다."
  },
  {
    "objectID": "posts/2023-11-10-discourse1/index.html#discourse-커뮤니티-만들기",
    "href": "posts/2023-11-10-discourse1/index.html#discourse-커뮤니티-만들기",
    "title": "Discourse 기반 커뮤니티 구축",
    "section": "Discourse 커뮤니티 만들기",
    "text": "Discourse 커뮤니티 만들기\nDiscourse는 다양한 설치 방식을 지원하지만 공식적으로 지원되는 유일한 방법은 Docker 기반입니다.\nDocker를 사용하면 빌드에 시간이 걸리는 단점이 있지만 Discourse를 제작하는데 사용된 Rails 웹 애플리케이션 프레임워크의 복잡한 설정을 하지 않아도 되며, 쉽게 배포하고 업데이트 할 수 있습니다.\n반면에 macOS / Ubuntu / Windows 환경에 직접 설치하는 방법도 공식 GitHub에 자세히 작성되어 있습니다.\n\n1. 메일서버 설정\n메일 서비스는 계정 관리, 알림에 쓰이며 필수적으로 구성해야 합니다. 일반적인 Gmail과 같은 서비스가 아닌 transactional 이메일 서버를 사용해야 합니다.\n저는 지원되는 이메일 서비스 중 Mailjet을 사용했습니다. Mailjet은 무료 요금제를 사용했을 때 일일 200건/월 6000건의 이메일을 보낼 수 있기에 적합하다고 생각했습니다.\n\n도메인 인증받기\nMailjet 서비스에 회원 가입을 했다면 설정 페이지에 들어가서 도메인을 등록해줍니다.\n\nPending 우측 톱니바퀴 버튼 → Validate → TXT DNS 레코드 등록을 통해서 도메인을 인증해줍니다.\n\n도메인 인증이 되었다면 Authenticate this domain 버튼을 클릭해서 SPF 및 DKIM 설정을 진행합니다.\n\nTXT DNS 레코드를 추가해서 SPF 및 DomainKeys를 설정해서 mailjet 서비스가 도메인을 사용할 수 있도록 합니다.(반영까지는 시간이 걸릴 수 있으니 새로고침 해주시면 됩니다)\n\n설정이 모두 되었다면 위 사진처럼 Active 라고 나옵니다.\n\n\n발신 이메일 정하기\n도메인 인증 바로 밑에 발송 주소를 정할 수 있는 부분이 있으며 메일의 발송자를 지정할 수 있는 부분입니다.\nDiscourse는 알림 용도로 메일 서비스를 사용하기 때문에 noreply를 붙여서 설정했습니다.\n\n\n\nAPI Key 발급받기\n계정관리 페이지에서 Main account의 API KEY를 확인하고 SECRET KEY를 받을 수 있습니다. 추후에 다시 다운받으려면 재발급이 필요하기에 꼭 어딘가에 저장해둬야 합니다.\n\n\n\n\n2. Discourse 설치\nDiscourse를 도커에 설치하는 방법은 공식 가이드를 참조했습니다.\n먼저 명령어를 실행해서 도커 이미지를 다운로드 받습니다. root 권한은 빌드 등에 필요합니다.\n$ sudo -s # root 권한 얻기\n$ git clone https://github.com/discourse/discourse_docker.git /var/discourse\n$ cd /var/discourse\n$ chmod 700 containers # 권한설정\n$ vim containers/discourse.yml # 설정파일 생성(이름을 다르게 만들어서 구분이 용이하게 하려고 합니다.)\n공식 가이드 에서는 discourse-setup 라는 자동 설정 툴을 사용해서 질문에 답변을 하는 형태로 직관적인 설정을 지원하나, 디테일한 설정을 위해 설정 파일을 직접 만들어 주었습니다.\n## discourse.yml\n## 밑에 expose에서 443을 제외했기 때문에 ssl template를 주석처리 했습니다.\ntemplates:\n  - \"templates/postgres.template.yml\"\n  - \"templates/redis.template.yml\"\n  - \"templates/web.template.yml\"\n  ## Uncomment the next line to enable the IPv6 listener\n  #- \"templates/web.ipv6.template.yml\"\n  - \"templates/web.ratelimited.template.yml\"\n  ## Uncomment these two lines if you wish to add Lets Encrypt (https)\n  #- \"templates/web.ssl.template.yml\"\n  #- \"templates/web.letsencrypt.ssl.template.yml\"\n\n## which TCP/IP ports should this container expose?\n## If you want Discourse to share a port with another webserver like Apache or nginx,\n## see https://meta.discourse.org/t/17247 for details\n## Nginx에서 직접 SSL 설정을 하려고 하므로 443을 제외했습니다.\nexpose:\n  - \"80:80\"   # http\n    #  - \"443:443\" # https\n\nparams:\n  db_default_text_search_config: \"pg_catalog.english\"\n\n  ## Set db_shared_buffers to a max of 25% of the total memory.\n  ## will be set automatically by bootstrap based on detected RAM, or you can override\n  #db_shared_buffers: \"256MB\"\n\n  ## can improve sorting performance, but adds memory usage per-connection\n  #db_work_mem: \"40MB\"\n\n  ## Which Git revision should this container use? (default: tests-passed)\n  ## 기본 설정은 beta 버전으로 설치됩니다. 안정성을 위해 stable 버전으로 꼭 지정해주어야 합니다.\n  version: v3.1.2\n\n## 다중 언어를 지원하므로 꼭 ko로 하지 않아도 됩니다.\nenv:\n  LC_ALL: en_US.UTF-8\n  LANG: en_US.UTF-8\n  LANGUAGE: en_US.UTF-8\n  # DISCOURSE_DEFAULT_LOCALE: en\n\n  ## How many concurrent web requests are supported? Depends on memory and CPU cores.\n  ## will be set automatically by bootstrap based on detected CPUs, or you can override\n  #UNICORN_WORKERS: 3\n\n  ## Discourse를 배포할 도메인\n  ## TODO: The domain name this Discourse instance will respond to\n  ## Required. Discourse will not work with a bare IP number.\n  DISCOURSE_HOSTNAME: 'community.zarathu.com'\n\n  ## Uncomment if you want the container to be started with the same\n  ## hostname (-h option) as specified above (default \"$hostname-$config\")\n  #DOCKER_USE_HOSTNAME: true\n\n  ## 관리자 이메일\n  ## TODO: List of comma delimited emails that will be made admin and developer\n  ## on initial signup example 'user1@example.com,user2@example.com'\n  DISCOURSE_DEVELOPER_EMAILS: 'office@zarathu.com'\n\n  ## 메일서버 설정값\n  ## TODO: The SMTP mail server used to validate new accounts and send notifications\n  # SMTP ADDRESS, username, and password are required\n  # WARNING the char '#' in SMTP password can cause problems!\n  DISCOURSE_SMTP_ADDRESS: in-v3.mailjet.com\n  DISCOURSE_SMTP_PORT: 587\n  DISCOURSE_SMTP_USER_NAME: API KEY\n  DISCOURSE_SMTP_PASSWORD: \"SECRET KEY\"\n  #DISCOURSE_SMTP_ENABLE_START_TLS: true           # (optional, default true)\n  DISCOURSE_SMTP_DOMAIN: community.zarathu.com # mailjet에 등록한 도메인\n  DISCOURSE_NOTIFICATION_EMAIL: noreply@community.zarathu.com # 등록해둔 발신자 주소\n\n  ## If you added the Lets Encrypt template, uncomment below to get a free SSL certificate\n  #LETSENCRYPT_ACCOUNT_EMAIL: me@example.com\n\n  ## The http or https CDN address for this Discourse instance (configured to pull)\n  ## see https://meta.discourse.org/t/14857 for details\n  #DISCOURSE_CDN_URL: https://discourse-cdn.example.com\n  \n  ## The maxmind geolocation IP address key for IP address lookup\n  ## see https://meta.discourse.org/t/-/137387/23 for details\n  #DISCOURSE_MAXMIND_LICENSE_KEY: 1234567890123456\n\n## 도커 컨테이너와 연결될 호스트 볼륨 경로 / 로그파일 경로\n## The Docker container is stateless; all data is stored in /shared\nvolumes:\n  - volume:\n      host: /var/discourse/shared/standalone\n      guest: /shared\n  - volume:\n      host: /var/discourse/shared/standalone/log/var-log\n      guest: /var/log\n\n## 플러그인 설정\n## 빌드 전/후 실행될 명령어를 작성할 수 있음\n## Plugins go here\n## see https://meta.discourse.org/t/19157 for details\nhooks:\n  after_code:\n    - exec:\n        cd: $home/plugins\n        cmd:\n          - git clone https://github.com/discourse/docker_manager.git\n\n## Any custom commands to run after building\nrun:\n  - exec: echo \"Beginning of custom commands\"\n  ## If you want to set the 'From' email address for your first registration, uncomment and change:\n  ## After getting the first signup email, re-comment the line. It only needs to run once.\n  #- exec: rails r \"SiteSetting.notification_email='info@unconfigured.discourse.org'\"\n  - exec: echo \"End of custom commands\"\n위 설정 파일은 /var/discourse/sample/standalone.yml 을 베이스로 만들어진 예시입니다.\nDiscourse는 SSL을 위한 Let’s encrypt 인증서 관리를 자동으로 해주지만 추후에 와일드카드 인증서를 사용하기 위해 비활성화 시켰습니다. 기본 SSL을 활성화 시키려면 설정 파일에서 아래 부분을 수정해주시면 됩니다.\n## discourse.yml\ntemplates:\n    ...\n  - \"templates/web.ssl.template.yml\"\n  - \"templates/web.letsencrypt.ssl.template.yml\"\n...\nexpose:\n  - \"80:80\"   # http\n  - \"443:443\" # https\n...\nenv:\n    ...\n    LETSENCRYPT_ACCOUNT_EMAIL: me@example.com\nDiscourse 버전은 공식 GitHub Tags 에서 버전명을 확인한 뒤에 안정적인 최신 버전으로 수정할 수 있습니다.\n\n\n3. Discourse 시작\n설정 파일을 저장하고 Discourse를 실행하는 단계입니다. Discours는 bootstrap(빌드)을 하는데 약 2-8분이 소요되므로 설정을 수정하면 꽤 오랜 시간을 기다려야 합니다.\n설정 파일의 이름이 discourse.yml 이므로 파일명을 꼭 명시해줘야 합니다. 만약 파일명을 잘못 입력한다면 충돌이 일어나서 DB가 초기화될 수 있습니다.(경험담)\n## /var/discourse/launcher 명령어 설정파일명\n$ /var/discourse/launcher rebuild discourse\nrebuild 명령어는 아래 3개의 명령어를 실행하는 것과 동일하게 작동합니다. 만약 설정파일을 수정했다면, 간단하게 위 명령어를 사용해서 다시 배포하면 새로운 수정 사항이 반영됩니다.\n## equivalent with launcher rebuild\n$ /var/discourse/launcher stop discourse\n$ /var/discourse/launcher bootstrap discourse\n$ /var/discourse/launcher start discourse\n작업이 완료됐다면, 설정 파일에 명시한 HOST_NAME인 http://community.zarathu.com 으로 접속할 수 있으며 관리자 계정 생성을 하고 가이드를 따라서 초기 설정을 해주시면 됩니다. (SSL이 비활성화된 상태이므로 http로 접속가능)\n\n\n4. 외부 Nginx를 이용한 배포\n서비스 관리의 단순화, 서버 리소스 효율성, 와일드카드 인증서를 사용하기 위해 단일 Nginx 도커 인스턴스를 사용해서 Discourse를 운영하기로 결정했습니다.\n이를 위해 설정 파일을 수정해서 외부 포트 노출을 제거합니다.\n## discourse.yml\n...\n\n## \"80:80\" -&gt; \"80\"\nexpose:\n  - \"80\"   # http\n    #  - \"443:443\" # https\nDocker Network의 Bridge Network를 통해 Nginx와 Discourse 컨테이너를 연결해줍니다. Nginx 컨테이너는 실행되고 있다고 가정합니다.\n## mybridge라는 이름의 bridge 네트워크 생성\n$ docker network create --driver bridge mybridge\n\n## nginx, discourse 컨테이너를 mybridge 네트워크에 연결\n$ docker network connect mybridge nginx\n$ docker network connect mybridge discourse\nNginx sites-available 설정 파일을 생성합니다.\n## community.conf\n\n## 80포트(http)\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name community.zarathu.com;\n\n    add_header Content-Security-Policy upgrade-insecure-requests;\n        \n        ## https로 upgrade\n    location / {\n        return 301 https://$server_name$request_uri;\n    }\n}\n\n## 443 포트(https)\nserver {\n    listen 443 ssl http2;\n    server_name community.zarathu.com;\n    underscores_in_headers on;\n\n        ## 따로 만들어둔 와일드카드 인증서 경로\n    ssl_certificate /etc/letsencrypt/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/privkey.pem;\n    ssl_protocols        SSLv3 TLSv1 TLSv1.1 TLSv1.2;\n    ssl_ciphers          HIGH:!aNULL:!MD5;\n\n        ## 보안을 위한 헤더\n    add_header Strict-Transport-Security max-age=31536000;\n    add_header X-Content-Type-Options \"nosniff\" always;\n    add_header X-Frame-Options \"SAMEORIGIN\" always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n    add_header Content-Security-Policy upgrade-insecure-requests;\n\n    ssl_stapling on;\n    ssl_stapling_verify on;\n    client_max_body_size 4G; # 사이트 업로드 크기 제한\n\n        ## Docker network를 이용한 프록시\n        ## discourse 컨테이너 내부 80포트로 연결됩니다.\n    location / {\n        proxy_pass http://discourse;\n\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n                proxy_set_header X-Forwarded-Proto https;\n                proxy_set_header X-Forwarded-Scheme https;\n\n        proxy_buffer_size          128k;\n        proxy_buffers              4 256k;\n        proxy_busy_buffers_size    256k;\n    }\n    location = /robots.txt {\n        return 200 \"User-agent: *\\nDisallow: /\";\n    }\n}\n설정 파일을 활성화 시키기 위해서 sites-enabled에 심볼릭 링크를 생성합니다.\n## nginx 설정 파일 위치는 /etc/nginx 로 가정\n$ ln -s /etc/nginx/sites-available/community.conf /etc/nginx/sites-enabled/community.conf\n변경된 설정을 반영하기 위해 Nginx를 다시 로드해줍니다. 기존에 배포된 서비스의 중단을 최소화 하기 위해 reload를 사용했습니다.\n## nginx 컨테이너의 nginx reload 명령 실행\n## docker exec -it 컨테이너명 nginx -s reload\n$ docker exec -it nginx nginx -s reload\n\n\n5. 개발 서버를 위한 다중 사이트 구성\nDiscourse는 설정 변경 후 bootstrap(빌드)을 하는데 매우 많은 시간이 소요되며 그 동안 기존의 서비스의 운영이 중단됩니다. 또한 여러 설정을 수정하거나 플러그인을 추가했을 때 오류가 발생할 수 있으므로 매우 위험합니다. 따라서 테스트를 할 수 있도록 개발용 Discourse 커뮤니티를 배포하게 되었습니다.\n메인 커뮤니티와 동일한 파일 경로를 공유하고 같은 DB를 사용하게 할 수도 있지만 사이트의 안정성을 위해 완전한 분리를 하려고 하므로 새로운 discourse를 설치했습니다.\n$ git clone https://github.com/discourse/discourse_docker.git /var/discourse-dev\n$ cd /var/discourse-dev\n$ chmod 700 containers # 권한설정\n$ vim containers/discourse-dev.yml # 설정파일 생성(이름을 다르게 만들어서 구분이 용이하게 하려고 합니다.)\n새로운 Discourse 설정 파일을 생성합니다. 대부분의 내용은 메인 커뮤니티와 동일합니다.\n## discourse-dev.yml\n...\nDISCOURSE_HOSTNAME: 'community.dev.zarathu.com'\n...\nvolumes:\n  - volume:\n      host: /var/discourse-dev/shared/standalone\n      guest: /shared\n  - volume:\n      host: /var/discourse-dev/shared/standalone/log/var-log\n      guest: /var/log\n...\n개발용 Discourse 커뮤니티를 실행합니다.\n$ /var/discourse-dev/launcher rebuild discourse-dev\n이후에 외부 Nginx를 이용한 배포 설정도 메인 커뮤니티 설정과 동일하나 dev 커뮤니티 도메인과 컨테이너 이름 등을 수정해서 배포해 주시면 됩니다.\n배포가 완료됐다면 메인 커뮤니티와 개발용 커뮤니티의 데이터를 동기화하기 위해 메인 커뮤니티를 백업합니다.\n\n백업이 완료됐다면 목록에 표시되며 다운로드 버튼을 클릭해서 파일로 저장합니다.\n\n저장한 파일을 개발 커뮤니티에 업로드한 뒤 복구를 진행하면 메인 커뮤니티의 사용자, 글, 설정 등이 개발용 서버에도 동일하게 적용됩니다.\n\n디스코스를 설치 한 다음, 몇가지 커스텀을 거쳐 실제로 사용하는 커뮤니티의 이미지는 다음과 같습니다.\n\n이어지는 글에서 커스텀 과정을 소개합니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html",
    "href": "posts/2019-10-25-ruck2019/index.html",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "",
    "text": "김진섭 대표는 10월 25일(금) 광화문 한국마이크로소프트 11층에서 열린 R User Conference in Korea 2019(RUCK 2019) 참석, 맞춤형 의학연구 앱을 만들고 그것을 패키지로 만들어 CRAN에 배포한 경험을 발표하였습니다. 초록과 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#abstract",
    "href": "posts/2019-10-25-ruck2019/index.html#abstract",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Abstract",
    "text": "Abstract\n의학연구자들에게 제공한 맞춤형 ShinyApps 중, 범용으로 쓰일만한 것들을 Shiny module 로 만들고 웹으로 공개하였습니다. 큰 용량의 데이터는 개인 PC에서 직접 다룰 수 있도록 Rstudio Addins 을 포함한 R 패키지 를 만들어 github 에 배포하였습니다. 패키지 관리를 위해 (1) testthat, covr 로 코드 테스트를 수행한 결과 리포트를, (2) pkgdown 으로 패키지를 소개하는 웹사이트를 만들었고, (3) Travis CI 와 appveyor 로 앞의 과정과 여러 운영체제에서의 테스트를 자동화하였습니다. 최종적으로 CRAN 에 패키지를 배포하였고, 1.01 버전까지 업데이트하였습니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#slide",
    "href": "posts/2019-10-25-ruck2019/index.html#slide",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RUCK2019/ 를 클릭하면 볼 수 있으며, Chrome 에 최적화되었습니다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "",
    "text": "김진섭 대표는 5월 31일(금) 차라투(주)가 후원하는 Shinykorea 밋업에 참석, shiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "요약",
    "text": "요약\n\n\n의학연구자들에게 맞춤형 ShinyApps를 제공함.\n\n범용으로 쓰일만한 것들을 Shiny module로 만든 후, 웹과 RStudio Addins로 배포.\n\n\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 리포트를 제공 중(R Markdown 활용).\n심평원/보험공단 빅데이터 연구에서 R Markdown 리포트로 연구지원 중."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html",
    "href": "posts/2022-09-30-GAM/index.html",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "",
    "text": "김진섭 대표는 성균관대학교 바이오헬스 규제과학과 강의에서 비선형모델인 GAM(Generalized Additive Model) 을 소개할 예정입니다. 전체 강의자료는 https://github.com/jinseob2kim/R-skku-biohrs 에 있습니다."
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#요약",
    "href": "posts/2022-09-30-GAM/index.html#요약",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "요약",
    "text": "요약\nGAM 은 비선형관계를 다루는 통계방법이다\n\nLOWESS: 구간 촘촘하게 나눈 후 평균값\nCubic spline(cs): 구간 몇개로 나눈 후 각각 3차함수 fitting\nNatural cubic spline(ns): cs 맨 처음과 끝구간만 선형 fitting\nSmoothing spline(GAM default): 최적화때 smoothing penalty(λ) 부여\n\n종속변수 형태따라 여러종류\n\nContinuous: normal\nBinary: logistic\nCount: poisson, quasipoisson(평균 ≠ 분산 일 때)\nSurvival: coxph"
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#slide",
    "href": "posts/2022-09-30-GAM/index.html#slide",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/gam 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html",
    "href": "posts/2022-01-20-dbbackup/index.html",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 3주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html#목차",
    "href": "posts/2022-01-20-dbbackup/index.html#목차",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "목차",
    "text": "목차\n\nSSH-key 생성 및 GitHub 등록\nDockerfile 작성\nCronFile 및 Shell File 작성\n\n\n\nSSH-Key 생성 및 GitHub 등록\nGitHub Repository에 SSH를 통한 접근 인증을 위해서는 SSH Public Key 생성 및 등록 과정을 걸쳐야 합니다.\n\nSSH-Key 생성\nssh-keygen -t rsa \n\n\n\n생성된 key는 인증키(private key)의 경우 ‘/.ssh/id_rsa’에 저장되어 있고 공개 키(public key)는’/.ssh/id_rsa.pub’로 저장되어 있습니다. 저희는 공개키를 사용하기에 공개키를 복사해주세요.\ncat ~/.ssh/id_rsa.pub\n위 명령어를 실행하면 공개키가 출력되는 모습을 확인하실 수 있습니다. 이제 GitHub로 이동하겠습니다.\n\n\n\nGitHub 등록\n\n\n\n\n\n\n\n\n본인 GitHub계정에 로그인한 이후 Settings → SSH and GPG keys → new SSH key에 복사한 SSH Key를 붙여주면 됩니다. Title은 임의로 정하셔도 상관없습니다. SSH-Key 등록 이후 본격적인 진행에 앞서 앞으로의 코드에 이해를 돕고자 현재 작업하고 있는 로컬 디렉토리의 계층도를 안내하겠습니다.\n\n\n\n\n\n\n\n\nDockerFile 작성\nDockerContainer를 DockerFile을 통해 만들도록 하겠습니다. 코드의 전문은 아래와 같습니다.\n\n\n\nFROM mysql:8.0.27\n생성되는 컨테이너는 MYSQL:8.0.27 이미지를 바탕으로 한다는 내용의 코드입니다\n\nRUN apt-get update\nRUN apt-get install cron -y\nRUN apt-get install git-all -y \n생성되는 컨테이너에 주기적인 백업을 위한 Cron, Git 명령어 수행을 위한 Git을 설치하는 코드입니다.\n\nRUN mkdir ~/.ssh \nCOPY /rsa/id_rsa /root/.ssh/id_rsa\nCOPY /rsa/id_rsa.pub /root/.ssh/id_rsa.pub\n앞서 생성한 SSH-key를 컨테이너에서 활용 가능하도록 컨테이너 생성시 .ssh폴더를 생성한 뒤 해당 키 값들을 복사하여 생성하는 코드입니다.\n\nCOPY cron /etc/cron.d/cron\nCOPY backup.sh /etc/cron.d/backup.sh \nCOPY cronstart.sh /etc/cron.d/cronstart.sh \n로컬에 있는 cron, backup.sh, cronstart.sh 파일들을 복사하여 컨테이너 생성시 해당 디렉토리에 넣는 코드입니다.해당 파일의 내용은 아래에서 살펴보도록 하겠습니다.\n\n\n\nCronFile 및 Shell File작성\n\n\nCronFile 작성\n\n\n\n앞서 컨테이너의 /etc/cron.d/cron에 복사한 cron파일입니다.\nCron을 작동시키는 시간을 맞추기 위해서 TZ = Asia/Seoul를 설정하여 현재 서울의 시간대와 일치시켰습니다.\n* * * * * /etc/cron.d/backup.sh &gt;&gt; /var/log/cron.log 2&gt;&1\n매분 마다 /etc/cron.d/backup.sh 파일을 작동시키고, 해당 파일의 작동 결과를 /var/log/cron.log 파일에 기록할 수 있도록 설정하였습니다. Backup.sh의 내용은 아래에서 살펴보겠습니다.\n(Cron 설정에 관련해서는 임의로 설정하셔도 상관없습니다.)\n\n\n\nShell 파일 작성\n\n\n\n앞서 Cron 설정으로 일정주기 마다 실행되는 Backup.sh 파일입니다. mysqldump를 활용해서 test db를 백업한 내용을 “$FileDir/$YmdH”.sql에 저장하고 이후 Git으로 Push를 진행해준다.\n여기까지cron을 활용하여 GitHub에 주기적으로 백업을 진행하는 내용의 코드는 완성되었습니다. 이에 더하여 컨테이너를 실행하면 cron이 자동으로 시작되는 기능을 추가하겠습니다.\n앞선 DockerFile에서 다음과 같은 코드를 확인할 수 있습니다.\nENTRYPOINT [\"/etc/cron.d/cronstart.sh\"]\nENTRYPOINT는 컨테이너가 시작되었을때 스크립트 혹은 명령을 실행합니다. 위 코드에서는 /etc/cron.d/cronstart.sh 파일을 실행하는 것입니다. /etc/cron.d/cronstart.sh 파일의 내용은 아래에서 확인하겠습니다.\n\n\n\ncronstart.sh\n#!/bin/bash\nservice cron start \nbash /usr/local/bin/docker-entrypoint.sh mysqld\n컨테이너가 시작되면 service cron start로 cron을 시작합니다. 이후 bash /usr/local/bin/docker-entrypoint.sh 파일에 mysqld 인자를 넘기며 실행시켜 해당 컨테이너에 mysqld를 작동시킵니다.\n\ndockerfile을 활용하여 컨테이너를 구축하고 실제로 작동이 잘되는지 확인해보도록 하겠습니다.\ndocker build -t test .\ndocker run -i --name test test \n\n\n\n컨테이너 생성과 동시에 정상적으로 DB 백업이 이루어지는 것을 확인할 수 있습니다.\n\n\n\n\n결론\nDB의 내용을 GitHub에 주기적으로 백업하는 방식에 대해 알아보았습니다. DB의 내용을 주기적으로 자동으로 백업이 가능하다는 유의미한 결과를 보이기는 하지만 코드상 DB의 암호가 노출된다는 점 등을 미루어 보았을 때 취약점이 존재하는것 같습니다. 개선된 방향으로의 학습이 필요할것 같습니다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html",
    "href": "posts/2022-02-07-gtsummary/index.html",
    "title": "gtsummary 패키지 소개",
    "section": "",
    "text": "본 자료에서는 데이터 셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. gtsummary 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다. gtsummary 패키지에 관한 기본 개념 및 함수들을 예제를 통해 다루어 보자."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#setup",
    "href": "posts/2022-02-07-gtsummary/index.html#setup",
    "title": "gtsummary 패키지 소개",
    "section": "Setup",
    "text": "Setup\n\n## Setup\n\n# install.packages(\"tidyverse\")\n# install.packages(\"data.table\")\n# install.packages(\"gtsummary\")\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(gtsummary)"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#road-file",
    "href": "posts/2022-02-07-gtsummary/index.html#road-file",
    "title": "gtsummary 패키지 소개",
    "section": "Road file",
    "text": "Road file\n예제에 사용할 데이터를 fread함수를 통해 불러오자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n\n## Load file\n\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt &lt;- fread(url, header=T)\ndt"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "href": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "title": "gtsummary 패키지 소개",
    "section": "tbl_summary",
    "text": "tbl_summary\ntbl_summary 함수를 사용하여 기본 테이블을 작성할 수 있다. 출력값으로 데이터 셋의 각 열에 대한 기술 통계량을 반환한다. 데이터 셋에 섞인 범주형 변수와 연속형 변수를 자동적으로 인식해 그에 맞는 값을 반환하며, 범주형 변수의 기본 출력값은 n(%)이고 연속형 변수의 기본 출력값은 median(IQR)이다. 결측값은 테이블에 Unknown으로 출력된다.\nfread를 통해 불러온 데이터에서 몇 개의 변수를 추출해 요약 테이블을 만들어보자.\n\n# select variables\n\ndt2 &lt;- dt %&gt;% select(\"EXMD_BZ_YYYY\", \"Q_PHX_DX_STK\", \"Q_SMK_YN\",\n                     \"HGHT\", \"WGHT\" ,\"TOT_CHOL\", \"TG\")\ndt2\n\n\n\n\n  \n\n\n\ntbl_summary 함수를 통해 간단한 요약 테이블을 만들어보자.\n\n# create table\n\ndt2 %&gt;% tbl_summary()\n\n\n\n\n\n\nCharacteristic\n\nN = 1,6441\n\n\n\n\nEXMD_BZ_YYYY\n\n\n\n2009\n214 (13%)\n\n\n2010\n236 (14%)\n\n\n2011\n223 (14%)\n\n\n2012\n234 (14%)\n\n\n2013\n243 (15%)\n\n\n2014\n254 (15%)\n\n\n2015\n240 (15%)\n\n\nQ_PHX_DX_STK\n12 (1.1%)\n\n\nUnknown\n573\n\n\nQ_SMK_YN\n\n\n\n1\n995 (61%)\n\n\n2\n256 (16%)\n\n\n3\n391 (24%)\n\n\nUnknown\n2\n\n\nHGHT\n165 (158, 171)\n\n\nWGHT\n64 (56, 73)\n\n\nTOT_CHOL\n193 (170, 218)\n\n\nTG\n106 (72, 163)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\n\n변수 유형이 자동으로 구분되어 연속형 변수는 median(IQR), 범주형 변수는 n(%)의 형태로 출력된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "href": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "title": "gtsummary 패키지 소개",
    "section": "그룹별 통계",
    "text": "그룹별 통계\nby\ntbl_summary 함수에는 다양한 옵션이 존재한다. by 옵션을 이용하여 그룹별 통계량을 계산할 수 있다. by 옵션에 그룹별 통계를 수행할 변수를 지정하여 사용 가능하다. 다음 예시에서 연도 변수인 EXMD_BZ_YYYY를 기준으로 그룹별 통계량을 출력해보자.\n\ndt2 %&gt;% tbl_summary(by = EXMD_BZ_YYYY)\n\n\n\n\n\n\nCharacteristic\n\n2009, N = 2141\n\n\n2010, N = 2361\n\n\n2011, N = 2231\n\n\n2012, N = 2341\n\n\n2013, N = 2431\n\n\n2014, N = 2541\n\n\n2015, N = 2401\n\n\n\n\nQ_PHX_DX_STK\n2 (1.5%)\n2 (1.2%)\n1 (0.7%)\n2 (1.3%)\n2 (1.2%)\n2 (1.3%)\n1 (0.6%)\n\n\nUnknown\n82\n74\n85\n78\n75\n95\n84\n\n\nQ_SMK_YN\n\n\n\n\n\n\n\n\n\n1\n125 (59%)\n132 (56%)\n140 (63%)\n146 (62%)\n141 (58%)\n157 (62%)\n154 (64%)\n\n\n2\n34 (16%)\n42 (18%)\n35 (16%)\n36 (15%)\n35 (14%)\n38 (15%)\n36 (15%)\n\n\n3\n53 (25%)\n62 (26%)\n48 (22%)\n52 (22%)\n67 (28%)\n59 (23%)\n50 (21%)\n\n\nUnknown\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n165 (159, 171)\n165 (159, 171)\n165 (157, 171)\n164 (159, 172)\n165 (159, 171)\n164 (158, 172)\n164 (158, 172)\n\n\nWGHT\n64 (55, 72)\n64 (56, 73)\n63 (56, 72)\n64 (57, 74)\n64 (57, 72)\n63 (56, 72)\n64 (57, 74)\n\n\nTOT_CHOL\n192 (170, 216)\n193 (168, 220)\n190 (168, 214)\n196 (173, 224)\n190 (168, 218)\n193 (171, 216)\n194 (171, 217)\n\n\nTG\n105 (71, 148)\n107 (70, 158)\n104 (74, 164)\n108 (69, 164)\n107 (76, 160)\n108 (75, 162)\n111 (71, 167)\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n\n\n\ntbl_strata\nby 옵션을 통해 그룹별 통계량을 계산한 것처럼 tbl_strata 함수를 이용하면 여러 계층으로 그룹을 묶을 수 있다. tbl_strata(data, strata, .tbl_fun, …) 형식을 사용하며 strata에 그룹화할 칼럼, .tbl_fun 인자에는 출력할 tbl_summary formula를 지정한다.\n\ntbl_strata(data = dt2,\n           strata = EXMD_BZ_YYYY,\n           .tbl_fun =\n             ~ .x %&gt;%\n             tbl_summary(by = Q_SMK_YN) %&gt;%\n             add_p() %&gt;%\n             add_n(),\n           .header = \"**{strata}**, N={n}\")\n\n\n\n\n\n\n\nCharacteristic\n2009, N=214\n2010, N=236\n2011, N=223\n2012, N=234\n2013, N=243\n2014, N=254\n2015, N=240\n\n\nN\n\n1, N = 1251\n\n\n2, N = 341\n\n\n3, N = 531\n\n\np-value2\n\nN\n\n1, N = 1321\n\n\n2, N = 421\n\n\n3, N = 621\n\n\np-value2\n\nN\n\n1, N = 1401\n\n\n2, N = 351\n\n\n3, N = 481\n\n\np-value2\n\nN\n\n1, N = 1461\n\n\n2, N = 361\n\n\n3, N = 521\n\n\np-value2\n\nN\n\n1, N = 1411\n\n\n2, N = 351\n\n\n3, N = 671\n\n\np-value2\n\nN\n\n1, N = 1571\n\n\n2, N = 381\n\n\n3, N = 591\n\n\np-value2\n\nN\n\n1, N = 1541\n\n\n2, N = 361\n\n\n3, N = 501\n\n\np-value2\n\n\n\n\n\nQ_PHX_DX_STK\n130\n1 (1.5%)\n1 (3.6%)\n0 (0%)\n0.5\n162\n2 (2.3%)\n0 (0%)\n0 (0%)\n&gt;0.9\n138\n1 (1.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n156\n2 (2.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n168\n2 (2.1%)\n0 (0%)\n0 (0%)\n&gt;0.9\n159\n2 (2.2%)\n0 (0%)\n0 (0%)\n&gt;0.9\n156\n1 (1.0%)\n0 (0%)\n0 (0%)\n&gt;0.9\n\n\nUnknown\n\n58\n6\n18\n\n\n44\n10\n20\n\n\n53\n14\n18\n\n\n52\n12\n14\n\n\n44\n7\n24\n\n\n67\n8\n20\n\n\n58\n12\n14\n\n\n\nHGHT\n212\n160 (155, 166)\n166 (164, 174)\n170 (165, 175)\n&lt;0.001\n236\n160 (156, 166)\n168 (164, 173)\n172 (165, 176)\n&lt;0.001\n223\n159 (155, 166)\n170 (166, 174)\n172 (169, 177)\n&lt;0.001\n234\n161 (156, 167)\n169 (165, 172)\n172 (166, 177)\n&lt;0.001\n243\n160 (156, 165)\n170 (166, 172)\n171 (167, 174)\n&lt;0.001\n254\n160 (155, 165)\n169 (165, 174)\n173 (168, 177)\n&lt;0.001\n240\n161 (155, 166)\n170 (164, 173)\n173 (169, 177)\n&lt;0.001\n\n\nWGHT\n212\n59 (52, 67)\n68 (61, 74)\n71 (62, 77)\n&lt;0.001\n236\n60 (54, 67)\n70 (66, 77)\n70 (62, 78)\n&lt;0.001\n223\n60 (53, 67)\n69 (64, 75)\n72 (64, 79)\n&lt;0.001\n234\n61 (54, 69)\n70 (63, 77)\n72 (64, 80)\n&lt;0.001\n243\n59 (53, 69)\n68 (63, 73)\n69 (62, 75)\n&lt;0.001\n254\n59 (53, 66)\n69 (62, 76)\n72 (64, 79)\n&lt;0.001\n240\n61 (54, 69)\n70 (63, 79)\n74 (62, 84)\n&lt;0.001\n\n\nTOT_CHOL\n212\n192 (166, 215)\n195 (177, 224)\n198 (174, 217)\n0.6\n236\n193 (166, 220)\n200 (181, 219)\n186 (168, 218)\n0.5\n223\n186 (165, 212)\n195 (172, 222)\n198 (176, 228)\n0.2\n234\n198 (173, 224)\n200 (171, 238)\n192 (175, 214)\n0.8\n243\n187 (165, 214)\n191 (176, 227)\n196 (170, 220)\n0.3\n254\n193 (170, 216)\n194 (174, 220)\n193 (174, 214)\n&gt;0.9\n240\n189 (170, 216)\n194 (170, 216)\n204 (180, 222)\n0.12\n\n\nTG\n212\n87 (59, 125)\n130 (79, 189)\n139 (100, 173)\n&lt;0.001\n236\n86 (64, 133)\n113 (78, 176)\n142 (101, 230)\n&lt;0.001\n223\n93 (67, 129)\n129 (78, 194)\n148 (91, 200)\n&lt;0.001\n234\n94 (65, 133)\n130 (91, 188)\n132 (94, 200)\n0.001\n243\n96 (72, 140)\n105 (75, 162)\n127 (84, 217)\n0.009\n254\n98 (73, 140)\n118 (87, 165)\n134 (82, 202)\n0.005\n240\n98 (64, 145)\n125 (97, 255)\n150 (90, 248)\n&lt;0.001\n\n\n\n\n\n1 n (%); Median (IQR)\n\n\n\n2 Fisher's exact test; Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "href": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "title": "gtsummary 패키지 소개",
    "section": "Modifying function arguments",
    "text": "Modifying function arguments\ntbl_summary 함수에는 다양한 옵션이 존재하며, 이러한 옵션 조정을 통해 원하는 테이블을 작성할 수 있다. 다음은 tbl_summary 함수의 주요 옵션에 대한 설명이다.\n\nlabel : 테이블에 출력되는 변수명 지정\ntype : 변수 유형 지정 (ex. 연속형, 범주형)\nstatistic : 요약 통계량 지정\ndigits : 자릿수 지정\nmissing : 결측값이 있는 행을 표시할지 여부\nmissing_text : 결측행의 변수명 지정\nsort : 빈도에 따라 범주형 변수의 level 정렬\npercent : 열/행의 백분율 출력\ninclude : 테이블에 포함할 변수 지정\n\n\n\n\n\n\n\n\n\n다음은 옵션을 활용한 예시이다. 연도 변수 EXMD_BZ_YYYY의 그룹별 통계량을 출력하고, Q_SMK_YN 변수를 “smoking y/n”로 바꾸어보자. 이때 연속형 변수의 출력값을 {mean}({sd})으로, 범주형 변수의 출력값을 {n}/{N} ({p}%) 형태로 바꾸어보자. 결측값의 변수명은 “Missing”으로 수정한다.\n\ndt2 %&gt;%\n  tbl_summary(\n    by = EXMD_BZ_YYYY,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),\n    label = Q_SMK_YN ~ \"smoking y/n\",\n    missing_text = \"Missing\"\n  )\n\n\n\n\n\n\nCharacteristic\n\n2009, N = 2141\n\n\n2010, N = 2361\n\n\n2011, N = 2231\n\n\n2012, N = 2341\n\n\n2013, N = 2431\n\n\n2014, N = 2541\n\n\n2015, N = 2401\n\n\n\n\nQ_PHX_DX_STK\n2 / 132 (1.5%)\n2 / 162 (1.2%)\n1 / 138 (0.7%)\n2 / 156 (1.3%)\n2 / 168 (1.2%)\n2 / 159 (1.3%)\n1 / 156 (0.6%)\n\n\nMissing\n82\n74\n85\n78\n75\n95\n84\n\n\nsmoking y/n\n\n\n\n\n\n\n\n\n\n1\n125 / 212 (59%)\n132 / 236 (56%)\n140 / 223 (63%)\n146 / 234 (62%)\n141 / 243 (58%)\n157 / 254 (62%)\n154 / 240 (64%)\n\n\n2\n34 / 212 (16%)\n42 / 236 (18%)\n35 / 223 (16%)\n36 / 234 (15%)\n35 / 243 (14%)\n38 / 254 (15%)\n36 / 240 (15%)\n\n\n3\n53 / 212 (25%)\n62 / 236 (26%)\n48 / 223 (22%)\n52 / 234 (22%)\n67 / 243 (28%)\n59 / 254 (23%)\n50 / 240 (21%)\n\n\nMissing\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n164 (9)\n165 (9)\n164 (10)\n165 (9)\n165 (9)\n164 (9)\n164 (9)\n\n\nWGHT\n64 (13)\n65 (12)\n65 (13)\n66 (12)\n65 (12)\n64 (12)\n66 (13)\n\n\nTOT_CHOL\n195 (37)\n195 (39)\n194 (38)\n199 (35)\n192 (36)\n195 (36)\n195 (36)\n\n\nTG\n129 (90)\n136 (101)\n138 (108)\n129 (89)\n132 (98)\n138 (127)\n141 (113)\n\n\n\n\n1 n / N (%); Mean (SD)"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "",
    "text": "김진섭 대표는 7월 15일(금) 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 “데이터과학자가 갖춰야할 기술” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "요약",
    "text": "요약\n분석: 의학 + 연구는 R 추천\n\n데이터 전처리는 data.table 과 %&gt;%.\nR로 ppt/xlsx 만들기(분석결과 반출)\n일반, 반복측정, 표본추출, Propensity, meta, 시계열, ML\n코드관리는 Github\n\n서비스: 리포트(Rmarkdown), 웹 애플리케이션(Shiny)\n서버: 분석환경 or 웹 서비스\n\n리눅스, 클라우드, Docker"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/short-2022summer 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html",
    "href": "posts/2023-04-28-tidycdisc/index.html",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "",
    "text": "Clinical trials 데이터가 CDISC 형태로 통일되어 가며, CDISC에 대한 관심도가 많아지고 있다. 오늘 CDISC와 대략적인 data format에 대해 배워보고, CDISC ADaM 데이터를 시각화 해주는 Shiny App 오픈소스 패키지인 tidyCDISC에 대해서 소개하려고 한다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#cdisc란-왜-중요할까",
    "href": "posts/2023-04-28-tidycdisc/index.html#cdisc란-왜-중요할까",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "1. CDISC란? 왜 중요할까?",
    "text": "1. CDISC란? 왜 중요할까?\n임상시험에서 수많은 종류의 데이터들이 수집된다. 임상시험 데이터는 의약품의 안전성과 유효성을 증명하며, 임상효과를 확인하고 이상반응을 조사하는 데 사용될 수 있어 매우 중요하다.\n그러나 문제는 각 기관마다 통일된 규정이 없어서 데이터들의 변수 이름도 제각각이고, 데이터의 구조와 정의가 다 다르다.\n▶️ 데이터를 정리하고 분석하는지에 대한 국제적인 표준이 없다면, 데이터를 해석하고 설명하는 데만 상당한 시간과 노력이 필요하여서 비효율적이다.\n이런 비효율성을 없애고자 CDISC라는 비영리 단체가 1997년에 설립되어 규제기관, 제약회사, 임상연구 조직 등을 규합해서 임상시험 데이터의 표준을 정의하였다.\n이제 미국 FDA나 일본의 PMDA와 같은 규제기관들도 신약개발 및 임상시험에서 데이터를 CDISC 표준으로 제출하기를 요구하게 되는 만큼 CDISC 표준은 널리 사용되고 있기에, 이에 대해서 잘 알고 있는 것이 중요하다.\n  image from Kor J Clin Pharmacol Ther \nCDISC에는 데이터와 관련된 SDTM (Study Data Tabulation Model) 과 ADaM (Analysis Data Model)이 있다.\n\nSDTM은 임상시험의 데이터를 제출하기 위해 정의한 표준으로, Raw Data를 정해진 형식으로 정리/정의한다.\nSDTM을 이용하여 데이터분석이 가능하도록 변환한 형태로 변환하는게 ADaM이다.  How we build ADaM from SDTM\n\n 즉, ADaM은 데이터의 도출과 분석, SDTM은 Raw Data를 형식에 맞게 잘 정리하여 테이블에 정렬하는 것!\n이 글에서는 ADaM에 대해서 알아본 후, tidyCDISC에 대해서 소개하고자 한다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#adam이란",
    "href": "posts/2023-04-28-tidycdisc/index.html#adam이란",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "2. ADaM이란?",
    "text": "2. ADaM이란?\n우선 ADaM의 데이터 구조에 대해서 배우기 전에, CDISC에서 정의한 ADaM의 Fundamental Principles 를 확인해 보자.\n  image from PharmaSUG \nADaM의 Fundamental Principles 는 데이터셋 구조가 아닌 분석 요구와 이해에 중점을 둔다. ADaM은 분석 가능한 데이터셋을 만드는 역할을 한다.\n ADaM에서 정의된 Data Structure는 4개가 있다:\n\nADSL (subject-level analysis dataset)\nBDS (basic data structure)\nOCCDS (occurence data structure)\nOTHER\n\n  copy of “ADaMIG v1.1 Figure 1.6.1 Categories of Analysis Datasets”\n\n이런 Data Structure가 왜 필요할까?\n\n분석을 위한 structure가 필요하기 때문이다!\n\n분석 목적에 따라 맞는 특정한 data structure를 사용한다.\n\n\n\n2.1. ADSL (Analysis Data Subject Level)\nADaM의 첫 번째 데이터 구조는 ADSL(Analysis Data Subject Level)이다.\nADSL은:\n\n개별 연구대상자를 가진다, contains one record per subject\n한 행에 대상자의 정보, 임상시험의 정보 등 SDTM의 한 대상자의 모든 자료가 들어가고, 다른 ADaM datasets의 분석에 필요한 data가 있다.\n주요 변수는 ID (USUBJID), 약물그룹(TRT01P), 시작일(RFSTDTC), 종료일 (RFENDTC) 등이 있다.\n\nADSL example in tidyCDISC\n 다른 변수들의 의미는 여기를 확인할 수 있다.\n\n\n2.2. BDS (Basic Data Structure)\nADaM의 두 번째 데이터 구조는 BDS(Basic Data Structure)이다.\n\n한 대상자에 대한 반복적인 혹은 여러 번의 결과가 나타나 있는 데이터이다.\n대상자, 분석 변수, 분석 시점별로 하나 이상의 데이터가 존재한다.\nBDS 에서는 분석하려는 매개변수(예: PARAM 및 관련 변수들)을 설명하고 분석할 값 (예: AVAL 및 AVALC 등 관련 변수들)을 포함하는 중앙 변수 집합이 포함된다.\n\nPARAM: 분석하고자 하는 값에 대한 설명\nAVAL/AVALC: 분석하고자 하는 값\n\nADSL 및 OCCDS와 같은 다른 데이터 구조의 기초 또는 시작점이기 때문에 “Basic”라고 한다.\n기본 데이터 집합(예: 치료, 인구학 및 안전성 데이터)을 처리하는 데 사용되는 데이터 구조이다.\n반복측정이 계획되어 있거나, 이미 반복적으로 측정한 값이다.\nBDS는 부작용이나 기타 발생 데이터(other occurrence data)의 발생률 분석은 지원하지 않는다.\n모든 ADSL 변수가 BDS dataset에 있을 필요가 없다.\n\n\n2.2.1. ADLB (Laboratory Data Analysis Dataset):\n\n검사 데이터, laboratory test results data.\n주요 변수는 ID (USUBJID), 검사항목 (LBTESTCD, LBTEST), 결과값 (LBORRES)이 있다.\n\n CDISC. (2011). Analysis Data Model Implementation Guide: ADaM Version 1.1.\n\n\n2.2.2. ADEFF (Analysis Dataset Definition):\n\n메타데이터 (다른 데이터를 설명해 주는 데이터) 테이블이다.\n분석하려는 data set의 내용 및 구조를 설명한다.\nADaM dataset를 만드는데 ADEFF table이 사용된다.\n\n The ADEFF table should be completed before creating the analysis datasets to ensure consistency in variable definitions and to allow traceability of the analysis datasets back to their source data.\n table from CDISC.org\n\n\n2.2.3. ADTTE (Analysis Data Time-to-Event):\n\n임상 시험에서 기록하고자하는 사건(event)의 발생 시간\nmore detailed information about ADTTE could be found here \n\n [PARAMCD:Parameter Code, STARTDT: Time to event origin date for subject, ADT: Analysis date, SRCDOM: Source Data] \n\n\n\n2.3. OCCDS (Occurence Data Structure)\nADaM Dataset structure 중 세 번째 OCCDS는 한 대상자에 대한 반복적인 결과가 나타난다는 점에서 BDS와 비슷하다.\n 하지만, OCCDS는 BDS처럼 반복적으로 측정되지 않으며, 한 대상자에 대한 결과가 한 건도 발생하지 않거나, 이와 반대로 무수히 많이 발생할 수도 있다.\n OCCDS 는 부작용과 같은 discrete event를 분석하는 데 사용된다.\nCDISC ADaM structure for OCCDS v1.0\n OCCDS의 종류를 알아보자!\n\n2.3.1. ADAE (Adverse Events Analysis Datset):\n\n부작용 정보에 대한 dataset\nOne record per subject per adverse event\n주요 변수는 임상시험 과정: ID (USUBJID), 부작용종류 (AETERM), 발생일 (AESTDTC), 해결일 (AEENDTC), 중증도(AESER)\n\nADAE example in tidyCDISC\n\n\n2.3.2. ADCM (Concomitant Medications Analysis Datase)\n\n복용 약물정보\n주요 변수는 ID (USUBJID), 약물명 (CMTRT), 시작일 (CMSTDTC), 종료일 (CMENDTC), 용량 (CMDOSFRQ)\nOne record or multiple records per subject per recorded medication occurrence or constantdosing interval\n\nexample ADCM dataset: \n\n\n\n2.4. OTHER\nOTHER의 dataset은:\n\nADaM의 Fudamental Principles 및 기타 ADaM 규칙(naming convention 등등)을 따르지만, ADaM의 정의된 3개의 데이터 구조(ADSL, BDS, OCCDS)를 따르지 않는다.\n\n details on other"
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#r-shiny-app-tidycdisc",
    "href": "posts/2023-04-28-tidycdisc/index.html#r-shiny-app-tidycdisc",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "3. R Shiny App tidyCDISC",
    "text": "3. R Shiny App tidyCDISC\ntidyCDISC는 오픈소스 프로그램이며, ADaM-ish 데이터로 인터랙티브한 표, 그래프, 그리고 환자들의 프로필 생성을 할 수 있는 shiny app이다.\n\ntidyCDISC의 데모를 같이 실행해보자 ▶️ https://rinpharma.shinyapps.io/tidyCDISC/\ntidyCDISC의 세 가지 주요 기능:\n\nDrag-and-Drop Table Generator\nPopulation Explorer (Graph Generator)\nIndividual Explorer/ Patient Profile Viewer\n\n 각 기능에 대해 세 개의 R 패키지를 사용한다:\n\nTable Generator ▶️ GT 패키지\nPopulation Explorer ▶️ plotly 패키지지\nPatient Profile Viewer ▶️ timevis 패키지지\n\n 차근차근 각 패널이 어떤 기능이 있는지 확인해 보자!\n\n3.1. Data Upload 패널\n맨 처음 tidyCDISC 데모에 들어가면 아래와 같이 Data Upload 패널이 랜딩 페이지로 보일 것이다. 데모에는 CDISC Pilot Data 예시 데이터가 사용되고 있다.\n\ntidyCDISC 앱은 ADaM(-ish) 데이터가 없으면 사용할 수가 없다. 앱을 실행하려면 최소한 ADSL sas7bdat 파일이 필요하며, 더 많은 데이터가 있을수록 더 많은 기능과 인사이트를 탐색할 수 있다.\n\n\n\n3.2. Table Generator 패널\n다음 Table Generator 탭에 들어가면 인터랙티브한 테이블을 만들 수 있다.\nTable Generator 탭은 두 개로 구분된다. 왼쪽 영역은 테이블을 만드는 데 사용되는 드래그 앤 드롭 인터페이스이고 오른쪽 영역은 실시간 테이블 출력한다.\n우선 테이블을 생성하려면 왼쪽의 변수 블록을 “Variable” 드롭 영역으로 끌어다 놓고, ANOVA, CHG, MEAN, FREQ 등등을 “Stats” 드롭 영역에 끌어다 놓으면 된다.\n\n\n변수들 위에 Standard Analysis Tables 드롭 다운에는 규제 당국에 제출할 문서에 공통으로 포함되는 테이블 list가 나온다.\n\n테이블 중 하나를 선택하면, 해당 테이블을 생성할 때 필요한 변수와 Stats가 올바른 순서로 선택되어 원하는 테이블을 생성합니다.\n\nGroup Data By 드롭다운을 사용하여 범주형 변수들의 통계량을 계산할 수도 있다.\n\n\n\n밑에 Table Title로 테이블의 이름을 설정하고 파일로(RTF, CSV, 그리고 HTML) 저장이 가능하다.\n\n\n\n\n\n3.3. Population Explorer 패널\n다음으로 Population Explorer 탭에서 여러 차트로 데이터 시각화를 할 수 있다.\n\n\n\n\n\n\n\n\n\n\nType of Chart:\n\n원하는 차트의 종류를 고를 수 있다.\n\nGeneral plot controls:\n\n차트 유형에 따라 바뀐다.\n일반적으로 BDS 데이터 소스의 변수 또는 매개 변수 등을 사용하여 축을 설정하도록 된다.\n\n\n\n\n\n위 설정에 따라 메인 패널에는 아래와 같은 인터랙티브 그래프가 표시될 것이다.\n\n\n\n\n3.4. Individual Explorer 패널\n마지막 탭은 Individual Explorer이다. 이 탭에서는 특정 환자 데이터를 탐색하기 위해 사용됩니다. 처음 들어가시면 환자의 USUBJID로 환자를 선택할 수 있는 기능이 있습니다. 특정 그룹에 포함된 환자 데이터를 탐색해야 할 경우 (예를 들어, 나이가 10세 이하), Advanced Pre-Filtering을 이용하면 된다.\n\n Details on filtering. \n 특정 환자를 선택한 후, 밑에 Events에서는 환자 타임라인과 events에 대한 데이터 테이블을 확인할 수 있다.\n\nEvents 탭 바로 옆에 Visits탭에서는 BDS data sets에서의 PARAMS과 Study Visit의 plot과 데이터 테이블을 보여준다.\n\n환자의 특정 변수별 plot을 담은 파일을 png 혹은 html 파일로 다운 가능하다."
  },
  {
    "objectID": "posts/2023-04-28-tidycdisc/index.html#마치며",
    "href": "posts/2023-04-28-tidycdisc/index.html#마치며",
    "title": "ADaM in CDISC and tidyCDISC",
    "section": "마치며",
    "text": "마치며\n지금까지 CDISC의 ADaM에 대해서 4개의 data structure를 살표보며 알아갔고, ADaM 데이터를 활용한 오픈소스 프로그램인 tidyCDISC를 사용하는 방법에 대해서 배웠다.\n여러 해외 규제기관에서 임상이나 비임상시험 데이터 제출시 CDISC 적용을 의무하고 있는 만큼, CDISC 표준을 잘 이해하고 자료 관리에 있어 적용이 필요해 보인다.\nCDISC의 전문가가 되지 않는한 어렵고 복잡한 것을 알 필요는 없지만, 임상시험 결과가 ADaM을 이용해서 만들어야 하기 때문에 적어도 CDISC data format과 임상시험의 전체 흐름에 대해서 잘 파악하고 있는 것이 중요할 것 같다.\n\n\nReference\n\n\nJeong, Sunok, et al. “International Standard in Electronic Clinical Trial.” Journal of Korean Society for Clinical Pharmacology and Therapeutics, vol. 15, no. 1, Korean Society for Clinical Pharmacology and Therapeutics, 2007, p. 20. Crossref, https://doi.org/10.12793/jkscpt.2007.15.1.20.\n“Get Started With {tidyCDISC}.” Get Started With {tidyCDISC}, cran.r-project.org/web/packages/tidyCDISC/vignettes/getting_started.html.\n“SDTMIG v3.3.” SDTMIG v3.3 | CDISC, www.cdisc.org/standards/foundational/sdtmig/sdtmig-v3-3/html#Representing+Relationships+and+Data.\n“TidyCDISC an Open Source Application to Interactively Create Tables, Figures, and Patient Profiles.” YouTube, 17 Aug. 2020, www.youtube.com/watch?v=EFGkHrV0WbY.\nLi, Chengxin. “The Dataset Generation for Survival Analysis With the ADaM Basic Data Structure for Time-to-Event Analyses (ADTTE) Standard.” Pharmaceutical Programming, vol. 5, no. 1–2, Informa UK Limited, Dec. 2012, pp. 1–4. Crossref, https://doi.org/10.1179/1757092112z.0000000001."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html",
    "href": "posts/2021-09-28-shinyecrf2/index.html",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이티 입력웹을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "href": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "요약",
    "text": "요약\n4월 발표 개발완료 후 서비스 중\n\n수백개 변수 추가, 입력화면 디자인 개선(Thanks to 김진환)\nshinydashboard 적용, 환자수 대시보드 추가(Thanks to 고현준)\nshinymanager 로 로그인 모듈: 특정 ID만 삭제권한.\n\n타 연구 빠른적용 위해 모듈화 필요, 설문지 Builder 가능할까?"
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "href": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea2 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html",
    "href": "posts/2024-03-14-process-macro/index.html",
    "title": "Process macro 소개",
    "section": "",
    "text": "Process macro에 대해 알아보고 R에서 사용가능한 패키지를 소개합니다."
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#매개효과",
    "href": "posts/2024-03-14-process-macro/index.html#매개효과",
    "title": "Process macro 소개",
    "section": "매개효과",
    "text": "매개효과\n매개효과 분석은 설명변수가 반응변수에 영향을 미치는 경로, 매커니즘을 확인하기 위한 분석방법입니다. 단순매개모형(4번 모델)은 다음과 같은 다이아그램으로 표현할 수 있습니다.\n\n\n\n\nFigure.1\n\n\n\n먼저 R에서 process macro를 사용하려면 패키지를 설치하거나 파일을 다운받아야 합니다. 패키지로는 가톨릭대학교 문건웅 교수님이 만든 processR이라는 패키지를 다음과 같이 다운로드하고 불러올 수 있습니다.\n\ndevtools::install_github(\"cardiomoon/processR\")\nlibrary(processR)\n\nprocessR 패키지를 사용하려면 lavaan 패키지가 필요합니다. 아래 코드로 다운로드하고 불러올 수 있습니다.\n\ninstall.packages(\"lavaan\")\nlibrary(lavaan)\n\n아래 코드로 processR 패키지에서 지원하는 모델의 번호를 확인할 수 있습니다.\n\npmacro$no\n\n [1]  0.0  1.0  2.0  3.0  4.0  4.2  5.0  6.0  6.3  6.4  7.0  8.0  9.0 10.0 11.0\n[16] 12.0 13.0 14.0 15.0 16.0 17.0 18.0 19.0 20.0 21.0 22.0 23.0 24.0 28.0 29.0\n[31] 30.0 31.0 35.0 36.0 40.0 41.0 45.0 49.0 50.0 58.0 59.0 60.0 61.0 62.0 63.0\n[46] 64.0 65.0 66.0 67.0 74.0 75.0 76.0 25.0 26.0 27.0 58.2  4.3\n\n\n직접 다운받아 사용하시려면 process macro를 개발한 Andrew F. Hayes가 제공하는 파일을 여기서 내려받을 수 있습니다. process.R파일을 실행시키거나 분석을 진행할 R파일 상단에 source(\"process.R\")코드를 실행하면 함수를 사용할 수 있습니다. processR패키지와 process.R파일은 서로 다른 도구이니 혼동하지 않도록 주의해야 합니다. 이 포스트에서 processR 패키지에서 제공하는 함수는 코드 상단에 # processR로, process.R에서 제공하는 함수는 # process.R로 주석을 달아놓겠습니다.\n예시 데이터로 단순매개효과를 설명해보겠습니다.\n미국의 1,338명의 의료비용에 대한 데이터입니다.\n\ncost &lt;- read.csv(\"Medical_Cost.csv\")\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.924\n\n\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.552\n\n\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.462\n\n\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.471\n\n\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.855\n\n\n\n\n\nprocess.R의 process()함수를 실행해보겠습니다. 인자는 다음과 같습니다.\n\ndata = 데이터셋\nx = 설명변수\ny = 반응변수\nm = 매개변수\nmodel = 모델번호\nboot = 부트스트래핑 횟수\ntotal = 총효과 출력(0이면 출력하지 않음)\n\n\ncost$sex &lt;- ifelse(cost$sex == \"male\", 1, 0)\ncost$smoker &lt;- ifelse(cost$smoker == \"yes\", 1, 0)\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", m = \"bmi\", model = 4, boot = 0, total = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 4      \n    Y : charges\n    X : smoker \n    M : bmi    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: bmi\n\nModel Summary: \n          R      R-sq       MSE         F       df1       df2         p\n     0.0038    0.0000   37.2152    0.0188    1.0000 1336.0000    0.8910\n\nModel: \n             coeff        se         t         p      LLCI      ULCI\nconstant   30.6518    0.1870  163.8953    0.0000   30.2849   31.0187\nsmoker      0.0567    0.4133    0.1371    0.8910   -0.7541    0.8674\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8111    0.6579 50238769.3992 1283.9234    2.0000 1335.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -3459.0955  998.2795   -3.4651    0.0005 -5417.4628 -1500.7282\nsmoker   23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\nbmi        388.0152   31.7875   12.2065    0.0000   325.6564   450.3741\n\n************************ TOTAL EFFECT MODEL *************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.7873    0.6198 55804130.1996 2177.6149    1.0000 1336.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant  8434.2683  229.0142   36.8286    0.0000  7985.0017  8883.5348\nsmoker   23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\n************ TOTAL, DIRECT, AND INDIRECT EFFECTS OF X ON Y ************\n\nTotal effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23615.9635  506.0753   46.6649    0.0000 22623.1748 24608.7523\n\nDirect effect of X on Y:\n      effect        se         t         p       LLCI       ULCI\n  23593.9810  480.1805   49.1357    0.0000 22651.9905 24535.9715\n\nIndirect effect(s) of X on Y:\n       Effect\nbmi   21.9825\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n우선 Figure.1에서 보았던 경로의 이름을 정하겠습니다.\\(X → M : a\\)\\(M → Y : b\\)\\(X → Y : c'\\) 단순매개모형에서 설명변수가 0, 1로 이루어진 변수일때, \\(a = [\\bar{M}|(X = 1)] - [\\bar{M}|(X = 0)] = 0.0567\\)이며 X가 1일때 M의 평균과 X가 0일때 M의 평균의 차이와 같고 lm(bmi ~ smoker, data = cost)의 기울기와 같습니다.\\(b = [\\hat{Y}|(M = m, X = x)] - [\\hat{Y}|(M = m - 1, X = x)] = 388.0152\\)이며 이는 lm(charges ~ smoker + bmi, data = cost)에서 bmi의 기울기와 같고 아래처럼 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\ncost.1 &lt;- cost\ncost.1$bmi &lt;- cost.1$bmi - 1\n\npredict(model, cost)[1] - predict(model, cost.1)[1]\n\n       1 \n388.0152 \n\n\n\\(c' = [\\hat{Y}|(X = x, M = m)] - [\\hat{Y}|(X = x - 1, M = m)] = 23593.9810\\)이며 lm(charges ~ smoker + bmi, data = cost)의 smoker의 기울기와 같고 다음과 같이 계산할 수도 있습니다.\n\nmodel &lt;- lm(charges ~ bmi + smoker, data = cost)\n\nsmoke1 &lt;- cost\nsmoke1$smoker &lt;- 1\n\nsmoke0 &lt;- cost\nsmoke0$smoker &lt;- 0\n\npredict(model, smoke1)[1] - predict(model, smoke0)[1]\n\n       1 \n23593.98 \n\n\n단순매개모형에서 \\(ab\\)를 간접효과, \\(c'\\)을 직접효과, 이 둘을 더한 값을 \\(c\\)(총효과)라고 하며 총효과는 lm(charges ~ smoker, data = cost)의 기울기와 같습니다. 간접효과는 매개변수를 통했을 때 흡연자는 비흡연자보다 의료비용이 21.9825만큼 높다는 것을 의미하며, 직접효과는 매개변수가 고정되어있을 때 흡연자는 의료비용이 23593.981만큼 더 높다는 것을 의미합니다. 이제 processR 패키지를 실행해보겠습니다.\n\n# processR\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", M = \"bmi\")\nmeanSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\n \nY\nM\nY\n\n\n\n\ncharges\nbmi\nadjusted\n\n\n\n\nsmoker(X) = 0\nMean\n8434.268\n30.652\n8438.77\n\n\n\nSD\n5993.782\n6.043\n\n\n\nsmoker(X) = 1\nMean\n32050.232\n30.708\n32032.751\n\n\n\nSD\n11541.547\n6.319\n\n\n\n\nMean\n13270.422\n30.663\n\n\n\n\nSD\n12110.011\n6.098\n\n\n\n\n\n\n\nAdjusted mean은 \\(adjusted\\;mean(\\bar{Y}^*) = i_{Y} + b\\bar{M} + c'X\\)로 계산할 수 있습니다. 설명변수가 0일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 0\\)이고 설명변수가 1일때는 \\(\\bar{Y}^* = -3459.10 + 388.02 * 30.6634 + 23593.98 * 1\\)로 계산할 수 있습니다. 보정평균은 \\(X\\)일때 평균적인 \\(M\\)의 값을 가지는 사람은 보정평균만큼의 \\(Y\\)를 갖는다는 것을 의미합니다.\n아래처럼 각 계수를 깔끔하게 출력하는 함수도 존재합니다.\n\n# processR\nmodelsSummaryTable(labels = labels, data = cost)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\nbmi(M)\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\na\n0.057\n0.413\n0.137\n.891\n\nc'\n23593.981\n480.180\n49.136\n&lt;.001\n\n\nbmi(M)\n\n\n\n\n\n\nb\n388.015\n31.787\n12.207\n&lt;.001\n\n\nConstant\niM\n30.652\n0.187\n163.895\n&lt;.001\n\niY\n-3459.096\n998.279\n-3.465\n.001\n\n\nObservations\n\n1338\n\n\n1338\n\n\nR2\n\n0.000\n\n\n0.658\n\n\nAdjusted R2\n\n-0.001\n\n\n0.657\n\n\nResidual SE\n\n6.100 ( df = 1336)\n\n\n7087.931 ( df = 1335)\n\n\nF statistic\n\nF(1,1336) = 0.019, p = .891\n\n\nF(2,1335) = 1283.923, p &lt; .001\n\n\n\n\n\n\n간접효과, 직접효과, 총효과를 다음 함수로 출력할 수 있습니다.\n\n# processR\nmodel &lt;- tripleEquation(labels = labels)\nsemfit &lt;- sem(model = model, data = cost)\n\nmedSummaryTable(semfit)\n\n\n\n\n\nEffect\nEquation\nestimate\n95% CI\n\n\n\nindirect\n(a)*(b)\n21.983\n(-292.098 to 336.063)\n\n\ndirect\nc\n23593.981\n(22653.900 to 24534.062)\n\n\ntotal\ndirect+indirect\n23615.964\n(22624.816 to 24607.111)\n\n\nprop.mediated\nindirect/total\n0.001\n(-0.012 to 0.014)"
  },
  {
    "objectID": "posts/2024-03-14-process-macro/index.html#조절효과",
    "href": "posts/2024-03-14-process-macro/index.html#조절효과",
    "title": "Process macro 소개",
    "section": "조절효과",
    "text": "조절효과\n조절효과는 설명변수가 반응변수에 미치는 영향이 다른 변수에 의해 변화될 때, 이 변화를 조절효과라고 하며, 이러한 영향을 주는 변수를 조절변수라고 합니다.\n단순조절효과(1번모델)는 다음의 다이아그램으로 나타낼 수 있습니다.\n\n\n\n\nFigure.2\n\n\n\nprocess()함수로 단순조절효과를 알아보겠습니다. plot 인자는 출력결과 하단에 테이블을 만드어줍니다.\n\n# process.R\nprocess(data = cost, x = \"smoker\", y = \"charges\", w = \"age\", model = 1, plot = 1)\n\n\n********************* PROCESS for R Version 4.3.1 ********************* \n \n           Written by Andrew F. Hayes, Ph.D.  www.afhayes.com              \n   Documentation available in Hayes (2022). www.guilford.com/p/hayes3   \n \n*********************************************************************** \n               \nModel : 1      \n    Y : charges\n    X : smoker \n    W : age    \n\nSample size: 1338\n\n\n*********************************************************************** \nOutcome Variable: charges\n\nModel Summary: \n          R      R-sq           MSE         F       df1       df2         p\n     0.8495    0.7217 40903347.7298 1153.1995    3.0000 1334.0000    0.0000\n\nModel: \n              coeff        se         t         p       LLCI       ULCI\nconstant -2091.4206  582.5654   -3.5900    0.0003 -3234.2647  -948.5764\nsmoker   22385.5487 1278.7311   17.5061    0.0000 19877.0057 24894.0917\nage        267.2489   13.9285   19.1872    0.0000   239.9247   294.5731\nInt_1       37.9887   31.0950    1.2217    0.2220   -23.0116    98.9890\n\nProduct terms key:\nInt_1  :  smoker  x  age      \n\nTest(s) of highest order unconditional interaction(s):\n      R2-chng         F       df1       df2         p\nX*W    0.0003    1.4925    1.0000 1334.0000    0.2220\n----------\nFocal predictor: smoker (X)\n      Moderator: age (W)\n\nData for visualizing the conditional effect of the focal predictor:\n     smoker       age    charges\n     0.0000   22.0000  3788.0555\n     1.0000   22.0000 27009.3554\n     0.0000   39.0000  8331.2870\n     1.0000   39.0000 32198.3946\n     0.0000   56.0000 12874.5186\n     1.0000   56.0000 37387.4338\n\n******************** ANALYSIS NOTES AND ERRORS ************************ \n\nLevel of confidence for all confidence intervals in output: 95\n\n\n단순조절효과의 계수는 lm(charges ~ smoker + age + smoker * charges, data = cost)의 계수와 동일합니다.\n\\(\\hat{Y} = i_{Y} + b_{1}X + b_{2}W + b_{3}XW\\)일때, \\(b_{1} = 23385.55\\), \\(b_{2} = 267.25\\), \\(b_{3} = 37.99\\)이며 각 계수를 다음과 같은 의미를 가지고 있습니다.\n\n\\(b_{1} = W\\)가 \\(0\\)일때 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(X\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{X→Y} = b_{1} + b_{3}W\\)로 계산합니다.\n\\(b_{2} = X\\)가 \\(0\\)일때 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과이고 \\(W\\)가 \\(Y\\)에 미치는 조건부 효과는 \\(\\theta_{W→Y} = b_{2} + b_{3}X\\)로 계산합니다.\n\\(b_{3} = W\\)가 한 단위 바뀔 때, \\(X\\)의 한 단위 변화가 \\(Y\\)에 영향을 미치는 정도의 차이입니다.\n\nproceeR 패키지로 확인해보겠습니다.\n\nlabels &lt;- list(X = \"smoker\", Y = \"charges\", W = \"age\")\nmodel &lt;- lm(charges ~ smoker + age + smoker * age, data = cost)\n\n# processR\nm.summary &lt;- modelsSummary(list(model), labels = labels)\nmodelsSummaryTable(m.summary)\n\n\n\n\n\n\n\nConsequent\n\n\n\n\ncharges(Y)\n\n\nAntecedent\n\nCoef\nSE\nt\np\n\n\n\n\nsmoker(X)\nc1\n22385.549\n1278.731\n17.506\n&lt;.001\n\n\nage(W)\nc2\n267.249\n13.929\n19.187\n&lt;.001\n\n\nsmoker:age(X:W)\nc3\n37.989\n31.095\n1.222\n.222\n\n\nConstant\niY\n-2091.421\n582.565\n-3.590\n&lt;.001\n\n\nObservations\n\n1338\n\n\nR2\n\n0.722\n\n\nAdjusted R2\n\n0.721\n\n\nResidual SE\n\n6395.573 ( df = 1334)\n\n\nF statistic\n\nF(3,1334) = 1153.199, p &lt; .001"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html",
    "href": "posts/2023-07-01-officer/index.html",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "",
    "text": "이번 글에서는 R 패키지 officer를 사용하여 PPT 프레젠테이션에 벡터 그래픽을 만드는 과정에 대해 소개합니다. officer를 포함하여 몇몇 패키지들이 officeverse라고 불리는 생태계를 구성하고 있으며 PPT외에도 엑셀이나 워드로 R의 결과를 만들어 낼 수 있습니다만, 이번 글에서는 벡터 그래픽을 저장하는 목적으로의 officer에 한정합니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#오브젝트-만들기",
    "href": "posts/2023-07-01-officer/index.html#오브젝트-만들기",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "1. 오브젝트 만들기",
    "text": "1. 오브젝트 만들기\n\nppt &lt;- read_pptx()\n\nprint(ppt)\n\npptx document with 0 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\n\nread_pptx는 원래 ppt 파일을 R 오브젝트 형태로 읽기 위한 함수이지만, 만약 함수에 파일을 입력하지 않으면 새로운 ppt 오브젝트를 생성합니다.\n\nppt &lt;- read_pptx(\"mypptx.pptx\") # 기존 ppt 읽기 \nppt &lt;- read_pptx() # 새로운 ppt 생성\n\n한편 officer에는 read_pptx 외에도 read_docx(워드), read_xlsx(엑셀)도 존재합니다.\nppt 오브젝트를 콘솔에 입력하면, 몇개의 슬라이드로 구성되어있는지 확인할 수 있습니다. (layout과 master는 신경쓰지 않으셔도 좋습니다.)"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#슬라이드-만들기",
    "href": "posts/2023-07-01-officer/index.html#슬라이드-만들기",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "2. 슬라이드 만들기",
    "text": "2. 슬라이드 만들기\n처음 만든 ppt 오브젝트에는 pptx document with 0 slide(s), 즉 슬라이드가 없습니다.\n이 오브젝트에 슬라이드를 추가하는 것은 add_slide()로 할 수 있습니다.\n\nppt |&gt;\n  add_slide() # ppt &lt;- ppt |&gt; add_slide() 로 안해도 됨\n\npptx document with 1 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\nppt # pptx document with 1 slide(s)\n\npptx document with 1 slide(s)\nAvailable layouts and their associated master(s) are:\n             layout       master\n1       Title Slide Office Theme\n2 Title and Content Office Theme\n3    Section Header Office Theme\n4       Two Content Office Theme\n5        Comparison Office Theme\n6        Title Only Office Theme\n7             Blank Office Theme\n\n\nadd_slide()에는 layout과 master라는 옵션을 지정할 수 있고 가능한 값은 다음과 같습니다.\n\nTitle Slide\nTitle and Content (기본값)\nSection Header\nTwo Content\nComparison\nTitle Only\nBlank\n\n아마 눈치 채셨을 수도 있겠지만 레이아웃은 콘솔에서 ppt 오브젝트를 확인할 때 나오는 것들이며,\n우리의 목적은 슬라이드 구성이 아닌 이미지 저장이기 때문에 어떤 값을 선택해도 동일한 결과를 얻을 수 있습니다.\n아래의 이미지는 각 옵션들을 적용하여 만든 슬라이드의 결과물로 모두 동일한 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#벡터-그래픽스-슬라이드에-추가",
    "href": "posts/2023-07-01-officer/index.html#벡터-그래픽스-슬라이드에-추가",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "3. 벡터 그래픽스 슬라이드에 추가",
    "text": "3. 벡터 그래픽스 슬라이드에 추가\n앞서 만든 ggplot의 결과를 ph_with이라는 함수로 슬라이드에 추가할 수 있습니다.\n\nppt |&gt; ph_with( # paragraph의 ph가 아닐까 생각\n  dml(ggobj = plotObj), # 앞에서 만들었던 ggplot 이미지 오브젝트\n  location = ph_location_fullsize() # 쉬운 편집을 위해 이미지의 크기를 슬라이드에 가득 채움\n)\n\n여기서 dml은 DrawingML이라는 오피스 프로덕트(pptx)에 XML로 이미지를 만들기 위한 내용입니다. location에는 다른 옵션도 있지만 ph_location_fullsize를 권장합니다\n만약 여러개의 이미지를 여러장의 슬라이드로 집어넣어 만들고 싶다면 다음처럼 ph_with를 pipe (|&gt;)로 이어서 사용 할 수 있습니다.\n\nppt |&gt; \n  add_slide() |&gt; # 1번째 슬라이드 \n  ph_with(\n    dml(ggobj = plotObj),\n    location = ph_location_fullsize()\n  ) |&gt;\n  add_slide() |&gt; # 2번째 슬라이드\n  ph_with( \n    dml(ggobj = plotObj2),\n    location = ph_location_fullsize()\n  ) |&gt;\n  add_slide() |&gt; # 3번째 슬라이드 \n  ph_with( \n    dml(ggobj = plotObj3),\n    location = ph_location_fullsize()\n  )"
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#ppt-저장",
    "href": "posts/2023-07-01-officer/index.html#ppt-저장",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "4. ppt 저장",
    "text": "4. ppt 저장\n마지막으로 add_slide와 ph_with를 통해 만든 슬라이드는 print로 현재 작업중인 디렉토리에 (getwd()로 확인) 저장할 수 있습니다.\n\ngetwd() # ppt가 저장되는 위치\nppt |&gt; \n  print(target = \"myPrint.pptx\")\n\n이렇게 만들어진 pptx는 파워포인트와 키노트, 그리고 구글 슬라이드에서 작업할 수 있습니다."
  },
  {
    "objectID": "posts/2023-07-01-officer/index.html#정리",
    "href": "posts/2023-07-01-officer/index.html#정리",
    "title": "R의 officer 패키지를 활용하여 PPT 편집을 위한 벡터 그래픽스 만들기",
    "section": "정리",
    "text": "정리\n위 4개의 단계를 1개의 코드로 연결하면 다음과 같습니다.\n\nlibrary(officer)\nlibrary(rvg)\nlibrary(ggplot2)\n\n# 이미지 생성\nplotObj &lt;- iris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n# ppt\n\nread_pptx() |&gt; # ppt 생성, 별도의 오브젝트로 저장하지 않아도 됨.\nadd_slide() |&gt; # 슬라이드 추가\n  ph_with( # 이미지 추가\n    dml(ggobj = plotObj), \n    location = ph_location_fullsize() \n  ) |&gt;\nprint('image.pptx') # ppt 저장 \n\n한편, officer를 활용하여 더 자세한 ppt 생성과 편집도 가능하지만, 이 글에서는 다루지 않으며 quarto를 활용한 revealjs 슬라이드 생성하는 방법을 링크로 대신 첨부해드립니다."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html",
    "href": "posts/2023-03-17-pkgdown/index.html",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "",
    "text": "R 패키지를 개발할 때, 개발 자체도 힘들지만, 패키지를 잘 설명하는 문서화 역시 매우 중요합니다.\n아무리 좋은 기능을 개발해두었어도, 어떤 기능이 있는지, 어떤 방식으로 사용할 수 있는지… 등을 작성해 두지 않으면 (코드를 열어보기 전까진 모르기 때문에) 패키지를 사용하려는 사람들로부터 사랑받기 어렵습니다.\n특히나, 패키지를 설치해서 ? 를 통해 확인할 수 있는 것과, 설치 하기 전에 깃헙의 패키지 리포지토리에서 확인할 수 있는 것은 꽤 차이가 큽니다.\n하지만 문서화와 이 결과물을 웹페이지로 만드는 것은 많은 시간과 노력이 필요한 작업입니다.\n이런 문제를 해결하기 위해 pkgdown이라는 R 패키지가 등장했습니다. 이번 글에서는 pkgdown을 사용하여 R 패키지를 문서화하는 페이지를 만드는 방법을 소개합니다."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#pkgdown-설치-및-환경-설정",
    "href": "posts/2023-03-17-pkgdown/index.html#pkgdown-설치-및-환경-설정",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "pkgdown 설치 및 환경 설정",
    "text": "pkgdown 설치 및 환경 설정\n위에서 서술한 것처럼 pkgdown 또한 하나의 R package이기 때문에 설치를 해야합니다.\n\n# install.packages('pkgdown') CRAN version\nremotes::install_github('r-lib/pkgdown') # Github version\n\nlibrary(pkgdown)\nlibrary(usethis) \n\npkgdown은 패키지의 웹페이지를 만드는 역할을 하기 때문에 pkgdown의 “대상이 되는” R 패키지로 pkgdown.tutorial이라는 간단한 패키지를 먼저 만들었습니다. (위 링크 참조)\npkgdown.tutorial\n\n💡 ttest와 ttest2는 동일한 내용의 함수이며, roxygen2의 효과를 보기 위해 비교용도로 사용합니다.\n\n\n# ttest.R (= ttest2.R)\nttest &lt;- function(x, y = NULL, alternative = 'two.sided', \n                  mu = 0, paired = FALSE, var.equal = FALSE, \n                  conf.level = 0.95, ...){\n  t.test(x, y, alternative, mu, paired, var.equal, conf.level, ...)\n}\n\n패키지에 ttest와 ttest2라는 함수를 만들고 패키지 빌드 직후의 구성 상태는 아래와 같습니다.\n/pkgdown.tutorial\n  - .gitignore\n  - .Rbuildignore\n  - DESCRIPTION\n  - NAMESPACE\n  - pkgdown.tutorial.Rproj\n  - /R\n    - ttest.R\n    - ttest2.R\n이후 pkgdown.tutorial의 작업 디렉토리에서 (.Rproj를 열어) usethis::use_pkgdown()을 실행합니다.\n그 결과 아래 이미지처럼 _pkgdown.yml이라는 파일이 생기는 것을 확인 할 수 있습니다.\n\n\n💡 .gitignore에서 docs를 삭제해주세요.\n\nbuild_site\npkgdown의 핵심 코드를 하나만 고르라면 pkgdown::build_site()입니다.\n이는 현재 작업된 내용들을 기반으로 웹사이트를 만드는 역할을 하는 함수입니다.\n바로 실행해보면 아래와 같은 결과를 확인할 수 있습니다.\n\n💡 library(pkgdown)을 실행했다면 앞의 pkgdown::은 붙이지 않아도 좋습니다"
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#pkgdown의-구성-요소",
    "href": "posts/2023-03-17-pkgdown/index.html#pkgdown의-구성-요소",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "pkgdown의 구성 요소",
    "text": "pkgdown의 구성 요소\npkgdown에서 웹페이지 제작을 위해 제공하는 주요 요소들을 소개하겠습니다.\n_yml\n_pkgdown.yml은 보여지는 웹사이트를 구성하는 파일입니다.\n\n💡 yml은 들여쓰기 (indent)를 깐깐하게 사용하기 때문에 에러가 난다면 이를 확인해보는 것이 좋습니다.\n들여쓰기를 하나도 하지 않은 (처음 url과 같은 위치) 경우를 lv0이라 표현합니다.\n\n1. template (lv0)\ntemplate: \n  bootstrap: 5\n  bootswatch: flatly \n처럼 변경하여 웹사이트의 테마를 바꿀 수 있습니다. 아래의 예시에서는 flatly를 사용했습니다.\n\ntheme에서 사용할 수 있는 옵션은 bootswatch의 theme를 소문자로 입력한 값이며, 필요한 경우 bslib 옵션을 활용하여 더 자세한 커스터마이즈를 할 수 있습니다.\ntemplate:\n  bootstrap: 5\n  bslib:\n    bg: \"#202123\"\n    fg: \"#B8BCC2\"\n    primary: \"#306cc9\"\n2. navbar (lv0)\nnavbar는 웹페이지 윗부분의 네비게이션 바 구성을 설정할 수 있는 옵션입니다.\n만약 yml의 내용을\nnavbar:\n  structure:\n    left:  [intro, reference, articles, tutorials, news]\n    right: [search, github]\n처럼 작성한다면\n\n\nnavbar의 왼쪽정렬로 intro, reference., articles, tutorials, news\n\n\n오른쪽 정렬로 search, github를 배치할 수 있습니다.\n\n위의 예시에서 표기된 기본 제공되는 구성요소의 설명은 이러합니다.\n\n\nintro: Get Started 페이지\n\nreference: R패키지의 함수(예시의 ttest) 기능 설명\n\narticles: 추가로 만든 rmd 아티클 파일\n\ntutorials: 튜토리얼 (개인적으로는 헷갈리니 articles로의 사용을 권장합니다)\n\nnews: NEWS.md 설명\n\nsearch: 웹페이지의 검색창\n\ngithub: 패키지를 담고 있는 깃헙 리포지토리 링크 (패키지 DESCRIPTION 에서 설정)\n2-1. intro\nGet Started 페이지는 패키지와 동일한 이름을 갖는 rmd(예시는 pkgdown.tutorial.rmd)로 아티클을 추가해야만 합니다.\n아티클을 추가하는 것에 대해서는 아래에서 자세하게 다루겠습니다. (지금은 아래 코드를 실행만 하면 됩니다.)\n\n💡 usethis::use_article(“pkgdown.tutorial”, “intro”)\n\n2-2. reference\nttest.R에서 roxygen2를 활용하여 함수 description을 만들고 나면 그 결과가 reference에 나타납니다. (/man 디렉토리에 .rd 파일을 생성합니다.)\nroxygen2에서 사용 가능한 태그의 종류는 다양하며, 보통은 @import, @export, @title, @description, @details, @param, @returns, @examples 정도가 권장됩니다.\n앞서 만들었던 ttest.R에 아래 내용을 코드의 맨 위에 추가한 다음, CTRL/CMD + SHIFT + D를 통해 일부 내용만 reference를 만들어 보겠습니다. (ttest2.R은 비교용)\n#' @title ttest\n#' @description run t test\n#' @details\n#' alternative = \"greater\" is the alternative that x has a larger mean than y. For the one-sample case: that the mean is positive.\n#' If paired is TRUE then both x and y must be specified and they must be the same length. \n#' Missing values are silently removed (in pairs if paired is TRUE). \n#' If var.equal is TRUE then the pooled estimate of the variance is used. \n#' By default, if var.equal is FALSE then the variance is estimated separately \n#' for both groups and the Welch modification to the degrees of freedom is used.\n#' If the input data are effectively constant (compared to the larger of the two means) \n#' an error is generated.\n#' @param x a (non-empty) numeric vector of data values.\n#' @param y an optional (non-empty) numeric vector of data values.\n#' @returns A list with class \"htest\" containing the following components:\n#' @examples t.test(1:10, y = c(7:20))      # P = .00001855\n#' @export\n이후 pkgdown::build_site()를 실행하면 아래 이미지처럼 reference 페이지가 navbar에 생성 되는 것을 확인할 수 있습니다.\n\n2-3. articles\nusethis::use_article(&lt;ARTICLENAME&gt;, &lt;PAGETITLE&gt;)의 형태로 사용 할 수 있습니다.\n\n💡 여기서 ARTICLENAME에는 숫자, 문자 그리고 -와 _ 만 활용할 수 있습니다. (소문자를 권장합니다)\n\nusethis::use_article(\"using-ttest\", \"perform t-test\") 코드를 실행하면 using-ttest.Rmd라는 파일이 생성되며 build_site()를 통해 그 결과를 반영 할 수 있습니다.\n\n3. components (navbar &gt; lv1)\nnavbar에서 소개 되지 않은, 기본 제공 되지 않는 구성요소는 아래처럼 작성하여 사용할 수 있습니다.\nnavbar:\n components:\n   articles: \n    text: Articles\n    menu:\n    - text: Category A\n    - text: Title A1\n      href: articles/a1.html\n    - text: Title A2\n      href: articles/a2.html\n    - text: -------\n    - text: \"Category B\"\n    - text: Article B1\n      href: articles/b1.html\n이는 이렇게 해석할 수 있습니다.\nArticles라는 (text:) 메뉴의 하위 구성요소로\n\nCategory A (그룹)\nTitle A1 (a1.rmd에서 생성)\nTitle A2 (a2.rmd에서 생성)\n구분선 (——)\nCategory B (그룹)\nArticle B1 (b1.rmd에서 생성)\n\n\n💡 usethis::use_article(“a1”,“A1 article”)…로 a1,a2,b1 아티클을 추가하세요\n\n4. footer (lv0)\n크게 중요한 것은 아니지만, 모든 페이지에 공통으로 나타날 수 있게 하는 역할을 합니다.\nfooter:\n  structure: \n    left: developed_by\n    right: built_with\n\n5. DESCRIPTION\n자세한 설명은 링크를 참조하세요.\n원래는 패키지 개발을 하면서 채워졌어야 하지만, pkgdown.tutorial에서는 미처 채워지지 못한 부분들로 아래와 같이 채우겠습니다.\n\n💡 먼저 usethis::use_mit_license()등을 통해 라이센스를 설정하고, 그 다음 DESCRIPTION을 채우는 것을 권장합니다.\n\nPackage: pkgdown.tutorial\nTitle: tutorial pkgdown\nVersion: 0.0.1\nAuthors@R: \n    person(\"Jinhwan\", \"Kim\", , \"jinhwan@zarathu.com\", role = c(\"aut\", \"cre\"))\nDescription: contains base ttest function \nLicense: MIT + file LICENSE\nEncoding: UTF-8\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.2.3\nSuggests: \n    rmarkdown\nURL: https://github.com/jhk0530/pkgdown.tutorial\nURL을 추가 한 것에 유의하세요 (navbar의 github 버튼에 사용됩니다)\n\n\n오른쪽의 Links, License, Developers 등이 채워졌음을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-03-17-pkgdown/index.html#메인-페이지-추가",
    "href": "posts/2023-03-17-pkgdown/index.html#메인-페이지-추가",
    "title": "pkgdown을 활용한 R 패키지 문서화",
    "section": "메인 페이지 추가",
    "text": "메인 페이지 추가\n여기까지 잘 따라왔다면, 패키지의 구조는 아래 이미지와 같습니다.\n\n이제 usethis::use_readme_md()를 사용하여 README.MD를 추가하여 메인 페이지를 만들어줍니다.\ngithub repository를 만들면서, add readme를 통해 만들었어도 상관 없지만, 위 함수를 사용하면 최소 템플릿을 만들어 주기 때문에 조금 더 편리할 수 있습니다.\n\n💡 rmd를 선호한다면 usethis::use_readme_rmd()를 사용해도 좋습니다.\n\n최종 결과는 아래 이미지와 같습니다.\n\n단, 지금은 주소창이 https://로 시작하지 않는, 작업자의 pc에서만 확인 할 수 있는 형태라는 것을 확인해야합니다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html",
    "href": "posts/2021-08-19-pubicdatawithr/index.html",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "",
    "text": "김진섭 대표는 9월 12일 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에 참석, R 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "요약",
    "text": "요약\nR 로 보험공단/심평원 빅데이터 분석가능.\n\n공단표본코호트 V1 은 내부 분석환경 구축\n공단표본코호트 V2, 심평원 데이터는 원격 분석환경 이용\n대용량데이터 위한 R 패키지: data.table, fst, parallel\n\n자체개발 R 패키지 CRAN 배포, 원격분석환경에서도 이용가능.\n\nKaplan-meier 그림: jskm\n논문용 테이블: jstable\nGUI 분석: jsmodule\n\nShiny 로 웹기반 실시간 분석서비스.\n\n내부 분석환경: 웹에서 실시간 분석수행\n원격 분석환경: 모든 분석결과 반출 후 웹기반 시각화\nExcel/PPT 다운로드\n\nCDM 다기관 메타분석 서비스\n\nTable 1 합치기, Forest/Funnel plot 등"
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/publicdata_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html",
    "href": "posts/2018-11-24-basic-biostatistics/index.html",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "",
    "text": "김진섭 대표는 11월 28일(수) 중앙보훈병원 정신건강의학과를 방문, 의학 연구에 필요한 기술 통계(descriptive statistics)에 대해 강의하고 자체 제작한 웹 애플리케이션과 Rstudio Addins를 이용하여 실습을 진행하였습니다. 강의 내용을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n의학연구에서 R 활용 능력을 대략 5단계로 구분할 수 있다.\n\n데이터 정리는 미리 excel로 완료, 통계분석만 R 이용.\nR로 데이터 정리와 통계분석을 모두 수행.\nR로 그림을 그린다.\n논문에 들어갈 Table과 figure를 모두 R로 만든다.\n글쓰기, 참고문헌 등 논문의 모든 작업을 R에서 직접 수행한다.\n\n본 강의는 1단계에 해당하는 연구자가 R을 쉽게 이용할 수 있도록 돕는 내용에 해당하며 R과 Rstudio의 설치과정은 생략한다. 혹시 설치를 못했다면 https://www.r-project.org 와 https://www.rstudio.com/products/rstudio/download 를 참조하여 설치하길 바란다. R의 전반적인 도움말은 help.start() 명령어를 활용하고 특정 함수를 보려면 help(which) 형태로 실행하면 된다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "의학 연구에서의 기술 통계",
    "text": "의학 연구에서의 기술 통계\n기술 통계는 원래 평균(mean), 중위수(median), 분산(variance), 빈도표(frequency table)등의 데이터를 설명하는 숫자들이나 히스토그램(histogram), 상자그림(box-plot)같은 그래프를 의미한다. 그러나 대부분의 의학 연구에서는 단순한 기술 통계가 아닌 그것들의 그룹 비교(ex: 성별, 질환 유무)가 Table 1에 기술 통계란 제목으로 제시된다.\n\n\n\n\nTable 1 example(Balaji 2011)\n\n\n\n보통 연구의 흐름은 기술 통계로 데이터를 보여주고 단변량(univariate) 분석을 통해 가설 검정을 수행한 후, 다변량(multivariate) 혹은 소그룹(subgroup) 분석 을 이용하여 다른 변수들의 효과를 보정한 결과를 보여주는 것으로 이루어진다. 그러나 단변량 분석에서 끝나는 간단한 연구도 많고 이것은 본질적으로 기술 통계의 그룹 비교와 같으므로, Table 1에 필요한 통계를 알고 쉽게 구현할 수 있다면 그것만으로 간단한 의학 연구를 수행할 수 있다.\n본 강의에서는 Table 1의 그룹 비교에 필요한 통계 방법들을 알아보고 R을 이용해서 실제 분석을 수행할 것이다. 통계 방법을 선택하는 기준은 크게 (1) 연속 변수(continuous variable) vs 범주형 변수(categorical variable), (2) 비교할 그룹 수, (3) 샘플 수 혹은 정규분포 여부 의 3가지가 있으며 추가로 짝지은 그룹인 경우를 살펴보겠다.\n마지막으로 자체 개발한 웹 애플리케이션과 Rstudio Addins을 사용하여 간단히 Table 1을 만들어 볼 것이다(Figure @ref(fig:appgif), @ref(fig:addingif))."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "연속 변수의 그룹 비교",
    "text": "연속 변수의 그룹 비교\n연속 변수의 그룹 비교는 2 그룹일 때는 t-test, 3 그룹 이상이면 ANOVA라고 생각하면 되며, 2 그룹일 때 ANOVA 결과는 t-test 결과와 거의(?) 같다. 따라서 연속 변수는 무조건 ANOVA라고 생각해도 대충 맞다.\nT-test\nT-test는 2 그룹의 평균값을 비교하는 통계 방법1으로 필요한 숫자는 각 그룹의 평균과 분산이다. 실제로 데이터 없이 두 그룹의 평균과 분산만 있어도 t-test를 수행할 수 있으며 https://www.evanmiller.org/ab-testing/t-test.html 를 통해 웹에서도 간단히 계산할 수 있다. 아래 남녀의 총 콜레스테롤 데이터를 이용해 t-test를 수행해 보자.\n\n\n\n\n\n\n이제 t.test 함수를 이용하여 남녀의 총 콜레스테롤 수치를 비교한다.\n\nnev.ttest &lt;- t.test(tChol ~ sex, data = data.t, var.equal = F)\nnev.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  tChol by sex\nt = -0.47138, df = 23.437, p-value = 0.6417\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -23.36286  14.68429\nsample estimates:\nmean in group Female   mean in group Male \n            149.3750             153.7143 \n\n\n여자의 평균 콜레스테롤 값은 149.4, 남자는 153.7 이고 \\(p\\)-value는 0.642임을 확인할 수 있다.\n위의 t.test 함수의 옵션 중 var.equal = F는 등분산 가정 없이 분석하겠다는 뜻으로 옵션을 적지 않아도 기본적으로 F가 적용된다. 등분산 가정이란 두 그룹의 분산이 같다고 가정하는 것인데, 계산이 좀 더 쉽다는 이점이 있으나 아무 근거 없이 분산이 같다고 가정하는 것은 위험한 가정이다. 위의 분석에 var.equal = T를 적용해보자.\n\nev.ttest &lt;- t.test(tChol ~ sex, data = data.t, var.equal = T)\nev.ttest\n\n\n    Two Sample t-test\n\ndata:  tChol by sex\nt = -0.48157, df = 28, p-value = 0.6339\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -22.79682  14.11825\nsample estimates:\nmean in group Female   mean in group Male \n            149.3750             153.7143 \n\n\n앞서는 Welch t-test였는데 이름이 바뀐 것을 확인할 수 있고 \\(p\\)-value도 0.634로 아까와 다르다. 논문 리뷰어가 요구하는 등의 특별한 경우가 아니고서야 위험한 등분산가정을 할 필요가 없고, 이제부터 var.equal 옵션은 신경쓰지 않아도 좋다.\n\nANOVA2\n\n3 그룹 이상일 때는 2그룹씩 짝을 지어서 t-test를 여러 번 수행할 수 있다. 그러나 Table 1에서는 대부분 하나의 \\(p\\)-value만 제시하고 이것은 전체적으로 튀는 것이 하나라도 있는가?를 테스트하는 ANOVA를 이용한다.3 ANOVA는 비교할 모든 그룹에서 분산이 같다는 등분산 가정 하에 분석을 수행하며, 실제로 2 그룹일 때 ANOVA를 수행하면 등분산 가정 하에 수행한 t-test와 동일한 결과를 얻는다. 위에도 언급했듯이 모든 그룹에서 분산이 같다는 것은 너무 위험한 가정이나, 3 그룹 이상인 경우 마땅한 대안이 없고 Table 1에서 엄밀한 통계를 요구하는 것도 아니기 때문에 그냥 ANOVA를 쓰는 것이 관행이다. 아래 세 그룹의 총 콜레스테롤 데이터를 활용해 분석을 수행해 보자.\n\n\n\n\n\n\n이제 aov함수를 이용하여 3 그룹의 평균 콜레스테롤 값을 한번에 비교한다.\n\nres.aov &lt;- aov(tChol ~ group, data = data.aov)\nsummary(res.aov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\ngroup        2   3421  1710.3   2.103  0.142\nResiduals   27  21956   813.2               \n\n\n결과에서 나온 \\(p\\)-value인 0.142가 Table 1에 이용되며 의미는 “3 그룹에서 총콜레스테롤 값이 비슷하다(다른 것이 있다고 할 수 없다)” 이다.\n비모수 통계: 정규분포 아닐 때\nT-test나 ANOVA는 모두 변수가 정규분포를 이룬다고 가정하고 분석을 수행하는데, 언뜻 생각하기에 의학에서 정규분포가 아닌 변수는 없을 것 같지만4 일부 지표(ex: CRP, 자녀 수)들은 정규분포를 따르지 않는다고 알려져 있다. 이 때는 변수의 값 자체가 아닌 순위 정보만을 이용하는 비모수 검정을 이용한다. T-test에 대응되는 비모수 분석은 Wilcoxon rank-sum test 혹은 Mann–Whitney U test 로 불리며 앞서 이용한 남녀별 총 콜레스테롤 데이터로 분석을 수행하면 아래와 같다.\n\nres.wilcox &lt;- wilcox.test(tChol ~ sex, data = data.t)\nres.wilcox\n\n\n    Wilcoxon rank sum exact test\n\ndata:  tChol by sex\nW = 114, p-value = 0.951\nalternative hypothesis: true location shift is not equal to 0\n\n\n위 결과에서 \\(p\\)-value는 0.951임을 확인할 수 있다. ANOVA에 대응되는 비모수 분석은 Kruskal–Wallis one-way ANOVA이며 역시 앞서 이용한 그룹별 총 콜레스테롤 데이터에 적용하면 아래와 같다.\n\nres.kruskal &lt;- kruskal.test(tChol ~ group, data = data.aov)\nres.kruskal\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tChol by group\nKruskal-Wallis chi-squared = 2.6013, df = 2, p-value = 0.2724\n\n\n마찬가지로 \\(p\\)-value는 0.272임을 확인할 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "범주형 변수의 그룹 비교",
    "text": "범주형 변수의 그룹 비교\n범주형 변수의 그룹 비교는 그룹 수나 정규분포를 고려할 필요가 없어 연속 변수일 때보다 훨씬 간단하며 딱 하나, 샘플 수가 충분한지만 확인하면 된다.\n샘플 수 충분: Chi-square test\nChi-square test는 두 범주형 변수가 관계가 있는지 없는지를 파악하는 테스트로5 아래의 혈압약과 당뇨약 복용 여부를 조사한 데이터로 분석을 수행해 보겠다.\n\n\n\n\n\n\n두 약물 복용 여부를 테이블로 나타내면\n\ntb.chi &lt;- table(data.chi)\ntb.chi\n\n        DM_medi\nHTN_medi  0  1\n       0 15 13\n       1 14  8\n\n\n이며 언뜻 봐서는 관계가 있는지 아닌지 잘 모르겠다. 이제 chisq.test함수를 이용해서 Chi-square test를 수행하자.\n\nres.chi &lt;- chisq.test(tb.chi)\nres.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.chi\nX-squared = 0.18246, df = 1, p-value = 0.6693\n\n\n\\(p\\)-value는 0.669가 나오고 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 말할 수 있다.\n샘플 수 부족: Fisher’s exact test\n이번엔 다른 사람들의 혈압, 당뇨약 복용 데이터로 chi-square test를 수행해 보겠다.\n\n\n\n\n\n\n아까와 마찬가지로 테이블로 두 약물 복용상태를 비교하면 아래와 같다.\n\ntb.fisher &lt;- table(data.fisher)\ntb.fisher\n\n        DM_medi\nHTN_medi  0  1\n       0 31  8\n       1  9  2\n\n\n혈압약과 당뇨약을 모두 복용한 사람이 2명으로 좀 작아보이지만 무시하고 chi-square test를 수행하면 결과는 나오나 Warning 메시지가 뜬다.\n\nchisq.test(tb.fisher)\n\nWarning in chisq.test(tb.fisher): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.fisher\nX-squared = 4.5971e-31, df = 1, p-value = 1\n\n\n이는 두 약을 모두 복용한 사람이 2명뿐이라서 일어나는 문제로, 일반적으로 분석할 테이블에서 샘플 수가 너무 작은 항이 있으면 chi-square test의 계산이 부정확해진다. 이 때는 fisher’s exact test를 수행하며 아래와 같이 fisher.test함수를 이용하면 된다.\n\nres.fisher &lt;- fisher.test(tb.fisher)\nres.fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb.fisher\np-value = 1\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07627205 5.55561549\nsample estimates:\nodds ratio \n 0.8636115 \n\n\n\\(p\\)-value는 1로 확인되고 마찬가지로 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 할 수 있다.\n여기서 의문점이 들 수 있다. 무조건 fisher’s test만 하면 간단한데 도대체 chi-square test는 왜 하는 것일까? 샘플 수가 작을 때는 fisher’s test만 하는 것이 실제로 더 간단하고 방법론적으로도 아무 문제가 없다. 그러나 샘플 수나 그룹 수가 늘어날수록 fisher’s test는 필요한 계산량이 급격하게 증가하는 문제가 있어 chi-square test를 먼저 수행하는 것을 권유한다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "추가: 짝지은 2 그룹",
    "text": "추가: 짝지은 2 그룹\n각 사람의 혈압을 한 번은 사람이 직접, 한 번은 자동혈압계로 측정했다고 하자. 이 때 직접 잰 혈압과 자동혈압계의 측정값을 비교한다면 t-test로 충분할까? t-test는 혈압 재는 방법마다 평균을 먼저 구한 후 그것이 같은지를 테스트하므로 짝지은 정보를 활용하지 못한다. 이 때는 각 사람마다 두 혈압값의 차이를 먼저 구한 후 평균이 0인지를 테스트하면, 짝지은 정보를 활용하면서 계산도 더 간단한 방법이 된다.\n연속변수: Paired t-test\n위에 언급한 대로 각 사람마다 차이값을 먼저 구한 후 그 평균이 0인지를 테스트하는 방법이 paired t-test이다. 아래의 수축기 혈압 데이터를 통해 t-test와의 차이점을 알아보자.\n\ndata.pt &lt;- data.frame(SBP_hand = round(rnorm(30, mean = 125, sd = 5)), SBP_machine = round(rnorm(30, mean = 125, sd = 5)))\nrownames(data.pt) &lt;- paste(\"person\", 1:30)\ndatatable(data.pt, rownames = T, caption = \"data.pt: systolic blood pressure measured by hand & machine\")\n\n\n\n\n\n위 데이터는 30명의 사람이 앞서 말한 두 가지 방법으로 수축기 혈압을 측정한 데이터이다. 먼저 t-test를 수행하자.\n\npt.ttest &lt;- t.test(data.pt$SBP_hand, data.pt$SBP_machine)\npt.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.45768, df = 57.863, p-value = 0.6489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.224307  2.024307\nsample estimates:\nmean of x mean of y \n    125.0     125.6 \n\n\n위 결과를 보면 각 방법의 평균을 먼저 구한 후 그것을 비교한 것을 확인할 수 있고 \\(p\\)-value는 0.649이다. 이제 paired t-test를 수행하자.\n\npt.ttest.pair &lt;- t.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\npt.ttest.pair\n\n\n    Paired t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.46171, df = 29, p-value = 0.6477\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.257804  2.057804\nsample estimates:\nmean difference \n           -0.6 \n\n\n이번에는 사람마다 차이값을 먼저 구한 후 그것이 0인지 테스트 한 것을 확인할 수 있고 \\(p\\)-value는 0.648이다.\nPaired t-test의 비모수버전은 wilcoxon-signed rank test 이며 아래와 같이 실행한다.\n\npt.wilcox.pair &lt;- wilcox.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with zeroes\n\npt.wilcox.pair\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nV = 214, p-value = 0.9482\nalternative hypothesis: true location shift is not equal to 0\n\n\n본 강의에서는 다루지 않겠지만 짝지은 3개 이상의 그룹은 repeated measure ANOVA6라는 방법을 이용한다.\n범주형 변수: Mcnemar test, Symmetry test for a paired contingency table\n이번에는 측정값이 0,1과 같은 범주형 변수인 경우를 살펴보자. 아래 데이터를 활용해 약 복용 전후로 복통증상 발생에 차이가 있는지 알아본다고 하자.\n\n\n\n\n\n\n이 데이터를 2 by 2 테이블로 정리하면 아래와 같다.\n\ntable.mc &lt;- table(data.mc)\ntable.mc\n\n           Pain_after\nPain_before 0 1\n          0 8 8\n          1 9 5\n\n\n먼저 앞서 배운 Chi-sqaure test 를 이용한 결과를 보자.\n\nmc.chi &lt;- chisq.test(table.mc)\nmc.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table.mc\nX-squared = 0.17514, df = 1, p-value = 0.6756\n\n\n이것은 약 복용 전 복통 증상과 복용 후의 복통 증상이 얼마나 관계가 있는지 알아보는 테스트로 짝지은 정보를 활용하지 않는다. 이번엔 짝지은 정보를 활용하는 mcnemar test를 수행하자.\n\nmc.mcnemar &lt;- mcnemar.test(table.mc)\nmc.mcnemar\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  table.mc\nMcNemar's chi-squared = 0, df = 1, p-value = 1\n\n\nMcnemar test는 약 복용후 증상발생이 달라진 사람 즉, discordant pair만 분석에 이용한다. 따라서 condordant pair 의 구성과 어떻더라도 통계결과는 동일하게 나온다.\n한편 측정값이 3개 이상일 때는 chi-square test는 그대로 이용할 수 있으나, mcnemar test는 그대로 쓰지 못하고 symmetry test for a paired contingency table7라는 일반화된 테스트를 사용한다. 본 강의에서는 간단한 실행법만 살펴볼 것이며 먼저 rcompanion R package를 설치하자.\n\n## For symmmetry test\n#install.packages(\"rcompanion\")\nlibrary(rcompanion)\n\n예제 테이블(3 \\(\\times\\) 3)을 아래와 같이 불러온 후\n\n## Example data\ndata(AndersonRainGarden) \nAndersonRainGarden       \n\n             Yes.after No.after Maybe.after\nYes.before           6        0           1\nNo.before            5        3           7\nMaybe.before        11        1          12\n\n\nnominalSymmetryTest 함수로 분석을 수행한다.\n\n## Symmetry test\nnominalSymmetryTest(AndersonRainGarden)\n\n$Global.test.for.symmetry\n  Dimensions  p.value\n1      3 x 3 0.000476\n\n$Pairwise.symmetry.tests\n                                       Comparison p.value p.adjust\n1       Yes.before/Yes.after : No.before/No.after  0.0736   0.0771\n2 Yes.before/Yes.after : Maybe.before/Maybe.after 0.00937   0.0281\n3   No.before/No.after : Maybe.before/Maybe.after  0.0771   0.0771\n\n$p.adjustment\n  Method\n1    fdr\n\n$statistical.method\n        Method\n1 McNemar test"
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "실습",
    "text": "실습\n웹 애플리케이션\nAnpanman 에서 만든 기초통계 앱을 소개한다(Figure @ref(fig:appgif)). 5메가 이하의 excel, csv 형태 혹은 sas, spss 프로그램으로 만든 데이터를 업로드하면 Table 1과 회귀분석, 로지스틱 회귀분석을 간단하게 수행하고 결과를 excel로 바로 다운받을 수 있다.\n\n\n\n\nApplication mady by Anpanman\n\n\n\nRstudio Addins\n5메가보다 큰 데이터는 R에서 데이터를 읽은 후, 자체적으로 만든 jsmodule R package를 설치하여 앱을 이용할 수 있다.\n\n## For private package install \ninstall.packages(\"devtools\")   \ndevtools::install_github(c(\"jinseob2kim/jstable\", \"jinseob2kim/jsmodule\")) \n\n패키지를 설치한 후 Rstudio 프로그램의 Addins 탭을 누르면 Basic statistics 항목이 보일 것이다. 데이터를 읽고 그것의 이름을 드래그 한 상태로 Basic statistics 를 누르면 된다(Figure @ref(fig:addingif)).\n\n\n\n\nRstudo Addins made by Anpanman8\n\n\n\n직접 R에서 코드를 실행하고 싶은 유저는 tableone R package를 참고하기 바란다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "마치며",
    "text": "마치며\n지금까지 의학 연구에서 쓰이는 그룹 비교 통계들을 알아보고 웹 앱과 Rstudio Addins 을 이용하여 직접 Table 1을 만들어보았다. 앞으로 연구자들은 어려운 통계 프로그램을 이용할 필요 없이 Anpanman의 서비스를 활용, 빠르게 통계 분석을 수행하고 테이블과 그림을 바로 다운받을 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#footnotes",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#footnotes",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n1 그룹의 평균값을 특정 숫자와 비교할 수도 있다.↩︎\n본 강의에서는 One-way ANOVA만 다룬다.↩︎\n사후(post-hoc) 분석을 이용, 어떤 것이 튀는지를 알아볼 수도 있다.↩︎\n정규분포에 대한 내용은 https://jinseob2kim.github.io/Normal_distribution.html 를 참고하기 바란다.↩︎\n세 범주형 변수일 때도 이용할 수 있으나 본 강의에서는 생략한다.↩︎\nhttps://statistics.laerd.com/statistical-guides/repeated-measures-anova-statistical-guide.php↩︎\nhttp://rcompanion.org/handbook/H_05.html↩︎\nhttps://github.com/jinseob2kim/jsmodule↩︎"
  },
  {
    "objectID": "posts/2023-02-14-shiny.likert/index.html",
    "href": "posts/2023-02-14-shiny.likert/index.html",
    "title": "shiny.likert 패키지 소개",
    "section": "",
    "text": "개요\n\n이전 글을 보고 오면 이해에 조금 더 도움이 됩니다*\n\n순서를 가진 범주형 데이터는 A~ E, 좋음 ~ 안좋음. 상위 10% ~ 하위 10%, NPS 1-10과 같은 예시들로 꼭 의료 도메인이 아니더라도 다양한 분야에서 활용되고 있습니다.\n이러한 데이터를 수집하는 방법은 보통, 설문조사를 위해 쓰이는 Google Forms나 Typeform과 같은 온라인 설문조사 도구를 활용할 수 있습니다.\n이제 이 결과를 활용하기 위한 방법은 정말 많지만, 설문 데이터를 시각화를 하는 방법은 대부분 의 경우 pie chart 혹은 barchart 정도만 활용하게 됩니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\nR이나 Python 과 같은 프로그래밍 경험이 없거나 혹은 google studio, tableau public과 같이 “상용 시각화 툴”에서 제공하지 않는 차트라면 likert chart와 같은 방법을 활용하는 것은 꽤나 골치 아픈 일입니다.\n다행히 R에서는 Shiny라는 R의 기능을 웹으로 보내주는 라이브러리가 있고, 이를 활용하면 누구나 web application의 주소만 알고 있으면 해당 페이지에 접속하여 R의 기능을 사용하여 likert chart를 만들어 낼 수 있습니다.\n이 글에서는 likert 패키지를 웹에서 사용할 수 있게 하는 shiny app을 만드는 과정에 대하여 가볍게 다뤄보겠습니다.\nshiny design\n\nUI 디자인에 대한 내용은 언급하지 않습니다.\n\nshiny application에 제일 먼저 필요한 기능은 사용자의 설문조사 결과 데이터 (csv)를 업로드 하는 기능입니다.\n처음에는 googlesheets4라이브러리를 사용하여 많이 쓰이는 google sheets를 url만 복사하여 사용하게 만드는 것도 고려했으나, 이렇게 하기 위해서는 해당 시트를 외부에 노출 하거나, 구글 권한 문제를 shiny에서 같이 해결해야하기 때문에 너무 복잡하여 고려하지 않고, 대신 다운로드 받은 csv만 작업할 수 있게 합니다.\n\n이후 사용자의 혹은 예시로 github에 올려둔 데이터를 사용자가 업로드 하면 여러 column 중에 어떤 부분을 차트로 그릴지 선택해야 합니다.\n다행히 likert 패키지에서는 항목만 같다면 여러개의 차트도 동시에 그릴수 있기 때문에 multiple select의 형태로 만들어줍니다.\n\ncolumn을 선택하고 나면, column의 내용과 차트 옵션이 나타납니다.\n\n단, column의 내용. 데이터의 경우 좋음 ~ 안좋음. 동의함 ~ 동의하지 않음 등과 같이 사용자가 설계한 내용에 따라 다른 항목을 가질 수 있고, 이러한 항목의 순서가 likert에서는 중요하기 때문에 입력한 column의 순서를 사용자가 다시 바꿀 수 있게 구현합니다.\n\n이후 옵션을 조절한 후 draw button 을 클릭하여 차트를 만들어낼 수 있습니다.\n그러나 사용자는 처음에 어떤 옵션이 어떤 역할을 하는지 알 수 없기 때문에 차트가 만들어지고 난 이후에 옵션을 변경해도 차트에 반영되도록 구현합니다.\n\n차트는 ggplot + plotly를 같이 활용하여 interactive하게 만들어 지기 때문에 사용자가 png로 다운로드하여 활용할 수 있게 구현합니다.\n\n이 상태에서의 shiny application은 말 그대로 “works-on-my-machine ¯_(ツ)_/¯” 이기 때문에 이를 다른 사람도 웹에서 사용할 수 있게 배포를 해야합니다.\ndeploy shinyapps\nshiny application을 배포하는 방법은 shinyapps.io, shiny server, Rstudio connect 3가지로 볼 수 있는데 각각의 특징은 이러합니다.\n\nshinyapps.io:\n\n\nposit의 클라우드 인프라를 이용한 배포 방식. Rstudio와 바로 연동하여 편리하게 올릴 수 있다는 장점이 있다.\n다양한 요금제를 제공하며 무료 요금제의 경우 사이즈가 작은 shiny application을 5개까지 운영할 수 있다.\n\n\nshiny server:\n\n\n자체 서버/인프라를 가지고 있는 경우(온프레미스) 이를 설치하여 배포하는 방법\n비용이 들지 않음.\n\n\nRstudio connect:\n\n\n이전의 shiny server pro에 몇 기능을 더 추가한 옵션.\nRstudio와 바로 연동하여 편리하게 배포 할 수 있다.\ncommercial product이기 때문에 팀 단위로 비용이 든다.\n\n다행히 shiny.likert는 복잡한 기능, 많은 패키지 등을 사용하지 않는 “가벼운” Application이기 때문에, 그냥 제 개인 shinyapps.io 계정에 배포를 해도 문제가 없습니다.\n이 app을 배포하는 방법은 간단합니다. Rstudio에서 편집한, 잘 돌아가는 app.R로 이동하여 오른쪽 위의 connect 버튼을 누르고 이후의 몇번 더 클릭만 하면 됩니다. 물론 shinyapps.io 계정은 미리 만들어두어야 합니다.\n\n몇분 정도 지난 후 배포를 마치고 나면 https://jhkim.shinyapps.io/shiny-likert/ 와 같은 주소를 통해 누구나 shiny.likert 패키지를 활용하여 likert chart를 만들 수 있습니다.\n아래의 이미지는 shiny.likert를 이용하여 만든 이미지 입니다.\n\nshiny app to R package\nshinyapps를 통해 배포하는 방법은 편리하지만, 3가지 문제점을 가지고 있습니다.\n\n사용량이 많은 경우 요금제가 막힘\n클라우드 무료 요금제의 인스턴스는 로컬에 비해 성능이 많이 모자람 (물론 제약이 생길 정도까진 아닙니다)\nR을 사용할 수 있는 사람도 shiny.likert를 사용하기 위해 shinyapps만 사용해야함\n\n물론 이 외에도 나중에 유지보수를 위해 로컬에서 작업해야하는 경우를 위해 package의 형태로도 개발하겠습니다.\n이전에 만든 shiny package 중 하나는 끌어다 사용하는 라이브러리의 api 변화로 로컬에서 작동하게 하려면 최근 버전에 맞추어 수정을 해야합니다.\n이 방법은 dean attali님의 아티클을 참조하였습니다.\n\n현재 작업중인 디렉토리에 R Package를 생성\n\n\ndevtools::create_package(getwd())\n\n\nR package의 Description을 변경.\n\n\n\nImports를 추가합니다.\n\n\n\ninst 디렉토리를 만들고, shinyApp이라는 디렉토리를 만들어 작업했던 app.R이나 www/styles.css를 이동합니다.\n\n\n\ninst는 고정이고, shinyApp은 이름을 바꾸어도 상관없습니다.\n\n\n\nR 디렉토리에 shiny.likert라는 함수를 추가합니다. 내용은 아래와 같습니다.\n\n\n\nshinyApp과 package 이름 shiny.likert에 주의합니다.\n\n\n#' @importFrom shiny runApp\n#' @export\nshiny.likert &lt;- function(){  \n    appDir &lt;- system.file(\"shinyApp\", package = 'shiny.likert' )  \n    shiny::runApp(appDir)\n}\n\n\n이후 roxygen2를 활용하여 export등의 documentation을 하고, package를 build합니다.\n\n그 결과, 아래의 코드를 통해 누구나 로컬에서도 shiny.likert를 활용할 수 있습니다.\n\nremotes::install_github('zarathucorp/shiny.likert')\nlibrary(shiny.likert)\nshiny.likert()\n\ngithub package\nshiny.likert는 github에 올려진 R package이기 때문에 shinyreadme와 polaroid를 사용하여 readme.md를 수정하고, pkgdown을 활용해 사이트를 제작합니다.\npkgdown에서의 Get started는\n\npkgdown::build_article(\"shiny.likert\", \"shiny.likert\")\n\n를 통해 만들 수 있습니다. 이외의 과정들은 별도로 설명하지 않으며, 결과는 아래와 같습니다.\n\n\n\n\nReuseCC BY-NC 4.0CitationBibTeX citation:@online{kim2023,\n  author = {Kim, Jinhwan},\n  title = {Shiny.likert {패키지} {소개}},\n  date = {2023-02-15},\n  url = {https://blog.zarathu.com/jp/posts/2023-02-14-shiny.likert},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKim, Jinhwan. 2023. “Shiny.likert 패키지 소개.” February\n15, 2023. https://blog.zarathu.com/jp/posts/2023-02-14-shiny.likert."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html",
    "href": "posts/2020-10-05-yu-seminar/index.html",
    "title": "의학통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 영남대학교 “의사과학자 역량 배가 프로젝트” 에 참석, 그동안 해온 지원업무에 대해 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#요약",
    "href": "posts/2020-10-05-yu-seminar/index.html#요약",
    "title": "의학통계지원 소개",
    "section": "요약",
    "text": "요약\n다양한 의료데이터를 다뤄본 경험으로, 의학연구 활성화에 기여하겠습니다.\n\n의과대학, 유전체연구 박사과정, 삼성전자 무선사업부를 거치며 임상, 유전체, 모바일헬스 등 다양한 데이터를 다루었습니다.\n현재 연구지원법인 차라투 를 운영 중입니다.\nShiny 밋업 의 진행과 후원을 맡아, 의료/축산/게임/반도체/신용평가/IPTV 등 다양한 분야의 사람들과 함께 Shiny 를 알아가는 중입니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#slide",
    "href": "posts/2020-10-05-yu-seminar/index.html#slide",
    "title": "의학통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/yu/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html",
    "href": "posts/2021-04-02-shinyecrf/index.html",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 4월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#요약",
    "href": "posts/2021-04-02-shinyecrf/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "요약",
    "text": "요약\n삼성서울병원 심혈관중재실 의뢰: 환자데이터 입력웹(eCRF).\n\nTychobra의 Shiny CRUD 참고해 용병 1인과 개발 중.\nshinymanager 로 로그인 모듈: 어떤 ID가 생성, 수정했는지 기록.\nDB: RSQLite 이용, 파일로 관리.\nDT 사용: proxy 기능으로 빠른 업데이트 가능. 테이블 안에 클릭(수정)버튼 삽입.\n버튼 1개 당 shiny module 1개.\n의료데이터 입력/관리/분석 통합서비스 목표."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#slide",
    "href": "posts/2021-04-02-shinyecrf/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html",
    "href": "posts/2020-10-29-survivalpractice/index.html",
    "title": "생존분석 실습",
    "section": "",
    "text": "김진섭 대표는 성균관의대 사회의학교실 김종헌 교수님 수업에 참가, Kaplan-meier curve, 비례위험가정 및 모형적합도, Time-dependent covariate 그리고 모수적 생존분석을 중심으로 R 코드를 실습할 예정입니다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#요약",
    "href": "posts/2020-10-29-survivalpractice/index.html#요약",
    "title": "생존분석 실습",
    "section": "요약",
    "text": "요약\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "href": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "title": "생존분석 실습",
    "section": "Kaplan-meier plot",
    "text": "Kaplan-meier plot\nKaplan-meier plot 은 R 기본 plot에서도 제공하지만, survminer 패키지의 ggsurvplot 함수에서 다양한 옵션을 제공한다. 본 실습에서는 본사가 개발한 jskm 패키지의 jskm 함수를 survival 패키지 내장 데이터 veteran 에 적용하겠다. 우선 패키지를 불러온 후 survfit 으로 구간별 생존율을 구하자.\n\nlibrary(DT);library(survival);library(jskm)\ndatatable(veteran, rownames = F, caption = \"Example data\", options = list(scrollX = T))\n\n\n\n\nsfit &lt;- survfit(Surv(time, status) ~ trt, data = veteran)\nsummary(sfit, times = c(100, 200, 300, 365), extend = T)\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = veteran)\n\n                trt=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     34      34   0.5020  0.0606       0.3962        0.636\n  200     12      19   0.1947  0.0501       0.1176        0.322\n  300      5       6   0.0885  0.0371       0.0390        0.201\n  365      4       1   0.0708  0.0336       0.0279        0.180\n\n                trt=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     21      45    0.333  0.0578       0.2367        0.467\n  200     13       7    0.216  0.0517       0.1354        0.345\n  300      8       4    0.146  0.0454       0.0797        0.269\n  365      6       2    0.110  0.0407       0.0530        0.227\n\n\ntrt 1 은 “Standard”, 2 는 “Test” 이며 jskm 을 적용하면 아래와 같다.\n\njskm(sfit)\n\n\n\n\n\n\n\n라벨을 수정하고, risk table 과 log-rank p-value 를 추가하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T)\n\n\n\n\n\n\n\n십자가 무늬는 실제 censoring 이 발생한 부분이며 mark = F 로 숨길 수 있다. 생존율이 아닌 누적발생률을 %로 보는 코드는 아래와 같다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cumhaz = T, surv.scale = \"percent\" )\n\n\n\n\n\n\n\np-value 위치는 pval.coord legend 위치는 legendposition 옵션을 이용한다. 선을 흑백으로 바꾸려면 linecols = \"black\" 을 추가한다. legendposition 은 x,y 값 모두 0~1 scale 임을 주의하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, pval.coord = c(100, 0.1), legendposition = c(0.85, 0.6), linecols = \"black\")\n\n\n\n\n\n\n\n마지막으로 특정 시간을 기준으로 나누어보는 landmark analysis 옵션을 소개한다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cut.landmark = 365)\n\nWarning: Removed 2 row(s) containing missing values (geom_path)."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "href": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "title": "생존분석 실습",
    "section": "연속변수의 최적 cut-off 구하기",
    "text": "연속변수의 최적 cut-off 구하기\nmaxstat 패키지를 이용한다.\n\nlibrary(maxstat)\nmtest &lt;- maxstat.test(Surv(time, status) ~ karno, data = veteran, smethod = \"LogRank\")\nmtest\n\n\nMaximally selected LogRank statistics using none\n\ndata:  Surv(time, status) by karno\nM = 4.6181, p-value = NA\nsample estimates:\nestimated cutpoint \n                40 \n\ncut &lt;- mtest$estimate\nveteran$karno_cat &lt;- factor(as.integer(veteran$karno &gt;= cut))\n\nsfit2 &lt;- survfit(Surv(time, status) ~ karno_cat, data = veteran)\njskm(sfit2, ystrataname = \"Karno\", ystratalabs = paste(c(\"&lt;\", \"≥\"), cut), table = T, pval = T)"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "href": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "title": "생존분석 실습",
    "section": "비례위험가정 확인",
    "text": "비례위험가정 확인\nLogrank test, Cox model 로 추정할 때 비례위험을 가정하므로 이것이 깨지면 큰일이다. 본 글에서는 비례위험가정을 확인하는 그림 2개와 테스트를 소개한다. 자세한 내용은 https://3months.tistory.com/357?category=743476 를 참고하기 바란다.\nLog-log plot\n\\(\\log(t)\\) 와 \\(\\log(-\\log(S(t)))\\) 관계를 그림으로 보는 방법이다. 왜 로그를 이용하는지는 모수적 생존분석에서 이야기하겠다.\n\nplot(sfit, fun=\"cloglog\", lty=1:2, col=c(\"Black\", \"Grey50\"), lwd=2, font.lab=2, main=\"Log-log KM curves by Treat\", \n     ylab=\"log-log survival\", xlab=\"Time (log scale)\")\nlegend(\"bottomright\",lty=1:2,legend=c(\"Standard\", \"Test\"), bty=\"n\", lwd=2, col=c(\"Black\", \"Grey50\"))\n\n\n\n\n\n\n\n두 선이 평행한지 확인하면 되고 직선인지 곡선인지는 상관없다. 모수적 생존분석에서 다룰 weibull 모형에서는 직선인지도 확인해야 한다.\nObserved-expected plot\n비례위험을 가정하는 cox model 예상과 비교하는 방법이다.\n\nplot(sfit, lty=\"dashed\", col=c(\"Black\", \"Grey50\"), lwd=2, font=2, font.lab=2, main=\"Observed Versus Expected Plots by Treat\", \n     ylab=\"Survival probability\", xlab=\"Time\")\npar(new = T)\n\n#expected\nexp &lt;- coxph(Surv(time, status) ~ trt, data = veteran)\nnew_df &lt;- data.frame(trt = c(1, 2))\nkmfit.exp &lt;- survfit(exp, newdata = new_df)\nplot(kmfit.exp, lty = \"solid\", col=c(\"Blue\", \"Red\"), lwd=2, font.lab=2)\n\n\n\n\n\n\n\nGoodness of fit\ncox.zph 함수로 통계검정을 수행한다.\n\ncox.zph(exp)\n\n       chisq df    p\ntrt     3.54  1 0.06\nGLOBAL  3.54  1 0.06\n\nplot(cox.zph(exp), var = \"trt\")\nabline(h = 0, lty = 3)\n\n\n\n\n\n\n\n선이 시간 상관없이 일정할수록, 즉 x축과 평행할수록 비례위험가정을 만족한다고 판단한다. 위 그림은 x축과 평행은 아니지만 경향성이 있다고 볼수도 없는 애매한 느낌이며 p 는 0.06 이다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "href": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "title": "생존분석 실습",
    "section": "모형 비교",
    "text": "모형 비교\nCox 모형에서 얻은 log-likelihood 값으로 여러 모형을 비교할 수 있다. 모형들은 n수가 전부 동일 해야 비교 가능하므로, 에러 나올땐 먼저 결측치를 확인하자.\n\nexp$loglik\n\n[1] -505.4491 -505.4442\n\nexp2 &lt;- coxph(Surv(time, status) ~ trt + age, data = veteran)\nexp3 &lt;- coxph(Surv(time, status) ~ trt + age + celltype, data = veteran)\n\nanova(exp, exp2, exp3)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(time, status)\n Model 1: ~ trt\n Model 2: ~ trt + age\n Model 3: ~ trt + age + celltype\n   loglik   Chisq Df P(&gt;|Chi|)    \n1 -505.44                         \n2 -505.14  0.6162  1    0.4325    \n3 -492.43 25.4161  3 1.264e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nstep 함수를 이용, AIC 기반 최적 모형을 고를 수 있다. scope 옵션으로 빠지면 안 될 변수를 미리 정한다.\n\nstep(exp3, scope = list(lower = ~ 1))\n\nStart:  AIC=994.86\nSurv(time, status) ~ trt + age + celltype\n\n           Df     AIC\n- age       1  993.04\n- trt       1  993.65\n&lt;none&gt;         994.86\n- celltype  3 1014.27\n\nStep:  AIC=993.04\nSurv(time, status) ~ trt + celltype\n\n           Df     AIC\n- trt       1  992.05\n&lt;none&gt;         993.04\n- celltype  3 1012.89\n\nStep:  AIC=992.05\nSurv(time, status) ~ celltype\n\n           Df     AIC\n&lt;none&gt;         992.05\n- celltype  3 1010.90\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ celltype, data = veteran)\n\n                    coef exp(coef) se(coef)     z        p\ncelltypesmallcell 1.0013    2.7217   0.2535 3.950 7.83e-05\ncelltypeadeno     1.1477    3.1510   0.2929 3.919 8.90e-05\ncelltypelarge     0.2301    1.2588   0.2773 0.830    0.407\n\nLikelihood ratio test=24.85  on 3 df, p=1.661e-05\nn= 137, number of events= 128"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "href": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "title": "생존분석 실습",
    "section": "Time-dependent analysis",
    "text": "Time-dependent analysis\n자세한 내용은 https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf 를 참고하기 바란다.\n비례위험가정 깨졌을 때 (time-dependent coefficients)\n어떤 공변량이 비례위험가정을 만족하지 않을 경우, 먼저 survSplit 으로 time 을 쪼개 몇 개의 그룹으로 나눈다.\n\nvet2 &lt;- survSplit(Surv(time, status) ~ ., data = veteran, cut=c(90, 180), episode = \"tgroup\", id = \"id\")\ndatatable(vet2, rownames = F, caption = \"Time split data\", options = list(scrollX = T))\n\n\n\n\n\n이제 공변량의 계수를 시간그룹 별로 따로 구한다.\n\nvfit2 &lt;- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data=vet2)\nsummary(vfit2)\n\nCall:\ncoxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n    data = vet2)\n\n  n= 225, number of events= 128 \n\n                                  coef exp(coef)  se(coef)      z Pr(&gt;|z|)    \ntrt                          -0.011025  0.989035  0.189062 -0.058    0.953    \nprior                        -0.006107  0.993912  0.020355 -0.300    0.764    \nkarno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15 ***\nkarno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530    \nkarno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                             exp(coef) exp(-coef) lower .95 upper .95\ntrt                             0.9890      1.011    0.6828    1.4327\nprior                           0.9939      1.006    0.9550    1.0344\nkarno:strata(tgroup)tgroup=1    0.9524      1.050    0.9409    0.9641\nkarno:strata(tgroup)tgroup=2    1.0081      0.992    0.9831    1.0337\nkarno:strata(tgroup)tgroup=3    0.9917      1.008    0.9637    1.0205\n\nConcordance= 0.725  (se = 0.024 )\nLikelihood ratio test= 63.04  on 5 df,   p=3e-12\nWald test            = 63.7  on 5 df,   p=2e-12\nScore (logrank) test = 71.33  on 5 df,   p=5e-14\n\n\n반복측정 공변량이 있을 때\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6015946/pdf/atm-06-07-121.pdf 예제를 이용하였다.\n\nlibrary(survsim)\n\nLoading required package: eha\n\n\nLoading required package: statmod\n\nN=100 #number of patients\nset.seed(123)\ndf.tf&lt;-simple.surv.sim(#baseline time fixed\n n=N, foltime=500,\n dist.ev=c('llogistic'),\n anc.ev=c(0.68), beta0.ev=c(5.8),\n anc.cens=1.2,\n beta0.cens=7.4,\n z=list(c(\"unif\", 0.8, 1.2)),\n beta=list(c(-0.4),c(0)),\n x=list(c(\"bern\", 0.5),\n c(\"normal\", 70, 13)))\n\nfor (v in 4:7){\n  df.tf[[v]] &lt;- round(df.tf[[v]])\n}\n\nnames(df.tf)[c(1,4,6,7)]&lt;-c(\"id\", \"time\", \"grp\",\"age\")\ndf.tf &lt;- df.tf[, -3]\n\ndatatable(df.tf, rownames = F, caption = \"df.tf: Original data\", options = list(scrollX = T))\n\n\n\n\n nft&lt;-sample(1:10,\n N,replace=T)#number of follow up time points\ncrp&lt;-round(abs(rnorm(sum(nft)+N,\n mean=100,sd=40)),1)\ntime&lt;-NA\nid&lt;-NA\ni=0\nfor(n in nft){\ni=i+1\ntime.n&lt;-sample(1:500,n)\ntime.n&lt;-c(0,sort(time.n))\ntime&lt;-c(time,time.n)\nid.n&lt;-rep(i,n+1)\nid&lt;-c(id,id.n)\n}\ndf.td &lt;- cbind(data.frame(id,time)[-1,],crp)\ndatatable(df.td, rownames = F, caption = \"df.td: Time dependent CRP\", options = list(scrollX = T))\n\n\n\n\n\ndf.tf 는 기본정보가 담긴 데이터, df.td 는 time-dependent covariate 가 담긴 데이터이다. tmerge 함수를 2번 실행하면 두 정보를 합칠 수 있다. 먼저 df.tf 만 이용해서 tstart, tstop 변수를 만들자.\n\ndf &lt;- tmerge(df.tf, df.tf, id = id, status1 = event(time, status))\n\ndatatable(df, rownames = F, caption = \"df: add tstart/tstop\", options = list(scrollX = T))\n\n\n\n\n\ntmerge 함수의 첫번째는 baseline data, 둘째는 time-dependent covariate 가 담긴 데이터가 들어가지만, tstart, tstop 를 만들기 위해 모두 df.tf 를 넣었다. status1 이라는 변수를 event(time, status) 로 지정함으로서 tstart, tstop 을 인식할 수 있다. status1 변수 자체는 status 와 동일하다. 이렇게 만든 df 에 time-dependent 정보가 담긴 df.td 를 결합하면 원하는 데이터를 얻을 수 있다. tmerge 의 자세한 내용은 https://ww2.amstat.org/meetings/sdss/2018/onlineprogram/ViewPresentation.cfm?file=304494.pdf 를 참고하기 바란다.\n\ndf2 &lt;- tmerge(df, df.td, id = id, crp = tdc(time, crp))\n\ndatatable(df2, rownames = F, caption = \"df2: final\", options = list(scrollX = T))\n\n\n\n\n\ncrp 변수를 tdc(time, crp) 로 만들었다. 이제 cox model 을 실행할 수 있는데, 반복측정정보를 cluster 옵션에 넣는 것을 잊지 말자.\n\nmodel.td &lt;- coxph(Surv(tstart, tstop, status1) ~ grp + age + crp, data = df2, cluster = id)\nsummary(model.td)\n\nCall:\ncoxph(formula = Surv(tstart, tstop, status1) ~ grp + age + crp, \n    data = df2, cluster = id)\n\n  n= 376, number of events= 67 \n\n         coef exp(coef)  se(coef) robust se     z Pr(&gt;|z|)  \ngrp 0.5022750 1.6524764 0.2525914 0.2555150 1.966   0.0493 *\nage 0.0005535 1.0005536 0.0081077 0.0072342 0.077   0.9390  \ncrp 0.0007922 1.0007925 0.0027391 0.0023373 0.339   0.7347  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ngrp     1.652     0.6052    1.0015     2.727\nage     1.001     0.9994    0.9865     1.015\ncrp     1.001     0.9992    0.9962     1.005\n\nConcordance= 0.554  (se = 0.04 )\nLikelihood ratio test= 4.21  on 3 df,   p=0.2\nWald test            = 4.34  on 3 df,   p=0.2\nScore (logrank) test = 4.18  on 3 df,   p=0.2,   Robust = 4.55  p=0.2\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "href": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "title": "생존분석 실습",
    "section": "모수적(parametric) 생존분석",
    "text": "모수적(parametric) 생존분석\nCox model 은 baseline hazard 없이도 HR 을 구할 수 있는 장점이 있다. 아래 식\n\\[h(t) = h_0(t) \\cdot \\exp(\\sum \\beta_i x_i)\\]\n에서 \\(h_0(t)\\) 를 몰라도 \\(\\beta\\) 들을 구할 수 있다는 뜻이고, cox model 이 준모수적(semi-parametric) 모형으로 불리는 이유이기도 하다. 그러나 Cox model 로 예측모형을 만들 때 이것은 단점이 된다. \\(t\\) 년 생존율을 구할 수 없기 때문이다. 생존함수 \\(S(t)\\) 는 아래처럼 계산하는데\n\\[S(t) = \\int_{0}^{t} h(u) \\,du\\]\nbaseline hazard 를 모르므로 \\(h(t)\\) 도 알 수 없고 따라서 \\(S(t)\\) 도 수식으로 표현할 수 없다. Cox model 로 예측모형을 만든 연구는 (1) 데이터에서 시간 \\(t\\) 마다 \\(S(t)\\) 의 값을 직접 구해 이용하거나, (2) 인구집단통계에서 \\(S(t)\\) 를 얻어온다.\n그러면 baseline hazard 가 어떤 형태라고 가정하면 어떨까? 이것이 모수적 생존분석이며 cox model 과 장단점을 비교하면 아래와 같다.\nCox model\n– distribution of survival time unkonwn\n– Less consistent with theoretical \\(S(t)\\) (typically step function)\n+ Does not rely on distributional assumptions\n+ Baseline hazard not necessary for estimation of hazard ratio\nParametric Survival Model\n+ Completely specified \\(h(t)\\) and \\(S(t)\\)\n+ More consistent with theoretical \\(S(t)\\)\n+ time-quantile prediction possible\n– Assumption on underlying distribution\n아래는 대표적인 분포들이며 본 글에서는 흔히 쓰는 weibull 을 다루려 한다.\n\n\n\n\n\n\n\n\n아까 비례위험가정 얘기할 때 weibull 모형은 log-log 그래프가 직선인지도 확인해야 한다고 했는데, 그 이유는 아래 식에 나와있듯이 \\(\\log(-\\log(S(t)))\\) 와 \\(\\log(t)\\) 가 정비례관계이기 때문이다.\n\\[\n\\begin{align}\nS(t) &= \\exp(-\\lambda t^p) \\\\\n-\\log(S(t)) &= \\lambda t^p \\\\\n\\log(-\\log(S(t))) &= \\log(\\lambda) + p\\log(t) \\\\\n\\log(-\\log(S(t))) &\\propto \\log(t)\n\\end{align}\n\\]\n\\(p\\) 를 scale parameter 라 하며 \\(p = 1\\) 이면 baseline hazard 가 시간에 따라 일정함을 의미하며, 자세한 내용은 https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf 를 참고하자. R의 survreg 함수를 이용하며, 결과해석은 cox model 과 동일한데 scale parameter 값이 추가로 나온다(scale parameter를 미리 정할 수도 있다).\n\nmodel.weibull &lt;- survreg(Surv(time, status) ~ trt, data = veteran)\nsummary(model.weibull)\n\n\nCall:\nsurvreg(formula = Surv(time, status) ~ trt, data = veteran)\n             Value Std. Error     z      p\n(Intercept) 4.7218     0.3275 14.42 &lt;2e-16\ntrt         0.0478     0.2079  0.23  0.818\nLog(scale)  0.1585     0.0673  2.35  0.019\n\nScale= 1.17 \n\nWeibull distribution\nLoglik(model)= -748.1   Loglik(intercept only)= -748.1\n    Chisq= 0.05 on 1 degrees of freedom, p= 0.82 \nNumber of Newton-Raphson Iterations: 5 \nn= 137 \n\n\nScale = 1.17 임을 확인할 수 있고, trt 그룹별 \\(S(t)\\) 를 그려보면 아래와 같다.\n\npcut &lt;- seq(0.01, 1, by = 0.01)  ## 1%-99%\nptime &lt;- predict(model.weibull, newdata = data.frame(trt = 1), type = \"quantile\", p = pcut, se = T)\nmatplot(cbind(ptime$fit, ptime$fit + 1.96*ptime$se.fit, ptime$fit - 1.96*ptime$se.fit), 1 - pcut,\n        xlab = \"Days\", ylab = \"Survival\", type = 'l', lty = c(1, 2, 2), col=1)\n\n\n\n\n\n\n\n\\(S(t)\\) 를 구할 수 없는 cox model 의 그림과 비교해보자.\n\nmodel.cox &lt;- exp\nkmfit.exp &lt;- survfit(exp, newdata = data.frame(trt = 1))\nplot(kmfit.exp, lty = c(1, 2, 2), col=1, lwd=2, xlab = \"Days\", ylab = \"Survival\")\n\n\n\n\n\n\n\n지금까지 생존분석 때 고려할 내용을 다루었으며 처음의 요약을 반복하면 아래와 같다.\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다.\n\n자세한 내용은 중간중간 링크한 자료들을 참고하기 바란다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html",
    "href": "posts/2023-09-27-high-dpi-slide/index.html",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "",
    "text": "지난 게시글에서 officer 패키지를 활용해 R으로 만든 그림을을 벡터 이미지로 저장하는 방법을 다루었습니다. 이렇게 저장한 벡터 이미지는 확대를 해도 깨지지 않고 파워포인트에서 편집이 가능하다는 장점이 있습니다. 하지만 파워포인트 슬라이드를 그림으로 내보내기하면 저해상도의 이미지로 저장된다는 문제가 있습니다. 따라서 이번 글에서는 파워포인트로 저장한 벡터 이미지를 300DPI의 고해상도로 내보내는 방법을 알아보고자 합니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#개요",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#개요",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "",
    "text": "지난 게시글에서 officer 패키지를 활용해 R으로 만든 그림을을 벡터 이미지로 저장하는 방법을 다루었습니다. 이렇게 저장한 벡터 이미지는 확대를 해도 깨지지 않고 파워포인트에서 편집이 가능하다는 장점이 있습니다. 하지만 파워포인트 슬라이드를 그림으로 내보내기하면 저해상도의 이미지로 저장된다는 문제가 있습니다. 따라서 이번 글에서는 파워포인트로 저장한 벡터 이미지를 300DPI의 고해상도로 내보내는 방법을 알아보고자 합니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#dpi란",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#dpi란",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "DPI란?",
    "text": "DPI란?\n\n\n이미지 출처: https://itwiki.kr/w/DPI\n\n\nDPI란 Dot Per Inch의 약자로, 인쇄물에서 1인치(= 2.54cm)에 몇 개의 점이 찍히는지를 나타내는 단위입니다. DPI 값이 높을 수록 고해상도의 결과물을 얻을 수 있으며, 깨끗한 이미지를 얻기 위해서는 300DPI 이상이 권장됩니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#고해상도로-슬라이드-내보내기",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#고해상도로-슬라이드-내보내기",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "고해상도로 슬라이드 내보내기",
    "text": "고해상도로 슬라이드 내보내기\n1. officer 패키지를 사용해 파워포인트로 이미지 저장하기\n우선 지난 게시글에서 다루었던 officer과 rvg 패키지를 활용해 벡터 이미지를 파워포인트로 저장하겠습니다.\n\nlibrary(officer)\nlibrary(rvg)\nlibrary(ggplot2)\n\n# 이미지 생성\nplotObj &lt;- iris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n# ppt\nread_pptx() |&gt; # ppt 생성, 별도의 오브젝트로 저장하지 않아도 됨.\nadd_slide() |&gt; # 슬라이드 추가\n  ph_with( # 이미지 추가\n    dml(ggobj = plotObj), \n    location = ph_location_fullsize() \n  ) |&gt;\nprint('image.pptx') # ppt 저장 \n\n파워포인트의 파일 탭에서 다른 이름으로 저장/이미지로 저장을 선택하면 슬라이드가 각각 JPEG 파일로 저장됩니다. 저장된 이미지의 속성을 살펴보면 파워포인트 이미지 내보내기의 디폴드 해상도인 96DPI로 저장된 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\n2. 내보내기 해상도 설정 변경하기\n슬라이드를 고해상도 이미지로 저장하려면, 파워포인트의 내보내기 해상도 설정을 변경해야 합니다. 설정을 변경하기에 앞서, 모든 Windows 기반 프로그램을 종료하시길 바랍니다. 실행 중인 프로그램은 Ctrl + Shift + ESC 단축키를 통해 확인할 수 있습니다.\n\n시작 단추를 우클릭한 뒤, 실행을 선택합니다.\n열기 상자에 regedit을 입력한 다음 확인을 선택합니다.\n\n사용 중인 파워포인트 버전에 따라 아래 레지스트리 하위 키를 찾습니다.\n파워포인트 버전별 레지스트리 하위 키는 다음과 같습니다.\n\n\n\nOption 하위 키를 선택하고, 편집 탭의 새로 만들기/DWORD(32비트) 값을 선택합니다\nExportBitmapResolution을 입력한 다음 엔터키를 누릅니다.\nExportBitmapResolution이 선택되어 있는지 확인한 다음 편집 탭의 수정을 선택합니다.\nDWORD 값 편집 대화 상자에서 10진수를 선택한 뒤, 값 데이터에 300을 입력하고 확인을 선택합니다.\n\n\n\n파일 메뉴에서 끝내기를 선택해 레지스트리 편집기를 종료합니다.\n3. 슬라이드를 고해상도 그림으로 내보내기\n앞서 저장했던 파워포인트 파일을 다시 열어 파일 탭의 다른 이미지로 저장/이미지로 저장을 선택해 다시 슬라이드를 JPEG 파일로 저장합니다. 이번에는 300DPI의 고해상도 이미지로 잘 저장된 것을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-27-high-dpi-slide/index.html#정리",
    "href": "posts/2023-09-27-high-dpi-slide/index.html#정리",
    "title": "R로 만든 PPT 슬라이드 고해상도로 저장하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 레지스트리 편집기에서 해상도 설정을 변경하여 파워포인트 슬라이드를 고해상도 이미지로 내보내는 방법에 대해 알아보았습니다. officer 패키지를 다룬 지난 글과 함께 R을 통해 이미지를 자유자재로 다루는 데 도움이 되기를 기대합니다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html",
    "href": "posts/2021-07-25-googleauth/index.html",
    "title": "googleAuth",
    "section": "",
    "text": "7월 17일 Virtual Box를 이용해 ShinyProxy 환경을 구축하는 강의가 있었습니다. 강의 시간에 직접 해보지 못한 Social Login 중 Google Authentication 방법에 대해 알아보겠습니다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#요약",
    "href": "posts/2021-07-25-googleauth/index.html#요약",
    "title": "googleAuth",
    "section": "요약",
    "text": "요약\n\n지난 Lecture에서 구축한 환경설정 간단하게 리뷰\nApplication.yml을 project folder에 생성하여 Social login을 사용하도록 설정\nGoogle Developer Console에서 OAuth 설정을 통해 Client Id, Key 발급과 Redirection URL 설정"
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#contents",
    "href": "posts/2021-07-25-googleauth/index.html#contents",
    "title": "googleAuth",
    "section": "Contents",
    "text": "Contents\nReview of previous lecture\n\n\nShinyManager (https://datastorm-open.github.io/shinymanager/)\n\nShiny App의 Authentication service 제공\nSigle thread 기반으로 구성되기 때문에 동시에 여러 유저가 로그인 불가\n\n\n\nShinyProxy (https://www.shinyproxy.io/)\n\nShinyManager를 사용할 때 동시에 하나의 유저만 로그인할 수 있는 한계를 극복하기 위해 도입\n여러 사용자의 동시 접근을 가능한 Shiny App deploy\n\n\n\nGoogle OAuth 설정으로 넘어가기 전에…\n\n강의시간에 만든 Virtual box의 가상이미지 실행 후 shinyproxy docker 이미지가 실행되고 있는지 확인\n\n\nsudo docker images | grep shinyproxy\n\n\n\nRstudio server 로그인 후 Terminal에서 shinyproxy 실행\n\n브라우저에서 127.0.0.1:8787로 접속 후 Virtual Box 이미지 생성 시 입력했던 아이디와 패스워드로 로그인\nRstudio Terminal에서 shinyproxy 실행\n\n\njava -jar shinyproxy-2.5.0.jar\n\n\n\n127.0.0.1:8080으로 접속하여 admin 계정을 통해 login 기능 작동 확인\n\n\n\n\n\n\n\n\n\nGoogle OAuth\n\nFacebook, Github 등 다양한 소셜 로그인 중 하나\nGoogle Developer Console https://console.cloud.google.com/apis/에서 프로젝트 생성 후 Client Id와 Client Key 발급\n발급받은 Id, Key를 application.yml 파일에 입력\nGoogle Devloper Console에서 로그인 후 redirection할 URL을 입력하면 끝!\n구체적인 과정은 네이버 블로그를 따라하세요\nShinyProxy Setting\n\napplication.yml : Authentication 방법을 정의하는 파일로 simple, LDAP, openid, social 등의 방법 선택 가능 (authentication: {원하는 로그인 방식})\n\nRStudio server에서 홈 디렉토리에 application.yml 파일 생성(아래 코드를 복사하세요)\n\nproxy:\n  title: Shiny App\n  logo-url: https://www.openanalytics.eu/shinyproxy/logo.png\n  landing-page: /\n  heartbeat-rate: 10000\n  heartbeat-timeout: 60000\n  port: 8080\n  authentication: social\n  admin-groups: scientists\n  openid:\n    auth-url: https://accounts.google.com/o/oauth2/v2/auth\n    token-url: https://www.googleapis.com/oauth2/v4/token\n    jwks-url: https://www.googleapis.com/oauth2/v3/certs\n    client-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n    client-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n    # Docker configuration\n  social:\n    google:\n      app-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n      app-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n  docker:\n    url: http://localhost:2375\n    port-range-start: 20000\n  specs:\n  - id: 01_hello\n    display-name: Hello Application\n    description: Application which demonstrates the basics of a Shiny app\n    container-cmd: [\"R\", \"-e\", \"shinyproxy::run_01_hello()\"]\n    container-image: openanalytics/shinyproxy-demo\n    access-groups: [scientists, mathematicians]\n  - id: 06_tabsets\n    container-cmd: [\"R\", \"-e\", \"shinyproxy::run_06_tabsets()\"]\n    container-image: openanalytics/shinyproxy-demo\n    access-groups: scientists\n\nlogging:\n  file:\n    shinyproxy.log\n\n\n\n\nauthenticaiton을 openid로 설정할 경우\n\n\n위 코드의 openid 부분과 동일하게 auth, token, jws url 작성\nclient-id와 client-secret을 Google Developer Console에서 확인 후 입력\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/login/oauth2/code/shinyproxy\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행 (localhost:8080로 접근하기 위해서는 redirect url에 http://localhost:8080/login/oauth2/code/shinyproxy 등록)\n\n\nauthentication을 social로 설정할 경우\n\n\n위 코드의 social 부분과 동일하게 작성\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/signin/google\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행\n마무리\n\nShinyProxy는 Spring framework 기반으로 작성되어 application.yml 수정을 통해 여러 인증방법을 적용할 수 있습니다.\nFacebook, Github 등 여러 API provider들이 구글과 비슷한 인증 서비스를 제공하고 있어 손쉽게 여러 social 인증을 추가할 수 있습니다.\n발급받은 client id와 secret은 외부로 노출되어서는 안됩니다(Github Repo에 commit 금지!)\nsocial과 openid 모두 적용할 수 있으나 redirect url에 차이가 있음에 주의 (정확한 차이는 연구 필요)"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html",
    "href": "posts/2023-05-24-surveyDashboardR/index.html",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "",
    "text": "R Shiny의 활용성은 무궁무진합니다. 다양한 Shiny 전용 패키지와 함수, Javascript, CSS와의 연계성 등 다양한 기능을 활용해 반응형 웹페이지 제작이 가능합니다.\n본 글에서는 이렇게 무궁무진한 R Shiny로 웹 기반 방역관리 위험도 평가 대시보드를 제작한 경험을 공유하며 사용한 여러 기능을 소개하려고 합니다.\nUI의 전반적인 레이아웃은 Shinydashboard를 사용했으며 본문에선 Shinydashboard와 Shiny의 기본 함수에 대한 설명은 생략합니다. 두 패키지에 대한 기본적인 함수 및 구조 설명은 아래 링크를 참조하세요.\nShinydashboard package : Shiny Dashboard\nShiny package : Mastering Shiny"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#db-dbi-rsqlite",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#db-dbi-rsqlite",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "1. DB : DBI & RSQLite\n",
    "text": "1. DB : DBI & RSQLite\n\n본 대시보드는 사용자별로 데이터 무결성이 유지되어야 했고 지속적인 데이터 수집이 필요했기에 DB를 따로 운영하기로 했습니다.\nDB를 다룰 수 있는 DBI 패키지와 간단한 DBMS인 SQLite를 사용할 수 있는 RSQLite를 사용하여 DB 설계 및 유지 보수를 진행했습니다.\n다음과 같이 DB connect / disconnect 함수를 따로 지정하여 초기 설정을 해줍니다.\n\nlibrary(DBI);library(RSQLite)\n\n# connection\ncon &lt;- function() {\n  DBI::dbConnect(SQLite(),\n            dbname = \"[path]/[DBname].sqlite\")    \n  \n  #기존에 SQLite DB가 존재한다면 Connected, 아니면 create DB\n}\n\n# disconnection\ndiscon &lt;- function(){\n  dbDisconnect(con())\n}\n\nDBI 패키지의 대부분의 함수는 parameter로 DBIconnection object를 요구합니다. 따라서 다음과 같이 con() 이라는 함수를 parameter로 호출하여 DBI의 함수를 사용할 수 있습니다.\n\n# Example :  DBname : database.sqlite\n\nDBI::dbExecute(con(), \"CREATE TABLE table1 (\n                                 keyvalue INTEGER(10) PRIMARY KEY,\n                                 value1 DATE,\n                                 value2 VARCHAR(20) CHECK(value2 IN ('a', 'b', 'c'))\n                                 )\")\ndiscon()\n\nDBI::dbExecute(con(), \"INSERT INTO table1 VALUES (?, ?, ?)\", c(1, \"2023-05-24\", \"a\"))\nDBI::dbGetQuery(con(), \"SELECT * FROM table1\")\n\ndbExecute 함수를 통해 실행하고자 하는 SQL문을 실행시킵니다. 이전에 생성한 con() 함수를 통해 DB에 연결한 뒤 SQL문을 문자열 형태로 입력하여 실행합니다. 또한 SQL문에 동적으로 R의 변수를 넣어야하는 경우, ?를 통해 SQL문을 입력한 뒤에 ?에 넣고자 하는 순서에 맞게 뒤에 vector 형태로 입력하면 됩니다.\ndbGetQuery의 경우SELECT문같이 SQL Query의 결과를 갖고 오고 싶은 경우 사용합니다. 추출 결과를 dataframe 형태로 가져옵니다.\n본 대시보드 제작 과정에선 RSQLite를 이용하여 Table 4개를 운용하였고, SQLite에서도 당연히 참조 무결성 제약조건(PK-FK)을 생성할 수 있기때문에 직접 R에서 코드를 작성하지 않고 SQL문을 통해 미리 제약조건을 생성할 수 있습니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#log-in-register-shinyauthr-customizing",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#log-in-register-shinyauthr-customizing",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "2. Log in / Register : shinyauthr Customizing",
    "text": "2. Log in / Register : shinyauthr Customizing\nShiny 패키지 중 Log in/Log out UI를 제공하는 패키지는 대표적으로 shinymanager와 shinyauthr가 있습니다. 그러나 Shinymanager에는 회원가입 기능을 추가하기가 어려운 부분이 있어 shinyauthr를 사용하되, 살짝 코드를 수정하여 Log in/Log out/Register option을 Web에 추가하였습니다.\nshinyauthr package의 주요 함수는 다음과 같습니다.\n\n\nloginUI : log in UI를 보여주는 함수로 화면에 나타나는 메시지를 수정가능합니다.\n\nloginServer : log in의 base가 되는 database를 설정하고 id, password 설정, 쿠키 로그인 설정이 가능합니다.\n\nlogoutUI : log out UI를 보여주는 함수로, 화면에 나타나는 메시지 및 css style을 수정할 수 있습니다.\n\nlogoutServer : log out시 실행되는 함수로, 사용자의 권한(user_auth)를 boolean reactive 형태로 관리합니다.\n\nlogin logic : loginUI에 ID/PW 입력 -&gt; loginServer에서 확인 후 권한 부여\nlogout logic : logoutUI의 logout button 클릭 -&gt; logoutServer에서 reactive하게 user_auth 상태 변경\n (자세한 내용은 shinyauthr package 참조) \nUI의 일반적인 구성은 다음과 같습니다.\n\nui &lt;- dashboardPage(\n  \n  skin = \"black\",\n  header = dashboardHeader(\n    title = (\"방역관리 위험도 평가\"),\n    tags$li(class = \"dropdown\", style = \"padding: 8px;\", shinyauthr::logoutUI(\"logout\"))\n  ),\n  \n  sidebar = dashboardSidebar(\n    # menu \n  ),\n  \n  body = dashboardBody(\n    shinyauthr::loginUI(\"login\",\n                        title = h4(HTML(\"&lt;center&gt; 이름과 전화번호를 입력해주세요 &lt;/center&gt;\")), \n                        user_title = \"이름\", \n                        pass_title = \"전화번호\", \n                        login_title = \"로그인\",\n                        error_message = h6(\"유효하지 않은 정보입니다. 처음이시라면 회원가입을 눌러주세요.\", style = \"color : red\"),\n                        additional_ui = tags$a(\n                          actionBttn(\n                            inputId = \"register\",\n                            label = \"회원가입\",\n                            style = \"fill\", \n                            color = \"danger\",\n                            size = \"xs\"\n                          )\n                        )\n    ),\n    # menu 별 UI\n  )\n)\n\n본 UI는 shinydashboard를 기반으로 구성되어있습니다. 따라서 header에 logout 버튼을 생성하려 했고, dashboradBody에 login UI를 배치해 메인 화면에 바로 로그인 화면이 나오도록 하였습니다. 또한 loginUI의 parameter를 원하는 텍스트로 설정하였고, 회원가입은 additional_ui parameter를 이용하여 추가적으로 shinyWidgets::actionBttn 에 따른 UI가 생성되도록 설계하였습니다. 아래는 로그인 화면과 회원가입 버튼 클릭 시의 화면입니다.\n\n\n\n\n로그인 화면\n\n\n\n\n회원가입 화면\n\n\n\n그러나 loginServer 함수의 경우, 기본 ID/PW 저장 DB 세팅이 tidyverse 패키지의 tibble형태로 작성되어 있기 때문에 동적 형태의 DB를 지원하고 있지 않습니다.\n예를 들어 회원 가입 후 즉시 로그인하기 위해선 새로운 사용자의 정보가 반영된 DB를 reactive하게 Call 해야하는데 이 부분에 제약사항이 있었습니다.\n따라서 reactive하게 DB 정보를 받아올 수 있도록 함수를 수정해야 했습니다. (다음 링크 참조 : Shinyauthr loginServer Customizing)\n\nserver &lt;- function(input, output, session) {\n  \n#login/logout function--------------------------------------\n\n  credentials &lt;- Myloginserver(\n      id = \"login\",\n      log_out = reactive(logout_init()),\n      reload_on_logout = TRUE\n  )\n  \n  logout_init &lt;- shinyauthr::logoutServer( \n    \"logout\", \n    reactive(credentials()$user_auth)\n  )\n  \n  userdata &lt;- reactive({\n    credentials()$info\n  })\n  \n}\n\nMyloginserver는 customizing 된 shinyauthr::loginServer 함수이며 미리 생성된 사용자 정보 DB table을 바탕으로 로그인이 되도록 설정되어 있습니다. 기본적으로 shinyauthr::loginServer 함수는 info와 user_auth라는 변수를 담고 있습니다.\n\ninfo의 경우, 로그인의 기반이 되는 DB 내에서 사용자의 ID/PW에 해당하는 row의 컬럼 값들을 table 형태로 저장하고 있습니다.\nuser_auth의 경우, 권한이 있느냐 없느냐를 나타내며 로그인이 완료될 시 TRUE, 아닐 시 FALSE 값을 가지게 됩니다.\n\n따라서 credentials이라는 변수에 loginServer 함수를 저장하면 reactive한 user_auth값과 info값을 지니게 됩니다.\nreload_on_logout = TRUE로 설정하면 로그 아웃 시 세션이 초기화되어 자동으로 로그인 화면으로 돌아갑니다. 기본적으로 세션이 초기화되면 credentials()$user_auth == FALSE가 되어 로그인이 취소되고 권한이 사라지므로, 좀 더 확실한 로그아웃을 위해 다음과 같은 옵션을 설정하였습니다.\nlogout_init은 logoutServer 함수로 logoutUI의 ID를 받아온 뒤, reactive 함수에 따라 logout 버튼을 보여줄 지 숨길 지 반응형으로 설정할 수 있습니다. 기본적으로 권한이 있느냐 없느냐에 따라 logout 버튼을 보여주거나 숨겨야하기때문에 위와 같이 설정해줍니다.\n또한 shinyauthr 패키지는 로그인 / 로그아웃 UI만 보여주고 로그인 상태에 따른 UI 변환 기능은 없기 때문에, 수동으로 로그인 완료시에만 보여주고 싶은 UI에는 옵션을 추가해야 합니다.\nreq(credential()$user_auth)\n다음과 같은 옵션을 uiOutput을 이용하여 renderUI에 추가하거나, 아래와 같이 MenuOutput 과 renderMenu를 이용하여 shinydashboard의 Menu 자체를 숨길 수 있습니다.\n\n#UI\n\nui &lt;- dashboardPage(\n  \n  # others\n  \n  sidebar = dashboardSidebar(\n    sidebarMenu(\n      id = \"tabs\",\n      menuItemOutput(\"check\")\n    )\n  ),\n  \n  body = dashboardBody(\n    tabItems(\n      tabItem(tabName = \"check\",\n              # UI contents\n      )\n    )\n  )\n)\n\n#Server\nserver &lt;- function(input, output, session) {\n  \n  output$check &lt;- renderMenu({\n    req(credentials()$user_auth)\n    menuItem(\"방역관리자 업무 점검\", icon = icon(\"check\", lib =\"glyphicon\"), tabName = \"check\")\n  })\n}"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#필수-응답-설문-설정-shinyvalidate",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#필수-응답-설문-설정-shinyvalidate",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "3. 필수 응답 설문 설정 : shinyvalidate\n",
    "text": "3. 필수 응답 설문 설정 : shinyvalidate\n\nshinyvalidate package는 selectInput, numericInput 등 Input function에 대해 사용자의 입력값에 대한 제약조건을 걸 수 있는 package입니다.\n기본적으로 필수적으로 응답해야는 부분에 대해 설정할 수 있으며, ‘&lt;’ 등의 연산자를 사용한 제약조건, ‘email’ 형식 제약조건 등 여러가지 option이 있습니다.\n본 개발에선 필수 응답 설문 항목에 대한 설정을 위해 다음과 같이 shinyvalidate package를 사용하였습니다.\n\n[validation name] &lt;- shinyvalidate::InputValidator$new()\n[validation name]$add_rule(\"input$[input variable]\", sv_required(\"[Warning message]\"))\n\n기본적인 Logic은 다음과 같습니다.\n\n\nInputValidator$new()를 통한 validation 변수 선언\n\nadd_rule을 통한 input variable별 Warning Message 작성\n\n\n\n회원가입 시 shinyvalidate 사용 예시\n\n다음과 같이 필수 입력 항목의 경우, 응답하지 않을 시 기본적으로 빨간색 테두리와 작성한 경고 메시지가 뜨게 됩니다.\n추가적으로 이러한 필수 응답 항목에 답하지 않을 시 다음단계로 지나가지 못하게 제약조건을 추가할 수 있습니다.\n    1. req([validation name]$is_valid())\n\n    2. if([validation name]$is_valid())\n다음과 같은 옵션을 추가하여 renderUI 혹은 actionButton click시의 전제 조건으로 추가하여 Web 설계가 가능합니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#reactable-onclick-활용-ui에서-동적으로-db-table-update하기",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#reactable-onclick-활용-ui에서-동적으로-db-table-update하기",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "4. Reactable onClick 활용 : UI에서 동적으로 DB Table Update하기",
    "text": "4. Reactable onClick 활용 : UI에서 동적으로 DB Table Update하기\n본 대시보드의 관리자 버전에선 관리자가 컨설팅 완료 여부를 입력해야하는 기능이 필요했습니다.\n또한 DB의 값이 관리자의 Web 내의 완료 여부 입력에 따라 같이 변경되어야 했습니다.\n따라서 reactable 형태로 DB의 테이블을 보여준 뒤 cell 마다 onClick 옵션을 추가하여 binary 형태로 값을 자유롭게 변경할 수 있도록 기능을 추가했습니다.\n아래는 reactable에 구현한 예시 코드입니다.\n\n consult_rt &lt;- function(table){\n    rt &lt;-reactable(\n      data = table, # DB에서 받아온 테이블\n      onClick = JS(\"function(rowInfo, colInfo) {\n                      var tb_index = {'colId': colInfo.id, 'rowId': rowInfo.id };\n                      Shiny.setInputValue('consult_index', tb_index, { priority: 'event' })}\"),\n      \n        #...이외 내용 생략\n        \n\n    )\n}\n\n먼저 onClick parameter를 통해 각 cell을 클릭할 시의 reactive action을 설정해줍니다.\nJS 함수를 통해 JavaScript 코드를 호출하였고, tb_index라는 variable을 선언하여 row Id와 column Id를 저장한 뒤 Shiny 내 consult_index라는 input 변수에 tb_index의 값이 저장되도록 하였습니다.\n따라서 이 input 변수를 다음과 같이 활용하였습니다.\n\nobserveEvent(input$consult_index,  {\n    # table : reactable에 사용한 table로 똑같이 DB에서 받아옴\n\n    rowid &lt;- as.integer(input$consult_index$rowId) + 1\n    colid &lt;- input$consult_index$colId \n    colname &lt;- consult_list[match(colid, consult_list_name)] #원래 DB 컬럼명\n    userKey &lt;- table[rowid][[\"KeyName\"]] #해당 row의 DB Key\n    value &lt;- table[rowid][[colname]] #실제 DB에서의 값\n    \n    colname &lt;- consult_list[match(colid, consult_list_name)]\n    date &lt;- as.character(Sys.Date())\n    if(value %&gt;% is.na()){\n      message &lt;- h5(\"컨설팅 완료 상태로 변경되었습니다.\")\n      query &lt;- paste0(\"UPDATE consult SET \", colname, \" = ? WHERE PKcolumn = ?\")\n      dbExecute(con(), query, c(date, userKey))\n      discon()\n    }else{\n      message &lt;- h5(\"컨설팅 미완료 상태로 변경되었습니다.\")\n      query &lt;- paste0(\"UPDATE consult SET \", colname, \" = ? WHERE PKcolumn = ?\")\n      dbExecute(con(), query, c(NA, userKey))\n      discon()\n    }\n    \n})\n\nconsult_incex에 저장된 row Id와 column Id는 index 형태로, input$consult_index$rowId 형식으로 값을 받아올 수 있습니다.\n받아온 index는 0부터 시작하고, DB table이 저장된 data.table 형태의 table은 index가 1부터 시작하기 때문에 +1 해주어 row의 index를 받아왔습니다.\n이렇게 row의 index와 column의 index를 rowid, colid에 저장한 다음 table[rowid][[\"Key Name\"]]을 통해 현재 row의 DB 내 key 값을 받아왔습니다.\n현재 DB는 컨설팅 완료 시에는 컨설팅 완료 날짜를, 미완료 시에는 NA로 저장되어 있기 때문에 이를 ifelse 구문을 활용하여 각각의 경우에 맞게 코드를 작성하였습니다.\n위에서 받아온 row의 DB Key 값을 이용해 SQL문으로 DB table에 접근하여 값을 update 해주었습니다.\n (이렇게 변경된 DB table이 반영된 reactable을 사용자에게 동적으로 보여주기 위해선 UI 함수와 reactable을 재호출해야하는데, 이 부분에 대해선 생략하겠습니다.) \n또한 shinyalert를 사용하여 값이 변경되었음을 팝업 메시지로 띄워주었습니다.\n아래는 실제 UI에서 구현된 예시입니다.\n\n\n\n\n변경 전\n\n\n\n\n변경 후"
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#이-외",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#이-외",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "5. 이 외",
    "text": "5. 이 외\n(1) reactable : reactive download to csv\nadmin을 위한 web page 제작 과정에서, DB table 들로부터 받아온 사용자 정보를 reactable을 이용하여 다음과 같이 제작하였습니다.\n\n\n사용자 정보\n\n이러한 table을 csv로 다운로드 받을 수 있게 하면서도, 만약 사용자가 일부 사용자의 정보를 확인하고 싶어 검색 기능을 사용했을 때 보여지는 subset table을 reactive하게 csv로 다운로드 받을 수 있게 하려했습니다.\n이 부분은 Reactable 공식 문서의 JavaScript API 이용 부분을 차용하여 작성하였습니다.\n\nbox(width = 12, title = \"전체 사용자 정보\" %&gt;% h5c,\n                    reactableOutput(\"infotable\"), \n                    br(),\n                    htmltools::browsable(\n                      tagList(\n                        tags$button(\n                          tagList(fontawesome::fa(\"download\"), \"Download CSV\"),\n                          onclick = \"Reactable.downloadDataCSV('user_info', '[filename].csv')\"\n                        )\n                      )\n                    )\n    )\n\nUI 작성 Part에서 box를 통해 reactable을 이용한 사용자 정보를 보여주려고 한다면 위와 같이 filename 부분을 수정하여 작성하면 현재 Web에 보여지는 reactable을 Download to csv가 가능합니다.\n(2) lapply를 이용한 설문 UI 간단하게 만들기\n만약 설문의 스타일이 간단하거나 (Ex: 예/아니오 유형, 체크 유형, 점수 유형) 반복되는 경우 lapply함수를 통해 좀 더 간단하고 정갈하게 UI 및 Server 코드를 작성할 수 있습니다.\n예를 들어, 해당하는 항목에 체크하는 형식의 설문이라면, 아래와 같이 미리 설문내용만 list 형태로 만들어 놓을 수 있습니다.\n\n# example 5개 Question\n\nQ_list &lt;- c(\n    \" 시설 위험도 평가 후 시설 별 맞춤형 방역관리 지침을 마련하였는가?\", \n    \" 정기적으로 종사자들에게 방역 수칙 교육·안내 하였는가?\",\n    \" 발열 및 호흡기 증상 유무를 확인하고 증상이 있는 경우 즉시 검사받도록 안내하였는가?\",\n    \" 감염병 예방수칙 홍보 안내문을 잘 보이는 곳에 배치 하였는가?\",\n    \" 환기 대장 및 소독 대장을 배치 하였는가?\"\n)\n\n# Shiny Input 함수의 ID를 각 설문 항목별 name으로 지정 (이 때 DB를 사용하실 거라면 DB의 컬럼명으로 ID를 지정하면 간편합니다)\nnames(Q_list) &lt;- paste0(\"q\", 1:5)\n\n다음과 같이 설문 내용을 정리한 변수를 기반으로 Shiny 코드를 작성할 수 있습니다.\n\n# UI code \n  lapply(names(Q_list)[1:5], function(x){\n                              checkboxInput(x, Q_list[[x]], value = F)\n                             })\n\n# Server code\n  \n  ## 답변 여부에 따라 Yes or No 형식으로 저장 \n  inputlist &lt;- lapply(names(Q_list), function(x){\n      if(input[[x]]){\n        \"Yes\"\n      }else{\n        \"No\"\n      }\n  })\n  \n  ## DB에Insert (DB 컬럼 순서에 맞게) \n  DBI::dbExecute(con(), paste0(\"INSERT INTO [table name] values ('\", paste(unlist(inputlist), collapse = \"', '\"), \"')\") )\n  discon()\n\nShiny의 UI 내에서도 간편하게 위와 같이 lapply 함수를 사용하여 Input 함수를 나타내는 것이 가능합니다.\nInput[[x]]로 사용자의 입력 내용에 접근 가능하므로 다음과 같이 inputlist로 Input 값에 접근에 DB에 저장할 수 있습니다.\n설문 내용을 value로, Input ID 및 DB columnname을 name으로 할 시 좀더 간편하게 코드 작성이 가능합니다.\n답변 유형(Yes or No, 점수 등등)에 따라 설문 datatable을 만들어 놓은 뒤 이에 맞게 datatable 컬럼 별로 lapply 함수를 잘 사용하여 UI 코드를 작성하면 보다 간편하게 Shiny를 이용하실 수 있습니다."
  },
  {
    "objectID": "posts/2023-05-24-surveyDashboardR/index.html#마치며",
    "href": "posts/2023-05-24-surveyDashboardR/index.html#마치며",
    "title": "R Shiny 기반 방역관리 위험도 평가 대시보드",
    "section": "마치며",
    "text": "마치며\nR만을 이용하여 Server와 UI를 동시에 제작하고 배포할 수 있다는 것이 R Shiny 의 가장 큰 장점입니다. 또한 오픈 소스가 활성화되어 있어 다양한 사용자 개발 library를 이용할 수 있으므로 사용하고 싶은 기능은 웬만하면 사용이 가능하며, 기존 library의 함수를 원하는 방식으로 변형하여 사용할 수도 있습니다. JavaScript나 CSS와도 호환이 잘 되기 때문에 UI 디자인까지 R이라는 한 Tool로 작업이 가능하기 때문에 상당히 편리합니다.\n본 글에선 이러한 R Shiny 웹 App의 기반이 되는 로그인 시스템, DB 운용 등 잘 알려지지 않은(?) 여러가지 세세한 기능과 가능성에 대해 소개해보았습니다.\n이외에 여러 package를 공부해가며 나만의 Shiny Web App을 제작해보는 과정 역시 재미있으니, 많은 분들께서 R Shiny를 이용하여 멋진 웹 페이지 제작에 참여해보시길 바라겠습니다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 1주차 강의록을 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다. 본 블로그에 정리 내용이 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 첫 번째에 해당하며 2주차에 tidyverse 를 다룰 것이다. data.table 은 이번 교육에는 포함시키지 않았는데, R에 익숙해지면서 느린 속도가 점점 거슬린다면 data.table 을 시작할 때이다.\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nproject 생성\n\n\n\n개인 PC에서 실습을 원한다면 http://www.r-project.org 와 https://rstudio.com/products/rstudio/download/#download 에서 R과 RStudio 를 설치하자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "\nR 기초연산 : 벡터(vector)",
    "text": "R 기초연산 : 벡터(vector)\nR 의 기본 연산단위는 벡터이며, x &lt;- c(1, 2, 3) 은 1,2,3 으로 이루어진 길이 3인 벡터를 x 에 저장한다. 대입연산자는 = 와 &lt;- 둘 다 가능하지만 함수의 인자로도 쓰이는 = 와 구별하기 위해 &lt;- 를 권장한다. 자주 쓰는 연산을 실습하자.\n\nx &lt;- c(1, 2, 3, 4, 5, 6)            ## vector of variable\ny &lt;- c(7, 8, 9, 10, 11, 12)\nx + y                                  \n\n[1]  8 10 12 14 16 18\n\nx * y\n\n[1]  7 16 27 40 55 72\n\nsqrt(x)                            ## root\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490\n\nsum(x)                                \n\n[1] 21\n\ndiff(x)                            ## difference\n\n[1] 1 1 1 1 1\n\nmean(x)                            ## mean  \n\n[1] 3.5\n\nvar(x)                             ## variance\n\n[1] 3.5\n\nsd(x)                              ## standard deviation\n\n[1] 1.870829\n\nmedian(x)                          ## median\n\n[1] 3.5\n\nIQR(x)                             ## inter-quantile range\n\n[1] 2.5\n\nmax(x)                             ## max value\n\n[1] 6\n\nwhich.max(x)                       ## order of max value\n\n[1] 6\n\nmax(x, y)                          ## max value among x & y\n\n[1] 12\n\nlength(x)                          \n\n[1] 6\n\n\nmax(x, y) 는 x, y 각각의 최대값이 아닌, 전체에서 최대인 값 1개를 보여줌을 기억하자. 잠시 후 각각의 최대값 구하는 연습문제가 나온다.\n벡터에서 특정 항목을 골라내려면 그것의 위치 혹은 조건문을 이용한다.\n\nx[2]                               ## 2 번째\n\n[1] 2\n\nx[-2]                              ## 2 번째만 빼고\n\n[1] 1 3 4 5 6\n\nx[1:3]                             ## 1-3 번째\n\n[1] 1 2 3\n\nx[c(1, 2, 3)]                      ## 동일 \n\n[1] 1 2 3\n\nx[c(1, 3, 4, 5, 6)]                ## 1, 3, 4, 5, 6  번째\n\n[1] 1 3 4 5 6\n\nx &gt;= 4                             ## 각 항목이 4 이상인지 TRUE/FALSE\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\nsum(x &gt;= 4)                        ## TRUE 1, FALSE 0 인식 \n\n[1] 3\n\nx[x &gt;= 4]                          ## TRUE 인 것들만, 즉 4 이상인 것들         \n\n[1] 4 5 6\n\nsum(x[x &gt;= 4])                     ## 4 이상인 것들만 더하기. \n\n[1] 15\n\nx %in% c(1, 3, 5)                  ## 1, 3, 5 중 하나에 속하는지 TRUE/FALSE\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nx[x %in% c(1, 3, 5)]               \n\n[1] 1 3 5\n\n\n벡터만들기\nseq 로 일정 간격인, rep 로 항목들이 반복되는 벡터를 만들 수 있다.\n\nv1 &lt;- seq(-5, 5, by = .2); v1             ## Sequence\n\n [1] -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2\n[16] -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2  0.0  0.2  0.4  0.6  0.8\n[31]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[46]  4.0  4.2  4.4  4.6  4.8  5.0\n\nv2 &lt;- rep(1, 3); v2                       ## Repeat\n\n[1] 1 1 1\n\nv3 &lt;- rep(c(1, 2, 3), 2); v3              ## Repeat for vector\n\n[1] 1 2 3 1 2 3\n\nv4 &lt;- rep(c(1, 2, 3), each = 2); v4       ## Repeat for vector : each\n\n[1] 1 1 2 2 3 3\n\n\n\nfor, if/else, ifelse 문\nfor loop 는 같은 작업을 반복할 때 이용하며 while 도 비슷한 의미이다. 예시를 통해 배워보자.\n\nfor (i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\ni &lt;- 0\nfor (j in c(1, 2, 4, 5, 6)){\n  i &lt;- i + j\n}\ni\n\n[1] 18\n\n\nif 와 else 는 조건문을 다룬다. else 나 else if 문은 선행 조건문의 마지막과 같은 줄이어야 함을 기억하자.\n\nx &lt;- 5\nif (x &gt;= 3 ){\n  x &lt;- x + 3\n}\nx\n\n[1] 8\n\nx &lt;- 5\nif (x &gt;= 10){\n  print(\"High\")\n} else if (x &gt;= 5){\n  print(\"Medium\")\n} else {\n  print(\"Low\")\n}                                          ## else if, else 주의: 반드시 } 와 같은 줄에 위치하도록.\n\n[1] \"Medium\"\n\n\nifelse 는 벡터화된 if/else 문으로 벡터의 각 항목마다 조건문을 적용하는데, 엑셀의 if 문과 비슷하다.\n\nx &lt;- 1:6\ny &lt;- ifelse(x &gt;= 4, \"Yes\", \"No\")           ## ifelse (조건,참일때,거짓일때)\ny\n\n[1] \"No\"  \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\"\n\n\n함수 만들기\n막 R을 배우는 단계에서는 함수를 만들어 쓸 일이 거의 없겠지만, 결측치 포함된 데이터에서 평균이나 분산을 구할 때 귀찮을 수 있다. R은 결측치가 하나라도 포함되면 평균값, 분산값으로 NA를 출력하기 때문이다. 이를 해결하기 위해서라도 아래처럼 기초 함수 만드는 법은 알고 있는 것이 좋다.\n\nx &lt;- c(1:10, 12, 13, NA, NA, 15, 17)      ## 결측치가 포함되어 있다면..\nmean(x)                                           \n\n[1] NA\n\nmean0 &lt;- function(x){\n  mean(x, na.rm = T)\n}                                         ## mean함수의 na.rm 옵션을 TRUE로 바꿈. default는 F\n\nmean0 &lt;- function(x){mean(x, na.rm = T)}  ## 한줄에 쓸 수도 있다. \nmean0(x)\n\n[1] 8\n\n\n둘 이상의 변수를 포함한 함수도 다음과 같이 만들 수 있다.\n\ntwomean &lt;- function(x1, x2){\n  a &lt;- (x1 + x2)/2\n  a\n}\ntwomean(4, 6)\n\n[1] 5\n\n\nApply 문 : apply, sapply, lapply\n\n벡터를 다루는 연산을 잘 활용하면, 벡터의 각 항목에 대해 for loop 을 쓰는 것보다 간편하게 코드를 작성할 수 있다. 행렬에서 행마다 평균을 구하는 예를 살펴보자.\n\nmat &lt;- matrix(1:20, nrow = 4, byrow = T)   ## 4행 5열, byrow = T : 행부터 채운다. \nmat\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\n\n모든 행에 대해 for loop 을 이용, 평균을 구하여 저장하는 코드는 아래와 같다.\n\nout &lt;- NULL                                ## 빈 벡터, 여기에 하나씩 붙여넣는다.\nfor (i in 1:nrow(mat)){\n  out &lt;- c(out, mean(mat[i, ]))\n}\nout\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고 여기에 결과를 붙여가는 모습이 번거로워 보인다. sapply 또는 lapply 를 사용하면 행 또는 열 단위 연산을 간단히 수행할 수 있다.\n\nsapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return vector\n\n[1]  3  8 13 18\n\nlapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return list type\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 18\n\nunlist(lapply(1:nrow(mat), function(x){mean(mat[x, ])}))     ## Same to sapply\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고, 이어붙이는 과정이 생략되어 간단한 코드가 되었다. list 는 벡터보다 상위개념으로 모든 것을 담을 수 있는 큰 그릇에 비유할 수 있는데, 본 강의에서는 unlist 를 취하면 벡터나 행렬을 얻게 된다는 정도만 언급하고 넘어가겠다. 사실 행렬의 행/열 단위 연산은 apply 혹은 row***, col*** 시리즈의 함수가 따로 있어, 더 간단히 이용할 수 있다.\n\napply(mat, 1, mean)                                          ## 1: 행\n\n[1]  3  8 13 18\n\nrowMeans(mat)                                                ## 동일\n\n[1]  3  8 13 18\n\nrowSums(mat)                                                 ## 행별로 합\n\n[1] 15 40 65 90\n\napply(mat, 2, mean)                                          ## 2: 열\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\ncolMeans(mat)                                                ## 열별로 합\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\n연습문제 1\nsapply나 lapply를 이용하여, 아래 두 벡터의 최대값을 각각 구하여라.\n\nx &lt;- 1:6\ny &lt;- 7:12\n\n\n정답 보기\n\nlapply(list(x, y), max)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 12\n\n  sapply(list(x, y), max)\n\n[1]  6 12\n\n\n\n멀티코어 병렬연산으로 apply 를 빠르게 수행할 수도 있는데 본 강의에서는 생략한다. 궁금하신 분은 과거 정리 내용 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "데이터 불러와서 작업하기",
    "text": "데이터 불러와서 작업하기\n이제부터는 실제 데이터를 읽어서 그 데이터를 매니징 하는 방법을 배워보도록 하겠다.\n데이터 불러오기, 저장하기\n데이터를 불러오기 전에 미리 디렉토리를 지정하면 그 다음부터는 편하게 읽고 쓸 수 있다.\n\ngetwd()                                                     ## 현재 디렉토리 \nsetwd(\"data\")                                               ## 디렉토리 설정\n## 동일\nsetwd(\"/home/js/Homepage/blog/_posts/2020-02-16-rdatamanagement-basic/data\")\ngetwd()\n\n폴더 구분을 / 로 해야 한다는 점을 명심하자 (\\\\ 도 가능). R 은 유닉스 기반이기 때문이다. 이제 실습 데이터를 읽어볼텐데, 가급적이면 데이터 포맷은 csv로 만드는 것을 추천한다. 콤마로 분리된 가장 간단한 형태로, 용량도 작고 어떤 소프트웨어 에서도 읽을 수 있기 때문이다. 물론 Excel, SPSS, SAS 파일도 읽을 수 있는데, 변수명이나 값에 한글이 있으면 encoding 에러가 생길 수 있으므로 미리 처리하자.\n\nex &lt;- read.csv(\"example_g1e.csv\")\nhead(ex)\n\nURL 링크를 이용할 수도 있다.\n\nex &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n\n\n\n  \n\n\n\nExcel 파일은 readxl, SAS나 SPSS는 haven 패키지를 이용한다.\n\n#install.packages(c(\"readxl\", \"haven\"))                    ## install packages    \nlibrary(readxl)                                            ## for xlsx\nex.excel &lt;- read_excel(\"example_g1e.xlsx\", sheet = 1)      ## 1st sheet\n\nlibrary(haven)                                             ## for SAS/SPSS/STATA   \nex.sas &lt;- read_sas(\"example_g1e.sas7bdat\")                 ## SAS\nex.spss &lt;- read_sav(\"example_g1e.sav\")                     ## SPSS\nhead(ex.spss)\n\n아래와 같이 Excel, SAS, SPSS 데이터는 read.csv 와 형태가 좀 달라보인다. 이것은 최근 R에서 인기있는 tidyverse 스타일의 데이터인데, 자세한 내용은 다음 강의에서 다룰 예정이니 일단 넘어가자.\n\n\n\n  \n\n\n\n파일 저장은 write.csv 를 이용하며, 맨 왼쪽에 나타나는 행 넘버링을 빼려면 row.names = F 옵션을 추가한다.\n\nwrite.csv(ex, \"example_g1e_ex.csv\", row.names = F)\n\nhaven 패키지에서 write_sas 나 write_sav 도 가능하다.\n\nwrite_sas(ex.sas, \"example_g1e_ex.sas7bdat\")\nwrite_sav(ex.spss, \"example_g1e_ex.sav\")"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "읽은 데이터 살펴보기",
    "text": "읽은 데이터 살펴보기\n본격적으로 데이터를 살펴보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n데이터 살펴보기\nhead 로 처음 6줄, tail 로 마지막 6줄을 볼 수 있다. 데이터 간단히 확인하려고 쓰인다.\n\nhead(ex)                                                   ## 처음 6행\ntail(ex)                                                   ## 마지막 6행\nhead(ex, 10)                                               ## 처음 10행\n\n\n\n\n  \n\n\n\nstr 은 head 와는 다른 방식으로 데이터를 확인한다. int 는 정수, num 은 실수형을 의미한다.\n\nstr(ex)\n\n'data.frame':   1644 obs. of  32 variables:\n $ EXMD_BZ_YYYY  : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...\n $ RN_INDI       : int  562083 334536 911867 183321 942671 979358 554112 487160 793017 219397 ...\n $ HME_YYYYMM    : int  200909 200911 200903 200908 200909 200912 200911 200908 200906 200912 ...\n $ Q_PHX_DX_STK  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTDZ : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTN  : int  1 0 0 NA NA NA NA NA NA 1 ...\n $ Q_PHX_DX_DM   : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_DLD  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_PTB  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q_HBV_AG      : int  3 2 3 3 3 2 2 3 3 3 ...\n $ Q_SMK_YN      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Q_DRK_FRQ_V09N: int  0 0 0 0 0 0 0 0 0 0 ...\n $ HGHT          : int  144 162 163 152 159 157 160 159 156 146 ...\n $ WGHT          : int  61 51 65 51 50 55 56 54 53 48 ...\n $ WSTC          : int  90 63 82 70 73 73 67 66 67 78 ...\n $ BMI           : num  29.4 19.4 24.5 22.1 19.8 22.3 21.9 21.4 21.8 22.5 ...\n $ VA_LT         : num  0.7 0.8 0.7 0.8 0.7 1.5 1.5 1.2 1.2 1.5 ...\n $ VA_RT         : num  0.8 1 0.6 0.9 0.8 1.5 1.5 1.5 1 1.5 ...\n $ BP_SYS        : int  120 120 130 101 132 110 119 111 138 138 ...\n $ BP_DIA        : int  80 80 80 62 78 70 78 60 72 84 ...\n $ URN_PROT      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ HGB           : num  12.6 13.8 15 13.1 13 11.9 11.2 12.2 11 12.8 ...\n $ FBS           : int  117 96 118 90 92 100 84 88 74 107 ...\n $ TOT_CHOL      : int  264 169 216 199 162 192 152 166 155 178 ...\n $ TG            : int  128 92 132 100 58 109 38 42 86 87 ...\n $ HDL           : int  60 70 55 65 40 53 43 58 52 35 ...\n $ LDL           : int  179 80 134 114 111 117 101 99 85 125 ...\n $ CRTN          : num  0.9 0.9 0.8 0.9 0.9 0.7 0.8 1 0.6 0.7 ...\n $ SGOT          : int  25 18 26 18 24 15 8 16 15 21 ...\n $ SGPT          : int  20 15 30 14 23 12 6 11 13 21 ...\n $ GGT           : int  25 28 30 11 15 14 10 12 13 23 ...\n $ GFR           : int  59 74 79 61 49 83 97 65 96 70 ...\n\n\nnames 로 변수들 이름을 확인할 수 있다. 공백이나 특수문자는 “.” 로 바뀌고, 이름이 같은 변수들은 뒤에 숫자가 추가되어 구별된다. read.csv(..., check.names = F) 옵션으로 원래 이름을 유지할 수 있으나 에러의 원인이 되므로 추천하지 않는다.\n\nnames(ex)\n\n [1] \"EXMD_BZ_YYYY\"   \"RN_INDI\"        \"HME_YYYYMM\"     \"Q_PHX_DX_STK\"  \n [5] \"Q_PHX_DX_HTDZ\"  \"Q_PHX_DX_HTN\"   \"Q_PHX_DX_DM\"    \"Q_PHX_DX_DLD\"  \n [9] \"Q_PHX_DX_PTB\"   \"Q_HBV_AG\"       \"Q_SMK_YN\"       \"Q_DRK_FRQ_V09N\"\n[13] \"HGHT\"           \"WGHT\"           \"WSTC\"           \"BMI\"           \n[17] \"VA_LT\"          \"VA_RT\"          \"BP_SYS\"         \"BP_DIA\"        \n[21] \"URN_PROT\"       \"HGB\"            \"FBS\"            \"TOT_CHOL\"      \n[25] \"TG\"             \"HDL\"            \"LDL\"            \"CRTN\"          \n[29] \"SGOT\"           \"SGPT\"           \"GGT\"            \"GFR\"           \n\n\n샘플수, 변수 갯수는 dim, nrow, ncol 로 확인한다.\n\ndim(ex)                                                    ## row, column\n\n[1] 1644   32\n\nnrow(ex)                                                   ## row\n\n[1] 1644\n\nncol(ex)                                                   ## column\n\n[1] 32\n\n\n클래스는 class로 확인한다. read.csv 는 data.frame, Excel/SAS/SPSS 는 tibble & `data.frame 인데, data.frame 은 행렬이면서 데이터에 특화된 list, tibble 은 앞서 언급했던 tidyverse 스타일의 data.frame 인 정도만 알고 넘어가자.\n\nclass(ex)\n\n[1] \"data.frame\"\n\nclass(ex.spss)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsummary 로 모든 변수들의 평균, 중위수, 결측치 등을 한 번에 확인할 수 있다. R은 결측치를 NA 로 표시하며, 안타깝지만 분산은 나오지 않는다.\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI          HME_YYYYMM      Q_PHX_DX_STK   \n Min.   :2009   Min.   :   2270   Min.   :200901   Min.   :0.0000  \n 1st Qu.:2010   1st Qu.: 230726   1st Qu.:201011   1st Qu.:0.0000  \n Median :2012   Median : 487160   Median :201210   Median :0.0000  \n Mean   :2012   Mean   : 490782   Mean   :201216   Mean   :0.0112  \n 3rd Qu.:2014   3rd Qu.: 726101   3rd Qu.:201406   3rd Qu.:0.0000  \n Max.   :2015   Max.   :1010623   Max.   :201512   Max.   :1.0000  \n                                                   NA's   :573     \n Q_PHX_DX_HTDZ     Q_PHX_DX_HTN   Q_PHX_DX_DM      Q_PHX_DX_DLD   \n Min.   :0.0000   Min.   :0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.00   Median :0.0000   Median :0.0000  \n Mean   :0.0241   Mean   :0.25   Mean   :0.0693   Mean   :0.0399  \n 3rd Qu.:0.0000   3rd Qu.:0.25   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00   Max.   :1.0000   Max.   :1.0000  \n NA's   :566      NA's   :492    NA's   :547      NA's   :566     \n  Q_PHX_DX_PTB       Q_HBV_AG        Q_SMK_YN     Q_DRK_FRQ_V09N \n Min.   :0.0000   Min.   :1.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:0.000  \n Median :0.0000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :0.0276   Mean   :2.235   Mean   :1.632   Mean   :1.026  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :1.0000   Max.   :3.000   Max.   :3.000   Max.   :7.000  \n NA's   :703      NA's   :2       NA's   :2       NA's   :6      \n      HGHT            WGHT            WSTC             BMI       \n Min.   :134.0   Min.   : 31.0   Min.   : 57.00   Min.   :12.30  \n 1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00   1st Qu.:21.50  \n Median :165.0   Median : 64.0   Median : 81.00   Median :23.70  \n Mean   :164.5   Mean   : 65.1   Mean   : 80.69   Mean   :23.92  \n 3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00   3rd Qu.:26.20  \n Max.   :188.0   Max.   :118.0   Max.   :114.00   Max.   :37.20  \n                                                                 \n     VA_LT           VA_RT            BP_SYS          BP_DIA     \n Min.   :0.100   Min.   :0.1000   Min.   : 81.0   Min.   : 49.0  \n 1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0   1st Qu.: 70.0  \n Median :1.000   Median :1.0000   Median :120.0   Median : 78.0  \n Mean   :0.984   Mean   :0.9792   Mean   :122.3   Mean   : 76.6  \n 3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0   3rd Qu.: 82.0  \n Max.   :9.900   Max.   :9.9000   Max.   :180.0   Max.   :120.0  \n                                                                 \n    URN_PROT          HGB             FBS            TOT_CHOL    \n Min.   :1.000   Min.   : 5.90   Min.   : 61.00   Min.   : 68.0  \n 1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00   1st Qu.:170.0  \n Median :1.000   Median :14.10   Median : 94.00   Median :193.0  \n Mean   :1.078   Mean   :14.11   Mean   : 97.23   Mean   :194.9  \n 3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00   3rd Qu.:218.0  \n Max.   :5.000   Max.   :18.30   Max.   :290.00   Max.   :363.0  \n NA's   :4                                                       \n       TG              HDL             LDL              CRTN        \n Min.   :  13.0   Min.   : 23.0   Min.   :  19.0   Min.   : 0.4000  \n 1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0   1st Qu.: 0.8000  \n Median : 106.0   Median : 54.0   Median : 112.0   Median : 0.9000  \n Mean   : 134.9   Mean   : 55.9   Mean   : 118.7   Mean   : 0.9891  \n 3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0   3rd Qu.: 1.0000  \n Max.   :1210.0   Max.   :593.0   Max.   :8100.0   Max.   :16.5000  \n                                  NA's   :16                        \n      SGOT            SGPT             GGT              GFR        \n Min.   :  6.0   Min.   :  3.00   Min.   :  6.00   Min.   :  3.00  \n 1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00   1st Qu.: 76.00  \n Median : 23.0   Median : 20.00   Median : 24.50   Median : 87.00  \n Mean   : 25.6   Mean   : 25.98   Mean   : 36.34   Mean   : 89.74  \n 3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00   3rd Qu.:101.00  \n Max.   :459.0   Max.   :779.00   Max.   :408.00   Max.   :196.00  \n                                                   NA's   :467     \n\n\n특정 변수 보기\ndata.frame 에서 특정변수는 $ 를 이용, 데이터이름$변수이름 로 확인할 수 있다. 앞서 언급했듯이 data.frame 은 행렬과 list의 성질도 갖고 있어 해당 스타일로도 가능하다.\n\nex$EXMD_BZ_YYYY                                            ## data.frame style\nex[, \"EXMD_BZ_YYYY\"]                                       ## matrix style\nex[[\"EXMD_BZ_YYYY\"]]                                       ## list style\nex[, 1]                                                    ## matrix style with order\nex[[1]]                                                    ## list style with order\n\n2개 이상 변수선택은 행렬 스타일을 이용한다.\n\nex[, c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"BMI\")]                  ## matrix syle with names\nex[, c(1, 2, 16)]                                          ## matrix syle with names\nex[, names(ex)[c(1, 2, 16)]]                               ## same\n\n\n\n\n  \n\n\n\n특정 변수는 벡터형태로 나타나므로 처음에 다루었던 벡터다루기를 그대로 활용할 수 있다. 예를 들어 년도 변수인 EXMD_BZ_YYYY의 첫 50개만 확인하면 아래와 같다.\n\nex$EXMD_BZ_YYYY[1:50]                                      ## data.frame style\nex[1:50, 1]                                                ## matrix style\nex[[1]][1:50]                                              ## list style\n\n\n\n [1] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[16] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[31] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[46] 2009 2009 2009 2009 2009\n\n\nunique 로 변수가 어떤 값들로 이루어져 있는지, table 로 해당 값들이 몇개씩 있는지 확인한다.\n\nunique(ex$EXMD_BZ_YYYY)                                   ## unique value\n\n[1] 2009 2010 2011 2012 2013 2014 2015\n\nlength(unique(ex$EXMD_BZ_YYYY))                           ## number of unique value\n\n[1] 7\n\ntable(ex$EXMD_BZ_YYYY)                                    ## table\n\n\n2009 2010 2011 2012 2013 2014 2015 \n 214  236  223  234  243  254  240 \n\n\n새로운 변수 만들기\n연속형 변수인 BMI 에서 원하는 조건에 맞는 정보를 뽑아내는 연습을 해 보자.\n\nmean(ex$BMI)                                              ## mean\n\n[1] 23.92257\n\nBMI_cat &lt;- (ex$BMI &gt;= 25)                                 ## TRUE of FALSE\ntable(BMI_cat)                         \n\nBMI_cat\nFALSE  TRUE \n 1077   567 \n\nrows &lt;- which(ex$BMI &gt;= 25)                               ## row numbers\nhead(rows)                                      \n\n[1]  1 14 18 21 23 24\n\nvalues &lt;- ex$BMI[ex$BMI &gt;= 25]                            ## values\nhead(values)\n\n[1] 29.4 27.5 27.7 28.0 30.7 25.6\n\nlength(values)\n\n[1] 567\n\nBMI_HGHT_and &lt;- (ex$BMI &gt;= 25 & ex$HGHT &gt;= 175)              ## and\nBMI_HGHT_or &lt;- (ex$BMI &gt;= 25 | ex$HGHT &gt;= 175)               ## or\n\n데이터에 새로운 변수로 추가하는 방법은 간단하다.\n\nex$zero &lt;- 0                                              ## variable with 0\nex$BMI_cat &lt;- (ex$BMI &gt;= 25)                              ## T/F\nex$BMI_cat &lt;- as.integer(ex$BMI &gt;= 25)                    ## 0, 1\nex$BMI_cat &lt;- as.character(ex$BMI &gt;= 25)                  ## \"0\", \"1\"\nex$BMI_cat &lt;- ifelse(ex$BMI &gt;= 25, \"1\", \"0\")              ## same\ntable(ex$BMI_cat)\n\n\n   0    1 \n1077  567 \n\nex[, \"BMI_cat\"] &lt;- (ex$BMI &gt;= 25)                         ## matrix style\nex[[\"BMI_cat\"]] &lt;- (ex$BMI &gt;= 25)                         ## list style\n\n변수 클래스 설정: 데이터 읽은 후 가장 먼저 해야할 것.\n앞서 데이터의 클래스가 data.frame 임을 언급했었는데, 각 변수들도 자신의 클래스를 갖으며 대표적인 것이 숫자형(numeric), 문자형(character), 팩터(factor) 이다. 그 외 T/F 로 나타내는 논리(logical), 날짜를 나타내는 Date 클래스가 있다. 숫자는 integer(정수), numeric(실수) 이 있는데, 전부 실수형(numeric)으로 해도 상관없어 설명은 생략한다. 범주형은 character 와 factor 두 종류가 있는데, 전자는 단순 문자인 반면 후자는 레벨(level) 이 있어 reference 나 순서를 설정할 수 있다. read.csv 로 읽으면 숫자는 int/num, 문자는 전부 factor 가 기본값이므로, 숫자 변수 중 0/1 같은 것들은 직접 factor 로 바꿔줘야 한다. ID와 설문조사 변수를 범주형으로 바꿔보자.\n\nvars.cat &lt;- c(\"RN_INDI\", \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_PHX_DX_HTN\", \"Q_PHX_DX_DM\", \"Q_PHX_DX_DLD\", \"Q_PHX_DX_PTB\", \n              \"Q_HBV_AG\", \"Q_SMK_YN\", \"Q_DRK_FRQ_V09N\")\nvars.cat &lt;- names(ex)[c(2, 4:12)]                              ## same\nvars.cat &lt;- c(\"RN_INDI\", grep(\"Q_\", names(ex), value = T))     ## same: extract variables starting with \"Q_\"\n\nvars.conti &lt;- setdiff(names(ex), vars.cat)                     ## Exclude categorical variables\nvars.conti &lt;- names(ex)[!(names(ex) %in% vars.cat)]            ## same: !- not, %in%- including\n\nfor (vn in vars.cat){                                          ## for loop: as.factor\n  ex[, vn] &lt;- as.factor(ex[, vn])\n}\n\nfor (vn in vars.conti){                                        ## for loop: as.numeric\n  ex[, vn] &lt;- as.numeric(ex[, vn])\n}\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI       HME_YYYYMM     Q_PHX_DX_STK Q_PHX_DX_HTDZ\n Min.   :2009   4263   :   7   Min.   :200901   0   :1059    0   :1052    \n 1st Qu.:2010   38967  :   7   1st Qu.:201011   1   :  12    1   :  26    \n Median :2012   56250  :   7   Median :201210   NA's: 573    NA's: 566    \n Mean   :2012   84322  :   7   Mean   :201216                             \n 3rd Qu.:2014   99917  :   7   3rd Qu.:201406                             \n Max.   :2015   115809 :   7   Max.   :201512                             \n                (Other):1602                                              \n Q_PHX_DX_HTN Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG    Q_SMK_YN  \n 0   :864     0   :1021   0   :1035    0   :915     1   :  77   1   :995  \n 1   :288     1   :  76   1   :  43    1   : 26     2   :1102   2   :256  \n NA's:492     NA's: 547   NA's: 566    NA's:703     3   : 463   3   :391  \n                                                    NA's:   2   NA's:  2  \n                                                                          \n                                                                          \n                                                                          \n Q_DRK_FRQ_V09N      HGHT            WGHT            WSTC       \n 0      :805    Min.   :134.0   Min.   : 31.0   Min.   : 57.00  \n 1      :379    1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00  \n 2      :249    Median :165.0   Median : 64.0   Median : 81.00  \n 3      :121    Mean   :164.5   Mean   : 65.1   Mean   : 80.69  \n 4      : 28    3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00  \n (Other): 56    Max.   :188.0   Max.   :118.0   Max.   :114.00  \n NA's   :  6                                                    \n      BMI            VA_LT           VA_RT            BP_SYS     \n Min.   :12.30   Min.   :0.100   Min.   :0.1000   Min.   : 81.0  \n 1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0  \n Median :23.70   Median :1.000   Median :1.0000   Median :120.0  \n Mean   :23.92   Mean   :0.984   Mean   :0.9792   Mean   :122.3  \n 3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0  \n Max.   :37.20   Max.   :9.900   Max.   :9.9000   Max.   :180.0  \n                                                                 \n     BP_DIA         URN_PROT          HGB             FBS        \n Min.   : 49.0   Min.   :1.000   Min.   : 5.90   Min.   : 61.00  \n 1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00  \n Median : 78.0   Median :1.000   Median :14.10   Median : 94.00  \n Mean   : 76.6   Mean   :1.078   Mean   :14.11   Mean   : 97.23  \n 3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00  \n Max.   :120.0   Max.   :5.000   Max.   :18.30   Max.   :290.00  \n                 NA's   :4                                       \n    TOT_CHOL           TG              HDL             LDL        \n Min.   : 68.0   Min.   :  13.0   Min.   : 23.0   Min.   :  19.0  \n 1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0  \n Median :193.0   Median : 106.0   Median : 54.0   Median : 112.0  \n Mean   :194.9   Mean   : 134.9   Mean   : 55.9   Mean   : 118.7  \n 3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0  \n Max.   :363.0   Max.   :1210.0   Max.   :593.0   Max.   :8100.0  \n                                                  NA's   :16      \n      CRTN              SGOT            SGPT             GGT        \n Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00   Min.   :  6.00  \n 1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00  \n Median : 0.9000   Median : 23.0   Median : 20.00   Median : 24.50  \n Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98   Mean   : 36.34  \n 3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00  \n Max.   :16.5000   Max.   :459.0   Max.   :779.00   Max.   :408.00  \n                                                                    \n      GFR              zero      BMI_cat      \n Min.   :  3.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 76.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 87.00   Median :0   Median :0.0000  \n Mean   : 89.74   Mean   :0   Mean   :0.3449  \n 3rd Qu.:101.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :196.00   Max.   :0   Max.   :1.0000  \n NA's   :467                                  \n\n\nsummary 를 보면 설문조사 변수들이 처음과 달리 빈도로 요약됨을 알 수 있다. 한 가지 주의할 점은 factor 를 numeric 으로 바로 바꾸면 안된다는 것이다. 방금 factor 로 바꾼 Q_PHX_DX_STK 를 numeric 으로 바꿔서 테이블로 요약하면, 원래의 0/1 이 아닌 1/2로 바뀐다.\n\ntable(\n  as.numeric(ex$Q_PHX_DX_STK)\n  )\n\n\n   1    2 \n1059   12 \n\n\nfactor를 바로 바꾸면 원래 값이 아닌, factor에 내장된 레벨(순서값) 로 바뀌기 때문이다. 제대로 바꾸려면 아래처럼 character 로 먼저 바꿔준 후 숫자형을 적용해야 한다.\n\ntable(\n  as.numeric(as.character(ex$Q_PHX_DX_STK))\n  )\n\n\n   0    1 \n1059   12 \n\n\n마지막으로 Date 클래스를 살펴보자. 검진년월 변수인 HME_YYYYMM 를 Date 로 바꿔 볼텐데, Date는 년/월/일 이 모두 필요하므로 일은 1로 통일하고 paste 로 붙이겠다.\n\naddDate &lt;- paste(ex$HME_YYYYMM, \"01\", sep = \"\")                ## add day- use `paste`\nex$HME_YYYYMM &lt;- as.Date(addDate, format = \"%Y%m%d\")           ## set format                  \nhead(ex$HME_YYYYMM)\n\n[1] \"2009-09-01\" \"2009-11-01\" \"2009-03-01\" \"2009-08-01\" \"2009-09-01\"\n[6] \"2009-12-01\"\n\nclass(ex$HME_YYYYMM)\n\n[1] \"Date\"\n\n\n결측치 다루기\n변수 클래스만큼 중요한 것이 결측치 처리이다. 앞서 “함수만들기” 에서 봤듯이 결측치가 있으면 평균같은 기본적인 계산도 na.rm = T 옵션이 필요하다. 결측치가 있는 LDL 변수의 평균을 연도별로 구해보자. 그룹별 통계는 tapply 를 이용한다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, mean)                          ## measure/group/function\n\n\n\n\n\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n\n\n150.9486\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n2009년만 결측치가 없고, 나머지는 결측치가 있어 평균값이 NA 로 나온다.na.rm = T 옵션으로 결측치를 제외하면 원하는 결과를 얻는다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, \n       function(x){\n         mean(x, na.rm = T)\n         })    \n\n    2009     2010     2011     2012     2013     2014     2015 \n150.9486 112.9914 112.9450 117.5259 111.1577 116.5455 111.5294 \n\n\n더 큰 문제는, 대부분의 R 통계분석이 결측치를 갖는 샘플을 분석에서 제외한다는 점이다. 그래서 결측치를 신경쓰지 않고 분석하다보면, 원래 샘플 수와 분석에 이용된 샘플 수가 달라지는 문제가 생길 수 있다. LDL과 HDL 의 회귀분석 결과를 예로 살펴보자.\n\nsummary(lm(LDL ~ HDL, data = ex))\n\n\nCall:\nlm(formula = LDL ~ HDL, data = ex)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-103.8  -28.2   -6.6   15.4 7974.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 138.2747    15.2318   9.078   &lt;2e-16 ***\nHDL          -0.3499     0.2570  -1.362    0.174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201.9 on 1626 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.001139,  Adjusted R-squared:  0.0005244 \nF-statistic: 1.854 on 1 and 1626 DF,  p-value: 0.1735\n\n\n“16 observations deleted due to missingness” 라는 글자가 보일 것이다. LDL 이 결측인 16명은 분석에서 제외했다는 뜻이다.\n연습문제 2: 결측치 처리\n결측치를 처리하는 제일 간단한 방법은 “하나라도 결측치 있는 샘플은 제외” 로, na.omit 함수를 이용하면 된다.\n\nex.naomit &lt;- na.omit(ex)\nnrow(ex.naomit)\n\n[1] 620\n\n\n1644 명에서 620 명으로 샘플 수가 줄어든 것을 확인할 수 있다. 필자는 보통 결측치 처리에 다음의 3가지 원칙을 적용한다.\n\n결측치 너무 많으면(예: 10% 이상) 그 변수는 삭제\n연속변수는 중간값(median)\n범주형변수는 최빈값(mode)\n\n이제 문제이다. 아까 변수형을 정리한 ex 데이터에 위 3가지 원칙을 적용, 새로운 데이터 ex.impute 을 만들어 보아라. 단 최빈값 함수는 아래와 같이 getmode 로 주어진다.\n\ngetmode &lt;- function(v){\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ngetmode(ex$Q_PHX_DX_STK)\n\n[1] 0\nLevels: 0 1\n\n\n\n정답 보기\n\nvars.ok &lt;- sapply(names(ex), function(v){sum(is.na(ex[, v])) &lt; nrow(ex)/10})\nex.impute &lt;- ex[, vars.ok]                                     ## only missing &lt; 10%\n\nfor (v in names(ex.impute)){\n  if (is.factor(ex.impute[, v])){                              ## or class(ex[, v]) == \"factor\"\n    ex.impute[, v] &lt;- ifelse(is.na(ex.impute[, v]), \n                             getmode(ex.impute[, v]), \n                             ex.impute[, v])\n  } else if (is.numeric(ex[, v])){                             ## or class(ex[, v]) %in% c(\"integer\", \"numeric\")\n    ex.impute[, v] &lt;- ifelse(is.na(ex.impute[, v]), \n                             median(ex.impute[, v], na.rm = T), \n                             ex.impute[, v])\n  } else{                                                      ## when date\n    ex.impute[, v]\n  }\n}\n\nsummary(ex.impute)\n\n  EXMD_BZ_YYYY     RN_INDI        HME_YYYYMM            Q_HBV_AG    \n Min.   :2009   Min.   :  1.0   Min.   :2009-01-01   Min.   :1.000  \n 1st Qu.:2010   1st Qu.:133.8   1st Qu.:2010-11-01   1st Qu.:2.000  \n Median :2012   Median :275.0   Median :2012-10-01   Median :2.000  \n Mean   :2012   Mean   :272.7   Mean   :2012-08-31   Mean   :2.235  \n 3rd Qu.:2014   3rd Qu.:405.2   3rd Qu.:2014-06-01   3rd Qu.:3.000  \n Max.   :2015   Max.   :547.0   Max.   :2015-12-01   Max.   :3.000  \n    Q_SMK_YN     Q_DRK_FRQ_V09N       HGHT            WGHT      \n Min.   :1.000   Min.   :1.000   Min.   :134.0   Min.   : 31.0  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:158.0   1st Qu.: 56.0  \n Median :1.000   Median :2.000   Median :165.0   Median : 64.0  \n Mean   :1.631   Mean   :2.023   Mean   :164.5   Mean   : 65.1  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:171.0   3rd Qu.: 73.0  \n Max.   :3.000   Max.   :8.000   Max.   :188.0   Max.   :118.0  \n      WSTC             BMI            VA_LT           VA_RT       \n Min.   : 57.00   Min.   :12.30   Min.   :0.100   Min.   :0.1000  \n 1st Qu.: 74.00   1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000  \n Median : 81.00   Median :23.70   Median :1.000   Median :1.0000  \n Mean   : 80.69   Mean   :23.92   Mean   :0.984   Mean   :0.9792  \n 3rd Qu.: 87.00   3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000  \n Max.   :114.00   Max.   :37.20   Max.   :9.900   Max.   :9.9000  \n     BP_SYS          BP_DIA         URN_PROT          HGB       \n Min.   : 81.0   Min.   : 49.0   Min.   :1.000   Min.   : 5.90  \n 1st Qu.:110.0   1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90  \n Median :120.0   Median : 78.0   Median :1.000   Median :14.10  \n Mean   :122.3   Mean   : 76.6   Mean   :1.078   Mean   :14.11  \n 3rd Qu.:130.0   3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40  \n Max.   :180.0   Max.   :120.0   Max.   :5.000   Max.   :18.30  \n      FBS            TOT_CHOL           TG              HDL       \n Min.   : 61.00   Min.   : 68.0   Min.   :  13.0   Min.   : 23.0  \n 1st Qu.: 86.00   1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0  \n Median : 94.00   Median :193.0   Median : 106.0   Median : 54.0  \n Mean   : 97.23   Mean   :194.9   Mean   : 134.9   Mean   : 55.9  \n 3rd Qu.:103.00   3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0  \n Max.   :290.00   Max.   :363.0   Max.   :1210.0   Max.   :593.0  \n      LDL              CRTN              SGOT            SGPT       \n Min.   :  19.0   Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00  \n 1st Qu.:  90.0   1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00  \n Median : 112.0   Median : 0.9000   Median : 23.0   Median : 20.00  \n Mean   : 118.6   Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98  \n 3rd Qu.: 134.0   3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00  \n Max.   :8100.0   Max.   :16.5000   Max.   :459.0   Max.   :779.00  \n      GGT              zero      BMI_cat      \n Min.   :  6.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 16.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 24.50   Median :0   Median :0.0000  \n Mean   : 36.34   Mean   :0   Mean   :0.3449  \n 3rd Qu.: 41.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :408.00   Max.   :0   Max.   :1.0000  \n\n\n\nSubset\n특정 조건을 만족하는 서브데이터는 지금까지 배웠던 것을 응용해 만들 수도 있지만, subset 함수가 편하다. 아래는 2012 이후의 자료만 뽑는 예시이다. 이제부터는 결측치를 전부 제외한 ex.naomit 데이터를 이용하겠다.\n\nex1 &lt;- ex.naomit                                               ## simple name\nex1.2012 &lt;- ex1[ex1$EXMD_BZ_YYYY &gt;= 2012, ]\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\nex1.2012 &lt;- subset(ex1, EXMD_BZ_YYYY &gt;= 2012)                  ## subset\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\n\n그룹별 통계\n결측치 다루기에서 그룹별 통계를 구할 때 tapply 를 이용했었다. tapply 를 여러 변수, 여러 그룹을 동시에 고려도록 확장할 수 있는 함수가 aggregate 로, 허리둘레와 BMI의 평균을 고혈압 또는 당뇨 여부에 따라 살펴보자.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN), mean)\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex1, mean)   ## same\n\n\n\n\n\nGroup.1\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n결측치가 있어도 잘 적용된다는 장점이 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.23958\n23.70961\n\n\n1\n83.87847\n24.99861\n\n\n\n\n\n당뇨여부도 그룹으로 다루려면 list 에 추가하면 된다.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN, ex1$Q_PHX_DX_DM), mean)\n\n\n\n\n\nGroup.1\nGroup.2\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\nGroup.1 이 첫번째 그룹은 고혈압 여부, Group.2 가 두번째 그룹인 당뇨 여부이다. 위와 마찬가지로 formula 형태를 이용할 수도 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\n표준편차를 같이 보려면 function(x){c(mean = mean(x), sd = sd(x))} 와 같이 원하는 함수들을 벡터로 모으면 된다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})\n\n\n\nWarning in `[&lt;-.data.frame`(`*tmp*`, , isn, value = structure(list(WSTC.mean =\nc(\"80.231068\", : provided 4 variables to replace 2 variables\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.231068\n9.546884\n\n\n1\n0\n83.939759\n9.124277\n\n\n0\n1\n87.555556\n7.551674\n\n\n1\n1\n88.000000\n6.177918\n\n\n\n\n\n아예 데이터의 모든 변수의 평균을 다 볼순 없을까? 아래처럼 “.” 으로 전체 데이터를 지정할 수 있다.\n\naggregate(. ~ Q_PHX_DX_HTN  + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})    \n\n  Q_PHX_DX_HTN Q_PHX_DX_DM EXMD_BZ_YYYY.mean EXMD_BZ_YYYY.sd RN_INDI.mean\n1            0           0       2013.493204        1.109498    269.30680\n2            1           0       2013.578313        1.105645    251.78313\n3            0           1       2013.333333        1.414214    269.77778\n4            1           1       2013.307692        1.031553    303.53846\n  RN_INDI.sd HME_YYYYMM.mean HME_YYYYMM.sd Q_PHX_DX_STK.mean Q_PHX_DX_STK.sd\n1  159.12594      16102.3184      422.8574        1.00776699      0.08787296\n2  154.03951      16121.8072      413.1641        1.01204819      0.10976426\n3   92.88807      16036.3333      551.2248        1.00000000      0.00000000\n4  142.18686      16018.6923      417.4666        1.07692308      0.27735010\n  Q_PHX_DX_HTDZ.mean Q_PHX_DX_HTDZ.sd Q_PHX_DX_DLD.mean Q_PHX_DX_DLD.sd\n1         1.00194175       0.04406526         1.0174757       0.1311630\n2         1.06024096       0.23937916         1.0722892       0.2605404\n3         1.00000000       0.00000000         1.0000000       0.0000000\n4         1.07692308       0.27735010         1.0769231       0.2773501\n  Q_PHX_DX_PTB.mean Q_PHX_DX_PTB.sd Q_HBV_AG.mean Q_HBV_AG.sd Q_SMK_YN.mean\n1         1.0271845       0.1627787     2.2291262   0.5236863     1.6970874\n2         1.0000000       0.0000000     2.1927711   0.5512255     1.3855422\n3         1.0000000       0.0000000     2.0000000   0.0000000     1.6666667\n4         1.0769231       0.2773501     2.2307692   0.4385290     1.5384615\n  Q_SMK_YN.sd Q_DRK_FRQ_V09N.mean Q_DRK_FRQ_V09N.sd  HGHT.mean    HGHT.sd\n1   0.8674234           2.0388350         1.3329287 166.613592   9.116636\n2   0.6777172           1.9759036         1.3612754 160.506024   9.254364\n3   0.8660254           1.8888889         0.3333333 168.333333  10.185774\n4   0.6602253           1.9230769         1.1151636 162.384615   9.639662\n  WGHT.mean   WGHT.sd WSTC.mean   WSTC.sd  BMI.mean    BMI.sd VA_LT.mean\n1 66.582524 13.211630 80.231068  9.546884 23.829903  3.276315  1.0190291\n2 65.313253 13.155661 83.939759  9.124277 25.179518  3.693922  0.8469880\n3 71.777778  8.913161 87.555556  7.551674 25.344444  2.711140  0.9111111\n4 65.076923  6.211032 88.000000  6.177918 24.723077  2.057164  0.7769231\n   VA_LT.sd VA_RT.mean  VA_RT.sd BP_SYS.mean  BP_SYS.sd BP_DIA.mean BP_DIA.sd\n1 0.5248189  1.0079612 0.3503677  119.889320  13.378266   75.452427  9.464616\n2 0.3201895  0.8638554 0.3444962  132.879518  14.344539   81.481928 11.015910\n3 0.1691482  0.8111111 0.2368778  128.555556   8.647415   83.333333 11.842719\n4 0.2350668  0.9000000 0.1080123  129.461538  12.149180   79.307692  7.846280\n  URN_PROT.mean URN_PROT.sd   HGB.mean     HGB.sd  FBS.mean    FBS.sd\n1     1.0543689   0.3430173 14.3749515  1.5952305  94.75534  12.71807\n2     1.2168675   0.6634756 14.1048193  1.6682036 103.60241  14.34330\n3     1.0000000   0.0000000 15.2555556  1.0284832 131.11111  19.62425\n4     1.0769231   0.2773501 13.4153846  0.9711215 125.30769  34.11838\n  TOT_CHOL.mean TOT_CHOL.sd   TG.mean     TG.sd  HDL.mean    HDL.sd  LDL.mean\n1     196.96505    34.20684 132.80777 107.56421 54.943689 12.881333 118.49903\n2     191.30120    32.64769 138.09639  81.93106 55.903614 16.123468 108.22892\n3     169.77778    47.79325 164.66667  68.40870 46.333333  9.394147  92.77778\n4     179.61538    39.92397 154.76923 139.23072 48.769231 10.288779 102.30769\n     LDL.sd CRTN.mean   CRTN.sd SGOT.mean   SGOT.sd SGPT.mean   SGPT.sd\n1  50.86475 0.8871845 0.1867988 24.151456  9.426161 24.609709 16.616090\n2  29.09167 0.9168675 0.2483208 25.289157  6.400324 22.963855  9.671993\n3  38.88694 0.9666667 0.2549510 35.777778 25.849457 47.666667 44.235167\n4  28.39420 0.9153846 0.1675617 31.769231 19.689904 36.307692 27.417709\n  GGT.mean   GGT.sd GFR.mean   GFR.sd zero.mean zero.sd BMI_cat.mean BMI_cat.sd\n1 34.47573 31.35216 92.01359 19.14246         0       0    0.3223301  0.4678230\n2 35.77108 31.18799 81.16867 17.75430         0       0    0.4337349  0.4986022\n3 44.22222 21.01058 87.55556 22.20423         0       0    0.5555556  0.5270463\n4 48.46154 66.83265 80.69231 14.34332         0       0    0.1538462  0.3755338\n\n\nSort\n정렬은 순위함수인 order 를 이용한다. 기본은 오름차순이며, 내림차순을 원한다면 (-) 붙인 값의 순위를 구하면 된다.\n\nord &lt;- order(ex1$HGHT)                                        ## 작은 순서대로 순위\nhead(ord)\n\n[1] 500 168   3 328 473 177\n\nhead(ex1$HGHT[ord])                                           ## Sort\n\n[1] 138 139 140 140 141 143\n\nord.desc &lt;- order(-ex1$HGHT)                                  ## descending\nhead(ex1$HGHT[ord.desc])\n\n[1] 188 186 185 185 184 183\n\n\n\nex1.sort &lt;- ex1[ord, ]\nhead(ex1.sort)\n\n\n\n\n  \n\n\n\nWide to long, long to wide format\n받은 데이터가 원하는 형태가 아닌 경우가 있다. 수축기 혈압을 10번 측정해서 각각 SBP1, SBP2, …, SBP10 변수에 기록된 데이터를 본다면, 이것들을 쫙 아래로 내려 측정시기, 측정값 2개의 변수로 정리하고 싶다는 마음이 들 것이다. 이럴 때 쓰는 함수가 melt, 반대로 데이터를 옆으로 늘릴 때 쓰는 함수가 dcast 이다(Figure @ref(fig:melt)3).\n\n\n\n\nmelt and dcast\n\n\n\n실습으로 수축기/이완기 혈압 변수를 합쳐서 아래로 내려보자.\n\nlibrary(reshape2)\nlong &lt;- melt(ex1, id = c(\"EXMD_BZ_YYYY\", \"RN_INDI\"), measure.vars = c(\"BP_SYS\", \"BP_DIA\"), variable.name = \"BP_type\", value.name = \"BP\")\nlong\n\n\n\n\n  \n\n\n\nid 는 유지할 변수, measure.vars 는 내릴 변수를 의미하고, variable.name, value.name 은 각각 그룹, 값의 변수이름을 의미한다. 이를 원래대로 되돌리려면 dcast 를 이용하는데, “유지할 변수 ~ 펼칠 변수” 형태로 formula 를 입력한다.\n\nwide &lt;- dcast(long, EXMD_BZ_YYYY + RN_INDI ~ BP_type, value.var = \"BP\")\nhead(wide)\n\n\n\n\n  \n\n\n\nMerge\nmerge 함수를 이용한다. “by” 옵션으로 기준이 되는 공통 컬럼을 설정하며, 기준 컬럼의 이름이 두 데이터 셋에서 다른 경우는 “by.x” 와 “by.y” 로 따로 설정한다. 실습을 위해 ex1 데이터를 2개로 나눈 후 merge 를 적용하겠다.\n\nex1.Q &lt;- ex1[, c(1:3, 4:12)]\nex1.measure &lt;- ex1[, c(1:3, 13:ncol(ex1))]\nhead(ex1.Q)\nhead(ex1.measure)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n전자는 설문조사 결과를, 후자는 측정값을 포함했고 “년도, ID, 검진년월” 은 공통변수이다. 이 공통변수로 merge 를 적용하면\n\nex1.merge &lt;- merge(ex1.Q, ex1.measure, by = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all = T)\nhead(ex1.merge)\n\n\n\n\n  \n\n\n\n합쳐진 원래 데이터를 얻을 수 있다. all = T 는 한 쪽에만 있는 샘플을 유지하는 옵션이며 빈 변수는 NA 로 채워진다. 공통인 샘플만 취하려면 all = F 로 바꾸자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "마치며",
    "text": "마치며\n이번 강의를 정리하자.\n\nRStudio cloud 로 클라우드 환경에서 실습을 진행했으며\n기초 벡터연산과 for, if, ifelse, 함수만들기, apply 문을 통해 기본 문법을 익혔고\n\n공단 검진 데이터를 실습자료를 읽어와 데이터를 살펴보는 법을 배웠다.\n\n변수 생성, 클래스 설정, 결측치 처리, 서브데이터, 그룹별 통계, 정렬\n\n\n마지막으로 Long/wide type 데이터 변환과 merge 를 다루었다.\n\n기타 기본적으로 알아야 할 R 명령어는 아래의 Base R Cheat Sheet 에서 확인할 수 있다.\n 다음 강의에서는 쉬운 문법으로 R 의 대세가 된 tidyverse 를 다룰 예정인데, 오늘 배운 기본 문법과 많은 비교가 될 것이다. 미리 알아보고 싶은 분은 본 블로그의 이전 글4 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#footnotes",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#footnotes",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://jinseob2kim.github.io/rbasic.html↩︎\nhttps://jinseob2kim.github.io/radv1.html↩︎\nhttps://t1.daumcdn.net/cfile/tistory/2433F13D55E1163907↩︎\nhttps://blog.zarathu.com/posts/2019-01-03-rdatamanagement↩︎"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "",
    "text": "본 내용은 필자가 준비하던 박사논문 선형모형의 다차원공간으로의 확장 의 후속으로 계획했던 내용으로, 선형모형 벡터공간에 허수축(Imaginary Axis) 을 도입, Inverted U-shape 을 선형관계로 재해석하였습니다. 아인슈타인 일반상대성이론의 4차원 시공간 중, 시간을 허수축으로 해석하는 것에서 아이디어를 얻었습니다. 이전 내용 의 요약은 아래 슬라이드를 참고해 주세요."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#시작하면서",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#시작하면서",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "",
    "text": "본 내용은 필자가 준비하던 박사논문 선형모형의 다차원공간으로의 확장 의 후속으로 계획했던 내용으로, 선형모형 벡터공간에 허수축(Imaginary Axis) 을 도입, Inverted U-shape 을 선형관계로 재해석하였습니다. 아인슈타인 일반상대성이론의 4차원 시공간 중, 시간을 허수축으로 해석하는 것에서 아이디어를 얻었습니다. 이전 내용 의 요약은 아래 슬라이드를 참고해 주세요."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Abstract",
    "text": "Abstract\n이전 연구 에서 저자는 선형모형을 휘어진 다차원공간으로 확장하여 \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제시하였는데, Inverted \\(U\\)-shape은 이 방법으로 표현할 수 없었다. 이에 본 연구에서는 선형모형의 무대를 Imaginary Axis를 포함한 다차원공간까지 확장하여 Inverted \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제안한다. 본 연구를 활용하여 Health science 연구자들이 새로운 관점에서 연구데이터를 해석할 수 있을 것이다.\nkeywords : Multidimension, Linear Model, Vector Space, Metric Tensor"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Introduction",
    "text": "Introduction\n이전 연구에서 선형모형을 다차원 벡터공간으로 확장한 Multidimensional Vectorized Linear Model(MDVLM)을 제안하였는데, 변수들간의 dependency를 적절하게 조절하여 기존 선형모형에서 정확히 표현하기 어려웠던 관계를 표현할 수 있었다(Jinseob Kim 2017). 특히 MDVLM을 통해 점점 느리게 감소하는 혹은 점점 빠르게 증가하는 모양을 표현할 수 있는데, 이는 MDVLM의 표현식 \\(Y^2= \\beta_1^2X_1^2 + 2r\\beta_1\\beta_2X_1X_2+ \\beta_2^2X_2^2 = (\\beta_1X_1+r\\beta_2X_2)^2 + (1-r^2)\\beta_2^2X_2^2\\)에서 \\(X_1\\)과 \\(Y\\)의 관계가 쌍곡선 모양을 보이기 때문이다(Figure 1).\n\n\n\n\nExpressable relationship in conventional linear model and MDVLM\n\n\n\n이제 자연스러운 질문이 생긴다. “MDVLM은 점점 빠르게 감소하는 혹은 느리게 증가하는 관계를 표현할 수 있을까?” \\(Y^2=X_1^2 - X_2^2\\)의 간단한 예를 통해 이를 살펴보도록 하자.\nLinear relationship can’t explained via MDVLM\n\\(Y^2 = X_1^2 - X_2^2\\)의 그래프(\\(X, Y &gt; 0\\))를 살펴보면 \\(Y\\)와 \\(X_2\\)는 타원모양(이 예시에서는 원)으로 감소하는 관계를 보이고 MDVLM으로 잘 표현할 수 없다(Figure 2(a)). MDVLM은 직선 혹은 쌍곡선 모양만 표현할 수 있기 때문이다.\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n즉, \\(Y^2 = X_1^2 - X_1^2\\) 간단한 표현임에도 불구하고, 선형모형을 벡터공간으로 일반화한 MDVLM으로도 잘 표현될 수 없다. 그렇다면 이것은 선형모형이 아닌 것일까? 분명히 점점 빠르게 감소하는 타원 모양은 선형관계가 아니다. 그러나 \\(X_2^2\\)이 1 증가할 때 마다 \\(Y^2\\)가 정확히 1이 감소하는 관계가 있는 것도 사실이다(Figure 2(b)). 이 또한 선형관계라고 할 수 있는 것 아닐까?\n이를 확인해 보기 위해 MDVLM에서와 마찬가지로 다차원 벡터공간에서 \\(Y^2 = X_1^2 - X_2^2\\)을 표현해보자.\n\\[\\vec{Y} = \\vec{X_1} + \\vec{X_2}\\]\n\\(\\vec{X_1}\\)와 \\(\\vec{X_2}\\)가 독립된 axis를 갖고 있다면 \\(Y^2 = \\vec{Y}\\cdot\\vec{Y} = X_1^2 + X_2^2\\)을 얻는다. 한편 \\(\\vec{X_2}\\)가 허수축(imaginar axis)를 갖고 있다고 생각하면 아래와 같이 \\(Y^2 = X_1^2 - X_2^2\\)을 얻는다.\n\\[Y^2 = \\vec{Y}\\cdot\\vec{Y} =  X_1^2 + (iX_2)^2 = X_1^2 - X_2^2\\]\n이 경우에도 \\(X_1\\)이 고정된 상태에서는 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}= d\\vec{X_2}\\)가 성립하며, 방향과 허수축을 고려했을 때 \\(dY\\)와 \\(dX_2\\)은 선형관계가 있다고 할 수 있다.\n허수 \\(i\\)는 실제로 존재하는 것이 아니지만 \\(i^2 = -1\\)임을 이용해서 내적이 음수인 허수축을 상상할 수 있고, 선형관계를 허수축을 포함한 벡터공간으로 확장할 수 있다. 허수축의 활용은 물리학에서 많이 볼 수 있는데, 대표적으로 아인슈타인의 특수상대성이론에서는 time coordinate를 허수축으로 두고 4차원 시공간(Minkowski space)에서의 거리 \\(ds\\)를 아래와 같이 정의한다(Corry 1997).\n\\[(ds)^2 = (dx)^2 + (dy)^2 + (dz)^2 + (idt)^2 = (dx)^2 + (dy)^2 + (dz)^2 - (dt)^2\\]\n일반적으로 \\(p\\)개의 실수축과 \\(q\\)개의 허수축으로 구성된 manifold를 pseudo-Riemannian manifolds라 하고 거리 \\(g\\)는 아래와 같이 정의한다(Kulkarni 1981).\n\\[g = dx_1^2 + \\cdots + dx_p^2 - dx_{p+1}^2 - \\cdots - dx_{p+q}^2\\]\n점점 느리게 증가하는 타원모양도 마찬가지로 MDVLM으로는 표현하기 어려우며 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 원을 예로 들 수 있다(Figure 3(a), 3(b)).\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n제안: MDVLM with Imaginary Axis\n지금까지의 고찰을 토대로 본 연구에서는 허수축을 포함한 다차원 벡터공간에서 선형관계를 표현하는 MDVLM with Imaginary Axis(MDVLM-IA)를 제안한다. 이것은 기존의 MDVLM에 Imaginary Axis의 개념을 추가하여 일반화한 것인데, MDVLM의 개념을 간단하게 리뷰한 후 여기에 허수축을 추가하여 본 연구의 모형과 추정방법을 설명하겠다. 그 후 몇 가지 example을 통해 이것이 어떻게 활용되는지 살펴볼 것이다."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Formula",
    "text": "Formula\n이전에 연구했던 MDVLM에 대해 간단히 리뷰를 한 후, 이것을 확장하여 본 연구의 모형과 추정을 설명하도록 하겠다.\nBrief Review of MDVLM\n음이 아닌 실수 \\(Y\\)와 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\) 들의 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ (\\beta_2X_2+\\beta_{02})\\vec{e_2} + \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p}\\]\n이 때, \\(\\vec{e_i}\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며, \\(X_i\\)와 \\(X_j\\)가 완전히 독립적인 정보라면 \\(\\vec{e_i}\\cdot\\vec{e_j} = 0\\)이고 일반적으로 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)의 값은 0에서 1까지의 값을 갖는다.\n한편 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\beta_2dX_2\\vec{e_2} + \\cdots + \\beta_pdX_p\\vec{e_p}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고, 기존의 선형모형의 해석에 vector의 개념만 추가된 것이다.\n\\(\\beta\\)값들의 추정은 다음과 같은 스칼라식을 토대로 이루어진다.\n\\[\n\\begin{aligned}\nY^2 = (\\vec{Y})\\cdot(\\vec{Y}) &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i}) \\\\\n&=\\sum_{i=1}^p(\\beta_iX_i+\\beta_{i0})^2 + 2\\sum_{i&lt;j}r_{ij}(\\beta_iX_i+\\beta_{i0})(\\beta_jX_j+\\beta_{j0})\\\\\n\\end{aligned}\n\\]\n이제 최소제곱추정을 위한 오차제곱의 합(Sum of Squared Error: SSE)을 다음과 같이 정의하면, 기존 선형모형의 최소제곱추정을 자연스럽게 확장한 것이 된다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i&lt;j}r_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n예를 들어 \\(r_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 optimization technique를 이용하며, Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다Nelder and Mead (1965).\nMDVLM with Imaginary Axis\nMDVLM에서 허수축을 갖는 \\(X_{p+1}\\),\\(\\cdots\\),\\(X_{p+q}\\)를 추가하여 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p} + (\\beta_{p+1}X_{p+1}+\\beta_{0(p+1)})\\vec{e_{p+1}} + \\cdots + (\\beta_{p+q}X_{p+q}+\\beta_{0(p+q)})\\vec{e_{p+q}}\\]\n\\(\\vec{e_1},\\cdots,\\vec{e_p}\\)들은 실수축을 가진 단위벡터로서 자기자신과의 내적값인 \\(\\vec{e_i}\\cdot\\vec{e_i}\\)의 값이 1 이다. 반면 \\(\\vec{e_{p+1}},\\cdots,\\vec{e_{p+q}}\\)는 허수축을 가진 단위벡터로서 자기자신과의 내적값은 -1이다. \\(1\\le i, j \\le p\\)일 때는 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)가 0에서 1까지의 값을 갖으며, \\(p+1\\le i, j \\le p+q\\)라면 -1에서 0까지의 값을 갖고, 그 외에는 \\(r_{ij}=0\\)이다. 즉 Axis들의 dependency는 실수축끼리, 혹은 허수축끼리만 정의한다.\n\\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\cdots + \\beta_pdX_p\\vec{e_p} + \\beta_{p+1}dX_{p+1}\\vec{e_{p+1}} + \\cdots + \\beta_{p+q}dX_{p+q}\\vec{e_{p+q}}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고 이는 MDVLM에서의 해석과 동일하다.\n추정을 위한 스칼라 관계식은 MDVLM때와 비슷하게 아래와 같이 표현할 수 있다.\n$$\n\\[\\begin{aligned}\nY^2 &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{1\\le i&lt;j \\le p}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{p&lt; i&lt;j\\le p+q}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j}))\\\\\n\\end{aligned}\\]\n\\[ $Y$의 값의 변화량 $dY$는 \\]\n\\[\\begin{aligned}\n(dY)^2 &= (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i})\\cdot (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 + \\sum_{1 \\le i&lt;j \\le p}2r_{ij}\\beta_i\\beta_jdX_idX_j) - (\\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2 + \\sum_{p &lt; i&lt;j \\le p+q}2r_{ij}\\beta_i\\beta_jdX_idX_j)\\\\\n\\end{aligned}\\]\n$$\n이고, 모든 \\(r_{ij}\\)들이 0이라면 \\(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 - \\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2\\)로 간단히 표현할 수 있다.\n최소제곱 추정을 위한 \\(SSE(\\beta)\\)값도 비슷하게 정의할 수 있으며 추정방법은 MDVLM의 경우와 같이 optimization technique를 이용한다.\n\\[\n\\begin{aligned}\nf(\\beta,X_k) &= (\\sum_{i=1}^p(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{1\\le i&lt;j \\le p}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{p&lt; i&lt;j\\le p+q}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) \\\\\nSSE(\\beta) &= \\sum_{k=1}^N (Y_k - \\sqrt{f(\\beta,X_k)})^2\n\\end{aligned}\n\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value, \\(X_k= (X_{k1},\\cdots,X_{k(p+q)})\\))"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Apply to Data",
    "text": "Apply to Data\n앞서 언급했던 \\(Y^2 = X_1^2 - X_2^2\\)과 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 경우의 데이터에 적용해보도록 하겠다. 모든 계산은 R 3.3.3의 optim 함수를 이용하였다.\nExample 1: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\\(Y^2 = X_1^2 - X_2^2\\)인 양수 \\(Y, X_1, X_2\\)의 쌍을 100번 random sampling 해서 MDVLM과 본 연구의 모형을 비교해 보았다.\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - X_2^2\\): Dependency and MSE\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-0.224 + 1.163X_1  -0.634X_2)^2\\)\n\\(Y^2 = X_1^2 -X_2^2\\)\n\n\nMSE\n0.176\n0\n\n\n\n\n\n실제로 \\(Y^2\\)과 \\(X_1^2, X_2^2\\)의 값을 이용해서 선형모형으로 추정하면 정확한 추정 결과를 얻는다. 그러나 제곱한 값이 아닌 원래값을 이용해서 선형모형에 적용하면 \\(Y =\\) \\(-0.224\\) \\(+\\) \\(1.163\\) \\(X_1 +\\) \\(-0.634\\) \\(X_2\\)이 되어 정확한 추정을 얻지 못하고, MDVLM으로 확장해도 이보다 정확한 추정은 얻을 수 없다.\nExample 2: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - (5-X_2)^2\\): Dependency and MSE\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-2.78 + 1.079X_1 + 0.567X_2)^2\\)\n\\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\nMSE\n0.047\n0"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Discussion",
    "text": "Discussion\n예상대로 \\(Y\\)가 빨리 감소하거나 천천히 증가하는 타원모양의 관계는 MDVLM으로 잘 표현할 수 없었으며, 허수축을 활용했을 때 정확히 표현할 수 있었다.\n본 연구가 Health Status를 설명하기 위해 처음으로 허수 \\(i\\)의 개념을 이용하였다는 의의가 있다. 본 연구에서는 \\(i^2 = -1\\)이라는 특징을 이용해서 타원모양을 허수축에서의 선형관계로 재해석하여 MDVLM보다 더 확장된 선형모형을 제시하였는데, 이를 통해 연구자들이 선형관계라는 직관적 해석을 잃지 않으면서 지금보다 훨씬 다양하게 건강현상을 설명할 수 있을 것이라 확신한다.\n허수 \\(i\\)의 또하나의 큰 특징은 복소수 표현을 통해 실수체계를 확장할 수 있다는 것인데, 이것을 적극적으로 활용한 분야 중 하나가 양자역학이다. 양자역학의 대표적인 방정식인 슈뢰딩거 방정식(Schrödinger equation)은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용인데 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특징이며, 복소수가 포함된 파동함수 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현할 수 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 건강상태를 설명하는 새로운 방법이 될 수 있을 것이라 예상한다.\n본 연구를 시작으로 향후 Health science에서 복소수를 활용한 모형이 활발히 제안되길 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html",
    "href": "posts/2018-11-08-ruck2018/index.html",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "",
    "text": "김진섭 대표는 11월 26일(금) 서울특별시 시민청에서 열린 R User Conference in Korea 2018(RUCK 2018) 에서 맞춤형 의학연구 애플리케이션 개발 환경 구축 경험에 대해 발표하였습니다. 초록과 발표 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#abstract",
    "href": "posts/2018-11-08-ruck2018/index.html#abstract",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Abstract",
    "text": "Abstract\n맞춤형 의학통계 앱 제작을 위해\n\nDocker swarm 기반의 Rstudio & shiny server 를 구축하고\n의학통계 앱에 필요한 R 패키지와 Shiny Application 들을 만들었습니다.\n\n미리 Rstudio와 shiny server가 설치된 도커(docker) 이미지를 만들고 이것을 도커 스웜을 이용해 배포함으로써 서버의 종류와 갯수에 구애받지 않는 마이크로서비스 아키텍처(microservice architecture)를 구축하였으며, 동적 프록시 서버(dynamic proxy server) 프로그램인 Traefik 을 이용하여 서비스가 추가될 때 마다(ex: 홈페이지, Jupyter) 이에 맞추어 https 보안이 적용된 서브도메인(subdomain) 주소를 부여하였습니다. 흔히 이용되는 의학통계 방법들을 Shiny Application으로 만들어 위의 환경에 배포하였으며 DT, tableone, epiDisplay, svglite 등의 기존 패키지와 자체적으로 개발한 패키지를 이용, 데이터 라벨(label) 정보가 적용된 논문용 테이블과 그림을 보여줄 수 있었습니다. 이번 발표에서는 이러한 개발 환경 구축 경험을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#slide",
    "href": "posts/2018-11-08-ruck2018/index.html#slide",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/swarm-setting/RUCK2018_JSKIM 를 클릭하면 볼 수 있으며 PC 환경에 최적화되었다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html",
    "href": "posts/2022-07-13-r-datatable2/index.html",
    "title": "data.table 패키지 기초",
    "section": "",
    "text": "data.table은 빠른 속도와 메모리 효율성에 가장 적합한 패키지입니다.\n대용량의 데이터를 분산처리 시스템 없이 처리할 수 있습니다.\n데이터 프레임(data.frame)을 대신하여 더 빠르고 편리하게 사용할 수 있는 데이터 타입입니다.\n\n장점으로는, 상대적으로 메모리 요구량이 적고, 속도가 매우 빠르다는 특징이 있습니다.\n단점으로는, 다소 난해한 문법으로 널리 사용되지 못하고 있다는 특징이 있습니다.\n\n본격적으로 data.table에 대해서 알아보기 전에, Setup 과정에 대해서 먼저 소개하려고 합니다. data.table은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\nlibrary(data.table)\nlibrary(curl)\n\n위의 과정을 통해 pacakge 설치 및 불러오기를 실행합니다.\ndata.table을 생성하는 데는 두 가지 방법이 있습니다.\n첫번째는, data.table() 함수를 통해 직접 생성하는 방식입니다. 다음의 예시로 살펴보겠습니다.\n\nEX=data.table(\n  ID=c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  MATH=c(100,96,94,88,92),\n  ENGLISH=c(96,86,97,92,93),\n  HISTORY=c(85,92,87,92,94))\n\n\n\n\n\nID\nMATH\nENGLISH\nHISTORY\n\n\n\nA\n100\n96\n85\n\n\nB\n96\n86\n92\n\n\nC\n94\n97\n87\n\n\nD\n88\n92\n92\n\n\nE\n92\n93\n94\n\n\n\n\n\nID, MATH, ENGLISH, HISTORY를 변수로 한, data.table이 형성된 것을 확인할 수 있습니다.\n두번째는, 기존의 데이터를 불러오는 방법이 있습니다.\nfread 함수는 대용량 파일을 빠르게 가져올 수 있는 함수입니다. 파일을 읽어와서 data.table 형식의 자료로 만들 때, 로컬 file path를 입력하거나, http://로 시작하는 URL을 입력하는 방법을 사용할 수 있습니다.\n\nlibrary(data.table) ; library(magrittr)\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하여,\ndf에는 data.frame 형식으로 데이터를 불러왔고, dt에는 fread 함수를 이용하여 data.table의 형식으로 데이터를 불러온 것을 확인할 수 있습니다.\nfread 함수로 파일을 불러오면 그 class는 data.frame에 data.table이 추가되며, 문법이 원래의 data.frame과 달라지는 점을 유의해야 합니다.\nclass 함수를 통해 df와 dt의 속성을 확인해보겠습니다.\n\nprint(class(df)) ; print(class(dt))\n\n[1] \"data.frame\"\n\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 확인할 수 있습니다.\n지금까지 data.table을 생성하는 두 가지 방법에 대해서 알아보았습니다.\n다음으로 data.table이 data.frame과 다른 점은, 행(Row)의 이름을 받지 않는 것을 기본값으로 한다는 것입니다.\n예시로 알아보도록 하겠습니다.\nR에 기본적으로 저장되어 있는 mtcars 데이터를 이용하도록 하겠습니다.\n\n# mtcars\nEX1&lt;-as.data.frame(mtcars)\nEX2&lt;-as.data.table(mtcars)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n실행하였을 때, EX1과 EX2의 행의 이름에서 차이점이 있음을 확인할 수 있습니다.\n만약, data.table에서도 행의 이름을 남겨 놓고 싶을 때는 다음과 같이 실행하면 됩니다.\n\nEX3&lt;-as.data.table(mtcars,keep.rownames=T)\n\n\n\n\n  \n\n\n\ndata 값 뒤에 keep.rownames=T로 설정하였을 때,\n각 행의 이름이 rn 컬럼에 남아 있는 것을 확인할 수 있습니다.\n\ndata.table의 기본 문법은 DT[i, j, by] 형태입니다.\n\ni는 행(row)과 관련되어, 행에 대해서 subset 하는 역할을 수행합니다.\nj는 열(column)을 선택하거나, 열 또는 테이블 전체(.SD)에 함수를 적용합니다.\nby는 집단을 나눕니다. j에서 지정한 열과 함수에 대한 실행을 그룹 별로(by group) 수행합니다.\n맨 마지막에 [order]를 붙여 오름차순이나 내림차순으로 정렬할 수 있습니다.\n\ndata.table에서만 확인할 수 있는 특수기호들이 있습니다.\n각 특수기호의 자세한 기능과 사용법은 이하에서 설명하기로 하고, 여기에서는 간단히 개념정도만 다뤄보려고 합니다.\n\n.SD : Subset of Data(by=로 나눠진 부분 데이터). 특수 기호를 사용하여 그룹 칼럼(by grouping columns)을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용합니다.\n.SDcols : 특수 기호를 사용하여 특정 다수의 칼럼을 지정하여 처리할 때 사용합니다.\n.N : 부분 데이터의 행의 수를 나타낼 때 사용합니다. 특정한 열을 잡아서 length() 함수를 이용해도 되지만 좀 더 간편하게 구할 수 있습니다.\n:= : DT[i, j, by]에서 칼럼 j를 추가/갱신/삭제할 때 특수기호 := 연산자를 사용하여 수행할 수 있습니다.\n\n이상에서 data.table을 이용하면서 가장 많이 쓰이는 특수기호들에 대해서 알아보았습니다. 각각의 특수기호들이 어떻게 실제로 쓰이는지에 대해서는 이하에서 등장할 때마다 자세하게 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#data.-table",
    "href": "posts/2022-07-13-r-datatable2/index.html#data.-table",
    "title": "data.table 패키지 기초",
    "section": "",
    "text": "data.table은 빠른 속도와 메모리 효율성에 가장 적합한 패키지입니다.\n대용량의 데이터를 분산처리 시스템 없이 처리할 수 있습니다.\n데이터 프레임(data.frame)을 대신하여 더 빠르고 편리하게 사용할 수 있는 데이터 타입입니다.\n\n장점으로는, 상대적으로 메모리 요구량이 적고, 속도가 매우 빠르다는 특징이 있습니다.\n단점으로는, 다소 난해한 문법으로 널리 사용되지 못하고 있다는 특징이 있습니다.\n\n본격적으로 data.table에 대해서 알아보기 전에, Setup 과정에 대해서 먼저 소개하려고 합니다. data.table은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\nlibrary(data.table)\nlibrary(curl)\n\n위의 과정을 통해 pacakge 설치 및 불러오기를 실행합니다.\ndata.table을 생성하는 데는 두 가지 방법이 있습니다.\n첫번째는, data.table() 함수를 통해 직접 생성하는 방식입니다. 다음의 예시로 살펴보겠습니다.\n\nEX=data.table(\n  ID=c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  MATH=c(100,96,94,88,92),\n  ENGLISH=c(96,86,97,92,93),\n  HISTORY=c(85,92,87,92,94))\n\n\n\n\n\nID\nMATH\nENGLISH\nHISTORY\n\n\n\nA\n100\n96\n85\n\n\nB\n96\n86\n92\n\n\nC\n94\n97\n87\n\n\nD\n88\n92\n92\n\n\nE\n92\n93\n94\n\n\n\n\n\nID, MATH, ENGLISH, HISTORY를 변수로 한, data.table이 형성된 것을 확인할 수 있습니다.\n두번째는, 기존의 데이터를 불러오는 방법이 있습니다.\nfread 함수는 대용량 파일을 빠르게 가져올 수 있는 함수입니다. 파일을 읽어와서 data.table 형식의 자료로 만들 때, 로컬 file path를 입력하거나, http://로 시작하는 URL을 입력하는 방법을 사용할 수 있습니다.\n\nlibrary(data.table) ; library(magrittr)\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하여,\ndf에는 data.frame 형식으로 데이터를 불러왔고, dt에는 fread 함수를 이용하여 data.table의 형식으로 데이터를 불러온 것을 확인할 수 있습니다.\nfread 함수로 파일을 불러오면 그 class는 data.frame에 data.table이 추가되며, 문법이 원래의 data.frame과 달라지는 점을 유의해야 합니다.\nclass 함수를 통해 df와 dt의 속성을 확인해보겠습니다.\n\nprint(class(df)) ; print(class(dt))\n\n[1] \"data.frame\"\n\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 확인할 수 있습니다.\n지금까지 data.table을 생성하는 두 가지 방법에 대해서 알아보았습니다.\n다음으로 data.table이 data.frame과 다른 점은, 행(Row)의 이름을 받지 않는 것을 기본값으로 한다는 것입니다.\n예시로 알아보도록 하겠습니다.\nR에 기본적으로 저장되어 있는 mtcars 데이터를 이용하도록 하겠습니다.\n\n# mtcars\nEX1&lt;-as.data.frame(mtcars)\nEX2&lt;-as.data.table(mtcars)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n실행하였을 때, EX1과 EX2의 행의 이름에서 차이점이 있음을 확인할 수 있습니다.\n만약, data.table에서도 행의 이름을 남겨 놓고 싶을 때는 다음과 같이 실행하면 됩니다.\n\nEX3&lt;-as.data.table(mtcars,keep.rownames=T)\n\n\n\n\n  \n\n\n\ndata 값 뒤에 keep.rownames=T로 설정하였을 때,\n각 행의 이름이 rn 컬럼에 남아 있는 것을 확인할 수 있습니다.\n\ndata.table의 기본 문법은 DT[i, j, by] 형태입니다.\n\ni는 행(row)과 관련되어, 행에 대해서 subset 하는 역할을 수행합니다.\nj는 열(column)을 선택하거나, 열 또는 테이블 전체(.SD)에 함수를 적용합니다.\nby는 집단을 나눕니다. j에서 지정한 열과 함수에 대한 실행을 그룹 별로(by group) 수행합니다.\n맨 마지막에 [order]를 붙여 오름차순이나 내림차순으로 정렬할 수 있습니다.\n\ndata.table에서만 확인할 수 있는 특수기호들이 있습니다.\n각 특수기호의 자세한 기능과 사용법은 이하에서 설명하기로 하고, 여기에서는 간단히 개념정도만 다뤄보려고 합니다.\n\n.SD : Subset of Data(by=로 나눠진 부분 데이터). 특수 기호를 사용하여 그룹 칼럼(by grouping columns)을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용합니다.\n.SDcols : 특수 기호를 사용하여 특정 다수의 칼럼을 지정하여 처리할 때 사용합니다.\n.N : 부분 데이터의 행의 수를 나타낼 때 사용합니다. 특정한 열을 잡아서 length() 함수를 이용해도 되지만 좀 더 간편하게 구할 수 있습니다.\n:= : DT[i, j, by]에서 칼럼 j를 추가/갱신/삭제할 때 특수기호 := 연산자를 사용하여 수행할 수 있습니다.\n\n이상에서 data.table을 이용하면서 가장 많이 쓰이는 특수기호들에 대해서 알아보았습니다. 각각의 특수기호들이 어떻게 실제로 쓰이는지에 대해서는 이하에서 등장할 때마다 자세하게 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "href": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "title": "data.table 패키지 기초",
    "section": "2. data.table에 접근하기",
    "text": "2. data.table에 접근하기\n이하에서는 위에서 불러온 dt(=09-15년 공단 건강검진 데이터)와 EX3(=mtcars) 데이터를 이용해서 실습하려고 합니다.\n2-1. 행(Row)에 접근하기\ndata.table에서 행(Row)에 접근하는 방법은 DT[i, j, by]에서 i 자리에 값을 넣는 것입니다. 즉, DT[c(row1, row2, …)]의 방식입니다.\nmtcars 데이터로 예시를 들어보겠습니다.\n여러 개의 자동차 종류 중, Datsun 710과 Hornet Sprotabout에 대해서만 알아보고 싶을 때는 다음과 같이 작성하면 됩니다.\n\nEX3[c(3,5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n만약 Mazda RX4 부터 Hornet Sportabout까지 알아보고 싶다면, 범위로 지정할 수도 있습니다.\n\nEX3[1:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n여기서 중요한 것은 DT[i, j, by]라는 기본적인 형식에서 i의 자리에 지금 내용을 채워 넣는 것인데, i 자리에 내용을 작성한 후 꼭 콤마(,)를 찍지 않아도 된다는 것입니다. 꼭 콤마(,)를 찍지 않아도 뒤에 특정한 열 을 선택하지 않으면 모든 열에 대해서 알아서 필터링을 하기 때문입니다.\n다음으로는 특정 조건을 만족하는 행(row)을 선택하는 방법에 대해서 알아보려고 합니다.\nDT[조건]의 형식을 이용하면 됩니다.\nmtcars 데이터에서 cyl&gt;=6이면서, carb==4인 조건을 만족하는 데이터를 찾고 싶은 경우, 다음과 같이 작성하면 됩니다.\n\nEX3[cyl&gt;=6 & carb==4]\n\n\n\n\n  \n\n\n\nKEY를 미리 설정해놓으면 더 빠르게 검색할 수 있는데, 이 내용에 대해서는 뒤에서 자세하게 다루도록 하겠습니다.\n다음으로는 특정 행(row)을 제외하는 방법에 대해서 알아보려고 합니다. 제외하려는 행 혹은 행의 범위 앞에 - 나 ! 를 붙여주면 됩니다.\n\nEX3[!1:5]\nEX3[-2]\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n위와 같이 실행했을 때, 제외하려는 데이터가 사라진 것을 확인할 수 있습니다.\n2-2. 열(Column)에 접근하기\n행(Row)을 선택할 때와 유사합니다. 기본적인 형식은 DT[i, j, by]의 j 자리에 넣는 것입니다.\nmtcars 데이터에서 ‘cyl’ 열(Column)을 가져오고 싶을 때는 다음과 같이 가져올 수 있습니다.\n\nEX3[,3] ; EX3[,.(cyl)] ; EX3[,\"cyl\"]\n\n\n\n\n\ncyl\n\n\n\n6\n\n\n6\n\n\n4\n\n\n6\n\n\n8\n\n\n6\n\n\n\n\n\n열(column)의 숫자로 불러와도 되고, 변수의 이름으로 불러오는 것도 가능합니다. 그런데, 여기서 중요하게 봐야할 점은 변수의 이름으로 가져올 때 앞에 .()의 형식을 이용했다는 점입니다.\n.()은 list()와 동일한 기능을 하는데, 조금 더 간편하게 쓸 수 있는 형식이라 생각하면 됩니다.\ndata.table에서는 변수의 이름만 넣었을 경우, 벡터의 형식으로 값을 불러옵니다. 그렇기 때문에 data.table의 형식을 유지하면서 데이터를 불러오려면 .() 혹은 list() 형식을 이용해야 합니다. 혹은 변수를 따옴표를 이용하여 작성하는 것도 동일한 결과를 가져옵니다.\n열(column)을 선택할 때, DT[,.(new_col_name=col)] 형식을 사용하여 새로운 열 이름을 지정해서 출력할 수도 있습니다.\n\nEX3[,.(MPG=mpg, CYL=cyl)]\n\n\n\n\n  \n\n\n\n위와 같이 mpg와 cyl에 대해서 변수 이름을 대문자로 바꿔준 것을 확인할 수 있습니다.\n다음으로는 변수로 열을 선택하는 방법에 대해서 알아보려고 합니다.\n예시를 위해 mpg, cyl, disp 세 변수를 묶는 VARS라는 새로운 변수를 임의로 설정하겠습니다.\n\nVARS&lt;-c(\"mpg\",\"cyl\",\"disp\")\n\n\nEX3[,..VARS]\n\n\n\n\n  \n\n\n\nVARS 라는 변수를 넣었을 때, 위에서 설정한 것처럼 mpg, cyl, disp에 대한 값들만 추출한 것을 확인할 수 있습니다. 여기서 중요한 것은, 변수 앞에 .. 을 넣어줬다는 것입니다.\ndata.table의 약속이라고 보면 되는데, 같은 결과를 도출하는 다른 형식들에 대해서 소개하려고 합니다.\n우선은, with = F가 있습니다.\n\nEX3[,VARS,with=F]\n\n\n\n\n  \n\n\n\nVARS 앞에 ..을 붙이지 않아도, with=F를 추가한다면 같은 결과를 도출하는 것을 확인할 수 있습니다.\n다음으로는 앞서 배운 .SD 과 .SDcols를 이용하는 방법에 대해 알아보겠습니다.\n\nEX3[,.SD,.SDcols=VARS]\n\n\n\n\n  \n\n\n\n.SD를 통해 전체 변수를 대상으로 하되, .SDcols로 특정 변수만을 설정하는 메커니즘입니다.\n또한 특정조건을 만족하는 값들에 대해 VARS의 변수 값을 알고 싶으면 다음과 같이 실행하면 됩니다.\n\nEX3[hp&gt;=130 & gear&gt;=4, ..VARS]\n\n\n\n\n  \n\n\n\nhp가 130을 넘고, gear가 4를 넘는 값들 중 VARS(mpg,cyl,disp) 변수에서 해당하는 값들을 보여주는 것을 확인할 수 있습니다.\n다음으로는 열을 제거하는 방법에 대해서 알아보려고 합니다. 행(row)을 제거할 때와 유사하게 - , ! 을 통해서 실행하면 됩니다. 그리고 같은 결과를 도출하는 다른 형식들에 대해서도 소개하려고 합니다.\n\nEX3[,-..VARS] ; EX3[,!..VARS] ; EX3[,.SD,.SDcols=-VARS]\n\n\n\n\n  \n\n\n\n마지막으로 열(column)의 값에 대해서 함수들을 이용해 값들을 가공하는 방법입니다.\nmpg와 hp의 평균에 대해서 구해보겠습니다.\n\nEX3[,.(mean(mpg), mean(hp))]\n\n\n\n\n  \n\n\n\n값을 실행할 경우, V1, V2라는 변수 아래에 값이 도출되는 것을 확인할 수 있습니다.\n위에서 배웠던, 변수에 새로운 이름을 부여하는 방식을 이용해보겠습니다.\n\nEX3[,.(MEAN_mpg=mean(mpg), MEAN_hp=mean(hp))]\n\n\n\n\n  \n\n\n\n위와 동일한 값에 변수의 이름이 생긴 것을 확인할 수 있습니다.\n.SD, .SDcols를 이용해서도 도출할 수 있습니다.\n\nEX3[,lapply(.SD,mean), .SDcols=(c(\"mpg\", \"hp\"))]\n\n\n\n\n\nmpg\nhp\n\n\n20.09062\n146.6875\n\n\n\n\n또한 행(row)의 자리에 특정 조건을 입력하여, 특정 조건을 만족하는 변수들에 대해서만 특정 함수를 적용할 수도 있습니다.\n\nEX3[gear==4, lapply(.SD,mean), .SDcols=c(\"mpg\",\"hp\")]\n\n\n\n\n\nmpg\nhp\n\n\n24.53333\n89.5\n\n\n\n\n2-3. by에 접근하기\nby는 집단을 나눕니다. 정확히는 옵션을 이용하여 그룹별로 함수를 적용할 수 있습니다.\nby = (그룹1, 그룹2, …)의 형식으로 두 개 이상의 그룹별로 함수를 적용할 수도 있는데, 이 때 괄호 앞에 있는 점(.)은 list를 의미하므로 꼭 포함시켜야 합니다. (ex. by=.(EXMD_BZ_YYYY, Q_SMK_YN) 와 같이 두 개 이상의 그룹으로 묶을 때는 .()의 형식을 이용해야 합니다.)\n어떻게 쓰이는지 바로 알아보도록 하겠습니다.\n여기에서는 dt(=09-15 공단 건강검진 데이터)를 이용해서 실습해보려고 합니다. EXMD_BZ_YYYY을 기준으로 집단을 분리한 후, 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같습니다.\n\ndt[,.(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by= EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY에 따라 각 연도를 기준으로, HGHT, WGHT, BMI가 정렬이 되었고, 그 값들의 평균을 그룹별로 구하여 나타낸 데이터 값입니다.\n만약 특정한 변수가 아닌, 모든 변수에 대해서 평균을 구하고 싶다면 .SD를 이용하면 됩니다.\n\ndt[,lapply(.SD,mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n위의 값은 평균을 낼 수 없는 변수들에 대해서도 일괄적으로 평균을 돌렸기 때문에 NA 값이 도출되었습니다. 값에 집중하기보다, 전체에 대한 함수를 적용하는 방식에 대해서 알아두면 좋을 것 같습니다. 만일 전체가 아닌 특정 변수에만 함수를 적용하고 싶다면 .SDcols 을 이용하면 됩니다.\n다음으로는 두 개 이상의 그룹 변수를 지정해 행(row)의 개수를 구해보겠습니다.\n키가 175cm 이상인 사람들에 대해서, 연도(EXMD_BZ_YYYY)와 흡연 여부(Q_SMK_YN)로 구분해보려고 합니다.\n\ndt[HGHT&gt;=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\n위에서 잠깐 언급한 .N 을 이용하여 특정조건에 부합하며 각 변수값에 해당되는 행(row)의 수를 구해보았습니다. 그러나, 여기에서 도출된 결과값의 문제는 Q_SMK_YN의 값이 섞여 있다는 것입니다.\n조금 더 정렬된 결과값으로 나타내고 싶을 때는, by 대신에 keyby를 이용하면 됩니다. keyby는 기존의 by에 오름차순/내림차순 기능이 포함되었다고 생각하면 됩니다. 만약 by를 이용하면서 정렬을 시키고 싶다면 마지막에 [order(정렬기준)]를 붙이면 됩니다.\n\ndt[HGHT&gt;=175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\ndt[HGHT&gt;=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)][order(EXMD_BZ_YYYY)]\n\n\n\n\n  \n\n\n\n연도를 기준으로 정렬을 하고 싶은 경우, 위와 같이 뒤에 [order(EXMD_BZ_YYYY)]를 붙여주면, 위의 값과 다르게 정렬된 것을 확인할 수 있습니다.\n다음으로, 특정 조건(HGHT&gt;=175)를 만족시키면서, 하나의 기준을 더 추가하여 분류하고 싶을 때는 다음과 같은 방식을 이용하면 됩니다.\n\ndt[HGHT&gt;=175, .N, keyby=.(Y2015 = ifelse(EXMD_BZ_YYYY&gt;=2015, \"&gt;=2015\", \"&lt;2015\"))]\n\n\n\n\n\nY2015\nN\n\n\n\n&lt;2015\n206\n\n\n&gt;=2015\n36\n\n\n\n\n\nHGHT가 175 이상인 사람들을 우선으로 뽑아놓고, 거기에서 Y=2015를 기준으로 행(row)의 갯수를 확인하였습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "href": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "title": "data.table 패키지 기초",
    "section": "3. 다른 기능들",
    "text": "3. 다른 기능들\n3-1. setkey()\n키를 설정합니다. 키를 활용하는 이유는 자료를 찾을 때, 그 탐색 및 처리 시간을 단축시키기 위함입니다.\nsetkey(DT,col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용합니다.\n만일 설정된 키를 제거할 경우, setkey(DT, NULL)를 활용합니다.\ndt 데이터를 이용하여, key를 설정하고 활용해보겠습니다.\n\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\ndt의 키로 EXMD_BZ_YYYY가 설정된 것을 확인할 수 있습니다. 다른 변수들도 setkey 함수에 추가로 입력하면, 그 변수들이 key로 저장된 것을 확인할 수 있습니다.\n다음으로는 키를 활용한 행(row) 선택에 대해서 알아보려고 합니다. dt[.(a)], dt[J(a)], dt[list(a)], dt[col==a] 중에서 아무거나 사용하여 행을 선택할 수 있습니다. 위에서 EXMD_BZ_YYYY를 key로 설정하였기 때문에, dt[J(a)]에서 a의 자리에 EXMD_BZ_YYYY에 포함되어 있는 값을 넣으면 그 값을 기준으로 데이터를 정리합니다.\n예시로,\n\ndt[J(2013)]\n\n\n\n\n  \n\n\n\n이 값의 경우, EXMD_BZ_YYYY가 2013인 값에 대하여 정리한 것을 확인할 수 있습니다.\n만약, key를 두 개 이상 설정해놓은 경우, a의 자리에 다른 조건을 연결하면 그 조건도 포함하고 있는 값이 도출됩니다.\n\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\ndt[J(2013,2)]\n\n\n\n\n  \n\n\n\n위의 과정은 key에 Q_HBV_AG를 추가한 뒤, 2013년도에 Q_HBV_AG가 2인 값들에 대해서 정리한 것입니다.\n3-2. Merge : data.table 병합\n다음으로는 두 개의 data.table에 대해서 공통된 column을 기준으로,\n하나의 data.table로 만드는 방법에 대해서 소개하려고 합니다.\ndt 파일의 설문조사에 관한 데이터(Q_)들을 하나의 변수 colvars로 편의를 위해 설정하였습니다.\n\ncolvars&lt;-grep(\"Q_\", names(dt), value=T)\n\n다음으로는 dt 데이터를 임의로 분리하여 dt1, dt2를 설정하겠습니다.\n\ndt1&lt;-dt[1:10, .SD, .SDcols=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\", colvars)]\ndt2&lt;-dt[6:15, -..colvars]\n\n본격적인 분석을 하기에 앞서, dt1 과 dt2에 대해서 간단히 살펴보겠습니다.\n행(row)을 기준으로는 6:10행까지가 겹치고,\n열(column)을 기준으로는 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM” 이 공통입니다.\ndt1, dt2 데이터를 이용해 merge 함수에 대해서 알아보도록 하겠습니다.\nmerge 에는 inner_join, full_join, left_join, right_join, anti_join 등이 있습니다.\n하나하나씩 예시와 함께 알아보도록 하겠습니다.\n처음으로 알아볼 것은 inner_join 입니다. 집합의 교집합 개념과 유사하지만, 약간의 차이점은 존재합니다.\n\ninner_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=F)\n\n\n\n\n  \n\n\n\n우선 inner_join을 실행함에 있어 단순하게 inner_join 함수를 이용해도 되지만, merge 함수에서 공통 변수인 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”을 직접 merge의 매개체로 설정할 수도 있습니다. 그리고 inner_join의 경우, merge 함수의 뒤에 all=F가 들어간다는 것을 유의하시면 될 것 같습니다. (뒤에 full_join과 비교)\ninner_join을 실행하였습니다. dt1과 dt2의 공통 행(row)에 속하는 6:10행까지를 기준으로 정렬하되, 각 값들이 dt1, dt2에서 가지고 있던 변수 값들도 그대로 가져온 것을 확인할 수 있습니다.\n결과값의 RN_INDI가 714509인 값을 살펴보겠습니다.\n이 변수값은 원래 dt1에서는 HGHT와 WGHT 등의 값을 가지고 있지 않았습니다. 그러나, inner_join을 하면서 dt2의 값을 그대로 받아와, HGHT, WGHT 등의 값을 부여받은 것을 확인할 수 있습니다.\n다음으로는 full_join 입니다. 집합의 합집합 개념과 유사합니다.\n\nfull_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=T)\n\n\n\n\n  \n\n\n\nfull_join을 실행하였습니다. dt1과 dt2의 모든 행이 나열된 것을 확인할 수 있습니다. (공통된 행(row)은 한번만 표시되었습니다. 또한 inner_join과 다르게 all=T 임을 확인할 수 있습니다.)\n여기서 유심히 봐야할 것은 1:5, 11:15행입니다.\n1:5행의 경우에는 dt에는 속해 있지만, dt2에는 속해있지 않습니다. 그렇기 때문에 1:5행은 HGHT, WGHT 등 dt2에만 있는 값들에 대해서는 받을 값이 존재하지 않아, NA로 표시된 것을 확인할 수 있습니다.\n반면, 11:15행의 경우에는 dt2에는 속해 있지만, dt1에는 속해있지 않습니다. 그렇기 때문에 Q_로 시작하는 변수값에 대해서 받을 수가 없어서 NA로 나온 것을 확인할 수 있습니다.\n다음으로는 left_join과 right_join 입니다.\n\nleft_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.x=T)\ndt2[dt1, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\nleft_join을 실행하였습니다. 변수의 값이 dt1의 row를 기준으로 설정된 것을 확인할 수 있습니다. 그러나, dt2의 column이 추가되어, HGHT, WGHT 등 기존의 dt1에 없던 변수들이 생긴 것을 확인할 수 있습니다. inner_join과 유사하게, dt2에 있는 변수들에 대해서는 left_join을 했을 때도, 원래 dt1에는 없었던 HGHT, WGHT 등의 값이 새로 생긴 것을 확인할 수 있습니다.\nright_join으로도 직접 실습하여 차이를 확인하시기 바랍니다.\n하나의 차이가 있다면, left_join을 했을 때는 all.x = T 였지만, right_join의 경우에는 all.y =T 를 사용합니다.\n\nright_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.y=T)\ndt1[dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\n마지막으로 anti_join이 있습니다. 예시부터 보이고 설명하도록 하겠습니다.\n\nanti_join(dt1,dt2)\ndt1[!dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n  \n\n\n\n직관적으로 확인할 수 있듯, dt2와 겹치지 않는 dt1의 값들에 대해서만 나타낸 것을 확인할 수 있습니다. 또한 다른 join들이 dt2의 column(variable)을 가져왔던 것과 다르게, anti_join은 오직 dt1의 변수들로만 구성된 것을 확인할 수 있습니다.\n만약에 anti_join(dt2,dt1) (또는, dt2[!dt1, on = c(“EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”))은 이라고 작성한다면 위와 반대로 dt2를 기준으로 데이터의 값이 도출됩니다.\n3-3. 수정 연산자 :=\ndata.table에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용합니다.\n수정 또는 생성하는 column이 두 개 이상이라면, DT[,c(“cola”, “colb”) := list(valA,valB)] 또는, DT[, “:=”(cola, colb)]의 형식을 사용합니다.\n즉, := 는 새로운 data.table을 생성하지 않고 기존의 데이터 테이블에 덮어씌우거나(수정), 새로운 컬럼을 추가합니다.\n다음 예시로 알아보겠습니다.\nBMI2 라는 새로운 변수를 data.table에 추가하려고 합니다.\n(BMI2 = WGHT/(HGHT/100)^2 를 하고, 소수점 첫째자리까지 반영)\n\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\n\n\n\n  \n\n\n\n열(column)의 맨 마지막에 BMI2가 새롭게 생긴 것을 확인할 수 있습니다.\n다음으로는 특정 조건을 충족하는 값들에 대해서 새로운 변수를 만들어 확인하는 것에 대해서 알아보려고 합니다.\n두 가지 조건을 설정하겠습니다. 하나는, BP_SYS가 140이 넘는지 그리고 BMI가 25가 넘는지에 대해서, factor로 바꾸어 0과 1로 나타내는 column에 대해서 추가하려고 합니다.\n\ndt[, ':=' (BP_SYS140 = factor(as.integer(BP_SYS&gt;=140)), BMI25 = factor(as.integer(BMI&gt;=25)))]\n\n\n\n\n  \n\n\n\ndt의 column에 새롭게 BP_SYS140과 BMI25 컬럼이 생성되어, 0과 1로 TRUE/FALSE를 보여주고 있습니다.\n그리고 := 을 이용해서 column을 삭제할 수도 있습니다. 위에서 만들어본 BMI25 column에 대해서 제거해보려고 합니다.\n\ndt[,BMI25 := NULL]\n\n\n\n\n  \n\n\n\n3-4. 데이터 재구조화\n마지막으로, data의 형태를 바꾸는 melt(wide to long), dcast(long to wide) 함수에 대해서 알아보겠습니다.\n우선 melt 함수입니다.\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있습니다. melt 함수의 기본 구조는 다음과 같습니다.\nmelt(data, id.vars, measure.vars, variable.name, value.name)의 구조를 가지고 있습니다.\n\nid.vars에는 data에서 그대로 유지할, 고정될 column에 대해 작성하면 됩니다.\nmeasure.vars에는 새로운 변수에 포함될 기존의 데이터 값들에 대해서 넣으면 됩니다.\nvariable.name에는 measure.vars에서 추출한 데이터들을 모은 변수에 대한 이름을 설정하면 됩니다.\nvalue.name에는 그 variable.name의 값들이 적히는 곳의 이름을 설정한다고 보면 됩니다.\n\n예시로 쉽게 설명해보겠습니다.\n\nmelt_EX&lt;-melt(dt,\n              id.vars=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n              measure.vars = c(\"TOT_CHOL\", \"TG\", \"HDL\", \"LDL\"),\n              variable.name = \"Lipie\",\n              value.name = \"Value\")\nmelt_EX\n\n\n\n\n  \n\n\n\n위의 예시에서 EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM 세 변수는 고정되어 있는 것을 확인할 수 있습니다. 그리고 TOT_CHOL, TG, HDL, LDL 값들이 Lipid라는 새로운 변수에 묶여있고, 그것들의 값이 Value에 나타나는 것을 확인할 수 있습니다.\nmelt 함수는 또한 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있습니다. melt 함수에 meausre = list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣습니다. 이 때 공통의 value.name을 지정할 수 있습니다.\n다음의 예시를 보겠습니다.\n\ncol1&lt;-c(\"BP_SYS\", \"BP_DIA\")\ncol2&lt;-c(\"VA_LT\", \"VA_RT\")\nmelt_EX2 &lt;- melt(dt,\n                 id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n                 measure.vars = list(col1, col2),\n                 value.name = c(\"BP\", \"VA\"))\nmelt_EX2\n\n\n\n\n  \n\n\n\n예시가 직관적이진 않지만 간단하게 설명을 하자면, col1(BP_SYS, BP_DIA)과 col2(VA_LT, VA_RT)의 내용이 매칭 되는 것입니다. 그래서 BP_SYS와 VA_LT일 때, variable에서 1로 나타나고, BP_DIA와 VA_RT일 때, 2로 나타나는 것입니다.\n(따라서, BP_SYS와 VA_RT의 값이 매칭되는 경우는 없습니다.)\n(또한 만약에 col1과 col2에 각각 새로운 변수가 하나씩 추가되었다면 그 값은 variable에 3으로 표시될 것입니다.)\n다음은 dcast 입니다.\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용합니다. 쉽게 설명하면, 방금 melt에서 행한 작업을 정확히 반대로 수행한다고 보면 됩니다.\ndcast의 기본 구조는 다음과 같습니다.\ndcast(data, formula, value.var, fun.aggregate)\n조금 더 실용적으로 작성하면 dcast(data, ID1+ID2+ID3+…~ variable, value.var = “val”)의 구조입니다.\n구조에 대해서 설명하자면,\n\ndata에는 dcast 함수를 실행할 데이터를 의미하고\nID1+ID2+ID3+…는 기존의 data부터 dcast 이후에도 고정적으로 유지될 변수들을 의미합니다.\nvariable은 melt 함수로 모아진 변수들을 다시 long to wide 하게 하기 위해 해당 변수들을 작성하는 곳입니다.\nvalue.var = ‘val’ 은 dcast 함수로 long to wide 하게 될 변수 값을 작성하는 공간입니다.\n그리고 variable과 value.var = 칸에는 melt를 하면서 지정한 변수 이름을 넣어주면 됩니다.\n\n예시로 알아보도록 하겠습니다.\n\ndcast_EX &lt;- dcast(melt_EX, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ Lipid, value.var=\"Value\")\ndcast_EX\n\n\n\n\n  \n\n\n\nLipid 변수에 하나로 묶어뒀던 TOT_CHOL, TG, HDL, LDL 변수가 다시 각각의 변수로 바뀐 것을 확인할 수 있습니다.\n다음은 dcast 함수를 조금 더 응용해서, 재구조화를 할 때 sum, mean 등의 집계함수를 이용해서 그룹별 요약 통계량을 나타내는 과정을 소개하겠습니다.\n\ndcast_EX2 &lt;- dcast(melt_EX, EXMD_BZ_YYYY ~ Lipid, value.var = \"Value\", fun.aggregate = mean, na.rm = T)\ndcast_EX2\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY, 즉 연도 변수를 기준으로 Lipid에 묶여있던 TOT_CHOL, TG, HDL, LDL 변수들의 평균에 대해서 (결측치를 제거하고) 나타낸 것을 확인할 수 있습니다.\n조금 더 구체적으로 fun.aggregate 뒤에는 기존에 존재하는 함수가 아닌, fun.aggregate = function(x){}의 형식으로 어떠한 함수도 이용할 수 있습니다.\n다음으로는 여러 개의 칼럼들을 묶어서 melt 한 data.table에 대해서도 dcast를 하는 것에 대해 설명하겠습니다. 기본적으로 dcast 구조와 동일하지만 약간의 차이가 있습니다. variable 칸에는 그대로 long to wide 하려는 변수 이름만을 작성하면 되지만, value.var = 칸에는 각각의 데이터 값의 이름을 다 적어야 한다는 차이점이 존재합니다.\n예시로 보이겠습니다.\n\ndcast_EX3 &lt;- dcast(melt_EX2, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ variable, value.var = c(\"BP\", \"VA\"))\ndcast_EX3"
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "href": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "title": "data.table 패키지 기초",
    "section": "4. 마치며",
    "text": "4. 마치며\n이상에서 R에서 데이터를 쉽고 빠르게 가공할 수 있는 data.table에 대하여 알아보았습니다.\n배운 내용을 가볍게 정리하고 마무리하도록 하겠습니다.\n1) 생성하기 : data.table은 기본적으로 설치되어 있는 프로그램이 아니기 때문에, package 설치가 필요하다.\n2) 기본구조 : data.table의 기본 문법은 DT [ i, j, by ] 형태이며 각각의 쓰임새는 다음과 같다.\n\ni : 행(row)을 선택\nj : 열(column)을 선택, data.table 전반에 함수를 적용\nby: 그룹을 구성, j에서 행한 함수를 by 그룹 별로 수행시킬 수 있음.\n\n3) 특수기호: data.table 에서만 확인할 수 있는 특수기호들이 있다. (.SD, .SDcols, .N, :=)\n4) KEY를 설정하여, 데이터에 조금 더 빠르게 접근할 수 있다.\n5) merge, melt, dcast 등의 함수를 통해 데이터를 보기 쉽게 가공할 수 있다.\n감사합니다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html",
    "href": "posts/2020-10-05-docker-rshiny/index.html",
    "title": "RStudio & Shiny Docker 소개",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 자체 제작한 RStudio & Shiny-server Docker image 를 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "href": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "title": "RStudio & Shiny Docker 소개",
    "section": "요약",
    "text": "요약\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앤다.\n공식 image 참고하여 자체개발. https://github.com/jinseob2kim/docker-rshiny"
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "href": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "title": "RStudio & Shiny Docker 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/docker-rshiny 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html",
    "href": "posts/2022-10-17-medical-scientist/index.html",
    "title": "의료데이터분석가 성장기",
    "section": "",
    "text": "김진섭 대표는 10월 19일(토) 융합형 의사과학자 심포지움 에서 “의료데이터분석가 성장기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#요약",
    "href": "posts/2022-10-17-medical-scientist/index.html#요약",
    "title": "의료데이터분석가 성장기",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n의학연구지원 \\(\\rightarrow\\) 임상시험분석시장 목표\n천하3분지계: 법학, 의학, 종교\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n김옥균, 맹상군, 유비\n문제해결 \\(\\rightarrow\\) 문제정의 \\(\\rightarrow\\) 아름다움 \\(\\rightarrow\\) 신내림"
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#slide",
    "href": "posts/2022-10-17-medical-scientist/index.html#slide",
    "title": "의료데이터분석가 성장기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/medical-scientist 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 5주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "목차",
    "text": "목차\n\nTraefik이란?\ndocker-compose.yml 파일 작성\nrules.yml 파일 작성\n실행결과\n결론\n\n\nTraefik이란?\n nginx와 같이 reverse프록시의 종류로서 별도의 제어 없이 실행중에 실시간으로 통신되는 요소끼리 찾아서 연결해주는 기능이 특징입니다. 또한 기본적으로 제공하는 대시보드 기능을 통하여 실시간으로 연결되어 있는 서비스들을 확인할 수 있고 또한 어떤 서버와 연결되어 있는지 파악이 가능합니다.\n해당 게시글은 Docker와 Traefik version2.2를 활용한 서비스에서 Reverse-Proxy를 구현하는 방법에 대해 설명 하도록 하겠습니다. 위 이미지에서 보이는 것처럼 Traefik은 들어오는 요청을 각각의 Docker Container에 배정해주는 역할을 수행합니다.\n아래 이미지는 Traefik 대시보드 페이지 입니다. \n\n\n\ndocker-compose.yml 파일 작성\n\ntraefik을 image로 가지고 있는 proxy 컨테이너와, whoami를 image로 가지고 있는 website 컨테이너를 생성하기 위한 docker-compose.yml에 작성된 코드입니다. 해당 코드를 활용하여 traefik으로 각 컨테이너를 어떻게 제어할 수 있는지에 대해 알아보도록 하겠습니다. 먼저 proxy 컨테이너 속 주요 코드를 살펴 보도록 하겠습니다.\n\n\nProxy 컨테이너\nimage: traefik:v2.2\ncommand:\n  - --entrypoints.web.address=:80\n  - --entrypoints.websecure.address=:443\n컨테이너의 이미지로서 traefik:v2.2를 사용하며 entrypoints로서 80번 포트로 들어오는 요청들은 web, 443번 포트로 들어오는 요청들은 websecure로 각각 명명하는 코드입니다\n\n- --certificatesresolvers.re.acme.email=*****@naver.com\n- --certificatesresolvers.re.acme.storage=./acme.json\n- --certificatesresolvers.re.acme.httpchallenge.entryPoint=websecure\n웹사이트에 Https를 적용하기 위해 Let’s Encrypt로부터 Certificate를 발급 받는 과정을 ACME protocol을 활용해서 자동으로 발급 받고 적용시킬 수 있도록 해주는 코드입니다.\n\n위 코드에서 ‘re’, ‘email 주소’, ‘acme.json’ 파일의 경로 혹은 파일명’은 사용자가 편하게 수정해도 괜찮습니다. 물론 entryPoint 값 또한 앞서 선언한 entrypoints 중에 본인이 희망하는 포트로 변경해도 괜찮습니다.\n\n해당 코드를 작성하기에 앞서 해당 경로에 acme.json파일을 생성해야 합니다. 이후 코드를 실행시키면 acme.json 파일에 certificate에 대한 내용이 담기게 됩니다.\n\n(아래 이미지는 acme.json 파일의 일부입니다.)\n\n\nports:\n  - 80:80\n  - 443:443\n  - 8080:8080\n80번 포트, 443번 포트, 8080포트로 들어오는 요청에 대한 포트 연결입니다.\n\n8080포트는 Traefik dashboard의 기본 포트입니다. 8080포트를 통하여 Traefik dashboard에 접속이 가능합니다.\n\nvolumes:\n  - /var/run/docker.sock:/var/run/docker.sock\n  - ./rules.yml:/etc/traefik/rules.yml:ro\n  - ./acme.json:/acme.json \ntraefik이 /var/run/docker.sock를 사용 가능하도록하여 docker container들의 정보를 사용이 가능합니다. 또한 앞서 작성한 acme.json 파일과 앞으로 작성할 rules.yml 파일 또한 사용이 가능하도록하는 코드입니다. rule.yml파일을 통하여 저희는 동작을 제어할것입니다.\n\n\n\nWebsite 컨테이너\nwebsite:\n  image: containous/whoami\n  labels:\n    - traefik.http.routers.website.rule=Host('food.dogdog.cf')\n    - traefik.http.routers.website.tls=true\n    - traefik.http.routers.website.tls.certresolver=re\n    - traefik.http.routers.website.entrypoints=websecure\nwebsite 컨테이너의 image를 containous/whoami로 설정합니다. 또한 http 서비스 제공시 routers 규칙을 traefik.http.routers.website.()를 통해 설정 합니다.’food.dogdog.cf’로 요청이 들어오면 ‘website’컨테이너로 라우팅이 되도록 설정합니다. 또한 https가 가능하도록 ’tls=true’로 설정하며, ’tls/certresolver=re’ 앞서 설정한 re 값을 certresolver값으로 설정합니다. 해당 컨테이너의 entrypoints는 앞서 설정한 :443포트로 들어오는 요청인 ’websecure’로 설정합니다. traefik 동작의 전반적인 이해를 돕고자 이미지를 첨부합니다.\n\n\n- traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https\n- traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\n- traefik.http.routers.redirs.entrypoints=web\n- traefik.http.routers.redirs.middlewares=redirect-to-https\nhttp로 ’food.dogdog.cf’를 통해 들어오는 요청들에 대해서 https로 redirect가 가능하도록 도와주는 코드입니다.\n\n\n\n\nrules.yml 파일 작성\n앞서 작성한 docker-compose.yml 파일을 통해서 서비스를 제공하는 기본적인 컨테이너 구축은 완료했습니다. 이제는 특정 요청에 대해서는 외부 서버에 있는 서비스를 이용하도록 하는 rules.yml 파일을 작성하도록 하겠습니다. \n\nroute1: \n  entryPoints:\n    -websecure\n  rule: Host('food.dogdog.cf')&&PathPrefix('/toy')\n  service: reverse-proxy\n  tls: {}\nentrypoints가 websecure로 들어오는 요청(:443포트로 들어오는 요청)에 대해서 만약 ‘food.dogdog.cf/toy’ 요청이면 reverse-proxy라는 서비스로 넘기며 해당 서비스의 내용을 수행하며. tls(https 서비스)를 사용하겠다는 코드입니다. 해당 코드형식만 유지하면 변수명 및 요청명에 대해서는 수정하셔도 됩니다.\n\n\n\n실행결과\n\n\n\n\n\n\n결론\nTraefik을 활용한 reverse-proxy를 구현하는 방법에 대해 알아봤습니다. 실습을 진행하며 파악한 것과 같이 Traefik에서는 let’s Encrypt를 통해 자동으로 https업로드, 대시 보드 제공을 통한 router, service, Middleware 상태 확인등의 기능을 확인할 수 있었습니다. Traefik은 이번 실습에서 다룬 기능보다 많은 기능이 제공되는는 유용한 오픈소스입니다"
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html",
    "href": "posts/2019-02-06-sccs/index.html",
    "title": "Self-controlled case series",
    "section": "",
    "text": "김진섭 대표는 2월 18일(월) 성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#요약",
    "href": "posts/2019-02-06-sccs/index.html#요약",
    "title": "Self-controlled case series",
    "section": "요약",
    "text": "요약\n\nSelf-controlled methods는 자기 자신을 대조군으로 비교, time-invariant confounders의 영향을 최소화할 수 있다.\nSelf-controlled case series (SCCS), case-crossover (CCO) design, sequence symmetry analysis (SSA)가 대표적이다.\nSCCS는 위험에 노출된 기간과 그렇지 않은 기간의 상대위험도 (RR) 를 구한다.\n한 사람에게 일어나는 각 사건(ex: 노출, 발생, 나이)이 변화할 때 마다 데이터를 만든다 (Long format data).\nMatched case-control study와 유사, Conditional logistic regression으로도 분석할 수 있다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#slide",
    "href": "posts/2019-02-06-sccs/index.html#slide",
    "title": "Self-controlled case series",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureSCCS/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html",
    "href": "posts/2018-11-08-medianorratio/index.html",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#abstract",
    "href": "posts/2018-11-08-medianorratio/index.html#abstract",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Abstract",
    "text": "Abstract\nHeritability는 trait의 유전적인 측면을 정량적으로 설명하는 지표로 genetic study에서 흔히 쓰이는 지표이다. \\(Y\\)가 continuous variable일 때는 Intraclass Correlation Coeffcient(ICC) 혹은 Variance Partition Coefficients(VPC)의 형태로 heritability를 간단히 표시할 수 있으나, binary trait에는 이를 그대로 적용할 수 없다. 이에 liability threshold model 등의 다양한 approximation 방법이 이용되고 있으나 결과 해석의 어려움이 문제로 지적되고 있다. 한편 binary trait의 경우 sibling recurrence risk ratio(\\(\\lambda_s\\))가 유전적인 부분을 표현하는 직관적인 지표로 쓰이고 있으나 prevalence값이 필요하고 covariate 보정이 안되는 한계점이 있다. 이에 저자는 binary multilevel study에서 이용되는 Median Odds Ratio(MOR)을 이용하여 binary trait의 유전적인 정도를 평가하는 편하고 직관적인 지표인 Median OR Ratio(\\(\\text{MORR}_{SU}\\))을 제안하였으며 Healthy Twin Study, Korea의 hypertriglycemia trait에 적용하여 간단하며 직관적으로 유전적인 정도를 측정할 수 있었다. 이 지표가 binary trait에서 유전적인 정도를 표현하는 새로운 지표로서 heritability나 \\(\\lambda_s\\)를 보완할 수 있을 것으로 확신한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#introduction",
    "href": "posts/2018-11-08-medianorratio/index.html#introduction",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Introduction",
    "text": "Introduction\nHeritability는 continuous trait에서 polygenic effect가 설명할 수 있는 정도를 측정하는 지표로 전체 분산 중 polygenic effect가 설명하는 비율(the portion of phenotypic variance in a population attributable to additive genetic factors)로 정의되며, trait의 정규분포 가정이 깨지지 않는다면 mixed effect model 등을 이용하여 쉽게 구할 수 있다.(Manolio et al. 2009; Vattikuti 2012). Mixed effect model의 결과에서 polygenic effect의 분산과 residual의 분산이 추정되고 이를 통해 전체분산과 polygenic effect가 설명하는 분율을 계산하는 것인데 이는 Intraclass Correlation Coeffcient(ICC) 또는 Variance Partition Coefficients(VPC)의 형태이며, mixed effect model을 이용함으로서 covariate(age, sex, etc..)들을 보정할 수 있는 장점이 있다.\n그러나 binary trait일 때는 이 방법으로 heritability를 계산할 수가 없다. 보통 질병에 걸릴 확률 \\(p\\)가 Logistic distribution을 따른다고 가정하고 logistic regression model을 이용하게 되는데 이 때, Continuous variable에서 한 것처럼 추정된 variance of polygenic effect의 분산은 probability의 logit function의 scale에서 계산된 것이며, 전체분산은 probability scale에서 계산이 되기 때문에 이를 단순하게 분자, 분모로 하여 계산할 수 없다(Browne et al. 2005). binary trait에서 heritability를 구하는 방법으로는 first order Taylor series expansion을 이용하여 logit함수에 대한 회귀식을 \\(Y\\)에 대한 회귀식으로 approximation하여 linearisation 하는 방법, simulation을 이용하는 방법, latent variable을 이용하는 방법 등이 제시되어 있는데 이들 방법은 근본적으로 근사값의 계산이라는 한계점이 있으며, 해석을 어떻게 해야 되는지도 문제가 된다(Browne et al. 2005; Bonnet 2016; Vigre et al. 2004; Davies et al. 2015; Tenesa and Haley 2013).\nbinary trait에서 유전적인 정도를 표현하는 또다른 방법으로 recurrence risk ratio라는 것이 있는데, 이 중 흔히 이용되는 것은 sibling recurrence risk ratio(\\(\\lambda_s\\))로 일반 인구집단의 prevalence에 비해 affected people의 sibling 집단에서의 prevalence가 몇배나 더 높은지를 표현하며 직관적으로 이해할 수 있는 지표라는 장점이 있다(Rybicki and Elston 2000). 그러나 이 값은 일반 인구 집단에서의 prevalence를 정확히 측정해야 한다는 부담을 갖고 있으며 ascertainment bias에 민감하며, polygenic effect의 변화없이 common environmental factor의 영향에 의해서도 변화할 수 있다는 문제점이 있다(S.-W. Guo 1998, 2002; S.-W. Guo 2000).\n한편 binary trait의 multilevel analysis에서 group level의 효과를 쉽게 이해하기 위해 Median Odds Ratio(MOR)라는 개념이 제시되었는데, 이것의 정의는 “the median value of the odds ratio between the group at highest risk and the group at lowest risk when randomly picking out two groups”이다(Merlo et al. 2006). MOR은 group변수의 분산만을 이용하여 쉽게 계산될 수 있으며 OR scale이기 때문에 이해하기도 쉬운데, 1보다 클수록 group effect가 크고 1에 가까울수록 group effect가 없다고 해석할 수 있다(Larsen et al. 2000; Larsen and Merlo 2005; Merlo et al. 2006).\n이에 저자는 multilevel logistic regression에서 이용하였던 MOR의 개념을 이용하여 binary trait의 유전적인 정도를 측정하는 새로운 지표인 Median OR Ratio between Sibling Pairs and Unrelated Pairs(\\(\\text{MORR}_{SU}\\))을 소개할 것이다. 본 지표는 sibling pairs끼리의 OR에 비해 unrelated pairs끼리의 OR의 값이 대략적으로(median) 얼마나 높은가를 나타내며, 유전적인 부분이 전혀 없는 질병이라면 sibling pair에서나 unrelated pair에서의 OR이 같게 되어 \\(\\text{MORR}_{SU}\\)의 값은 1이 된다. 반면에 이 값이 크면 클수록 sibling pair끼리의 질병발생 양상이 unrelated pair끼리의 그것보다 비슷해지고 이는 유전적인 부분이 큰 질병임을 뜻한다. 본 지표는 hierarchial generalized linear model(HGLM)를 통하여 계산할 수 있으며, covariates를 보정할 수 있다는 heritability의 장점과 쉽게 해석되는 \\(\\lambda_s\\)의 장점을 동시에 갖고 있다. 반대로 말하면 직관적인 해석이 어려운 heritability의 단점과 prevalence 정보가 필요하며 covariate보정이 어려운 \\(\\lambda_s\\)의 단점을 보완하였다고도 할 수 있다. (Lee and Nelder 1996). 본 연구에서는 \\(\\text{MORR}_{SU}\\)의 개념을 소개한 후 실제 데이터인 Healthy twin study, Korea의 hypertriglycemia trait에 적용하여 유전적인 부분을 해석해 볼 것이다(SUNG et al. 2006).\nMethod\nBreif review of Meidan OR\n먼저 MOR에 대해 간략하게 요약하여 설명하겠다(Larsen et al. 2000).\n\\(Y_{ij}\\)를 \\(j\\)th group의 \\(i\\)th individual의 health status라 하고(case: 1, control: 0), \\(X_{ij}\\)를 vector of covariates, \\(G_j\\)를 \\(j\\)th group의 effect라 정의하자. 이 때 multilevel logistic regression의 formula를 mixed effect model로 표시하면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\beta_0+X_{ij}'\\beta_1+G_j\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_j\\): random intercept \\(G_j\\sim \\text{iid  } N(0,V_g)\\))\n이 때 Conditional Odds는\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\exp{(\\beta_0)}\\exp{(X_{ij}'\\beta_1)}\\exp{(G_j)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 X가 고정되어 있을 때 임의로 뽑은 \\(j\\)th Group과 \\(k\\)th Group의 Odds ratio는\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{ij}=1|X,G_j)]}{{\\text{Odds}[Pr(Y_{ik}=1|X,G_k)]}}=\\exp{(G_j-G_k)}\n\\end{aligned}\n\\]\n가 되고 Odds가 큰그룹을 Odds가 작은 그룹과 비교한다면 \\(\\text{OR}=\\exp{|G_j-G_k|}\\)이다. 이제 \\((G_j-G_k)\\sim N(0,2V_g)\\) 임을 이용하여 이것의 중앙값(median)을 계산하면\n\\[\n\\begin{aligned}\n\\text{MOR} = \\exp{(\\sqrt{2V_g}\\times \\Phi^{-1}{(0.75)})}\\simeq \\exp{(0.95\\sqrt{V_g})}\n\\end{aligned}\n\\]\n이 되고 이를 MOR로 정의한다. MOR은 VPC와 다르게 오직 group variable의 분산인 \\(V_g\\)만 가지고 계산될 수 있어 그 자체값과 95% 신뢰구간을 간단히 계산할 수 있으며, MOR\\(=1\\) 이라면 group variable의 effect가 없다고 해석할 수 있고 MOR이 커질수록 group variable의 effect가 크다고 해석할 수 있다.\nFormula’s of \\(\\text{MORR}_{SU}\\)\n\n이제 \\(\\text{MORR}_{SU}\\)의 수식을 유도하여 보자. \\(Y_{i}\\)를 \\(i\\)th individual의 health status라 하고(case: 1, control: 0, \\(1\\le i \\le n\\)), \\(X_{i}\\)를 vector of covariates, \\(G_i\\)를 \\(i\\)th individual의 polygenic effect라 정의하자. 이제 polygenic model을 수식으로 나타내면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{i}=1|X_{i},G_i)]=\\beta_0+X_{i}'\\beta_1+G_i\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_i\\): polygenic effect of \\(i\\)th individual)\n한편 \\(G_i\\)들의 vector를 \\(G=(G_1,G_2,\\cdots,G_n)'\\)이라 하면, \\(G\\sim N(0,V_p\\Sigma)\\) 이 된다(\\(\\Sigma\\): genetic relationship matrix, \\(V_p\\): variance of polygenic effect).\n이제 Conditional Odds를 계산해보면\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{i}=1|X_{i},G_i)]=\\exp{(\\beta_0)}\\exp{(X_{i}'\\beta_1)}\\exp{(G_i)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 임의로 뽑은 \\(i\\)th individual과 \\(j\\)th individual의 Odds ratio는 \\(X\\)가 같을 때 다음과 같이 표현된다.\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{i}=1|X,G_i)]}{{\\text{Odds}[Pr(Y_{j}=1|X,G_j)]}}=\\exp{(G_i-G_j)}\n\\end{aligned}\n\\]\n이제 두 가지 경우를 생각하자.\n\n\\(i\\)와 \\(j\\)를 unrelated individuals에서 뽑았을 경우이다. 이 때는 앞서 MOR과 마찬가지로 \\((G_i-G_j) \\sim N(0,2V_g)\\)가 된다.\n\\(i\\),\\(j\\)를 sibling pair에서 뽑았다면, 즉 \\(i\\)와 \\(j\\)가 항상 sibling이라면 \\(Cov(G_i,G_j)=\\frac{1}{2}V_g\\) 이므로, \\((G_i-G_j) \\sim N(0,V_g)\\)가 된다.\n\n이제 unrelated individual과 sibling을 한 쌍씩 뽑아 각각 \\(G_i, G_j\\)와 \\(G_k, G_l\\)이라고 하면 Odds Ratio의 비인 OR Ratio(ORR)를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n\\text{ORR}_{SU}=\\frac{\\text{OR}_{unrelated}}{\\text{OR}_{sibling}}=\\frac{\\exp{|G_i-G_j|}}{\\exp{|G_k-G_l|}} =  \\exp{(|G_i-G_j|-|G_k-G_l|)}\n\\end{aligned}\n\\]\n\\(|G_i-G_j|-|G_k-G_l|\\)는 \\(|N(0,2V_g)| - |N(0, V_g)|\\) 의 분포를 따르는 것을 이용하여 median값을 계산하면 약 \\(0.2453\\times \\sqrt{V_p}\\)이고 최종지표인 Median OR Ratio(MORR)은 아래와 같다(Appendix).\n\\[\n\\begin{aligned}\n\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\n\\end{aligned}\n\\]\n즉 sibling pair들에서의 polygenic effect의 OR과 unrelated에서의 polygenic effect의 그것을 비교한 지표이며 범위는 1부터 무한대까지이다. \\(\\text{MORR}_{SU}\\)가 1이면 unrelated pair에서의 질병발생 양상의 차이가 sibling pair에서의 그것과 같게 되어 유전적인 부분이 전혀 없다고 해석할 수 있다. 값이 커질수록 sibling pair끼리는 unrelated pair끼리보다 질병발생 양상이 비슷하다고 볼 수 있으며 이는 곧 유전적인 부분이 큰 것으로 이해할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Apply to Real Data",
    "text": "Apply to Real Data\nHealthy Twin Study, Korea의 데이터에 본 지표를 적용하였다(SUNG et al. 2006). 가족-쌍둥이 구조로 이루어진 3,461명의 사람 중 지질검사와 음주, 흡연 정보가 있는 2,729명을 대상으로 150이상을 case, 150미만을 control로 정의하였으며 아무것도 보정하지 않은 Null model(Model 1), 성별과 연령을 보정한 모형(Model 2), 그리고 성별, 연령, 음주, 흡연력을 보정한 모형(Model 3)에 대해서 각각의 \\(V_p\\)값과 그에 따른 \\(\\text{MORR}_{SU}\\)값을 제시하였다(Table 1). 분석은 R 3.5.1버전에서 hglm package를 이용하였다(Ronnegard, Shen, and Alam 2010).\n\n\n\nVariance parameter, heritablity and MORR of polygenic effect\n\n\n\n\n\n\n\n\nModel 1 (95%CI)\nModel 2 (95%CI)\nModel 3 (95%CI)\n\n\n\n\\(V_p\\)\n0.59 (0.49-0.71)\n0.68 (0.57-0.81)\n0.66 (0.55-0.79)\n\n\nHeritablity\n0.15 (0.13-0.18)\n0.17 (0.15-0.2)\n0.17 (0.14-0.19)\n\n\n\\(\\text{MORR}_{su}\\)\n1.16 (1.13-1.19)\n1.18 (1.15-1.22)\n1.18 (1.14-1.21)\n\n\n\n\n\n(Heritability: Intraclass correlation coefficients(\\(\\frac{V_p}{V_p+\\frac{\\pi^2}{3}}\\)), Model 1: no covariate, Model 2: age & sex as covariates, Model 3: age, sex and alcohol/smoking status as covariates)\nModel 2를 살펴보면 age와 sex의 effect를 보정하고 난 후의 \\(V_p\\)값은 0.59(95% CI: 0.49-0.71)이고 이에 해당하는 \\(\\text{MORR}_{su}\\)의 값은 1.18(95% CI: 1.15-1.22)였으며 이는 sibling pair의 OR에 비해 unrelated pair의 OR의 값이 대략적으로 18% 더 높다고 해석할 수 있다. 한편 Null model(Model 1)의 \\(\\text{MORR}_{su}\\)의 값은 1.16(95% CI: 1.13-1.19)이며 age, sex, smoling status를 보정했을 때(Model 3)는 1.18(95% CI: 1.14-1.21)로 세 Model의 결과는 비슷했다. 이는 age, sex, alcohol, smoking status 보정 여부에 크게 상관없이 hypertriglycemia에 일정한 유전적인 영향이 존재함을 의미한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "href": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Conclusion",
    "text": "Conclusion\n저자가 제시한 \\(\\text{MORR}_{su}\\)의 개념을 이용하여 binary trait의 유전적인 정도를 직관적으로 설명할 수 있었다. 이는 해석이 어려운 heritability의 단점을 극복하였다는 의미가 있다. 또한 age, sex 등 다양한 covariates의 효과를 보정한 후의 유전적인 부분을 설명할 수 있고, 인구집단의 prevalence를 측정할 필요가 없다는 점에서 \\(\\lambda_s\\)의 단점을 보완한 지표라 할 수 있다.\n향후 heritability와 \\(\\lambda_s\\) 각각의 장점은 살리고 단점은 보완한 이 지표가 binary trait의 polygenic effect를 직관적으로 설명하는 방법으로 널리 쓰이길 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#appendix",
    "href": "posts/2018-11-08-medianorratio/index.html#appendix",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Appendix",
    "text": "Appendix\nCalculate MORR\n\\(|N(0,2V_p)| - |N(0, V_p)|\\)의 median을 \\(M\\)이라 하자. R의 distr package를 이용하면 아래와 같이 \\(V_g\\)값들에 따른 \\(M\\)값을 쉽게 계산할 수 있다(Ruckdeschel and Kohl 2014).\n\n\n\n\n\\(V_p, \\sqrt{V_p}\\) vs Median value of \\(|N(0,2V_p)| - |N(0, V_p)|\\)\n\n\n\n이 그래프의 직선의 기울기가 바로 0.2453이고 따라서 \\(\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\\)가 된다.\nCompare to Other Measures\n\n\n\n\nRelationship between \\(h^2\\) and \\(\\text{MORR}_{SU}\\)"
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html",
    "href": "posts/2019-11-30-rseleniumtip/index.html",
    "title": "RSelenium 이용 팁",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 1월 Shinykorea 밋업에 참석, RSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "href": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "title": "RSelenium 이용 팁",
    "section": "요약",
    "text": "요약\n웹에 로그인 후 클릭기반 데이터 다운받는 과정을 RSelenium 으로 자동화 하였다.\n\nSelenium docker image 를 이용, 복잡한 설치과정 없이 Selenium 을 실행하고 다운로드 경로를 설정하였다.\nfindElement 와 sendKeysToElement, clickElement 를 이용, 아이디와 비번을 입력하고 로그인버튼을 클릭하였다.\nclickElement 이 안될 때 mouseMoveToLocation 과 click 을 이용, 마우스로 클릭하였다.\n작업 팝업창을 바꾸는 switchToWindow 가 안될 때, queryRD 로 자체 함수를 만들어 작업하였다.\n50개 일별 데이터 다운로드에 성공하였다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "href": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "title": "RSelenium 이용 팁",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RSelenium 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html",
    "href": "posts/2021-07-05-channel.io-install-review/index.html",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "",
    "text": "얼마 전 회사 웹사이트에 채널톡 서비스를 추가했습니다. 우리 회사는 Hugo를 이용해 홈페이지를 운영합니다. Hugo는 Jekyll과 비슷한 서비스로 Static Site Generator의 한 종류입니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#서론",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#서론",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "",
    "text": "얼마 전 회사 웹사이트에 채널톡 서비스를 추가했습니다. 우리 회사는 Hugo를 이용해 홈페이지를 운영합니다. Hugo는 Jekyll과 비슷한 서비스로 Static Site Generator의 한 종류입니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설치",
    "text": "설치\n우선 채널톡 서비스는 Javascript 코드를 추가하는 것만으로도 쉽게 설치할 수 있습니다. 회원가입을 하고 나면 페이지에 추가해야 할 Javascript 코드가 제공됩니다. 설정을 위해, 제공되는 Javascript 코드를 파일화 하여 홈페이지 프로젝트 폴더의 /public/js에 넣고 config.toml파일에 해당 Javascript 파일을 사용하겠다고 설정해주는 것으로 간단히 설치가 끝납니다.\n\n\n\nchannel.js는 제가 임의로 지정한 이름입니다. 원하시는 이름으로 바꾸셔도 무방합니다.\n\n\n설치 직후에는 두 번째 행이\n\ncustom_js = []\n\n와 같은 상태입니다. 원하는 Javascript 파일의 경로와 이름을 큰따옴표로 묶어 대괄호 속에 넣어주면 사이트에 해당 파일이 적용됩니다. CSS또한 동일한 방법으로 custom_css에 원하는 파일의 경로와 이름을 입력한 후 선택적으로 적용할 수 있습니다. 이후 Netlify를 통해 사이트를 빌드 후 Publish 하였습니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설정",
    "text": "설정\n \n처음 실행 후 먼저 회사의 로고와 설명을 추가하고, 채널톡 주소도 설정하였습니다.\nZarathu는 Medical Research Supporters입니다. 위와 같이 설정하고 나면\n\n위와 같이 고객님들께서 상담 버튼을 누르자마자 나오는 메시지들과 이미지가 변경됩니다.\n고객님들께서 상담을 시작하시면\n\n위에서 설정한 것과 같은 메시지가 표시됩니다. 이는 회사의 응대를 기다리는 동안 정보를 입력하게 하여 상담을 원활히 진행하게 하는 것에 큰 도움이 됩니다.\n템플릿 설정도 있어, 매크로를 통해 빠르게 응대할 수도 있었습니다. \n\n마지막으로 운영 시간을 설정하였습니다. 출근, 퇴근 시 따로 설정할 필요 없이 자동으로 온/오프라인 설정을 할 수도 있었고 수동으로 설정할 수도 있었습니다. 저희는 회사 특성상 수동으로 설정하였습니다.\n\n종합적인 테스트를 해 보았습니다. 웹으로 들어가 테스트라고 메시지를 남기니\n\n와 같은 정보를 물을 수 있습니다. 이후 담당자를 배정한 후 테스트 계정과 소통할 수 있었습니다."
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "이번 태스크의 목적은 바로 구글, 네이버, 다음 API를 모두 이용하여 검색결과를 DB화 시키는 작업입니다.\n사용자로 부터 query와 크롤링 원하는 검색 글 수를 인풋으로 입력받아서, API로 부터 제공받은 데이터를 가공하여 새로운 데이터프레임으로 만드는 작업을 수행합니다. 데이터 프레임의 컬럼은\n[Title] - 글 제목, [Link] - 글 링크 , [Description] - 글 미리보기 내용, [Search engine] - 검색결과를 제공한 포털 (naver,google,daumkakao), [search_date] -글을 검색한 날짜\n입니다.\n전체 JupyterNoteBook Source Code\nhttps://github.com/zarathucorp/blog/blob/master/source_code/Custom_Search_Zarathu.ipynb\n\n\n\n흔히 동적 크롤러로 Selenium을 많이 활용합니다. 하지만 이런 크롤링에는 기업의 자산인 데이터 자산을 침해한다는 문제점이 있습니다. 이와 관련하여 부동산 정보 플랫폼 직방의 데이터를 크롤링한 경위로 스타트업 방픽은 소송에서 얻은 데이터를 폐기하고, 직방에 2000만원을 지급하도록 2월 3일에 판결을 내려졌습니다. 따라서 홈페이지를 통해 정보가 공개됐다고 하더라도, 이런 타 사이트에 대한 데이터 베이스를 수집하는 행위는 위험하다고 생각합니다. 따라서 크롤링을 해야한다면 해도 되는지 확인을 하고 하거나, 개발자를 위한 도구로 제공된 API가 먼저 있는지 없는지 검토를 하는 것이 안전해 보입니다. (특히 얻은 데이터로 수익을 창출하는 경우는 더 더욱)\n\nhttps://www.joongang.co.kr/article/25139935#home\n\n\n\nAPI란 무엇인지 위키백과의 설명을 통하면, “API(application programming interface 애플리케이션 프로그래밍 인터페이스[*], 응용 프로그램 프로그래밍 인터페이스)는 컴퓨터나 컴퓨터 프로그램 사이의 연결이다. 일종의 소프트웨어 인터페이스이며 다른 종류의 소프트웨어에 서비스를 제공한다”라고 설명하고 있습니다. 즉 컴퓨터끼리 연결하고 소통하는 방식인데, 주로 우리가 API를 연결해서 쓸 때는 클라이언트와 서버를 연결하여 원하는 요청을 처리하는 것이 보통입니다. 오늘 이 글에서 사용할 API는 REST API라고 합니다. API와 구별되는 REST API의 특징으로는 API를 이용할 때 규칙이 정해져 있다는 특징이 있습니다.\nURL을 통해 소통할 방법을 서버와 클라이언트 사이에 주고 받는다. 그때 어떤 자원에 대해 어떤 행위를 요청받을지 URL에 명시를 해주는 것이 원칙입니다.\n\nGET : 리소스 생성\nPOST: 조회\nPUT : 수정\nDELETE: 삭제\n\n등이 있고, 서버는 요청에 대해 상태코드로 응답합니다\n상태코드에 따라 정상과 비정상적으로 처리했는지 response 개체를 받아옵니다.\n\n200: 정상\n400: 비정상"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#왜-크롤링을-하지-않았는지",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#왜-크롤링을-하지-않았는지",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "흔히 동적 크롤러로 Selenium을 많이 활용합니다. 하지만 이런 크롤링에는 기업의 자산인 데이터 자산을 침해한다는 문제점이 있습니다. 이와 관련하여 부동산 정보 플랫폼 직방의 데이터를 크롤링한 경위로 스타트업 방픽은 소송에서 얻은 데이터를 폐기하고, 직방에 2000만원을 지급하도록 2월 3일에 판결을 내려졌습니다. 따라서 홈페이지를 통해 정보가 공개됐다고 하더라도, 이런 타 사이트에 대한 데이터 베이스를 수집하는 행위는 위험하다고 생각합니다. 따라서 크롤링을 해야한다면 해도 되는지 확인을 하고 하거나, 개발자를 위한 도구로 제공된 API가 먼저 있는지 없는지 검토를 하는 것이 안전해 보입니다. (특히 얻은 데이터로 수익을 창출하는 경우는 더 더욱)\n\nhttps://www.joongang.co.kr/article/25139935#home"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#api란",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#api란",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "",
    "text": "API란 무엇인지 위키백과의 설명을 통하면, “API(application programming interface 애플리케이션 프로그래밍 인터페이스[*], 응용 프로그램 프로그래밍 인터페이스)는 컴퓨터나 컴퓨터 프로그램 사이의 연결이다. 일종의 소프트웨어 인터페이스이며 다른 종류의 소프트웨어에 서비스를 제공한다”라고 설명하고 있습니다. 즉 컴퓨터끼리 연결하고 소통하는 방식인데, 주로 우리가 API를 연결해서 쓸 때는 클라이언트와 서버를 연결하여 원하는 요청을 처리하는 것이 보통입니다. 오늘 이 글에서 사용할 API는 REST API라고 합니다. API와 구별되는 REST API의 특징으로는 API를 이용할 때 규칙이 정해져 있다는 특징이 있습니다.\nURL을 통해 소통할 방법을 서버와 클라이언트 사이에 주고 받는다. 그때 어떤 자원에 대해 어떤 행위를 요청받을지 URL에 명시를 해주는 것이 원칙입니다.\n\nGET : 리소스 생성\nPOST: 조회\nPUT : 수정\nDELETE: 삭제\n\n등이 있고, 서버는 요청에 대해 상태코드로 응답합니다\n상태코드에 따라 정상과 비정상적으로 처리했는지 response 개체를 받아옵니다.\n\n200: 정상\n400: 비정상"
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#구글-크롤링-코드-전체",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#구글-크롤링-코드-전체",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "2.1 구글 크롤링 코드 전체",
    "text": "2.1 구글 크롤링 코드 전체\n응답받은 response 이 코드에선 data로 변수명을 저장했습니다. 이 객체에 접근해서 우리의 목표에 맞게 데이터에 접근해서 한 줄 한 줄 데이터 프레임에 입력하여 저장하는 작업을 하겠습니다.\n\n2.1.1 url에 내용의 신뢰도가 낮은 사이트의 검색결과는 저장하지 않도록 예외처리를 합니다. (선택사항)\n\nTrash_Link = [\"tistory\", \"kin\", \"youtube\", \"blog\", \"book\", \"news\", \"dcinside\", \"fmkorea\", \"ruliweb\", \"theqoo\", \"clien\", \"mlbpark\", \"instiz\", \"todayhumor\"] \n\nurl 링크에, 티스토리, 지식인,유튜브, 블로그,책,뉴스,디시인사이드,에펨코리아,루리엡,더쿠,클리앙,엠엘비파크,인스티즈,오늘의유머의 검색결과는 제거하도록 했습니다.\n\ndef Google_API(query, wanted_row):\n    \"\"\"\n    input : \n        query : str  검색하고 싶은 검색어 \n        wanted_row : str 검색 결과를 몇 행 저장할 것인지 \n    output : \n        df_google : dataframe / column = title, link,description  \n        사용자로 부터 입력받은 쿼리문을 통해 나온 검색 결과를 wanted_row만큼 (100행을 입력받았으면) 100행이 저장된 데이터 프레임을 return합니다.\n    \"\"\"\n\n    query= query.replace(\"|\",\"OR\") #쿼리에서 입력받은 | 기호를 OR 로 바꿉니다 \n    query += \"-filetype:pdf\" # 검색식을 사용하여 file type이 pdf가 아닌 것을 제외시켰습니다 \n    start_pages=[] # start_pages 라는 리스트를 생성합니다. \n\n    df_google= pd.DataFrame(columns=['Title','Link','Description']) # df_Google이라는 데이터 프레임에 컬럼명은 Title, Link, Description으로 설정했습니다.\n\n    row_count =0 # dataframe에 정보가 입력되는 것을 카운트 하기 위해 만든 변수입니다. \n\n\n    for i in range(1,wanted_row+1000,10):\n        start_pages.append(i) #구글 api는 1페이지당 10개의 결과물을 보여줘서 1,11,21순으로 로드한 페이지를 리스트에 담았습니다. \n\n    for start_page in start_pages:\n      # 1페이지, 11페이지,21페이지 마다, \n        url = f\"https://www.googleapis.com/customsearch/v1?key={Google_API_KEY}&cx={Google_SEARCH_ENGINE_ID}&q={query}&start={start_page}\"\n        # 요청할 URL에 사용자 정보인 API key, CSE ID를 저장합니다. \n        data = requests.get(url).json()\n        # request를 requests 라이브러리를 통해서 요청하고, 결과를 json을 호출하여 데이터에 담습니다.\n        search_items = data.get(\"items\")\n        # data의 하위에 items키로 저장돼있는 값을 불러옵니다. \n        # search_items엔 검색결과 [1~ 10]개의 아이템들이 담겨있다.  start_page = 11 ~ [11~20] \n        \n        try:\n          #try 구문을 하는 이유: 검색 결과가 null인 경우 link를 가져올 수가 없어서 없으면 없는대로 예외처리\n            for i, search_item in enumerate(search_items, start=1):\n              # link 가져오기 \n                link = search_item.get(\"link\")\n                if any(trash in link for trash in Trash_Link):\n                  # 링크에 dcinside, News 등을 포함하고 있으면 데이터를 데이터프레임에 담지 않고, 다음 검색결과로 \n                    pass\n                else: \n                    # 제목저장\n                    title = search_item.get(\"title\")\n                    # 설명 저장 \n                    descripiton = search_item.get(\"snippet\")\n                    # df_google에 한줄한줄 append \n                    df_google.loc[start_page + i] = [title,link,descripiton] \n                    # 저장하면 행 갯수 카운트 \n                    row_count+=1\n                    if (row_count &gt;= wanted_row) or (row_count == 300) :\n                      #원하는 갯수만큼 저장끝나면\n                        return df_google\n        except:\n          # 더 이상 검색결과가 없으면 df_google 리턴 후 종료 \n            return df_google\n\n    \n    return df_google\n\nlink를 입력 받을 때 try except구문을 활용하여 예외처리를 하였습니다.이를 자세히 설명하게 되면, 특정 키워드로 검색했을 때 나오는 검색결과가 충분하지 않는 경우 response객체에 items 값이 null이 되게 됩니다. 따라서 이후의 작업들이 문제가 생기게 되는데, 이를 try except 구문을 통해 처리했습니다."
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#네이버-크롤링-코드",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#네이버-크롤링-코드",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "3.1 네이버 크롤링 코드",
    "text": "3.1 네이버 크롤링 코드\n\n# Naver_client_id = '~~~~' # 발급받은 ID를 입력해주세요 \n# Naver_client_secret = '~~~' \n\ndef Naver_API(query,wanted_row):\n    query = urllib.parse.quote(query)\n\n    display=100 \n    #네이버 검색 API는 한 페이지를 요청했을 때 몇개의 건수를 보여줄 것인지 인자로 표시할 수 있습니다. \n    start=1\n    # start page를 1로 설정합니다.\n    end=wanted_row+10000\n    # 끝내는 페이지를 원하는 행의 갯수보다 더 많이 설정했는데 이유는 , trashlink 보다 많은 데이터를 저장합니다.  \n    sort='sim'\n    # 네이버 API 검색 결과는 검색결과 데이터를 정렬하는 순서의 기준을 정합니다. \n\n    df= pd.DataFrame(columns=['Title','Link','Description'])\n    # 마찬가지로 title,link,description의 컬럼을 가진 데이터프레임을 생성합니다. \n    row_count= 0 \n    # dataframe에 정보가 입력되는 것을 카운트 하기 위해 만든 변수입니다. \n    \n    for start_index in range(start,end,display):\n        url = \"https://openapi.naver.com/v1/search/webkr?query=\"+ query +\\\n            \"&display=\" + str(display)+ \\\n            \"&start=\" + str(start_index) + \\\n            \"&sort=\" + sort\n\n        #url에 요청할 정보에 대한 내용을 담아 변수 선언하고, \n        request = urllib.request.Request(url)\n        #urllib.request 모듈을 통해 요청을만들고,  \n        request.add_header(\"X-Naver-Client-Id\",Naver_client_id)\n        request.add_header(\"X-Naver-Client-Secret\",Naver_client_secret)\n        # 그 요청에 헤더를 만들어서 클라이언트 아이디와, 비밀번호를 헤더에 입력합니다. \n        try:\n            response = urllib.request.urlopen(request)\n            # 요청하여 받은 내용을 response로 저장합니다.\n            rescode = response.getcode()\n            # response 객체에 담긴 응답 코드를 받아옵니다\n            if(rescode==200):\n                response_body = response.read()\n                # response 내용을 읽어들여 response_body에 저장합니다. \n                items= json.loads(response_body.decode('utf-8'))['items']\n                # 전체 response를 json화 한 뒤 key값이 items로 되어있는 값에 저장을 합니다. \n                remove_tag = re.compile('&lt;.*?&gt;')\n                # html문법의 태그들을 제거하는 컴파일러를 정규식을 패키지를 통해 생성합니다.\n                for item_index in range(0,len(items)):\n                    link = items[item_index]['link']\n                    # 아이템에 링크에 접근합니다\n                    if any(trash in link for trash in Trash_Link):\n                      # link url에 출처가 신뢰도가 낮은 사이트의 정보라면 데이터프레임에 저장하지 않고 넘어갑니다. \n                        pass\n                    else:\n                        title = re.sub(remove_tag, '', items[item_index]['title'])\n                        description = re.sub(remove_tag, '', items[item_index]['description'])\n                        # html 태그를 제거한 후, 제목 설명,링크 저장 \n                        df.loc[row_count] =[title,link,description]\n                        row_count+=1\n                        if (row_count &gt;= wanted_row) or (row_count == 300):\n                            return df\n                        \n        except:\n            return df\n\n큰 흐름은 구글에서 작성한 코드와 유사합니다.\n먼저 네이버 클라이언트 ID와 KEY값을 변수로 넣어줍니다. 경우에 따라 자기 혼자 쓰는 건 상관없겠지만 , 보통 키를 저렇게 코드에 보이게 노출하는 것 보다는 INPUT을 통해 보이지 않게 입력하는 것이 좋을 것 같습니다. 그리고 구글에서는 URL에 서비스 요청할 정보와 요청자의 신원을 확인할 수 있는 정보들을 모두 입력했습니다. 네이버에서는 조금 다르게 URL에 넣는 인자로는 어떤 검색결과를 받을지 표시하고, 몇 건이 출력될지 등 파라미터를 줘서 코드를 작성할 수 있습니다. 또 URL에 통해 사용자를 키값 정보를 같이 담지 않고 요청에 헤더를 추가하여 그곳에 담아 요청합니다.\n이후는 이전과 동일하게 네이버 웹문서 검색 API가 제공하는 JSON 데이터 형식을 보고 필요한 키-밸류값에 접근하여 데이터를 수집한 뒤 데이터프레임을 반환합니다.\n구글에서는 요청에 대한 응답(response) items에 데이터가 불용어와 html문법의 태그로 돼있는 것들이 나와있지 않았지만, 네이버에서 보낸 응답에는 불용어가 많이 섞여있어서 그것을 제거하기 위해 re 패키지의 정규식을 이용하여 글자만 남기도록 했습니다."
  },
  {
    "objectID": "posts/2023-02-15-searchAPI-with-python/index.html#다음-api-활용-파이썬-코드",
    "href": "posts/2023-02-15-searchAPI-with-python/index.html#다음-api-활용-파이썬-코드",
    "title": "Python을 이용한 검색포털 API 활용",
    "section": "4.1 다음 API 활용 파이썬 코드",
    "text": "4.1 다음 API 활용 파이썬 코드\n\n# Kakao_API_key = '~~~~' # 발급받은 api키를 입력해주세요 \n\ndef Daum_API(query,wanted_row):\n    pages= wanted_row//10 \n    # 검색해야할 페이지를 10으로 나눈 몫으로 구합니다 \n    # 예: 100 행 검색이면 10페이지\n\n    method = \"GET\"\n    url = \"https://dapi.kakao.com/v2/search/web\"\n    header = {'authorization': f'KakaoAK {Kakao_API_key}'}\n    # 다음 카카오 API를 호출할 때는, header 딕셔너리로 생성하여 정해진 형식으로  API키를 넘겨줘야합니다. 형식은 위와 같습니다. \n\n    df= pd.DataFrame(columns=['Title','Link','Description'])\n    #데이터프레임을 생성합니다 \n    row_count=0\n    # row_count 변수를 생성합니다\n\n    for page in range(1,pages+10):\n      #여유있게 10페이지더 검색합니다 이유는 Link가 버려지는 경우를 위해서입니다. \n        params = {'query' : query, 'page' : page}\n        # 다른 API 호출방식과 달리 url string에 담아서 넘겨주는 형식이 아니라, 딕셔너리형태로 \n        # params와 header에 담아서 리퀘스트를 요청합니다. \n        request = requests.get( url, params= params, headers=header)\n        #요청한 내용을 받은 것을 request에 저장합니다. 변수명 request이지만 사실은 response\n        for i, item in enumerate(request.json()[\"documents\"], start=1):\n          #아이템 객체에서 url을 받아옵니다.\n            link = item['url']\n            try:\n              # date time이 null인 경우가 많아서 예외처리를 해서 경우를 나눴습니다. 앞의 4글자 년도 YYYY를 저장합니다. \n                written_year=int(item['datetime'][:4])\n            except:\n              # 작성일자가 null인 경우, 2023년으로 저장합니다.  \n                written_year = 2023\n\n            if (any(trash in link for trash in Trash_Link) or (written_year &lt;2020)):\n              # 출처가 신뢰도가 낮은 사이트거나, 작성된지 오래된 글의 경우\n                pass\n            else:\n                title= item[\"title\"]\n                description = item[\"contents\"]\n                df.loc[10*page+i] =[title,link,description]\n                #title과  본문 설명을 담아서 데이터프레임에 한줄한줄 append합니다. \n                row_count+=1\n                if (row_count &gt;= wanted_row) or (row_count == 300):\n                  # 행수가 원하는 로우만큼 채워지거나, Maximum_row개수인 300에 도달하면\n                  # html태그들을 제거한 후 반환합니다.\n                    remove_tag = re.compile('&lt;.*?&gt;')\n                    df['Title'] =df['Title'].apply(lambda x :re.sub(remove_tag, '',x))\n                    df['Description'] =df['Description'].apply(lambda x :re.sub(remove_tag, '',x))\n\n                    return df\n\n                \n\n    remove_tag = re.compile('&lt;.*?&gt;')\n    df['Title'] =df['Title'].apply(lambda x :re.sub(remove_tag, '',x))\n    df['Description'] =df['Description'].apply(lambda x :re.sub(remove_tag, '',x))\n    \n    return df \n\n카카오 API는 요청을 하면 보내주는 response 중 우리가 원하는 내용을 구글과 네이버는 items에 내용들이 담겨있었습니다. 카카오 다음검색 api는 보낸 응답에 documents라는 키값에 저장돼있음을 문서를 통해 확인할 수 있습니다. 문서를 통해 확인할 수 있고, 보통 api를 제공하는 사이트에서 테스트해볼 수 있어 입력물과 결과물의 형식을 요청해보지 않아도 확인가능합니다. 조금 다른 점이라면 이번에는 카카오 api에서 담겨있는 내용 중 작성일자를 Document-datetime에 저장돼있어서 검색 기간을 설정할 수 있는 기능을 구현했습니다.\n이렇게 나온 결과물에서 한번 더 필터링하는 것도 있을 수 있지만, 애초에 검색식을 이용하여 검색결과로 반환해야할 데이터를 줄일 수 있는데, 기간검색은 검색식으로 제어가 되지 않았습니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html",
    "href": "posts/2019-04-03-reviewmoneypin/index.html",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "",
    "text": "법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "법인 설립 후 세무/회계 고민",
    "text": "법인 설립 후 세무/회계 고민\n법인을 설립하고 느낀 가장 큰 문제가 세무와 회계였습니다. 세무사에게 맡기자니 매출도 없는데 비용이 부담되었고, 평소 회계에 관심이 많아 이 기회에 회계를 배우고 싶었습니다. 그러던 중 인터넷 검색을 통해 머니핀과 자비스를 알게되어 둘다 이용하기 시작했습니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n머니핀으로 결정한 계기",
    "text": "머니핀으로 결정한 계기\n두 서비스 모두 국세청, 법인카드, 기업통장을 연계하여 자동으로 수입, 지출을 기록할 수 있습니다. 계정과목을 지정하면 바로 재무제표에 반영되어 확인할 수 있고, 이것을 부가세 등 세금을 낼 때 이용할 수 있습니다. 작년 말부터는 머니핀만 사용하기 시작했는데 이유는 다음과 같습니다.\n\n\n앱 하나로 다 됩니다. 회계관리, 부가세/법인세 납부까지 앱에서 해결할 수 있습니다(Figure @ref(fig:screen)).\n\n\n자비스는 영수증 등록만 앱으로 등록하고 나머지 업무들은 전부 웹에서 해야 합니다.\n\n\n\n\n\n\n\n앱 화면(출처: 머니핀 홈페이지)\n\n\n\n\n\n운영진 피드백이 매우 좋습니다. 회계를 처음 겪는 입장에서 큰 도움이 되었습니다(Figure @ref(fig:talk)).\n\n앱 내 메신저를 통해 운영진에게 질문을 자주 하는데, 항상 빠르고 친절하게 설명해 주십니다. 질문을 할 때마다 미안한 마음이 들 정도입니다.\n\n\n\n\n\n\n\n운영진 피드백\n\n\n\n\n\n쌉니다. 작년에는 무료였고 지금은 기본요금제가 월 9,900원입니다(Figure @ref(fig:bill)).\n\n아직 거래가 별로 없는 1인기업이 세무대행을 이용하는 것이 부담이었습니다.\n\n\n\n\n\n\n\n요금제"
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n차라투의 머니핀 이용법",
    "text": "차라투의 머니핀 이용법\n가장 많이 쓰는 기능은 국세청 세금계산서, 법인카드, 통장내역 자동 반영입니다. 거래가 일어난 후 앱 새로고침을 누르면 바로 거래내역이 반영되고, 어떤 회계항목에 해당되는지 메뉴에서 선택할 수 있습니다(Figure @ref(fig:content)). 어느 계정에 넣을지 헷갈릴때는 운영진께 질문하면 친절하게 답변해 주십니다.\n\n\n\n\n거래내역 고르기(출저: 머니핀)\n\n\n\n부득이하게 현금이나 개인카드를 이용한 경우 영수증을 카메라로 찍어 업로드하면 해당 내역이 앱에 반영됩니다. 저의 경우는 법인 설립 전 지출했거나 깜빡 잊고 개인카드를 이용한 내역을 반영하는 데 이용했습니다(Figure @ref(fig:bill2)).\n\n\n\n\n영수증 업로드\n\n\n\n부가세를 낼 때는 그동안의 내역을 바탕으로 바로 신고서와 전자파일을 만든 후, 국세청에 업로드하면 됩니다(유료, Figure @ref(fig:tax)). 지난 3월에는 법인세를 납부했는데 이 또한 머니핀을 통해 쉽게 마무리하였습니다(유료).\n\n\n\n\n18년 부가세 신고\n\n\n\n차라투는 아직 1인기업이라 직원급여와 관련된 기능은 이용하지 못했습니다. 올해 말에는 꼭 같이할 팀원이 생겨서 이 기능도 이용해보고 싶네요."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "아쉬운 점",
    "text": "아쉬운 점\n차라투의 회계는 100% 머니핀에 의존하다보니, 앱에서 작은 문제만 생겨도 큰 어려움을 겪습니다. 다음은 앱을 이용하면서 아쉬웠거나 개선이 필요한 내용입니다.\n\n\n앱 안정성 문제\n\n앱이 갑자기 멈추거나, 홈택스/법인카드 거래내역 반영이 안될 때가 있습니다. 운영진께 문의하면 바로 처리해 주십니다.\n\n\n\n버그, 오류 문제.\n\n앱 안정성보다 이것이 더 문제인데요, 같은 내역이 2번 반영되는 등 기본 숫자가 잘못되는 경우가 있습니다. 이번에 법인세를 내면서 잘못된 숫자들이 있다는 것을 알게 되었고, 운영진의 도움으로 무사히 수정하였습니다.\n\n\n\n회계 항목 추가\n\n일반적인 항목들은 다 있어 별 문제점은 없습니다만, 거래처 경조사같은 몇 가지 세부 항목이 추가되었으면 좋겠습니다. 현재 운영진께 건의드린 내용입니다.\n\n\n\n머니핀도 창업한지 얼마 안된 스타트업인 만큼, 서비스 운영을 하며 여러 시행착오를 겪는 것으로 느껴집니다. 저희가 겪는 시행착오랑 비슷한 경우도 많아 감정이입이 됩니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "마치며",
    "text": "마치며\n창업 후 7개월간 머니핀을 사용한 느낌을 적어 보았습니다. 간단히 3줄 요약하자면 다음과 같습니다.\n\n앱 하나로 모든 세무, 회계를 처리할 수 있다.\n싸다.\n앱이 아직 불안정하다. 그러나 운영진의 피드백이 매우 빠르다.\n\n이제 막 사업을 시작하셨거나, 1인 법인 등 작은 규모의 업체를 운영하는 분께 적극 추천합니다."
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html",
    "href": "posts/2022-03-25-graph/index.html",
    "title": "R로 논문용 그래프 그리기",
    "section": "",
    "text": "이번 시간에는 R을 이용해서 데이터와 통계 분석 결과를 한 눈에 전달할 수 있는 그래프를 만들 것이다. 예제 데이터로 지난 시간에 사용한 건강검진 데이터를 이용한다.\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/main/data/example_g1e.csv\")\nrmarkdown::paged_table(head(data))"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram",
    "href": "posts/2022-03-25-graph/index.html#histogram",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\n연속형 데이터를 히스토그램으로 나타내보자.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\")\n\n\n\n\n\n\n\nbreaks=n 옵션을 이용해서 계급구간의 수를 설정하고, freq=F 옵션을 이용하면 y축을 빈도수가 아닌 확률밀도로 표시할 수 있다. 그래프의 색도 간단하게 설정할 수 있다.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\",\n     breaks = 30, freq=F, col=\"grey\", border=\"white\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot",
    "href": "posts/2022-03-25-graph/index.html#barplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n히스토그램과 유사하지만, X축에 표현하고자 하는 변수가 이산형 변수일 때는 빈도수를 바 그래프로 나타낼 수 있다. table() 함수를 이용해 빈도표를 만들고, 바 그래프로 나타낸다.\n\ntable &lt;- table(data$Q_SMK_YN)\nprint(table)\n\n\n  1   2   3 \n995 256 391 \n\nbarplot(table, main=\"Distribution of smoking\", names.arg=c(\"Never\", \"Ex-smoker\", \"Current\"), ylab=\"frequency\")\n\n\n\n\n\n\n\n연도에 따른 흡연 여부의 분포를 하나의 그래프로 나타낼 수 있다. beside=T 옵션을 사용하면 적층형 그래프가 그룹형 그래프로 바뀐다.\n\ntable &lt;- table(data$Q_SMK_YN, data$EXMD_BZ_YYYY)\nprint(table)\n\n   \n    2009 2010 2011 2012 2013 2014 2015\n  1  125  132  140  146  141  157  154\n  2   34   42   35   36   35   38   36\n  3   53   62   48   52   67   59   50\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"), beside=T)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot",
    "href": "posts/2022-03-25-graph/index.html#boxplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n범주형 변수(흡연 여부, X축)에 따른 연속형 변수(수축기 혈압, Y축)의 분포를 나타내는 데는 박스 그래프를 이용할 수 있다.\n\nboxplot(BP_SYS ~ Q_SMK_YN, data = data, names=c(\"Never\", \"Ex-smoker\", \"Current\"), \n        main=\"SBP average by smoking\", ylab=\"SBP(mmHg)\", xlab=\"Smoking\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n두 연속형 변수 간의 관계는 산점도로 한 눈에 보여줄 수 있다. pch=n 옵션은 점의 모양, cex=n 옵션은 점의 크기를 지정한다.\n\nplot(HGHT ~ WGHT, data=data,\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\n\n\n\n\n\n\n\n범주형 변수에 따른 점의 분포를 표현하고자 할 때는 점의 색깔(col= 옵션)로 구분해서 표현할 수 있다. 2009년과 2015년에 실시한 검사에서 수검자의 신장, 체중 분포에 차이가 있는지 확인해보자.\n또, legend() 함수를 이용하면 범례에 사용될 옵션을 따로 설정할 수 있다.\n\ndata2 &lt;- data %&gt;% filter(EXMD_BZ_YYYY %in% c(2009, 2015))\nplot(HGHT ~ WGHT, data=data2, col=factor(EXMD_BZ_YYYY),\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\nlegend(x=\"bottomright\", legend=c(\"2009\", \"2015\"), col=1:2, pch = 19)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#line-plot",
    "href": "posts/2022-03-25-graph/index.html#line-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Line plot",
    "text": "Line plot\nplot() 함수에 type=“l” 옵션을 사용하면 선 그래프를 그릴 수 있다.summarize 함수를 이용해 연도에 따른 흡연자 비율(Q_SMK_YN=3)을 계산한 뒤, 선 그래프로 표현해보자.\n\ntable &lt;- data %&gt;% group_by(EXMD_BZ_YYYY) %&gt;% \n  summarize(smoker= mean(Q_SMK_YN==3, na.rm=T))\nprint(table)\n\n# A tibble: 7 × 2\n  EXMD_BZ_YYYY smoker\n         &lt;int&gt;  &lt;dbl&gt;\n1         2009  0.25 \n2         2010  0.263\n3         2011  0.215\n4         2012  0.222\n5         2013  0.276\n6         2014  0.232\n7         2015  0.208\n\nplot(table$EXMD_BZ_YYYY, table$smoker, type=\"l\",\n     xlab=\"Year\", ylab=\"prop of current smoker\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n앞에서 만들었던 산점도를 ggplot2를 이용해 다시 만들어보며 ggplot의 기본 문법을 이해해 보자.\nggplot 문법의 첫번째 요소는 시각화할 데이터, x축과 y축 변수, 기하학적 object의 모양, 색, 크기를 지정하고 변수의 스케일을 결정하는 aesthetic mapping이다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY)))\n\n\n\n\n\n\n\n위 코드를 통해 기본적인 그래프의 배경이 그려진다. 여기에 + 연산자를 이용해 기하학적 object를 추가한다. + 연산자는 magrittr에서의 %&gt;%와 같이 ggplot2 함수들을 연결해주는 역할을 한다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point()\n\n\n\n\n\n\n\nggtitle(), xlab(), ylab() 함수를 이용해 각각 그래프 제목, X축 라벨과 Y축 라벨을 추가할 수 있다. scale_color_manual() 함수를 이용하면 범례에 사용될 옵션들을 지정할 수 있다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point() +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\")\n\n\n\n\n\n\n\nggplot 문법의 장점은 + 연산자를 이용해서 기존 그래프 위에 새로운 요소를 추가해서 덧씌우는 것이 용이하다는 점이다.\n위의 산점도에 geom_smooth() 함수를 추가하면 그래프 위에 추세선을 덧씌울 수 있다. 이때, aes(col=) 옵션을 ggplot() 함수에서 제외하고 geom_point() 함수 내로 이동시키면, aes(col=) 옵션은 geom_point object에만 영향을 미치고 geom_smooth object는 영향을 받지 않게 된다. 마찬가지로 각 object 함수 내에 지정된 alpha(투명도), size, color 옵션은 해당 object에만 영향을 미친다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT)) +\n  geom_point(aes(col=factor(EXMD_BZ_YYYY)), alpha=0.5) +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\") +\n  geom_smooth(color=\"brown\", size=0.8)\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-1",
    "href": "posts/2022-03-25-graph/index.html#boxplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\nggplot의 문법을 이해하고 나면 이를 응용해서 다양한 그래프를 그릴 수 있다.\n앞서 만들었던 흡연 여부에 따른 수축기 혈압의 분포 그래프를 다시 만들어 보자.\n\ndata2 &lt;- data %&gt;% filter(!is.na(Q_SMK_YN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n여기에 하나의 변수를 더 추가해서, facet으로 구분된 그래프를 만들 수 있다.\n\ndata2 &lt;- data2 %&gt;% filter(!is.na(Q_PHX_DX_HTN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_wrap(~Q_PHX_DX_HTN, labeller=label_both)\n\n\n\n\n\n\n\nfacet_grid를 이용해 2X2 grid 형태로 그래프를 그릴 수 있다. 고혈압 과거력 변수에 더해, 당뇨 과거력 변수에 따라서도 구분해서 그래프가 나타난다. labeller 함수를 사용해 facet의 label도 원하는 대로 설정할 수 있다.\n\ndata2 &lt;- data2 %&gt;% filter(!is.na(Q_PHX_DX_DM))\n\nHTN.labs &lt;- c(\"No HTN\", \"HTN\")\nnames(HTN.labs) &lt;- c(\"0\", \"1\")\nDM.labs &lt;- c(\"No DM\", \"DM\")\nnames(DM.labs) &lt;- c(\"0\", \"1\")\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_grid(Q_PHX_DX_DM~Q_PHX_DX_HTN,\n             labeller = labeller(Q_PHX_DX_HTN = HTN.labs, Q_PHX_DX_DM = DM.labs))"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot-1",
    "href": "posts/2022-03-25-graph/index.html#barplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n앞서 만들었던 바 그래프도 ggplot 패키지를 이용하면 table 변환 과정 없이 바로 그릴 수 있다.\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN))) +\n  geom_bar(fill=\"grey\", color=\"black\") +\n  ggtitle(\"Distribution of smoking\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\n\n\n연도에 따른 흡연 여부의 분포도 table 변환 과정 없이 그릴 수 있다. geom_bar()의 position=‘fill’ 옵션을 설정하면 100% 누적 비율 바 그래프가 그려진다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"fill\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"proportion\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015)\n\n\n\n\n\n\n\n누적 비율이 아닌 count를 나타내고 싶다면 geom_bar()의 옵션을 position=‘stack’으로 변경한다. 적층형 그래프가 아닌 그룹형 그래프로 나타내고 싶다면 position=‘dodge’로 변경한다.\n그래프의 X축과 Y축 위치를 뒤집고 싶을 때는 coord_flip() 함수를 이용한다. X축과 Y축의 위치가 서로 바뀌는데, 축의 scale과 label을 다시 설정하지 않아도 되기 때문에 편리하다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"dodge\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"count\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015) +\n  coord_flip()"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram-1",
    "href": "posts/2022-03-25-graph/index.html#histogram-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\nggpubr 패키지의 기본 문법을 활용해서 히스토그램을 그려보자. 고혈압 병력이 있는 군과 없는 군 간에 체중 분포에 차이가 있을지 확인해본다.\n\ndata3 &lt;- data2 %&gt;% mutate(HTN = as.factor(ifelse(Q_PHX_DX_HTN==1, \"Yes\", \"No\")))\np &lt;- gghistogram(data=data3, x=\"WGHT\",\n                     color=\"HTN\", fill = \"HTN\", add=\"mean\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\nplot1 &lt;- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               xlab=\"Weight(kg)\",\n               legend.title=\"HTN Dx history\")\nprint(plot1)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-2",
    "href": "posts/2022-03-25-graph/index.html#boxplot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n같은 분포를 박스 그래프로도 나타낼 수 있다. 여기에 stat_compare_means() 함수를 활용하면, 고혈압 병력군 간 체중 평균에 통계적으로 유의한 차이가 있는지 확인할 수 있다.\n\np &lt;- ggboxplot(data=data3, x=\"HTN\", y=\"WGHT\", color=\"HTN\") +\n        stat_compare_means(method = \"t.test\", label.x.npc = \"middle\")\n\nplot2 &lt;- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               ylab=\"Weight(kg)\",\n               xlab=\"HTN Dx history\",\n               legend=\"none\")\n\nprint(plot2)\n\n\n\n\n\n\n\n세 개 이상의 범주로 나누어졌을 때도 마찬가지로 통계적 유의성을 검정할 수 있다. scale_x_discrete() 함수의 사용에서 확인할 수 있듯이, ggpubr 패키지는 ggplot의 문법을 기반으로 하고 있다.\n\nmy_comparisons &lt;- list(c(\"1\", \"2\"), c(\"2\", \"3\"), c(\"1\", \"3\"))\np &lt;- ggboxplot(data=data3, x=\"Q_SMK_YN\", y=\"WGHT\", color=\"Q_SMK_YN\") +\n        stat_compare_means(comparisons = my_comparisons) +\n        stat_compare_means(label.y = 150) +\n        scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\nplot3 &lt;- ggpar(p,\n               main=\"Weight distrubution by smoking\",\n               ylab=\"Weight(kg)\",\n               xlab=\"Smoking\",\n               legend=\"none\")\n\nprint(plot3)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n위에서 여러번 만들었던 신장과 체중의 산점도를 다시 그려보자. add = “reg.line” 옵션을 이용해 그래프 위에 추세선을 그린 뒤, stat_cor() 함수로 두 변수 간의 상관계수와 p value를 구할 수 있다.\n\np &lt;- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", \n               add = \"reg.line\", conf.int = TRUE,\n               add.params = list(color = \"navy\", fill = \"lightgray\")) +\n        stat_cor(method = \"pearson\")\n\nplot4 &lt;- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot4)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\n같은 산점도를 고혈압 병력에 따른 두 그룹으로 나누어서 다른 색으로 표현하고, 상관계수와 p-value를 따로 계산할 수도 있다.\n\np &lt;- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", color=\"HTN\", alpha=0.5,\n               add = \"reg.line\", conf.int = TRUE) +\n        stat_cor(aes(color = HTN))\n\nplot5 &lt;- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot5)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\nggarange 함수를 사용하면 여러 개의 그래프를 한 페이지에 배열해서 보여줄 수 있다.\n\nggarrange(plot2, plot3,\n          labels = c(\"A\", \"B\"),\n          ncol = 2, nrow = 1)"
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html",
    "href": "posts/2019-01-09-doctorskku2019/index.html",
    "title": "진료실 밖 의사로서의 경험",
    "section": "",
    "text": "김진섭 대표는 2월 1일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "href": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "title": "진료실 밖 의사로서의 경험",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 1학기 수시 합격.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "href": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "title": "진료실 밖 의사로서의 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/doctorskku2019 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html",
    "href": "posts/2019-08-25-shinymanager/index.html",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 9월 Shinykorea 밋업에 참석, Shiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#요약",
    "href": "posts/2019-08-25-shinymanager/index.html#요약",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "요약",
    "text": "요약\n\nshinymanager 로 UI 종류에 상관없이, 간단하게 로그인기능을 추가한다.\nSQLite db 를 이용, 접속자와 그 log를 관리한다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#slide",
    "href": "posts/2019-08-25-shinymanager/index.html#slide",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/shinymanager 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html",
    "href": "posts/2021-09-11-googlelogin/index.html",
    "title": "Google Login",
    "section": "",
    "text": "Django allauth와 google OAuth를 이용해 zarathu 앱에 구글 로그인을 구현합니다.\nDjango admin 페이지에서 회원별 접근 권한을 관리합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "href": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "title": "Google Login",
    "section": "allauth 설치",
    "text": "allauth 설치\npython Django에서는 쉽게 사용할 수 있는 로그인 모듈을 제공합니다. 로그인 모듈 사용을 위해 Django allauth를 설치합니다.\n\npip install Django  \npip install django-allauth \n\nDjango allauth 사용을 위해서 settings.py에 다음과 같이 추가합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "href": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "title": "Google Login",
    "section": "구글 클라이언트 발급",
    "text": "구글 클라이언트 발급\nGoogle cloud platform (https://console.cloud.google.com)에서 웹 애플리케이션의 클라이언트를 발급합니다.\n자세한 방법은 https://cloud.google.com/endpoints/docs/frameworks/java/creating-client-ids?hl=ko#web-client 를 참고하여 주시기 바랍니다.\n클라이언트를 발급하면 클라이언트 ID와 보안 비밀번호를 얻게 되는데,\n이 ID와 비밀번호를 장고 admin 페이지의 소셜 어플리케이션 탭에 입력합니다.\n장고 admin 페이지는 장고 사이트 주소 뒤에 /admin 을 입력하여 접근할 수 있습니다.\n\n이제 장고 사이트에서 구글 로그인 링크를 누르면 400 오류:redirect_uri_mismatch 메시지가 뜹니다.\nGoogle cloud platform의 웹 애플리케이션 클라이언트에서 ’승인된 리디렉션 URI’에 https://domain.name/accounts/google/login/callback/ 을 입력해 주면 오류를 해결할 수 있습니다.\n\n리디렉션 uri에는 반드시 도메인 네임을 포함하여야 합니다. IP주소만으로는 실행할 수 없습니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "href": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "title": "Google Login",
    "section": "접근 권한 관리",
    "text": "접근 권한 관리\n장고로 배포한 페이지에서 shinyproxy로 shiny app을 배포한 주소로 링크를 연결할 것입니다.\n저희의 목표는 회원분들 개인마다 접근할 수 있는 앱을 지정해 주는 것입니다.\n이는 장고 admin 페이지와 html 파일에서 쉽게 구현할 수 있습니다.\n먼저, 장고 admin에서 다음과 같이 개인 사용자에게 그룹을 지정해줍니다.\n\n그리고 다음과 같이 링크를 연결할 페이지에서 해당 유저가 링크를 보기 위한 그룹에 속해있는지 확인하는 코드를 입력합니다.\n제 코드에서는 main.html에 입력하였습니다.\nmain.html 파일의 if 다음의 class가 해당 그룹에 속한 유저들에게만 노출됩니다.\n\n{% if request.user | has_group:\"&lt;group&gt;\" %}\n\n\nhas_group 함수의 내용은 다음과 같습니다.\n\nhas_group 함수는 제 코드에서는 goologin/templatetags/user_tags에 정의되어 있고 main.html에 다음 코드를 입력하여 불러옵니다.\n\n{% load user_tags %}"
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "href": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "title": "Google Login",
    "section": "참고사항",
    "text": "참고사항\n포트 포워딩\n장고의 기본 포트가 8000이기 때문에 브라우저 도메인 뒤에 8000포트가 붙게 됩니다.\n다양한 해결 방법이 있지만, 여기서는 프로그램 iptables를 이용하여 포트 포워딩을 수행하는 방법을 소개합니다.\n다음 명령어를 이용하면 80포트로 들어온 신호를 8000포트로 리다이렉트할 수 있습니다.\n\niptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8000 \n\nshinyproxy 배포\n장고 사이트에서 링크로 연결할 샤이니 앱을 배포하기 위해 shinyproxy를 이용합니다.\n먼저, rstudio server에서 도커 컨테이너를 빌드합니다.\n\nsudo docker build -t &lt;container image&gt;\n\napplication.yml 파일의 specs: 에 컨테이너 이미지를 입력하고 shinyproxy를 구동합니다.\n\njava -jar shinyproxy-2.5.0.jar\n\napplication.yml 파일에서 port, authentication, template-path, landing-page를 건드리면 배포할 포트를 결정하거나, 사용자 인증을 만들거나, 템플릿으로 이용할 html 파일을 지정하거나, 기본 페이지를 설정할 수 있습니다.\nshinyproxy에 관한 자세한 내용은 https://shinyproxy.io 를 참고하여 주시기 바랍니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html",
    "href": "posts/2022-08-26-shinyforpython/index.html",
    "title": "Shiny for Python",
    "section": "",
    "text": "이제 파이썬에서도 Dash 이외에도 shiny를 사용하여 반응형 웹앱을 쉽게 구현할 수 있게 되었습니다.\n아직 초기 단계라 많은 것이 구현되지는 않았고 Alpha단계이므로 API가 변경되거나 아래의 설명과 다른 점이 생길 수 있습니다. 글의 내용은 Shiny for Python를 참고했습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "title": "Shiny for Python",
    "section": "\n2.1 python과 Shiny 설치하기",
    "text": "2.1 python과 Shiny 설치하기\n우선 Shiny app을 저장할 디렉토리를 만듭니다.\n\n# make new directory for Shiny App\nmkdir myapp\n\n# change directory\ncd myapp\n\npython은 Python.org Anaconda 에서 설치할 수 있습니다.\npython이 설치되었다면 사용하는 디렉토리의 폴더에 python 가상 환경을 만들고 이를 활성화합니다.\n\n# Create a virtual environment in the .venv subdirectory\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\nshiny 를 설치합니다.\n\n# install shiny\npip install shiny"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "title": "Shiny for Python",
    "section": "\n2.2 Shiny Server에서 이용하기",
    "text": "2.2 Shiny Server에서 이용하기\nShiny for python은 Shinyapps.io, Shiny server, shinylive와 같은 다양한 방법으로 배포될 수 있습니다. 그중 shiny server에 배포하는 방법을 소개합니다.\nShiny Server는 v1.5.19 이상이 필요합니다. 만약 이전 버전을 사용하고 있다면 업데이트 합니다.\n\n#shiny-server v1.4.19\nwget https://download3.rstudio.org/ubuntu-18.04/x86_64/shiny-server-1.5.19.995-amd64.deb\ngdebi shiny-server-1.5.19.995-amd64.deb\n\nshiny server에서 python 파일을 실행하기 위해 config file을 수정해야 합니다.\nconfig file 은 /etc/shiny-server/shiny-server.conf 에 위치해있습니다.\n\n# Edit the file /etc/shiny-server/shiny-server.conf\nsudo vim /etc/shiny-server/shiny-server.conf\n\nconfig file 을 열어 코드 상단에 python 경로를 추가합니다.\n예를 들어 /home/js/myapp/venv/bin/python3를 사용하고 싶다면 코드는 아래와 같습니다.\n\n# Use system python3 to run Shiny apps\npython /home/js/myapp/venv/bin/python3;\n\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /srv/shiny-server;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n  }\n}"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "title": "Shiny for Python",
    "section": "\n3.1 Shiny App",
    "text": "3.1 Shiny App\n\n\npython\nR\n\n\n\n\n# import shiny\nfrom shiny import ui, render, App\n\n# ui\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"N\", 0, 100, 40),\n    ui.output_text_verbatim(\"txt\"),\n)\n\n# server\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"n*2 is {input.n() * 2}\"\n\n# This is a shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\n\n\n\nlibrary(shiny)\n\n# ui\nui = fluidPage(\n    sliderInput(\"n\", \"N\", 0, 100, 40),\n    textOutput(\"txt\")\n  )\n\nserver = function(input, output,session) {\n     output$txt = renderText(\"n*2 is\",input$n * 2,\"}\")\n  }\n\nshinyApp(ui, server)\n\n\n\n\n위는 ui에서 입력 받은 값을 server에서 2를 곱해주어 계산하고 이를 ui부분에서 출력하여 보여주는 예시입니다.\n\n먼저 from shiny import *를 통해 필요한 shiny 모듈을 불러옵니다.\nui의 ui.input_slider() 함수를 통해 입력값을 받습니다. 이처럼 인풋에 해당하는 부분은 ui.input_*() 함수를 통해 만들 수 있습니다. “n”은 해당 input의 이름에 해당하는 부분이며, N은 label, 0,100,40은 각각 min, max,value에 해당하는 인자입니다.\n입력된 인풋값을 서버에 전송하면 이를 토대로 계산하여 값을 return하고 @render.text 라는 decorator를 통해 텍스트 형태로 렌더링합니다.\n다시 ui 부분에서는 ui.output_text_verbatim() 함수를 통해 텍스트를 출력합니다. 이처럼 ui부분에서 출력할 때는 ui.output_* 함수가 사용됩니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "title": "Shiny for Python",
    "section": "\n3.2 Shiny for Python의 문법",
    "text": "3.2 Shiny for Python의 문법\n위의 예시를 이용하여 R과 Python의 문법을 간단하게 비교해보면 다음과 같습니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\n\nimport shiny\nfrom shiny import *\nlibrary(shiny)\n\n\nUI\napp_ui = ui.page_fluid(\nui = fluidPage(\n\n\ninput\n  ui.input_slider(“n”,“N”,0,100,40),   ui.output_text_verbatim(“txt”)   )\n  sliderInput(“n”,“N”,0,100,40)   textOutput(“txt”))\n\n\nServer\ndef server(in, out, session):\nserver=function(in,out,session){\n\n\ndecorator\n\n@output @render.text\n\n\n\noutput\n  def txt:     return input.x()\n output$txt = renderText (input $ x)   }\n\n\napp\napp = App(app_ui, server)\nshinyApp(ui, server)\n\n\n\n인풋에 해당하는 부분은 ui.input_*(), ui 부분에서 출력할 때는 ui.output_*()함수들을 사용합니다.\n서버 부분에서 def server(input, output, session): def txt(): 와 같이 파이썬의 함수 문법을 사용합니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\ndecorater\n\n@render.text … @reactive.event() @reactive.Calc() …@output\nrenderText() …  reactive({}) output$id\n\n\n예시의 server부분에서 @render.text @output이 사용된 것을 볼 수 있습니다. 이것은 함수를 인자로 받아 함수를 출력하는 decorator 라는 함수입니다. 보통 python 에서 여러 함수들이 부분적으로 중복될때 코드의 재사용을 용이하게 하기 위해 사용됩니다. 여기서는 그냥 함수의 일종이라고 생각하면 될 것 같습니다.\nR에서는 렌더링을 위해 renderPlot, renderText 같은 함수를 사용하지만 Python에서는 @render.text 같은 decorator를 사용합니다. 위의 예시에서는 txt() 라는 텍스트를 반환하는 함수에 @render.text라는 decorator가 적용됩니다. 이는 텍스트를 반환하는 함수를 리턴하는 함수로 텍스트를 렌더링해 주는 함수라고 생각할 수 있습니다.\ndecorator는 output, module, reactivity, rendering 등 많은 부분에서 사용됩니다. 하나의 함수에 여러개가 적용될 수 있으며 @render.text 는 @output 보다 먼저 적용되어야 하는 것과 같이 순서나 parameter가 정해져 있습니다.\n\n\n\npython\nR\n\n\nHTML\nui.div(), ui.a()\ntags$div, tags$a\n\n\nHTML 태그의 경우는 ui.tags.*() 를 통해서 사용할 수 있습니다. 예를 들어 li 태그의 경우 ui.tags.li()로 사용 가능합니다.\n일반적으로 많이 사용되는 div, span, a 같은 태그들은 ui.div()와 같이 ui 서브모듈에서 직접 사용할 수 있습니다.\n\n\n\npython\nR\n\n\nmutability\nobjects can be modified\nobjects cannot be modified\n\n\npython으로 shiny앱을 작성할때 가변성 처리(Handling mutability)도 고려해야합니다. Python에서 문자,숫자열,튜플 같은 간단한 객체들은 변경할 수 없지만 딕셔너리,리스트 같은 대부분의 객체들은 수정할 수 있습니다.\n이로 인해 반응형 프로그래밍에서 문제가 발생할 수 있습니다. 즉, 프로그램의 한 부분에서 객체를 수정하면 프로그램의 다른 부분과 값이 다른 문제가 발생할 수 있습니다.\n이러한 문제를 해결하기 위한 몇가지 방법들이 있습니다.\n첫번째는 두 값이 객체를 copy해서 동일한 객체를 먼저 가리키는 것을 피하는 것입니다. a = [1,2], b = a.copy와 같이 사용하게 되면 a의 값이 바뀌어도 b는 바뀌기 이전의 값을 가리키게 됩니다. 두번째는 객체를 변경하는 연산자나 매서드를 사용하는 것입니다. 예를 들어 a = [1,2]를 a = [1,2,3]로 만들고 싶을 때 a = a+[3]보다는 a.append(3)을 사용하는것이 바람직합니다. 마지막은 변경 불가능한 객체를 사용하는 것입니다. 리스트 대신 튜플을 사용하거나 pyrsistent 패키지의 리스트나 딕셔너리를 사용할 수 있습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "href": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "title": "Shiny for Python",
    "section": "\n4.1 shinylive 배포",
    "text": "4.1 shinylive 배포\nshinylive editor을 통해 간단하게 shinylive를 통해 배포해볼 수 있습니다 .\n이외에도 Netlify, GITHUB gist를 통해서도 가능하며 Sharing Shinylive applications를 참고할 수 있습니다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html",
    "href": "posts/2020-01-25-pgconference2020/index.html",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "",
    "text": "김진섭 대표는 2월 15일(토) 디시인사이드 가 후원하는 프로그래밍 갤러리 컨퍼런스 2020에 참석, R과 shiny로 웹 애플리케이션을 만든 경험을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#요약",
    "href": "posts/2020-01-25-pgconference2020/index.html#요약",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\n\n모임 소개\n\nshinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#slide",
    "href": "posts/2020-01-25-pgconference2020/index.html#slide",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/pgconference2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "",
    "text": "김진섭 대표는 2월 27일(수) Anpanman이 후원하는 Shinykorea 밋업에 참석, ShinyApps를 Rstudio Addins을 포함한 패키지로 만들어 CRAN에 배포신청한 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "요약",
    "text": "요약\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록\n\nShinyApps을 Rstudio Addins으로 만든 후, 이를 패키지로 만들어 github에 배포하였다.\ntestthat, covr로 코드 테스트를 수행하고 결과 리포트를 만들었으며, pkgdown으로 패키지를 소개하는 웹사이트를 만들었다.\nTravis CI와 appveyor로 2의 과정과 여러 운영체제에서의 테스트를 자동화하였다.\n최종적으로 CRAN에 패키지를 배포 신청했으나 거절되었다. 코멘트를 반영하여 재심사 중이다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Package",
    "text": "Package\nhttps://github.com/jinseob2kim/jsmodule"
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html",
    "href": "posts/2018-11-08-mdlm/index.html",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "",
    "text": "본 연구는 김진섭 대표가 박사학위 논문으로 계획했던 연구로, 결과적으로 학술지 게재와 심사통과에 실패했다는 것을 미리 알려드립니다. 계산법은 R package 로 만들었습니다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#abstract",
    "href": "posts/2018-11-08-mdlm/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Abstract",
    "text": "Abstract\n선형모형을 적용하기 어려운 \\(J,U\\)-shape같은 curved linear relationship을 비선형모형으로 분석하면 선형모형에 비해 해석이 어려워진다. 이에 본 연구에서는 선형관계의 컨셉은 유지하면서 \\(J,U\\)-shape 같은 curved linear relationship을 표현할 수 있는 모형을 제안한다. 이것은 선형모형의 무대를 1차원에서 휘어진 다차원 공간으로 확장함으로서 가능하며, curved linear relationship을 다차원공간에서의 선형관계로 재해석할 수 있다. 시뮬레이션 결과 선형관계는 기존의 선형모형과 동등한 성능으로 추정하면서 더 우수한 성능으로 curved relationship를 추정할 수 있었고, 실제 \\(U\\)-shape을 보이는 관계를 다차원공간에서의 선형관계로 쉽게 설명할 수 있었으며 \\(U\\)-shape의 cut-off값도 쉽게 계산할 수 있었다. 선형모형을 완벽히 포함하여 확장한 본 연구의 제안이 건강연구의 새로운 표준으로 자리잡을 수 있으리라 자신한다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#introduction",
    "href": "posts/2018-11-08-mdlm/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Introduction",
    "text": "Introduction\nMultivariable Linear Model은 분석결과의 해석이 간단하면서도 여러 독립변수들을 동시에 고려할 수 있는 장점이 있어 Health Science에서 널리 이용된다(Schneider, Hommel, and Blettner 2010). 그러나 모든 관계가 선형관계인 것은 아니며 흔한 non-linear relationship으로 \\(J,U\\)-shape같은 curved linear relationship이 있다(Calabrese and Baldwin 2001; Power, Rodgers, and Hope 1998; de Wit et al. 2009; Knutson and Turek 2006). 이런 관계를 단순히 선형모형으로 분석하게 되면 간단하긴 하나 정확한 추정을 할 수 없으며 exponential, Log나 제곱, 루트를 이용해 변수를 치환하여 선형모형을 이용할 수 있다(Jagodzinski and Weede 1981). 그러나 치환으로 선형관계를 만들 수 있는 경우는 극히 일부분에 지나지 않아 많은 경우에 비선형모형(non-linear model)을 활용하는데, 대표적인 방법으로는 독립변수의 고차항을 모형에 추가하거나(Polynomial Model) 비모수적인 방법으로 곡선을 추정하는 Additive Model, 그리고 Multi-layer를 이용한 neural network이 있다(Jagodzinski and Weede 1981; Buja, Hastie, and Tibshirani 1989; Hornik, Stinchcombe, and White 1989). 그러나 이런 비선형모형들은 휘어진 모양을 해석하기 때문에 직선으로 해석하는 선형모형에 비해 해석이 복잡할 수 밖에 없다.\n이와 비슷한 문제가 20세기 초 물리학에서도 있었는데 태양 주위에서 빛이 휘는 문제가 바로 그것이다. 이 현상은 뉴턴의 물리학으로 설명되지 않았었는데, 아인슈타인(Albert Einstein)은 빛이 휘는 것이 아니라 태양 근처의 4차원 시공간(spacetime)이 휘어진 것이라는 발상의 전환을 통해 이 문제를 설명하였다(Coles 2001). 이것이 유명한 일반상대성이론으로 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 의미한다(Verlinde 2011).\n이에 저자는 아인슈타인의 아이디어와 비슷하게 선형모형의 무대를 휘어진 다차원 공간으로 확장함으로서 \\(J,U\\)-shape을 선형관계로 해석할 수 있는 Multi-dimensional Linear Model(MDLM)을 제안한다. 이것은 기존의 선형모형에 차원(dimension)의 개념을 추가하여 일반화한 것으로 모든 독립변수(independent variable)들이 같은 dimension의 정보라면 기존의 Linear Model과 일치한다. 먼저 개념을 수식으로 정리한 후 계수들을 추정하는 방법을 설명할 것이며 다양한 시나리오를 시뮬레이션하여 MDLM의 유용성을 살펴보겠다. 마지막으로 실제 \\(U\\)-shape을 갖는 데이터에 본 모형을 적용하여 유용성을 평가할 것이다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#formula",
    "href": "posts/2018-11-08-mdlm/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Formula",
    "text": "Formula\n음이 아닌 실수 \\(Y\\)를 종속변수로 실수 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\)들을 독립변수라 하자.\n2 independent variables, 2 dimensions\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 선형관계를 2차원 벡터공간에서 표현하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\n\\end{aligned}\n\\] \\(\\boldsymbol{\\vec{g}}_i\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며 Figure @ref(fig:fig1)에 그림으로 표현되어 있다.\n\n\n\n\nMDLM with 2 variables, 2 dimensions\n\n\n\n이것은 방향의 개념을 제외하면 기존의 선형모형과 같으며, 만일 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)이 같은 방향이라면 아래와 같이 기존 선형모형과 일치하게 된다.\n\\[\n\\begin{aligned}\nY &= (\\beta_{01} + \\beta_1X_1) + (\\beta_{02} + \\beta_2X_2) \\\\\n  &= \\beta_{0} + \\beta_1X_1 + \\beta_2X_2\n\\end{aligned}\n\\] (\\(\\beta_0 = \\beta_{01}+\\beta_{02}\\))\n해석은 기존의 선형모형과 같이 변화량을 이용하며 \\(Y\\)와 \\(X_1\\), \\(X_2\\)의 변화량에 대해서 식을 재구성하면 아래와 같다.\n\\[\n\\begin{aligned}\nd\\boldsymbol{\\vec{Y}} &= \\beta_1dX_1\\boldsymbol{\\vec{g}}_1 +\\beta_2dX_2\\boldsymbol{\\vec{g}}_2\n= \\beta_1d\\boldsymbol{\\vec{X}_1} + \\beta_2d\\boldsymbol{\\vec{X}_2}\n\\end{aligned}\n\\] 즉, \\(X_2\\)가 고정되어 있을 때 \\(Y\\)는 \\(X_1\\)의 방향으로 \\(\\beta_1\\)만큼 증가한다고 할 수 있으며, \\(X_1\\)이 고정되어 있다면 \\(Y\\)는 \\(X_2\\)의 방향으로 \\(\\beta_2\\)만큼 증가한다고 볼 수 있다.\n벡터로 표현된 위 식을 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)의 내적값인 \\(g_{12}\\)를 이용해 스칼라로 표현하면 아래와 같다.\n\\[Y^2 = (\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)\\]\n만약 \\(g_{12}=0\\) 즉, \\(X_1, X_2\\)가 독립된 차원을 갖는다면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1=-\\frac{\\beta_{01}}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보이며 \\(X_2\\)와 \\(Y\\)의 관계도 마찬가지이다. 일반적으로 \\(Y^2 = (\\beta_{01} + \\beta_1X_1 + g_{12}(\\beta_{02} + \\beta_2X_2))^2+ (1-g_{12}^2)(\\beta_{02} + \\beta_2X_2)^2\\)로 식을 변형하면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1 = -\\frac{\\beta_{01}+g_{12}(\\beta_{02} + \\beta_2X_2)}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보임을 확인할 수 있다.\n\n\\(p\\) independent variable, 2 dimensions\n일반적으로 독립변수가 \\(p\\)개인 경우 \\(X_1, \\cdot, X_l\\)이 같은 차원, \\(X_{l+1}, \\cdot, X_p\\)가 같은 차원에 있다고 가정하면 다음과 같이 벡터식과 스칼라식을 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p )\\boldsymbol{\\vec{g}}_2 \\\\\\\\\nY^2 &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)^2 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)^2 \\\\\n+ & 2g_{12}(\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)(\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)\n\\end{aligned}\n\\]\n\n\\(p\\) independent variable, \\(p\\) dimensions\n마지막으로 독립변수들이 전부 다른 방향을 갖고 있다고 가정한다면 아래와 같은 벡터식과 스칼라식을 얻는다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{2}X_2 )\\boldsymbol{\\vec{g}}_2 + \\cdots (\\beta_{0p} + \\beta_pX_p)\\boldsymbol{\\vec{g}}_p \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\\\\\nY^2 &= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\cdot\n\\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)^2} + 2\\sum_{i &lt; j}{g_{ij}(\\beta_{0i} + \\beta_iX_i)(\\beta_{0j} + \\beta_jX_j)}\n\\end{aligned}\n\\]\n여기서 \\(g_{ij}\\)는 \\(X_i\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_i\\)와 \\(X_j\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_j\\)의 내적값으로 두 벡터의 dependency를 나타낸다. 위의 경우와 마찬가지로 모든 \\(g_{i}\\)들의 방향이 같다면 아래와 같이 기존의 선형모형과 같은 관계를 얻는다.\n\\[Y= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\] (단, (\\(\\beta_0 = \\beta_{01}+\\beta_{02}+\\cdots + \\beta_{0p}\\)) )"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#estimation",
    "href": "posts/2018-11-08-mdlm/index.html#estimation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Estimation",
    "text": "Estimation\n\\(\\beta = (\\beta_1, \\beta_2, \\cdots, \\beta_p, \\beta_{01}, \\beta_{02}, \\cdots, \\beta_{0p})\\) estimation을 위해 최소화해야 할 cost function은 아래와 같이 오차제곱의 합(Sum of Squared Error: SSE)으로 하면 자연스럽게 기존 선형모형의 최소제곱 추정을 일반화 할 수 있다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i&lt;j}g_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n위 식에서 \\(g_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\n\\(\\beta\\) Estimation\n기존 선형모형과 달리 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 직접 계산하기 어려워, optimization technique을 이용하며 Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다 (Nelder and Mead 1965; Fletcher 1964; Byrd et al. 1995). 위의 방법들을 활용하여 우리는 초기 \\(\\beta\\)값들부터 시작해서 반복적인 계산을 통해 수렴값을 얻게 된다. 한편 초기값이 바뀌면 \\(SSE(\\beta)\\)의 값은 같더라도 \\(\\beta\\)들의 부호가 다른 결과를 얻을 가능성이 있는데 이는 \\(SSE(\\beta)\\)가 \\(\\beta\\)들의 2차식으로만 구성되어 있기 때문이며, \\(\\beta\\)들의 부호가 바뀌더라도 \\(SSE(\\beta)\\)가 같다면 해석은 동일하다.\n추정된 \\(\\beta\\)값들의 standard error들은 \\(SSE(\\beta)\\)의 hessian matrix(\\(H\\))로 부터 구할 수 있다. 어떤 함수 \\(f(\\theta)\\)의 hessian matrix란 \\(f(\\theta)\\)를 2번 미분한 성분들로 이루어진 행렬인데, 일반적으로 \\(\\theta\\)의 variance와 반비례함이 알려져 있다(Dovi, Paladino, and Reverberi 1991). 여기에서는 1변수 함수의 예를 통해 직관적으로 이해해보도록 하자. \\(f(\\theta)\\)가 \\(\\theta_0\\)에서 최소값을 갖을 때 \\(f\\)를 1번 미분한 값은 0임을 이용, \\(f\\)를 \\(\\theta_0\\) 부근에서 2차항까지만 테일러 급수전개를 하면\n\\[\n\\begin{aligned}\nSSE(\\hat{\\theta} + d\\theta) &= SSE(\\hat{\\theta}) + H \\cdot\\dfrac{(d\\theta)^2}{2}\n\\end{aligned}\n\\]\n이고 \\(d\\theta\\)에 대해 정리하면\n\\[(d\\theta)^2 = 2\\cdot H^{-1}\\cdot (SSE(\\hat{\\theta}+d\\theta)-SSE(\\hat{\\theta}))\\]\n이 된다. 즉 hessian이 커질수록 \\(\\theta\\)의 분산에 해당하는 \\((d\\theta)^2\\) 이 감소함을 알 수 있다.\n일반적으로 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\hat{\\beta}\\)들의 variance-covariance matrix는 아래와 같이 표현되며, 대각성분에 루트를 취하면 \\(\\beta\\)들의 standard error 값이 되어 p-value와 confidence interval(CI)을 계산할 수 있다(Dovi, Paladino, and Reverberi 1991).\n\\[\\text{vcov}(\\hat{\\beta}) = 2\\cdot H^{-1}\\cdot MSE(\\hat{\\beta})\\]\n(\\(MSE\\): Mean Squared Error)\nEstimation of \\(g_{ij}\\)\n\n위에 설명한 추정은 \\(g_{ij}\\)가 고정되었을 때를 가정한 것인데, \\(g_{ij}\\)를 데이터에서 직접 구할 수도 있으며 이것은 Generalized Estimating Equation(GEE)에서 working correlation matrix를 직접 계산할 수 있는 것과 마찬가지이다(Pan and Connett 2002). 이 때는 \\(g_{ij}\\)들은 \\(\\beta\\)들과 달리 -1에서 1까지의 값을 갖는다는 제한조건이 있어 constrained optimization technique를 이용해야 하며 나머지는 앞서와 동일하다(Rios and Sahinidis 2013)."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#simulation",
    "href": "posts/2018-11-08-mdlm/index.html#simulation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Simulation",
    "text": "Simulation\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 여러가지 관계에 대한 Simulation을 이용해 MDLM의 유용성을 살펴볼 것이며 구체적으로 다음의 5개 모형을 비교하겠다.\n\nLinear Model (LM)\nMDLM with fixed \\(g_{12}=0\\) (MDLM 1)\nMDLM with non-fixed \\(g_{12}\\) (MDLM 2)\nQuadratic Model: Polynomial model with 2nd degree (Quadratic)\nGeneralized Additive Model (GAM)\n\n모형비교는 Root Mean Square Error(RMSE)와 Akaike Information Criterion(AIC)을 이용하였으며 GAM의 경우는 effective degree of freedom(edf)를 AIC 계산에 활용하였다(Wood 2001).\n편의상 \\(X_1\\)과 \\(X_2\\)는 각각 1부터 10까지의 자연수를 갖는 것으로 가정하였으며 따라서 샘플수는 100이다. 모든 계산은 R 3.5.1의 optim, constrOptim 함수를 이용하였다.\nScenario 1: \\(Y = X_1 + X_2\\)\n\n\\(Y \\sim N(X_1 + X_2, 1)\\) 로 샘플링해 데이터를 생성하였으며 100회의 시뮬레이션을 수행해 RMSE와 AIC를 비교하였다(Table 1).\n\n\n\nResults: \\(Y = X_1 + X_2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1 ± 0\n1.2 ± 0\n1 ± 0\n1 ± 0\n0.9 ± 0\n\n\nDF\n4\n5\n6\n6\n7.2 ± 0.7\n\n\nAIC\n290 ± 2.8\n335.1 ± 4.9\n291.2 ± 5\n293.1 ± 3.3\n282.3 ± 7.9\n\n\n\n\n\n비교 결과 선형모형이 적은 parameter로 효율적인 추정을 하고 있음을 알 수 있었으며 \\(g_{12}\\)를 고정하지 않은 MDLM(2)가 선형모형과 비슷한 성능을 보이는데 이는 MDLM이 선형모형을 포함한 개념임을 생각했을 때 자연스러운 결과이다.\nScenario 2: \\(Y^2 = X_1^2 + X_2^2\\)\n\n이번엔 \\(Y \\sim N(\\sqrt{X_1^2 + X_2^2}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 2).\n\n\n\nResults: \\(Y^2 = X_1^2 + X_2^2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.2 ± 0.1\n1 ± 0\n1 ± 0\n1.1 ± 0.1\n1.1 ± 0.1\n\n\nDF\n4\n5\n6\n6\n5.1 ± 0.1\n\n\nAIC\n321.3 ± 9.7\n292.6 ± 7.1\n293.2 ± 6.9\n318.2 ± 11.9\n317.7 ± 11.9\n\n\n\n\n\n이번엔 MDLM(1)이 선형모형보다 확실히 우수한 추정값을 보이는것을 확인할 수 있으며, \\(g_{12}\\)를 고정하지 않은 MDLM(2) 또한 MDLM(1)과 비슷한 성능을 보이는 것을 확인할 수 있다.\nScenario 3: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n마지막으로 Scenario2를 일반화한 경우를 시뮬레이션하였다. \\(\\beta\\)들은 -5~5, \\(g_{12}\\)는 -1~1의 값을 임의로 선택하여 \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)를 만족하는 \\(Y\\)를 샘플링하였다. 즉, \\(Y \\sim N(\\sqrt{(\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 3).\n\n\n\nResults: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.1 ± 0\n1.3 ± 0.4\n1 ± 0.1\n1.1 ± 0.1\n1.1 ± 0.1\n\n\nDF\n4\n5\n6\n6\n4.9 ± 1.6\n\n\nAIC\n311.6 ± 1.2\n347.2 ± 58.6\n294.7 ± 28.3\n306.8 ± 12.7\n304.8 ± 10.5\n\n\n\n\n\n시뮬레이션 결과 \\(g_{12}\\)를 추정할 수 있는 MDLM(2)가 다른 모형들보다 압도적으로 우수한 성능을 보이는 것을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Apply to Real data",
    "text": "Apply to Real data\n이번에는 실제 \\(U\\)-shape을 보이는 데이터를 MDLM으로 분석해보겠다. http://biostat.mc.vanderbilt.edu/dupontwd/wddtext/data/3.25.2.SUPPORT.csv의 데이터에는 응급실 내원 당시 평균 동맥압(mean arterial pressure, MAP)과 재실기간(length of stay,LOS)의 정보가 있는데 MAP와 LOS의 natural logarithm값의 관계가 \\(U\\)-shape이다. 이제 log(LOS)와 MAP의 관계를 아래와 같이 모델링 하였다.\n\\[\\boldsymbol{\\vec{\\text{log(LOS)}}} = \\beta_{00}\\boldsymbol{\\vec{g_{1}}} + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})\\boldsymbol{\\vec{g_{2}}}\\]\n즉 log(LOS)를 intercept와 MAP의 독립된 2차원으로 바라보는 관점으로 스칼라로 표현한 모형은 아래와 같다.\n\\[(\\text{log(LOS)})^2 = \\beta_{00}^2 + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})^2\\]\nMDLM을 이용하여 Intercept와 MAP의 2차원공간으로 log(LOS)를 \\(\\text{log(LOS)}^2 = 2.3669^2 + (-2.4276 + 0.0295\\cdot\\text{MAP})^2\\)의 모형으로 추정할 수 있었고 AIC값은 \\(2413\\)였다. 이것은 선형모형으로 추정한 \\(\\text{log(LOS)} = 2.2624 + 0.0027 \\cdot \\text{MAP}\\)(AIC \\(2434\\)), quadratic항을 추가한 \\(\\text{log(LOS)} = 3.3742 -0.0246 \\cdot \\text{MAP} + 2\\times 10^{-4} \\cdot \\text{MAP}^2\\)(AIC \\(2414\\))보다 좋은 추정 결과이다(Figure @ref(fig:fig2)). 또한 U shape의 cutoff값을 \\(\\dfrac{2.4276}{0.0295} = 82.29\\)로 간단히 계산할 수 있었다.\n\n\n\n\nRelation between mean arterial pressure(mmHg) and length of stay day(log scale)"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#discussion",
    "href": "posts/2018-11-08-mdlm/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Discussion",
    "text": "Discussion\nMDLM을 이용해서 기존의 선형모형을 완벽히 포괄하면서 선형모형의 개념을 휘어진 다차원 공간으로 확장할 수 있었고, 이를 통해 \\(J,U\\)-shape같은 curved linear relationship을 잘 추정하고 cut-off값도 쉽게 확인할 수 있는 장점을 확인할 수 있었다. 기존 선형모형의 틀은 유지하면서 그것만이 진실은 아닐 수 있다는 것을 보여줬다는 점, 이를 통해 선형관계가 아닌 것을 선형관계로 해석할 수 있는 방법을 제시하였다는 점에서 큰 의미가 있다. 향후 연구자들이 평면에서의 선형관계에 국한되지 않고 가설을 검증할 수 있을 것이며, 이를 토대로 Health science 연구에서 MDLM이 기존 선형모형을 포괄하는 새로운 표준으로 자리잡을 수 있으리라 예상한다.\nMDLM의 추정식은 제곱근이 포함되어 있어 \\(Y\\)가 음수값을 갖고 있는 경우에는 적용하기 어렵다는 한계가 있다. 그러나 Health Science 분야에서 음수값을 갖는 지표는 별로 많지 않아 큰 문제는 아닐 것으로 생각하며, 변수변환을 통해 (+)로만 이루어진 새로운 변수를 만들어 해결할 수도 있다. 이 문제는 물리학자 Paul Dirac이 특수상대성이론을 고려한 양자역학의 방정식을 만들 때 겪었던 문제와 비슷한데, 그는 방정식의 계수가 꼭 숫자일 필요가 없고 행렬일 수도 있다는 기발한 아이디어로 이를 해결했다(Dirac 1928). 예를 들어 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 }\\) 일 때, \\(\\beta_0, \\beta_1, \\beta_2\\)가 숫자일 필요가 없다는 것이다. \\(\\beta\\)들을 행렬로 간주한다면 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 } = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\) 꼴이 되어 제곱근을 없애는 것이 가능하다. 이 행렬들은 최소 \\(4\\times 4\\) 이상의 정방행렬이어야 함이 알려져 있으며, 대표적인 예로 Cliford Algebra를 만족하는 Dirac matrices 있는데 \\(\\beta\\)들의 예를 하나 들면 아래와 같다(Traubenberg 2009).\n\\[\n\\beta_0 = \\alpha_0 \\times \\begin{pmatrix}\n  1 & 0 &  0 &  0 \\\\\n  0 & 1 &  0 &  0 \\\\\n  0 & 0 &  -1 &  0 \\\\\n  0 & 0 &  0 & -1\n\\end{pmatrix}\n\\]\n\\[\n\\beta_1 = \\alpha_1 \\times \\begin{pmatrix}\n   0 &  0 & 0 & 1 \\\\\n   0 &  0 & 1 & 0 \\\\\n   0 & -1 & 0 & 0 \\\\\n  -1 &  0 & 0 & 0\n\\end{pmatrix}\n\\]\n\\[\n\\beta_2 = \\alpha_2 \\times \\begin{pmatrix}\n   0 & 0 & 1 &  0 \\\\\n   0 & 0 & 0 & -1 \\\\\n  -1 & 0 & 0 &  0 \\\\\n   0 & 1 & 0 &  0\n\\end{pmatrix}\n\\]\n(\\(\\alpha_0, \\alpha_1, \\alpha_2\\): 실수)\n회귀계수가 숫자가 아닌 행렬이 가능하다는 이 아이디어가 향후 본 연구의 제곱근 문제를 극복하는 열쇠가 될 수 있을 것이라 생각한다.\n\\(SSE(\\beta)\\)를 최소화하는\\(\\beta\\)를 구하기 위해 optimization tequnique을 활용한 것도 문제가 될 수 있는데, 얻은 \\(SSE(\\beta)\\)값이 진짜 최소값(global minimum)인지 보장할 수 없기 때문이다. 이를 local minima problem이라 한다. 그러나 machine learning의 유행과 더불어 optimization technique도 빠르게 발전되고 있어 조만간 이 문제가 해결되리라 예상한다. 게다가 최근 연구에서 high-dimensional space인 경우 local minima problem은 매우 희귀한 것으로 나타났는데, 모든 차원에서 local minima일 가능성은 매우 낮기 때문으로 여겨진다(Dauphin et al. 2014).\nIntroduction에서 언급했듯이 아인슈타인(Albert Einstein)은 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 설명하였는데, 수식으로 살펴보면 3차원 공간에서 기술된 뉴턴의 중력장 방정식 \\(\\nabla^2\\Phi = 4 \\pi G\\rho_0\\)를 휘어진 4차원에서의 방정식 \\(\\boldsymbol{R}_{uv}-\\dfrac{1}{2}\\boldsymbol{g}_{uv} = \\dfrac{8\\pi G}{c^4}\\boldsymbol{T}_{uv}\\)로 확장한 것이다(Verlinde 2011). 본 연구의 MDLM을 통해 선형공간의 무대를 다차원으로 확장하여 \\(U\\)-shape같은 curved linear relationship을 선형관계로 바라볼 수 있게 되었다는 점에서 물리학에서의 아인슈타인 방정식과 비슷한 의미를 가진다고 감히 주장해 본다. 실제로 \\(\\beta_i^2\\) 를 \\(g_{ii}\\), \\(g_{ij}\\beta_i\\beta_j\\)를 합쳐서 \\(g_{ij}\\)라 놓으면 \\(g_{ij}\\)는 아인슈타인 중력장 방정식을 표현하는데 쓰이는 계량텐서(metric tensor) \\(g_{uv}\\)와 같은 의미를 갖게 되는데, \\((dY)^2\\)를 표현하는 식은 \\(\\sum_{i,j}g_{ij}(dX_i)(dX_j)\\)로 휘어진 시공간에서 두 지점 사이의 거리를 나타내는 방법과 정확히 일치한다. 뉴턴의 방정식으로도 일상적인 운동을 잘 설명할 수 있으나 우주 공간같은 거시적인 스케일에서는 아인슈타인 방정식이 필요해지는데, 이와 마찬가지로 다차원 공간에서 기술된 본 연구의 MDLM이 population level에서 기존 선형모형보다 더 정확히 건강관련 현상을 설명할 수 있으리라 예상한다.\n한편 일반상대성이론은 원자 이하의 미시세계의 현상을 잘 설명하지 못한다는 문제점이 있으며,불확정성의 원리(uncertainty principle)와 슈뢰딩거 방정식(Schrödinger equation)으로 대표되는 양자역학(quantum mechanics)의 논리가 이곳을 지배한다. 슈뢰딩거 방정식은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용으로 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특이한 점이다. 복소수는 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현하게 되고 놀라운 정확도로 미시세계의 현상을 설명할 수 있다. 이것은 Health science에도 중요한 시사점이 될 수 있는데, Health science에서 가장 큰 문제점 중 하나가 population level의 연구결과가 개인의 건강상태를 잘 설명하지 못한다는 것이다. 상태를 확률로 기술한다는 점에서는 베이지안 접근법(bayesian approach)이 양자역학의 접근과 비슷하지만 복소수를 활용할 수 없다는 점에서 차이가 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 개인의 건강상태를 설명하는 새로운 방법이 될 것이라 과감히 추측해 본다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html",
    "href": "posts/2023-09-18-shinyexe/index.html",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "",
    "text": "이번 글에서는 R Shiny 앱을 별도의 설치나 외부 연결 없이 폐쇄 환경에서도 실행할 수 있는 (Standalone) exe 파일로 패키징하는 과정을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#개요",
    "href": "posts/2023-09-18-shinyexe/index.html#개요",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "",
    "text": "이번 글에서는 R Shiny 앱을 별도의 설치나 외부 연결 없이 폐쇄 환경에서도 실행할 수 있는 (Standalone) exe 파일로 패키징하는 과정을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#standalone-app",
    "href": "posts/2023-09-18-shinyexe/index.html#standalone-app",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Standalone App",
    "text": "Standalone App\n먼저 글의 맥락을 더 효과적으로 전달하기 위해 Standalone App에 대해 간단히 정의해보겠습니다.\nAn app that can run independently without any external help.\n즉, 외부에 의존하지 않고 독립적으로 실행 가능한 앱으로 정의하고 싶은데요.\n여기서 외부에는 보통 Shiny를 실행하기 위해 쓰이는 웹 브라우저(크롬)가 포함될 수 있습니다.\n한편, R과 Rstudio 또한 Shiny를 실행하기 위해서 (로컬에서) 필요한 외부로 볼 수 있습니다.\n이러한 외부의 도움을 받지 않고, 다운로드 이후 압축만 풀어서 바로 실행할 수 있는 것을 Standalone App이라고 하며, 이러한 예시에는 (아는 사람은 아는 ㅎㅎ) 피카츄 배구.exe가 있습니다.\n\n\n\n출처: https://gbworld.tistory.com/1362\n이후 내용에서 소개되는 Shiny를 standalone app으로 만드는 것은 일반적인 R Shiny 개발과는 상당히 다르고 동시에 복잡합니다.\n그렇다면 Standalone app으로 만드는 것은 Shiny를 사용자에게 제공하는 다른 방법들과 어떤 차이점이 있을까요?\n\n\n\n이에는 여러가지를 생각해 볼 수 있지만 크게 2가지의 차이점이 있습니다.\n\n사용자 경험\n\nelectron으로 만들어진 Standalone App은 Shiny를 제공하는 서버와의 네트워크 연결이 불필요합니다. Shiny를 사용하기 위해 브라우저를 열고 특정 URL에 접속하는 대신 설치된 프로그램을 실행하는 것으로 충분합니다.\n이로 인해 네트워크와의 데이터를 주고 받는 과정에 리소스가 쓰이지 않고, 사용자의 (로컬 PC) 자원을 활용하기 때문에 살짝 더 좋은 퍼포먼스를 보일 수 있습니다.\n\n폐쇄성 환경\n\n또한 네트워크가 연결되지 않는다는 점은 shiny에 입력하는 값이 사용자의 PC 외부로 나가지 않고, 동시에 외부의 리소스가 PC에 들어오지 않는다는 이야기이기도 합니다.\n그렇기 때문에 금융이나 병원등 망분리 / 폐쇄 되어 있는 개발 환경에서도 Shiny를 실행할 수 있고 더 뛰어난 보안성을 가지게 됩니다. (단, Shiny가 계산을 위해 외부의 API 같은 자원을 사용하려면 네트워크 연결이 필요합니다)"
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#electron",
    "href": "posts/2023-09-18-shinyexe/index.html#electron",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Electron",
    "text": "Electron\nElectron (정확히는 electron.js) 은 크로미움 (크롬)과 node.js를 활용하여 html과 css, js 같은 웹 개발 결과물 (shiny가 이에 포함됩니다) 을 임베디드 형태로 만들 수 있는 프레임워크입니다.\n\n\n\nStandalone App을 만들기 위해서 electron의 기술적인 원리를 이해할 필요는 없지만, 흐름을 표현하면 아래와 같습니다.\n\n\n\n이 기술을 활용하여 shiny로 standalone app을 만들려는 시도는 꽤 오래 전부터 있었고, 2020년에 공유된 Turn a shiny application into a tablet or desktop app 아티클도 있지만, 관련된 자료들이 2022년을 마지막으로 아카이브되어 업데이트 되지 않았기 때문에 최신의 내용을 반영한 업데이트가 필요했습니다.\n차라투에서는 연구를 통해 기존의 내용 중 일부를 최적화하고, 최근 내용들을 반영한 뒤, Windows와 M1 Mac 2개의 OS에서 Standalone App을 개발하여 분리 환경에서 Shiny를 사용해야 하는 (공공기관을 포함한) 고객에게 제공하였고, 이후 개발에 활용할 수 있는 템플릿과 가이드를 제공하고 있습니다.\n국내에는 윈도우 사용자가 더 많기 때문에, 이번 글에서는 윈도우를 기준으로 방법을 소개합니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#개발-준비",
    "href": "posts/2023-09-18-shinyexe/index.html#개발-준비",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "개발 준비",
    "text": "개발 준비\nStandalone shiny app을 개발하기 위해 shiny 개발에 필요한 R과 Rstudio 외에 추가 설치가 필요합니다.\n\n1. node.js 설치\n글이 작성되는 23년 9월을 기준으로, LTS인 18.17.1 버전을 설치합니다.\n\n\n\n이제 Rstudio를 관리자 권한으로 실행합니다. (아이콘을 오른쪽 클릭 후 선택)\n정상적으로 설치가 되었다면 Rstudio의 터미널에서 node -v, npm -v를 실행하여 설치 버전을 확인할 수 있습니다.\n\n\n2. electron-forge 설치\nelectron-forge는 electron을 조금 더 쉽게 사용할 수 있게 하는 패키지라고 생각하셔도 좋습니다.\n이는 npm을 사용해 (R의 install.packages와 유사) 설치할 수 있으며, 마찬가지로 Rstudio의 터미널에서 아래의 명령어를 입력하여 설치합니다.\nnpm i -g @electron-forge/cli\n\n\n3. 템플릿 포크 / 클론\n차라투 github에서 제공하는 템플릿을 자신의 계정에 포크 후, 클론하여 로컬 PC에 다운로드 받습니다.\nhttps://github.com/zarathucorp/shiny-electron-template-windows-2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. R project 열기\n템플릿 폴더의 shiny-elecgtron-template-windows-2023.Rproj를 Rstudio에서 실행합니다.\n이제 Rstudio 터미널의 작업 디렉토리가 해당 프로젝트의 위치로 변경됩니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#electron-app-만들기",
    "href": "posts/2023-09-18-shinyexe/index.html#electron-app-만들기",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Electron App 만들기",
    "text": "Electron App 만들기\n\n1. electron app 템플릿을 설치\nRstudio의 터미널에서 npx create-electron-app myApp을 실행하여 템플릿을 설치합니다.\n이때 myApp이 Standalone App의 이름이 되며 app을 제외한 다른 이름으로 변경할 수 있습니다.\n정상적으로 실행되었다면 디렉토리에 myApp 폴더가 새롭게 생기는 것을 확인할 수 있습니다.\n\n\n\n\n\n2. github 템플릿의 파일을 myApp으로 이동\nelectron app 에서 기본으로 제공하는 템플릿은 shiny를 개발하기에는 약간 다른 내용들이 있어서 차라투 github에 제공된 파일로 교체합니다.\n이때 만들어진 myApp으로 이동해야 하는 파일은 아래의 5개입니다.\n\nshiny (폴더)\nsrc (폴더)\nadd-cran-binary-pkgs.R\nget-r-win.sh\nstart-shiny.R\n\n\n\n\n이후 Rstudio의 터미널에서 cd myApp으로 디렉토리를 이동합니다.\n\n\n3. Standalone R 설치\nelectron 에 포함 시킬 local R을 현재 프로젝트에 설치합니다.\n단, 이때 기존에 사용중인 R의 버전과 동일한 버전을 설치해야 하며, 23년 9월에 최신 버전인 4.3.1을 기준으로 사용합니다.\nlocal R은 Rstudio의 터미널에서 sh ./get-r-win.sh를 실행하는 것으로 설치할 수 있습니다.\n정상적으로 실행되었다면 Done. 메세지와 함께 폴더에 r-win 이라는 폴더가 새롭게 만들어 진 것을 확인할 수 있습니다. r-win 내부의 구조는 아래와 같습니다.\n\n\n\n\n\n4. Shiny 패키지 설치\n예시에서 사용하는 shiny는 shiny 폴더의 app.R 코드를 사용합니다. (shiny 외의 다른 패키지를 사용하지 않는다면 해당 코드로 바꿔도 작동합니다) 이를 실행하기 위해 기본 R 에서 제공하는 패키지외에 (shiny를 포함한) 추가 CRAN 패키지를 설치합니다.\nRstudio의 터미널에서 Rscript add-cran-binary-pkgs.R을 입력하여 패키지를 설치합니다.\n실행전 (기본 R 패키지)\n\n\n\n실행후 (shiny를 포함한 패키지)\n\n\n\n\n\n5. node 패키지 설치\npackage.json의 내용을 다음과 같이 [fix] package-json의 내용으로 복사 붙여넣기합니다.\n이때 author와 repository는 본인의 내용에 맞게 수정해야합니다.\n\n\n\n이후 Rstudio 터미널에서 npm install을 입력하여 패키지를 설치할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#shiny-실행-및-패키징",
    "href": "posts/2023-09-18-shinyexe/index.html#shiny-실행-및-패키징",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "Shiny 실행 및 패키징",
    "text": "Shiny 실행 및 패키징\napp.R은 개발 의도대로, 정상적으로 실행된다는 가정하에 Electron으로 shiny 를 실행하기 위해 Rstudio의 터미널에 electron-forge start를 입력합니다.\n\n\n\n큰 문제 없이 실행이 되었다면 이제 electron-forge make로 패키지를 만들 차례입니다.(zip)\n패키지 빌드를 위한 약간의 시간이 지난 후 out 디렉토리에서 앱과 zip 파일을 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-09-18-shinyexe/index.html#정리",
    "href": "posts/2023-09-18-shinyexe/index.html#정리",
    "title": "electron forge를 활용하여 Standalone Shiny Application 제작하기",
    "section": "정리",
    "text": "정리\n이번 글에서는 Standalone의 특징과 electron을 활용하여 shiny를 standalone app으로 만드는 방법을 다뤄봤습니다.\nElectron을 활용하여 Standalone Shiny app을 만드는 것은 소개된 것처럼 R외의 개발 지식을 필요로 하기 때문에 경험이 없다면 다소 복잡하게 느껴질 수 있습니다. (글에서 다루지 않은 내용들도 있습니다)\n특히 electron 내부에서 local R과 electron을 위한 node 패키지들을 이미 포함해야 하기 때문에 간단한 shiny app도 용량이 200 메가바이트 정도부터 시작한다는 치명적인 단점도 존재합니다.\n그럼에도 불구하고 이는 Standalone 특유의 몇가지 특징들이 있어 사용자의 환경에 따라 적합한 shiny 개발 방법으로 고려해볼 수 있는 선택지 중 하나입니다.\n\n\n\n\n\n\n🤗 Let’s talk\n\n\n\n차라투에서는 R과 Shiny에 대한 컨설팅을 제공합니다. 진행중인 프로젝트 관련하여 도움이 필요하시다면 jinhwan@zarathu.com 으로 알려주세요!"
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "",
    "text": "김진섭 대표는 11월 14일 서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정 에 강사로 초청, Shiny 기초학습을 위한 워크샵을 진행할 계획입니다. 강의 슬라이드를 포함한 실습 자료를 미리 공유합니다. daattalli 의 NBA 2018/2019 선수 스탯 데이터를 이용한 워크샵 을 그대로 활용했습니다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습 목표",
    "text": "실습 목표\n\nRStudio cloud 를 이용, 클라우드 환경에서 R을 쓸 수 있다.\napp.R 파일에 Shiny의 ui와 server 코드를 입력할 수 있다.\nfluidPage의 sidebarLayout 레이아웃을 이용, 왼쪽에는 UI 옵션, 오른쪽에는 해당되는 결과를 보여줄 수 있다.\nDT 패키지로 데이터를, ggplot2 로 히스토그램을 보여줄 수 있다.\nReactivity 를 이해한다.\nshinyapps.io 에 app 을 배포할 수 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: RStudio cloud\n",
    "text": "실습환경 만들기: RStudio cloud\n\nStep 1: https://rstudio.cloud 회원 가입\nStep 2: https://rstudio.cloud/spaces/30306/join?access_code=s4hEiPXQF%2BjosPclQEzgTtR0mPWDuh7Dhr2O7wAg 들어가서 “Join Space” 클릭\nStep 3: 위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 선택. Repo 주소는 https://github.com/jinseob2kim/shiny-workshop-odsc2019\n\n\nStep 3\n\n모든 강의자료는 RStudio cloud 안에 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: 개인 PC",
    "text": "실습환경 만들기: 개인 PC\nStep 1: 패키지 설치\n\ninstall.packages(c(\"shiny\", \"ggplot2\", \"dplyr\", \"DT\", \"colourpicker\", \"readr\")) \n\nStep 2: https://github.com/jinseob2kim/shiny-workshop-odsc2019 들어간 후\nStep 3: 녹색 “Clone or download” 클릭 후 “Download ZIP” 을 눌러 자료 다운."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/shiny-workshop-odsc2019 를 클릭하면 볼 수 있고, 전체 워크숍 자료는 https://github.com/jinseob2kim/shiny-workshop-odsc2019에서 다운받을 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html",
    "href": "posts/2022-02-07-tableone/index.html",
    "title": "tableone 패키지 소개",
    "section": "",
    "text": "본 자료는 데이터셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. tableone 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "href": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "title": "tableone 패키지 소개",
    "section": "Categorical variable conversion",
    "text": "Categorical variable conversion\nfactorVars 인자를 사용하여 범주형 변수를 지정할 수 있다. 이때 vars 인자를 통해 전체 데이터 셋 중 테이블에 들어갈 변수를 설정할 수 있고, 지정하지 않을 시 데이터 셋의 모든 변수가 포함된다.\n\n# Variables\nmyVars &lt;- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars &lt;- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 &lt;- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n범주형 변수로 설정한 컬럼의 요약값이 mean(sd)에서 n(percentage)로 바뀐 것을 볼 수 있다.\n두 개의 범주가 있는 범주형 변수의 경우, 두 번째 범주의 요약값만 출력된다. 예를 들어 0과 1의 범주가 있을 때, 범주1의 개수와 백분율이 출력된다. 이는 옵션 설정을 통해 전체 범주의 요약값을 출력하도록 변경할 수 있다.\n3개 이상의 범주가 있을 때에는 모든 범주의 값이 요약되며, 백분율은 누락된 값을 제외한 후 계산된다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "href": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "title": "tableone 패키지 소개",
    "section": "Multiple group summary",
    "text": "Multiple group summary\nstrata 인자를 설정하여 그룹별 연산을 할 수 있다. strata는 dplyr 패키지의 group_by() 함수와 유사하며, 그룹 연산을 할 변수를 지정하여 사용할 수 있다.\n\nt2 &lt;- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nt2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)\n\n\n\n\n\n\n\n\n연속형 변수의 경우, 기본적으로 one-way ANOVA test가 적용되며 nonnormal일 경우 옵션 설정을 통해 Kruskal–Wallis one-way ANOVA test를 적용할 수 있다.\n범주형 변수의 경우, 기본적으로 chisq-test가 적용되며 print 함수의 exact 옵션 설정을 통해 fisher-test를 적용할 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "href": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "title": "tableone 패키지 소개",
    "section": "Showing all levels",
    "text": "Showing all levels\n범주형 변수에서 모든 범주의 요약값을 확인하려면 ShowAllLevels 또는 cramVars 옵션을 사용한다. ShowAllLevels = T 를 설정하거나 cramVars 옵션에 원하는 변수명을 지정하여 사용할 수 있다.\n\n1.use showAllLevels\n\n\n\nprint(t1, showAllLevels = T)\n\n\n\n\n\n\nlevel\nOverall\n\n\n\nn\n\n1644\n\n\nHGHT (mean (SD))\n\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n\n65.10 (12.53)\n\n\nBMI (mean (SD))\n\n23.92 (3.38)\n\n\nHDL (mean (SD))\n\n55.90 (19.47)\n\n\nLDL (mean (SD))\n\n118.69 (201.99)\n\n\nTG (mean (SD))\n\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n\n25.98 (27.18)\n\n\nQ_PHX_DX_STK (%)\n0\n1059 (98.9)\n\n\n\n1\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ (%)\n0\n1052 (97.6)\n\n\n\n1\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n1\n77 ( 4.7)\n\n\n\n2\n1102 (67.1)\n\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n1\n995 (60.6)\n\n\n\n2\n256 (15.6)\n\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n2.use cramVars\n\n\n\nprint(t1, cramVars=\"Q_PHX_DX_STK\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 0/1 (%)\n1059/12 (98.9/1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "href": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "title": "tableone 패키지 소개",
    "section": "nonnormal variables",
    "text": "nonnormal variables\n비모수통계를 사용하는 연속형 변수에는 nonnormal 옵션을 설정한다. nonnormal 설정 시 mean(sd)에서 median(IQR)로 요약값이 변경된다.\n\nprint(t1, nonnormal=\"LDL\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (median [IQR])\n112.00 [90.00, 134.00]\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#exact",
    "href": "posts/2022-02-07-tableone/index.html#exact",
    "title": "tableone 패키지 소개",
    "section": "exact",
    "text": "exact\nexact 옵션을 통해 fisher-test를 진행할 범주형 변수를 설정할 수 있다. 범주형 변수는 기본적으로 chisq-test가 적용되며, exact 옵션에 fisher-test를 적용할 변수를 지정하여 사용할 수 있다.\n\nprint(t2, exact=c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.045\nexact\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.281\nexact\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#smd",
    "href": "posts/2022-02-07-tableone/index.html#smd",
    "title": "tableone 패키지 소개",
    "section": "smd",
    "text": "smd\nsmd 옵션을 통해 smd(standardized mean difference)를 table1에 포함할 수 있다. default는 FALSE이고, smd=TRUE 설정 시 각 변수의 smd 값이 출력된다.\n\nprint(t2, smd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\nSMD\n\n\n\nn\n995\n256\n391\n\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n&lt;0.001\n\n0.972\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n&lt;0.001\n\n0.611\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n&lt;0.001\n\n0.181\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n&lt;0.001\n\n0.197\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n0.091\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n&lt;0.001\n\n0.341\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n&lt;0.001\n\n0.196\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n0.137\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n0.084\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n0.109\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n&lt;0.001\n\nNaN\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 2월 Shinykorea 밋업에 참석, 서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 공유할 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "요약",
    "text": "요약\n\n대표적인 감염병 모형은 SIR/SEIR 이고, 초기값과 parameter(예: 감염률)가 주어진 미분방정식으로 표현.\n확진자수 등 실제 데이터를 활용, parameter 들을 추정.\nSEIR 에 서울시 확진자수를 적용, 시간에 따라 변화하는 감염률을 계산함.\nParameter와 그 파생지표의 범위를 제한하고 신뢰구간을 계산하기 위해, 베이지안통계 이용 예정."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/covidmodel-seoul/modelling/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 2주차에는 %&gt;% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nProject 생성\n\n\n\n강의록은 과거 글 https://blog.zarathu.com/posts/2019-01-03-rdatamanagement/ 을 참고하자."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "요약",
    "text": "요약\ntidyverse는 직관적인 코드를 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\n\nmagrittr 패키지의 %&gt;% 연산자로 의식의 흐름대로 코딩한다.\ndplyr 패키지의 select, mutate, filter, group_by, summarize 함수는 %&gt;% 와 찰떡궁합이다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-snuhlab/tidyverse 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html",
    "href": "posts/2022-01-06-internship-django-1/index.html",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 1주차 동안 학습한 내용에 대해 공유합니다.\n차라투에서는 사용자에게 여러가지 통계 웹앱을 제공하는 Openstat 서비스를 준비중입니다. 서비스를 이용하는 사용자 본인이 불필요한 기능을 제거하고, 본인에게 필요한 기능만을 제공받을 수 있는 기능 도입 준비 중입니다. Django를 사용하여 사용자에게 보여질 기능을 DB로 유지하며 해당 DB기반으로 사용자에게 서비스를 제공해야합니다. 이에 사용자가 직접 DB의 필드값을 수정하며 서비스를 제공받는 기능을 구현했습니다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html#목차",
    "href": "posts/2022-01-06-internship-django-1/index.html#목차",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "목차",
    "text": "목차\n\nModel.Boolean 값으로 화면 출력 통제\n\nCheckBox를 활용하여 Model.BooleanField 핸들링\n\n\n\nModel.Boolean 값으로 화면 출력 통제\n\n\n[models.py]\nClass Post(models.Model):\n  user_id = models.CharField(max_length = 50, default=\"\")\n  postname = models.CharField(max_length =50)\n  content = models.TextField()\n  important = models.BooleanField(default = True)  \n\n Model 필드에 BooleanField 형식의 필드를 추가합니다.\n이 예제에서는 important라는 필드명을 사용했지만, 필드명은 다른 이름으로 사용해도 괜찮습니다.\n(기존 Model에 새로운 필드를 추가하는 경우, 기존 Model 속 데이터들의 필드들도 일괄적으로 변경됩니다. 새로운 필드에 값을 넣지 않을 경우, 에러가 발생하므로 Model에 새로운 필드를 추가할때는 default 값을 넣어주는것이 좋습니다.)\n\n\n\n[views.py]\nfrom .models import Post\n\ndef show_important_post(request):\n    postlist = Post.objects.all()\n    return render(request,'blog/ImportantPost_Posting.html',{'postlist':postlist})\n\n\npostlist = Post.objects.all()\n\n현재 생성된 Models의 Post를 import하고 현재 모델 Post 속에 담겨있는 모든 objects들을 postlist에 담습니다.  \n\nretrun render(request,‘blog/ImportantPost_Posting.html’,{‘postlist’:postlist})\n\n{‘postlist’:postlist}로 {key,value} 값을 맞춰서 blog/ImportantPost_Posting.html에 넘깁니다.\n(예시에서는 blog/ImportantPost_Posting.html에 넘겼지만 본인의 상황에 적합한 templates에 넘기면 됩니다.)   \n\n\n[templates-(ImportantPost_Posting.html)]\n{% block contents %}\n&lt;h1&gt;중요한 게시판 &lt;/h1&gt;\n    {% for list in postlist %}\n        {% if list.important is True %}\n        &lt;ul&gt;\n            &lt;li&gt;\n                작성자: {{list.user_id}} \n                &lt;a href=\"/blog/showImportant/{{list.pk}}/\"&gt;{{list.postname}}&lt;/a&gt;\n            &lt;/li&gt;\n        &lt;/ul&gt;\n        {% endif %}\n    {%endfor%}\n    &lt;button&gt;&lt;a href=\"/blog/showImportant/new_post/\"&gt;글쓰기&lt;/a&gt;&lt;/button&gt;\n    &lt;input type=\"button\" value = \"돌아가기\" onclick = \"back()\"&gt;\n{% endblock %}\n\n\n{{%for list in postlist%}}\n　　　{%if list.important is True%}\n{% endfor %}\n\n앞서 views에서 넘겨 받은 postlist를 순회합니다. 만약 postlist에 해당하는 부분을 전부 출력하면 현재 가지고 있는 POST DB에 존재하는 모든 내용을 출력하게 됩니다. 하지만 저희는 if 문을 활용하여 DB 속 모든 객체들 중 important 필드의 값이 True 인 경우만 &lt;li&gt; 태그에 넣어 화면에 출력하도록 합니다. \n\n\n{{list.postname}}\n\n위 예제에서는 list의 postname 필드를 출력하였지만, list의 다른 필드를 출력해도 상관없습니다.\n\n\n\n\n\nCheckBox활용 Model.BooleanField 핸들링\n\n\n[Views.py]\ndef edit_post(request,pk):\n    post = Post.objects.get(pk=pk)\nif request.method == 'POST':\n        if len(request.POST.getlist('important')) == 0:\n            important = False\n        else: important = True\n        post.postname = request.POST['postname']\n        post.content = request.POST['contents']\n        post.important = important\n        post.save()\n\n\npost = Post.objects.get(pk=pk)\n\nedit_post함수를 통해 request 메시지와 (pk:primary key)를 받으면 Model class인 Post에서 pk가 동일한 객체를 찾아 post에 넘겨줍니다.  \n\nif len(request.POST.getlist(‘important’))==0\n　　important = False\nelse: important = True\n\n’important’값은 체크박스를 통하여 값을 넘깁니다. 만약 위 예제 코드 속 다른 코드에서 쓰이는 것처럼 post.important = request.POST[‘important’] 형식을 사용하여 important 값을 넣으려고 시도한다면 문제가 생깁니다. 체크 박스를 체크하여 request 요청했다면 문제가 되지 않지만, 체크 박스를 체크하지 않고 값을 넘기면 MultiValueDictKeyError 문제가 발생하기에 코드를 이와 같이 수정해야 합니다.\nMultiValueDictKeyError를 피하기 위하여 우리는 important의 값을 list 형태로 가지고 옵니다. 만약 체크 박스를 체크했다면 list에 값이 들어있을 것이고 체크박스를 체크하지 않았다면 list에 값이 없을 것입니다. 그렇게 list의 길이를 len을 통해 측정하여 길이가 0이면 False를, 길이가 0이 아니면 True를 important 값에 넣어 post 필드 값을 수정하여 새롭게 save() 해줍니다.\n \n\n\n[templates-(editPost.html)]\n{% if Post.important is True %}\n  &lt;input type = \"checkbox\" name= \"important\" value= \"True\" checked&gt; important &lt;br&gt;\n{% else %}\n  &lt;input type = \"checkbox\" name = \"important\" value = \"False\"&gt; important &lt;br&gt;\n{% endif %}\n Post의 필드 중 important 부분을 체크박스를 통하여 수정하는 templates의 일부를 가지고 왔습니다. 현재 Post의 important 부분이 True 이면 체크된 상태로 화면에 출력되도록 하고 False이면 체크되지 않은 상태로 화면에 출력되도록 했습니다. \n \n\n\n\n출력\n\n\n\n\n\nimportant = True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportant = False\n\n\n\n\n\n\n\n\n\n\n\n상기 이미지에 보이는 것처럼 체크박스의 체크 유무가 필드값인 important로 대입되어 important의 값에 따라 True 인 경우에는 화면에 출력되고 False인 경우에는 출력되지 않는 모습을 확인할 수 있습니다.\n\n\n\n결론\nModel의 booleanField 값을 CheckBox를 통해 제어하며 화면에 출력을 통제하는 방법에 대해 알아보았습니다.\n해당 방법을 활용하여 User 맞춤 서비스 제공 등 다양한 방식으로 DB의 내용을 기준하여 서비스 제공에 기여할 것으로 기대됩니다."
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html",
    "href": "posts/2021-08-19-beadatascientist/index.html",
    "title": "R 활용 의학연구지원",
    "section": "",
    "text": "김진섭 대표는 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에 참석, “Be a data scientist - major actor in the future research” 라는 제목으로 R 활용 의학연구지원경험을 발표할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#요약",
    "href": "posts/2021-08-19-beadatascientist/index.html#요약",
    "title": "R 활용 의학연구지원",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\nShinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#slide",
    "href": "posts/2021-08-19-beadatascientist/index.html#slide",
    "title": "R 활용 의학연구지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/microbiome 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html",
    "href": "posts/2022-02-11-datatable/index.html",
    "title": "data.table 패키지 소개",
    "section": "",
    "text": "본 자료는 빠른 속도와 메모리 효율성에 강점이 있는 data.table 패키지에 관해 기본 개념과 문법, 함수들을 예제를 통해 다루어 볼 것이다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "href": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "title": "data.table 패키지 소개",
    "section": "Load & save data: fread & fwrite\n",
    "text": "Load & save data: fread & fwrite\n\nfread 함수로 빠르게 csv 파일을 읽어와서 data.table 자료로 만들 수 있고, fwrite 함수로 csv 파일을 쓸 수 있다. fread와 fwrite 는 이름답게 매우 빠르며 Base R의 함수보다 40배 더 빠른 속도를 자랑한다. 파일을 읽어와서 data.table 자료로 만들 때, 로컬 file path를 입력하거나 http:// 로 시작하는 url을 입력하는 방법을 사용한다. fread로 파일을 읽으면 그 class는 data.frame에 data.table이 추가되며 문법이 원래의 data.frame과 달라지는 점을 유의하자.\nfread를 통해 데이터를 불러와 data.table 형태로 만들어보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\nSetup\n\n## Setup\n\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 7.88.1 with LibreSSL/3.3.6\n\n\nLoad file\n\n# Load file\nurl &lt;- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndf &lt;- read.table(url,header=T)\ndt &lt;- fread(url,header=T)\n# Class\nprint(class(df))\n\n[1] \"data.frame\"\n\nprint(class(dt))\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 볼 수 있다.\n\n\n# dt = data.table(df)\n# df = data.frame(dt)\n## See \ndt\n\n\n\n\n  \n\n\n\nSave file\n\n# Save file\nwrite.csv(dt, \"aa.csv\",row.names=F)\nfwrite(dt, \"aa.csv\")"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#row-operation",
    "href": "posts/2022-02-11-datatable/index.html#row-operation",
    "title": "data.table 패키지 소개",
    "section": "row operation",
    "text": "row operation\n행을 선택하는 slice는 data.table에서 DT[c(row1, row2, …),] 또는 DT[c(row1, row2, …)]의 형식으로 data.frame과 동일하게 쓰인다.\n\ndt[c(3,5)]\n\n\n\n\n  \n\n\n\n\n특정한 조건을 만족하는 행을 선택하는 filter는 data.table에서 DT[cond]의 형식으로 쓰인다.\n(이때, cond는 논리형 벡터이다.)\n\ndt[BMI&gt;=30 & HGHT&lt;150]\n\n\n\n\n  \n\n\n\n미리 key들을 지정하면 더 빠르게 검색할 수 있다. 자세한 내용은 뒤에서 다루도록 하겠다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#column-operation",
    "href": "posts/2022-02-11-datatable/index.html#column-operation",
    "title": "data.table 패키지 소개",
    "section": "column operation",
    "text": "column operation\n열을 이름 또는 순번으로 선택하는 select는 DT[, .(cols)] 또는 DT[, list(cols)]의 형식으로 data.frame과 비슷하나 몇 가지 차이점이 있다. 변수 이름으로 선택하는 경우 앞에 .()이나 list를 붙이지 않으면 결과로 벡터가 반환된다.\n\n순번으로 열 선택\n\n\ndt[, c(13, 14)]\n\n\n\n\n  \n\n\n\n\n\n이름으로 열 선택\n\n\n## same\n# dt[, list(HGHT, WGHT)]\ndt[, .(HGHT, WGHT)]\n\n\n\n\n  \n\n\n\n\n열을 선택할 때 DT[, .(new_col = col)] 형식을 사용하여 열 이름을 지정해서 출력할 수 있다.\n\n# rename\ndt[, .(Height = HGHT, Weight = WGHT)]\n\n\n\n\n  \n\n\n\n\n\n변수로 열 선택\n\n\n## same\n# vars &lt;- c(13,14) \nvars &lt;- c(\"HGHT\", \"WGHT\")\n\ndt[, ..vars]\n\n\n\n\n  \n\n\n\n변수로 열을 선택하는 경우, 변수 앞에 ..을 붙이지 않으면 오류가 발생하므로 주의하도록 한다. .. 대신 with = F 를 뒤에 붙이거나 .SD, .SDcols 옵션을 사용하기도 한다. .SD는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, vars, with = F]\n\n\n\n\n  \n\n\n\n\n\n열 제외\n\n필요없는 열을 제외할 때는 - 또는 ! 를 붙인다.\n\nicols = c(1:12)\n\n## same\n# dt[, -..icols]\ndt[, !..icols]\n\n\n\n\n  \n\n\n\ndata.table의 column operation에서는 열을 선택할뿐만 아니라 연산하는 식을 처리할 수 있다. 앞에서 배운 내용을 통해 특정 조건을 만족하는 행을 대상으로 mean 연산을 수행하여 보자.\n\ndt[HGHT &gt;= 180 & WGHT &lt;= 80, .(m_chol = mean(TOT_CHOL), m_TG = mean(TG))]\n\n     m_chol     m_TG\n1: 181.1143 144.8857"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#by-operation",
    "href": "posts/2022-02-11-datatable/index.html#by-operation",
    "title": "data.table 패키지 소개",
    "section": "by operation",
    "text": "by operation\nby 옵션을 이용하여 그룹별로 함수를 적용할 수 있다. by=.(그룹1, 그룹2, …)을 사용해 두 개 이상의 그룹별로 함수를 적용할 수 있다. 이때 괄호 앞에 있는 점(‘.’)은 list()를 의미하므로 꼭 포함시키도록 한다. by 대신 keyby를 사용할 경우, 그룹별 집계 결과가 정렬되어 나타난다.\n연도 변수인 EXMD_BZ_YYYY을 기준으로 집단을 분리한 후 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같다.\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n\n기준으로 사용되지 않은 모든 열에 대해 평균을 구할때는 .SD를 사용한다. 이는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n\n이번에는 두 개의 그룹 변수를 지정해 행의 개수를 출력해보자.\n\ndt[HGHT &gt;= 175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\n정렬\n그룹별로 함수를 적용한 결과를 정렬하고자 할 때는 keyby를 사용하거나 마지막에 [order()]를 붙인다.\n\nkeyby\n\n\ndt[HGHT &gt;= 175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n  \n\n\n\nby를 사용한 예제와는 달리 Q_SMK_YN에 대해서 정렬된 것을 볼 수 있다.\n\n\n[order()]\n\n\n# BMI에 대해 내림차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n  \n\n\n\n\n# BMI에 대해 오름차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n\n\n\n  \n\n\n\nExpressions in by\n\nby 옵션에는 변수뿐만 아니라 식을 지정할 수도 있다.\n약물 치료 여부에 따른 환자수를 확인하려는 경우, 다음과 같이 식을 지정한다.\n\ndt[, .N, by=.(Q_PHX_DX_STK &gt; 0, Q_PHX_DX_HTDZ &gt; 0)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "href": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "title": "data.table 패키지 소개",
    "section": "key를 이용한 탐색 setkey()",
    "text": "key를 이용한 탐색 setkey()\nkey를 사용하면 데이터의 탐색 및 처리 속도가 매우 향상된다. setkey(DT, col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용한다. 설정되어 있는 키를 제거할 때는 setkey(DT, NULL)를 설정한다.\n\n키 설정\n\nsetkey를 활용해 데이터 테이블에 키를 설정하고, key 함수로 설정된 키를 확인할 수 있다.\n\n# 1 key\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\n\n# 2 keys\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\n\n키를 활용한 행 선택\n\ndt[.(a)], dt[J(a)], dt[list(a)], dt[col == a] 중에서 아무거나 사용하여 행을 선택할 수 있다.\n\n## same\n# dt[.(2011)]\n# dt[list(2011)]\n# dt[EXMD_BZ_YYYY==2011]\ndt[J(2011)]\n\n\n\n\n  \n\n\n\n\n## same\n# dt[.(2011, 2)]\n# dt[list(2011, 2)]\n# dt[EXMD_BZ_YYYY==2011 & Q_HBV_AG==2]\ndt[J(2011, 2)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 병합 merge()",
    "text": "데이터 테이블 병합 merge()\ndata.table은 key를 사용하거나, on=을 활용하여 두 데이터 데이블을 병합할 수 있다.\n\n\n\n\nMerge in data.table\n\n\n\n기존 데이터를 가공하여 새로운 data.table인 dt1, dt2에 저장하고 연도에 따라 merge해 보자.\n\n# dt1\n(dt1 &lt;- dt[c(1, 300, 500, 700, 1000)])\nsetkey(dt1, EXMD_BZ_YYYY)\n\n\n\n\n  \n\n\n\n\n# dt2\n(dt2 &lt;- dt[c(400, 600, 800 ,1200, 1500)])\nsetkey(dt2, EXMD_BZ_YYYY)\n\n\n\n\n  \n\n\n\n1. inner join\n두 데이터에 모두 존재하는 경우 dt1[dt2, on=‘key’, nomatch=0] 또는 merge(dt1, dt2, by=‘key’, all=F) 형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY', nomatch=0]\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all = F)\n\n\n\n\n  \n\n\n\n2. left_outer_join\n첫 번째 데이터에 존재하는 경우 dt2[dt1, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.x=T) 형식 사용\n\n\n\n\n\n\n\n\n\ndt2[dt1, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.x=T)\n\n\n\n\n  \n\n\n\n3. right_outer_join\n두 번째 데이터에 존재하는 경우 dt1[dt2, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.y=T)형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.y=T)\n\n\n\n\n  \n\n\n\n4. full_outer_join\n어느 한 쪽에 존재하는 경우 merge(dt1, dt2, by=‘key’, all=T) 형식 사용\n\n\n\n\n\n\n\n\n\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all=TRUE)"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 수정 연산자 :=",
    "text": "데이터 테이블 수정 연산자 :=\n데이터 테이블에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용한다. 수정 또는 생성할 열이 하나인 경우, dt[ , newcol1 := ] 형식을 쓰며 열이 두 개 이상인 경우 dt[, ‘:=’ (col1=, col2=)]을 사용한다.\n다음의 예시를 통해서 자세히 알아보자.\n기존의 데이터 테이블에서 HDL - LDL을 구해서 diff라는 이름의 열을 새로 생성한다. 그리고, HGHT와 WGHT는 HGHT*0.9, WGHT+5로 수정한다.\n열 생성/수정\n위 코드를 실행시키면 갱신이 눈에 보이지 않는 상태로 실행되며, 만약 갱신 결과를 눈에 보이도록 출력하려면 제일 뒤에 [] 를 붙여주어야 한다.\n\n# 열 생성\ndt[, diff := HDL-LDL][]\n\n\n\n\n  \n\n\n\n\n# 열 수정\ndt[, ':=' (HGHT = HGHT*0.9, WGHT = WGHT+5)][]\n\n\n\n\n  \n\n\n\n열 삭제\n데이터 테이블에서 열을 삭제하려면 col := NULL 형식을 사용한다.\n\n# BMI 삭제\ndt[, BMI := NULL]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#특수-기호",
    "href": "posts/2022-02-11-datatable/index.html#특수-기호",
    "title": "data.table 패키지 소개",
    "section": "특수 기호",
    "text": "특수 기호\n.SD\n.SD 는 ’Subset of Data’의 약자로, by로 지정한 그룹 칼럼을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용한다.\n\n# 모든 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n 또한 .SD 기호를 사용하여 연도별로 처음 두 개의 행을 추출할 수 있다.\n\ndt[, head(.SD, 2), by=EXMD_BZ_YYYY]\n\n\n\n\n  \n\n\n\n.SDcols\n.SDcols는 연산 대상이 되는 특정 칼럼을 지정하는 특수 기호이다. 특정 열을 대상으로 연산을 할 때 by 다음에 .SDcols = c(“col1”, “col2”, …)로 연산 대상을 지정한다.\n\n# HGHT, WGHT 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]\n\n\n\n\n  \n\n\n\n.N\n.N는 부분 데이터의 행의 수를 나타내며, 요약 통계치를 구할 때 대상 데이터의 수를 간편하게 구할 수 있다.\n\n# 특정 조건을 만족하는 행의 수\ndt[LDL &gt;= 150, .N]\n\n[1] 228\n\n\n\n# HGHT, WGHT 칼럼의 연도별 평균값과 행의 수\ndt[, c(.N, lapply(.SD, mean)), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#melt",
    "href": "posts/2022-02-11-datatable/index.html#melt",
    "title": "data.table 패키지 소개",
    "section": "melt",
    "text": "melt\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있다. melt(data, id.vars, measure.vars, variable.name, value.name) 형식으로 쓰이며, id.vars에는 고정 칼럼을 measure.vars는 stack 처리할 칼럼을 넣는다. melt함수를 써서 길게 재구조화한 후의 “variable”, “value” 변수 이름을 바꾸고 싶다면 variable.name=“new_var_name”, value.name=“new_val_name” 처럼 새로운 칼럼 이름을 부여하여 지정할 수 있다.\n\n# wide to long\ndt.long1 &lt;- melt(dt,\n                id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HGHT\", \"WGHT\"),\n                measure.vars = c(\"TOT_CHOL\", \"HDL\", \"LDL\"),\n                variable.name = \"measure\",\n                value.name = \"val\")\ndt.long1\n\n\n\n\n  \n\n\n\nEnhanced melt\nmelt 함수는 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있다.\n\n\nlist에 복수의 칼럼 이름을 입력하는 방법\n\nmelt 함수에 measure=list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣는다. 이때 공통의 value.name을 지정할 수 있다.\n\ncol1 &lt;- c(\"BP_SYS\", \"BP_DIA\")\ncol2 &lt;- c(\"HDL\", \"LDL\")\ndt.long2 &lt;- melt(dt, \n                 measure = list(col1, col2),\n                 value.name = c(\"BP\", \"Chol\"))\ndt.long2\n\n\n\n\n  \n\n\n\n\n\n특정 패턴을 정규 표현식으로 매칭하는 방법\n\nmelt 함수에 measure=patterns() 형식으로 특정 패턴을 따르는 복수의 칼럼을 정규 표현식을 통해 설정한다.\n\ndt.long3 &lt;- melt(dt, \n            measure=patterns(\"^Q_PHX_DX\", \"^BP\"), \n            value.name=c(\"Q_PHX_DX\", \"BP\"))\ndt.long3"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#dcast",
    "href": "posts/2022-02-11-datatable/index.html#dcast",
    "title": "data.table 패키지 소개",
    "section": "dcast",
    "text": "dcast\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용한다. dcast(data, formula, value.var, fun.aggregate) 형식으로 쓰이며, formula의 LHS에는 id.vars, RHS에는 variable을 입력하고 value.name에는 “value”를 입력한다. 이때 “variable”과 “value” 변수 이름을 바꿨다면, 새로 지정한 칼럼명을 넣는다.\n\n# long to wide\ndt.wide1 &lt;- dcast(dt.long1, EXMD_BZ_YYYY + RN_INDI + HGHT + WGHT ~ measure, value.var = \"val\")\ndt.wide1\n\n\n\n\n  \n\n\n\n\ndcast 함수에 집계 함수(fun.aggregate)를 사용하여 그룹별 요약 통계량을 계산한 결과를 재구조화하여 반환할 수 있다.\n\n# 연도별 TOT_CHOL, HDL, LDL의 평균값\ndt.wide2 &lt;- dcast(dt.long1, EXMD_BZ_YYYY ~ measure, value.var = \"val\", fun.aggregate = mean, na.rm =T)\ndt.wide2\n\n\n\n\n  \n\n\n\nEnhanced dcast\ndcast 함수의 value.vars에 복수의 칼럼을 넣어 여러 개의 칼럼을 동시에 재구조화할 수 있다.\n\ndt.wide3 &lt;- dcast(dt.long2,\n                  ... ~ variable,\n                  value.var = c(\"BP\", \"Chol\"))\ndt.wide3"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html",
    "href": "posts/2022-07-25-collapse/index.html",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "R의 고급 데이터 변환 및 통계 컴퓨팅을 위한 C/C++ 기반 패키지입니다.\n\n유연하고 간결한 구문을 통해 매우 빠르고 클래스에 구애받지 않습니다,\n기본 R, ‘dplyr’, ‘tibble’, ‘data.table’, ‘sf’, ‘plm’ 과 잘 통합됩니다.\n\n\n\n##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr)\nlibrary(data.table) \nlibrary(dplyr)\nlibrary(collapse)\nlibrary(microbenchmark)\n\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하겠습니다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#setup",
    "href": "posts/2022-07-25-collapse/index.html#setup",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr)\nlibrary(data.table) \nlibrary(dplyr)\nlibrary(collapse)\nlibrary(microbenchmark)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#load-file",
    "href": "posts/2022-07-25-collapse/index.html#load-file",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하겠습니다.\n\n# Exam data: 09-15\n\ndt &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf &lt;- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#load",
    "href": "posts/2022-07-25-collapse/index.html#load",
    "title": "collapse 패키지 소개",
    "section": "load",
    "text": "load\n\ncollapse는 데이터를 불러오는 함수가 존재하지 않기 때문에, data.table를 이용하여 읽습니다.\nfselect()는 컬럼명을 명시하거나 인덱스를 전달하면 원하는 컬럼을 불러올 수 있습니다.\n\n\n## data.table(Only specific column)\ndt1 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\",select = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"))\ndt2 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", select = 1:5)\ndt3 &lt;- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", drop = 6:10)\n\n## collapse(Only specific column)\ndt4 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM)\ndt5 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),1:5)\ndt6 &lt;- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),-(6:10)) \n\n\n예시로 dt6를 확인해보면, dt3와 마찬가지로 인덱스 6부터 10까지 제외되서 출력된 것을 볼 수 있습니다.\n\n\ndt3\n\n\n\n\n  \n\n\n\n\ndt6"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#row",
    "href": "posts/2022-07-25-collapse/index.html#row",
    "title": "collapse 패키지 소개",
    "section": "row",
    "text": "row\n\ncollapse::fselect와 dplyr::select는 유사하지만 fselect가 x100배 정도 빠릅니다.\nss()함수는 컬럼명이 아닌 인덱스로 행,열을 출력을 할 때 사용합니다. fsubset() 보다 빠르지만 기능이 제한적이기 떄문에 간단한 행,열 출력할 때 사용가능합니다.\n\n\n## data.table(row)\ndt[1:10]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)]\ndt[order(HME_YYYYMM)]\ndt[order(HME_YYYYMM, -HGHT)]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)]        \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)] %&gt;% .[order(HGHT)]  #same\n\n## collapse(row)                                       \nfsubset(dt, 1:10)                                                   #ss(dt,1:10)\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 )               \nroworder(dt, HME_YYYYMM)                                            \nroworder(dt, HME_YYYYMM, -HGHT)\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25)\n\n\n예시로 두번째 코드와 다섯번째 코드를 확인해보겠습니다.\n\n두번째\n\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 ) \n\n\n\n\n  \n\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)]\n\n\n\n\n  \n\n\n\n다섯번째\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)] \n\n\n\n\n  \n\n\n\n\nroworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column",
    "href": "posts/2022-07-25-collapse/index.html#column",
    "title": "collapse 패키지 소개",
    "section": "column",
    "text": "column\n\n열 이름을 정규식으로 선택하기 위해서는 fselect()가 아니라 get_vars() 함수를 이용해야합니다. regex = TRUE 정규식을 사용하겠다는 의미이고 return = “names” 컬럼명을 출력하겠다는 의미입니다. get_var()는 fselect()함수와 유사하지만, 수행속도가 좀 더 빠르며 벡터, 정수 형태로 값을 전달합니다.\n유의사항은 컬럼명을 리스트로 전달하면 ERROR 발생합니다.\nfsubset는 빠르고 부분적인 작업을 위해 base::subset 패키지 의 C 함수를 사용하는 향상된 버전입니다.\n\n\n## data.table(column)\ndt[, 1:10]\ndt[, c(\"HGHT\", \"WGHT\")]\ndt[, .(HGHT, WGHT)]\ndt[, .(Height = HGHT, Weight = WGHT)]   \ndt[, .(HGHT)] \ndt[, \"HGHT\"]\ncolvars1 &lt;- grep(\"Q_\", names(dt), value = T)\ndt[, ..colvars1]\ndt[, colvars1, with = FALSE]      \ndt[, .SD, .SDcols = colvars1]     \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars]\ndt[, !..colvars1]\ndt[, -..colvars1]\ndt[, .SD, .SDcols = -colvars1]\n\n## collapse(column)\nfselect(dt, 1:10)                 \nfselect(dt, c(\"HGHT\", \"WGHT\"))    #get_vars(dt, 1:10)\nfselect(dt, HGHT, WGHT)           #get_vars(dt, c(\"HGHT\", \"WGHT\"))\nfselect(dt, Height = HGHT, Weight = WGHT)\nfselect(dt, .(HGHT))              #ERROR\nfselect(dt, \"HGHT\")\ncolvars2 &lt;-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")    #regex = TRUE 정규식 사용/ return = \"names\" 컬럼명 출력\nfselect(dt, colvars2)                                            #fselect(dt, c(colvars))\nget_vars(dt, colvars2)                                           #get_var(dt, c(colvars))\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars2)\nfselect(dt, -(4:12))\nfselect(dt, -(Q_PHX_DX_STK:Q_DRK_FRQ_V09N))\n\n\n예시로 같은 값을 출력하는지 확인해 보겠습니다.\n\n\ncolvars1 &lt;- grep(\"Q_\", names(dt), value = T)\n\nqDT(colvars1)\n\n         colvars1\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ncolvars2 &lt;-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")\n\nqDT(colvars2)\n\n         colvars2\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars1]\n\n\n\n\n  \n\n\n\n\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars2)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column-summary",
    "href": "posts/2022-07-25-collapse/index.html#column-summary",
    "title": "collapse 패키지 소개",
    "section": "Column summary",
    "text": "Column summary\n\nfmean는 (열별) 평균을 계산하고, (선택적으로) 그룹화 및 가중치를 계산하는 일반적인 함수입니다.\ndapply는 데이터에 대한 정보(속성)를 잃거나 데이터의 클래스 또는 형식을 변경하지 않고 데이터의 행이나 열에 함수를 적용하는 효율적인 함수입니다.\n\n\n## data.tanble(Column summary)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))] \n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n#collapse(Column summary)\nfselect(dt,HGHT,WGHT,BMI) %&gt;% fmean()     #fmean(fselect(dt,HGHT,WGHT,BMI))\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n두번째 코드를 비교해보겠습니다.\n\n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n\n\n\n\nHGHT\nWGHT\nBMI\n\n\n164.5487\n65.09672\n23.92257\n\n\n\n\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n\n\n\n\nfmean\n\n\n\nHGHT\n164.54866\n\n\nWGHT\n65.09672\n\n\nBMI\n23.92257"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#by",
    "href": "posts/2022-07-25-collapse/index.html#by",
    "title": "collapse 패키지 소개",
    "section": "By",
    "text": "By\n\ncollap()는 ‘Fast Statistical Functions’ 사용하여 각 컬럼에 여러 함수를 적용할 수 있습니다.(#Fast Statistical Functions: fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct)\nadd_stub() 연산을 통해 열을 추가할 수 있는 명령입니다. ““를 통해 열 이름을 설정할 수 있습니다.\n\n\n##data.table(By)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY]\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = \"EXMD_BZ_YYYY\"]  #same\ndt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY]       #same\ndt[HGHT &gt;= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)]        \ndt[HGHT &gt;= 175, .N, by= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                               #same\ndt[HGHT &gt;= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                            #same(정렬)\n\n#collapse(By)                                                  \ncollap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16))                                # ~ ≒ by\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY)                       #same\nadd_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")  #dplyr::count()\n\n\n마지막 코드를 비교해보겠습니다. 결측치 값의 정렬 순서의 차이가 있지만, 같은 기능을 수행할 수 있습니다.\n\n\ndt[HGHT &gt;= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]\n\n\n\n\n  \n\n\n\n\nadd_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#new-variable",
    "href": "posts/2022-07-25-collapse/index.html#new-variable",
    "title": "collapse 패키지 소개",
    "section": "New variable",
    "text": "New variable\n\nftransform 새 열을 계산하거나 기존 열을 수정 및 삭제하는 데 사용할 수 있으며 항상 전체 데이터 프레임을 반환합니다.\nftransform은 base::transform 데이터 프레임 및 목록 의 향상된 버전입니다.\n\n\n## data.table(New variable)\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\ndt[, `:=`(BP_SYS140 = factor(as.integer(BP_SYS &gt;= 140)), BMI25 = factor(as.integer(BMI &gt;= 25)))]\n\ndt[, BMI2 := NULL]\n\n## collapse(New variable)\nftransform(dt, BMI2 = round(WGHT/(HGHT/100)^2, 1))\n\nftransform(dt,BP_SYS140 = factor(as.integer(BP_SYS &gt;= 140)),BMI25 = factor(as.integer(BMI &gt;= 25)))\n\nftransform(dt, BMI2 = NULL)\n\n\n첫번째와 두번째 코드를 확인해보겠습니다.\ndata.table\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\ncollapse"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "href": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "title": "collapse 패키지 소개",
    "section": "Specific symbol .N, .SD, .SDcols",
    "text": "Specific symbol .N, .SD, .SDcols\n\n## data.table(Specific symbol .N, .SD, .SDcols)\ndt[, .SD]\n\ndt[, lapply(.SD, class)]\n\ndt[, .N, keyby = \"RN_INDI\"]\n\n## collapse\nfselect(dt,1:32)\n\ndapply(dt,class)\n\nadd_stub(count(dt,RN_INDI),\"N\")\n\n\n두번째 코드를 비교해보겠습니다. 비슷하지만 출력하는 형태가 다릅니다. class()를 사용하여 확인해보면 형태가 다른 것을 알 수 있습니다.\n\n\ndt[, lapply(.SD, class)]\n\n\n\n\n  \n\n\n\n\ndapply(dt,class)\n\n\n\n\n  \n\n\n\n\nclass(dt[, lapply(.SD, class)])\n\n[1] \"data.table\" \"data.frame\"\n\nclass(dapply(dt,class))\n\n[1] \"character\""
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#order",
    "href": "posts/2022-07-25-collapse/index.html#order",
    "title": "collapse 패키지 소개",
    "section": "order",
    "text": "order\n\n#data.table(order)\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n#collapse(order)\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI)\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(-BMI)\n\n\n첫번째 코드를 통해 data.table과 collapse 출력 결과를 확인해보겠습니다.\n\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n  \n\n\n\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 행 연산",
    "text": "기본 행 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[1:10] ,\n               collapse = fsubset(dt, 1:10))\n\nWarning in microbenchmark(data.table = dt[1:10], collapse = fsubset(dt, : less\naccurate nanosecond times to avoid potential integer overflows\n\n\nUnit: microseconds\n       expr    min      lq     mean  median     uq     max neval cld\n data.table 43.091 43.8905 46.85316 44.4645 45.756 173.512   100   b\n   collapse  5.084  5.6990  6.12294  5.9655  6.232  15.703   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)],\n               collapse = fsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25 ))\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval cld\n data.table 67.732 70.6225 74.87338 72.2830 74.8455 196.636   100   b\n   collapse 25.994 29.4995 31.62248 30.4835 32.1235  48.339   100  a \n\nmicrobenchmark(data.table = dt[order(HME_YYYYMM, -HGHT)],\n               collapse = roworder(dt, HME_YYYYMM, -HGHT))\n\nUnit: microseconds\n       expr     min       lq      mean  median       uq      max neval cld\n data.table 136.653 142.5365 190.75250 148.543 154.8775 3990.366   100   b\n   collapse  56.744  62.3405  66.85255  65.764  68.7570  109.429   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25)][order(HGHT)],\n               collapse = roworder(dt, HGHT) %&gt;% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25))\n\nUnit: microseconds\n       expr     min       lq     mean   median       uq      max neval cld\n data.table 131.200 135.2795 144.9817 139.8920 142.9875  347.475   100   a\n   collapse  62.402  68.2445 110.8259  75.7065  78.4330 3636.495   100   a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 열 연산",
    "text": "기본 열 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, 1:10],\n               collapse = fselect(dt, 1:10))\n\nUnit: microseconds\n       expr    min      lq     mean  median     uq     max neval cld\n data.table 32.472 33.3945 37.27023 34.8705 37.720 112.258   100   b\n   collapse  3.649  4.0590  4.56576  4.3460  4.674  18.286   100  a \n\nmicrobenchmark(data.table = dt[, .(Height = HGHT, Weight = WGHT)],\n               collapse = fselect(dt, Height = HGHT, Weight = WGHT))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 145.837 149.6705 156.56383 152.2945 156.333 334.888   100   b\n   collapse   4.510   5.1455   6.05652   5.8630   6.560  18.040   100  a \n\n#base::grep() more faster \nmicrobenchmark(data.table = colvars &lt;- grep(\"Q_\", names(dt), value = T),\n               collapse = colvars &lt;-get_vars(dt, \"Q_\",regex = TRUE, return = \"names\"))\n\nUnit: microseconds\n       expr   min    lq    mean median     uq    max neval cld\n data.table 4.633 4.715 4.82078  4.715 4.8380  8.405   100  a \n   collapse 5.658 5.781 6.05119  5.863 5.9655 22.058   100   b\n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = get_vars(dt, colvars))\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval cld\n data.table 31.570 32.3695 34.59252 33.1895 34.0505 121.401   100   b\n   collapse  2.501  2.9520  4.11599  3.2390  3.4645  85.813   100  a \n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = fselect(dt, colvars) )\n\nUnit: microseconds\n       expr    min     lq     mean median      uq     max neval cld\n data.table 31.980 32.636 34.10544 33.087 33.7020 101.844   100   b\n   collapse  4.141  4.879  5.43537  5.248  5.5555  21.566   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI &gt;= 25), ..colvars],\n               collapse = fsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI &gt;= 25, colvars))\n\nUnit: microseconds\n       expr    min     lq     mean median      uq     max neval cld\n data.table 72.775 74.333 78.30959 75.809 78.7200 204.959   100   b\n   collapse 23.575 24.764 25.98047 25.133 26.2605  46.125   100  a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#평균-연산",
    "href": "posts/2022-07-25-collapse/index.html#평균-연산",
    "title": "collapse 패키지 소개",
    "section": "평균 연산",
    "text": "평균 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(mean(HGHT), mean(WGHT), mean(BMI))],\n               collapse = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 174.414 179.0265 184.97683 181.1175 184.336 386.015   100   b\n   collapse   7.216   7.9335   8.94415   8.6920   9.266  35.014   100  a \n\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))],\n               collaspe = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq     max neval cld\n data.table 170.027 172.1180 178.54762 174.1475 177.3045 343.211   100   b\n   collaspe   7.380   8.0975   9.17539   8.9585   9.4915  31.816   100  a \n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)],\n               collapse = dapply(fselect(dt,HGHT,WGHT,BMI),fmean))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq     max neval cld\n data.table 176.710 179.8260 185.93008 181.5275 185.0740 321.686   100   b\n   collapse  13.858  14.6575  16.01911  15.4775  16.6255  35.178   100  a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "href": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "title": "collapse 패키지 소개",
    "section": "그룹별 통계 연산",
    "text": "그룹별 통계 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY],\n               collapse.collap = collap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16)),\n               collapse.fmean = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n            expr     min       lq      mean   median       uq      max neval\n      data.table 203.442 210.6580 217.08270 214.5325 218.3660  366.704   100\n collapse.collap  57.728  62.7505  76.49821  65.6000  68.6340 1098.431   100\n  collapse.fmean  36.367  37.3510  38.55107  38.2120  39.0935   55.350   100\n cld\n   c\n  b \n a  \n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 205.984 209.0180 214.88469 210.9245 215.783 404.096   100   b\n   collapse  35.834  36.8795  38.00495  37.4740  38.294  62.853   100  a \n\n#data.table more faster\nmicrobenchmark(data.table = dt[HGHT &gt;= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)],\n               collapse = add_stub(count(fsubset(dt, HGHT &gt;= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\") )\n\nUnit: microseconds\n       expr      min        lq      mean    median       uq      max neval cld\n data.table  243.868  269.1855  316.4114  303.0925  348.951  543.619   100  a \n   collapse 2144.587 2261.2935 2491.8049 2338.5375 2538.556 5655.335   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "href": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "title": "collapse 패키지 소개",
    "section": "특수 심볼",
    "text": "특수 심볼\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .SD],\n               collapse = fselect(dt,1:32))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 211.888 216.0290 230.11783 223.3270 231.158 388.516   100   b\n   collapse   4.100   4.5715   5.47678   5.1455   5.576  23.124   100  a \n\nmicrobenchmark(data.table = dt[, lapply(.SD, class)],\n               collapse = dapply(dt,class))\n\nUnit: microseconds\n       expr     min       lq      mean  median       uq     max neval cld\n data.table 450.303 461.5165 489.46415 483.144 499.6875 656.574   100   b\n   collapse   7.421   8.1385   9.17088   9.020   9.5325  16.441   100  a \n\n#data.table more faster\nmicrobenchmark(data.table = dt[, .N, keyby = \"RN_INDI\"],\n               collapse = add_stub(count(dt,RN_INDI),\"N\"))\n\nUnit: microseconds\n       expr      min        lq      mean   median        uq      max neval cld\n data.table  195.365  217.2385  253.8142  251.863  277.6725  382.120   100  a \n   collapse 3653.633 3710.2540 3957.8870 3780.897 3938.3780 6161.275   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "href": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "title": "collapse 패키지 소개",
    "section": "정렬 연산",
    "text": "정렬 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(BMI))\n\nUnit: microseconds\n       expr     min      lq      mean   median       uq     max neval cld\n data.table 273.470 279.825 288.99301 284.1505 289.3165 523.980   100   b\n   collapse  42.025  44.403  47.12991  46.7810  47.9085  92.865   100  a \n\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %&gt;% roworder(-BMI))\n\nUnit: microseconds\n       expr     min      lq      mean   median      uq     max neval cld\n data.table 276.135 282.695 289.30994 285.7085 289.378 488.474   100   b\n   collapse  44.854  47.150  50.61163  49.3640  51.045 115.169   100  a"
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html",
    "href": "posts/2022-03-19-statreview/index.html",
    "title": "Reviewer들을 위한 의학통계",
    "section": "",
    "text": "김진섭 대표는 4월 8일(금) 제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 주제로 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#요약",
    "href": "posts/2022-03-19-statreview/index.html#요약",
    "title": "Reviewer들을 위한 의학통계",
    "section": "요약",
    "text": "요약\nTable 1\n\n연속변수 2그룹: 정규분포 t-test, 아니면 Wilcox-test\n연속변수 3그룹이상: 정규분포 ANOVA, 아니면 Kruskal–Wallis ANOVA\n범주형 변수: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n\n회귀분석\n\nUnivariate, multivariate 같이 보여주기, Subgroup 분석 추천\nStepwise selection 비추천: 예측모형 목적 아님, 임상맥락 고려X\n\n생존분석\n\nKaplan-meier 그림선 겹치면 안됨: Time stratification 필요\n보정할 변수가 Index time 이후면 안됨: Time-dependent covariate 필요\nPropensity score 매칭 후 pair 고려한 stratified cox 는 필수아님\n\n국민건강영양조사\n\n표본추출정보를 고려한 통계분석: Survey table1/GLM/Cox"
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#slide",
    "href": "posts/2022-03-19-statreview/index.html#slide",
    "title": "Reviewer들을 위한 의학통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/statreview 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html",
    "href": "posts/2023-04-21-sunburst/index.html",
    "title": "sunburstr 패키지 소개",
    "section": "",
    "text": "R에서 선버스트 차트를 그리는 방법은 다양하다. 그중에서 sunburstR 패키지가 도출 결과가 매우 깔끔하고, shiny에서도 실행이 가능하며 plotly나 ggplot을 사용하는 것보다 편리하다 느껴 이 글에서 소개하려고 한다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#선버스트-차트란",
    "href": "posts/2023-04-21-sunburst/index.html#선버스트-차트란",
    "title": "sunburstr 패키지 소개",
    "section": "1. 선버스트 차트란?",
    "text": "1. 선버스트 차트란?\n계층형 데이터 구조를 시각화할 때 이상적인 선버스트 차트는 계층 구조를 갖는 외부원과 내부원의 관계를 쉽게 보여주는 차트이다.\n\n\n\n\n\n\n\n\nsunburst chart\n\n\n\n\n\nhierarchical data\n\n\n\n\nimage from syncfusion*image from earo\n\n하나의 고리 또는 원이 계층 구조의 각 수준을 나타내며, 가장 안쪽에 있는 원이 계층 구조의 가장 높은 수준을 나타낸다. 복잡한 계층적 데이터를 명확하고 매력적인 방식으로 이해할 수 있는 능력에 있다.\n언제 사용하면 될까?\n\n전체 항목 중에서 개별 항목이 차지하는 비중을 파악하고 싶을 때\n\n\n트리맵 차트와 유사하게 데이터가 차지하는 비율을 면적으로 표현한 그래프이기 때문에 비중을 파악하기 용이하다.\n\n데이터의 패턴 및 관계를 나타낸다.\n\n일례로, 가장 많은 하위 범주를 가진 범주나 가장 인기 있는 하위 범주를 빠르게 파악할 수 있다.\n\n\n\n\n위계 구조를 가지는 데이터들의 패턴을 빠르게 확인하고 싶을 때\n\n\n데이터의 개별 요소들이 소속된 계층이 있는 경우 패턴 파악이 용이하다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#sunburstr-패키지",
    "href": "posts/2023-04-21-sunburst/index.html#sunburstr-패키지",
    "title": "sunburstr 패키지 소개",
    "section": "2. SunburstR 패키지",
    "text": "2. SunburstR 패키지\nR에 있는 sunburstr 패키지를 사용해 직접 계층적 데이터의 선버스트 차트를 만들어 보자.\n2.1. Setup\nsunburstr은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n\n# install.packages(\"data.table\")\n# install.packages(\"sunburstr\")\n\nlibrary(sunburstR);library(data.table);library(htmltools);library(magrittr)\n\n위의 과정을 통해 package 설치 및 불러오기를 실행합니다.\n2.2. Load data\n예제 데이터로는 2 단계에 걸쳐서 약을 복용한 149명 환자의 데이터를 사용한다.\n\n#load data\ndata &lt;- fread(\"https://raw.githubusercontent.com/seodaegal/blog-sy/main/blog_example_data/sunburst_example.csv\")\n\nrmarkdown::paged_table(data)\n\n\n  \n\n\n\n\n\ndata에서 Step.1은 첫 단계에서 복용한 약의 종류, Step.2는 첫 단계 이후에 두 번째로 복용한 약의 종류이다.\npersonCount는 특정 약을 복용한 사람의 수를 의미한다.\n2.3. Fix data\n데이터를 불러온 후, sunburstr 패키지를 적용하기 위해서는 sunburstr에 맞게 데이터를 재정렬해야 한다.\n\n\ndata 에는 단계마다 (Step.1, Step.2) 복용한 약이 한 개가 아니라 여러 개인 경우도 있어 각 단계 안에서도 복용한 약을 따로 분리해 주자.\n\n\n# size는 personCount로 지정\n\ndata2 &lt;-data[, names(data) := lapply(.SD, function(x) gsub(\"\\\\+\", \"-\", x)), .SDcols = names(data)]%&gt;%\n  .[, .(size= personCount, Step.1=Step.1, Step.2=Step.2)]%&gt;% \n  .[, c(\"st1level2\", \"st1level1\") := tstrsplit(Step.1, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;% #Step.1에서 복용한 약 종류는 2개라서, 2개의 다른 열로 나눈다\n  .[is.na(st1level1), st1level1 := st1level2]%&gt;% # 복용한 약의 종류가 하나라면 두번째 열에 똑같은 약을 입력하도록 한다다\n  .[, Step.1:= NULL]%&gt;%\n  .[, c(\"st2level1\", \"st2level2\", \"st2level3\") := tstrsplit(Step.2, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;% #Step.2에서 복용한 약 종류는 3개라서, 3개의 다른 열로 나눈다\n  .[is.na(st2level2), st2level2 := st2level1]%&gt;% \n  .[is.na(st2level3), st2level3 := st2level2] %&gt;% # 복용한 약이 3개가 아닐 경우 그전에 복용했던 약을 입력\n  .[,Step.2:=NULL]%&gt;%\n  setcolorder(., c(\"st1level1\", \"st1level2\", \"st2level1\", \"st2level2\",\"st2level3\", \"size\"))%&gt;% .[]\n\n# setcolorder( ) 함수로 열의 순서를 정한다\n\n\nrmarkdown::paged_table(data2)\n\n\n  \n\n\n\ndata2 에서\n\nst1 은 Step.1, st2 는 Step.2 에서 복용한 약을 의미한다.\n\nsize 는 해당 약을 복용한 personCount 이다.\n 열의 순서가 hierarchy에 영향을 미치기 때문에, 열의 순서를 ordering 하는데 주의가 필요하다!\n\n계층 구조가 높을수록 열의 순서가 왼쪽으로 오게 설정해야 한다.\n\n\n2.4. Building hierarchy on data\nd3r 패키지를 사용해 sunburstr에 적합한 환자들의 약 복용 데이터의 계층 구조를 구축한다. d3_nest 함수로 높은 순서대로 정리된 data2의 변수들을 그룹화한 뒤 데이터 갯수 변수 (count)를 지정한다.\n\n#datatable을 hierarchy로 convert\nlibrary(d3r)\nhierarchy &lt;- d3_nest(data2, value_cols = \"size\")\n\nhierarchy\n\n{\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"8\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Triflusal\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"61\",\"colname\":\"st1level2\"},{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"7\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"size\":\"1\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"3\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"36\",\"colname\":\"st1level2\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"cilostazol\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"4\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Aspirin\",\"size\":\"5\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"cilostazol\",\"size\":\"2\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"size\":\"11\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"cilostazol\",\"children\":[{\"name\":\"cilostazol\",\"children\":[],\"size\":\"6\",\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"},{\"name\":\"Triflusal\",\"children\":[{\"name\":\"Triflusal\",\"children\":[{\"name\":\"Clopidogrel\",\"children\":[{\"name\":\"Aspirin\",\"children\":[{\"name\":\"Triflusal\",\"size\":\"1\",\"colname\":\"st2level3\"}],\"colname\":\"st2level2\"}],\"colname\":\"st2level1\"}],\"colname\":\"st1level2\"}],\"colname\":\"st1level1\"}],\"name\":\"root\"} \n\n\n2.5. Sunburstr 패키지 사용\nsunburstr 패키지를 사용해 차트의 색깔, 크기 등등을 바꿀 수 있다.\n\n#colors\ncolors&lt;- c('#FFAA00', '#2D5F91','#91D4D2', '#E8655F')\nlabels &lt;- c(\"KYR_Aspirin_20230414\", \"KYR_Clopidogrel_20230414\", \"KYR_cilostazol_20230414\",  \"KYR_Triflusal_20230414\")\n\n# setting data as 'hierarchy', then setting legend, width, height, count\n\nsb&lt;-sunburst(\n  hierarchy,\n  legend = TRUE,\n  width = \"100%\",\n  colors= list(range = colors, domain = labels),\n  height = 400,\n  count= TRUE\n)\n\nsb\n\n\n\n\n\n\n\n\n\n\nLegend\n\n\n\n\n\n\n\n차트의 오른쪽 위에 Legend를 누르면 색깔별 약 종류에 대해서 볼 수 있다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#shiny에-sunburstr-사용하기",
    "href": "posts/2023-04-21-sunburst/index.html#shiny에-sunburstr-사용하기",
    "title": "sunburstr 패키지 소개",
    "section": "3. Shiny에 sunburstr 사용하기",
    "text": "3. Shiny에 sunburstr 사용하기\nShiny에 sunburstr를 적용하는 방법은 여기에 잘 설명 되어있다. Shiny의 기초는 여기를 참고해주세요.\n Shiny의 목적은 유저가 R을 사용하지 않고도 제공된 UI를 통해 통계 분석을 (지금의 경우 선버스트 차트와 데이터 테이블을 보여주는) 진행할 수 있도록 하는 것이다.\n위에서 배운 sunburstr을 이용해 shiny를 만들어 보자.\n3.1. Shiny setup\n우선 R studio에서 New file에서 Shiny Web app을 선택한 뒤, Single File (app.R)을 생성한다. 필요한 패키지들을 불러오고 위에서 정리한 데이터들을 가져오기 위해, 코드를 global 파일에 저장해 두자.\n새 Shiny 파일 생성\n\n\napp.R에 필요한 library와 global R code 불러오기\n\nlibrary(shiny);library(sunburstR);library(data.table);library(DT)\nsource(\"global.R\")\n\n\nglobal.R 에는 아래 코드 작성:\n\n#in global.R\n\ndata &lt;- fread(\"https://raw.githubusercontent.com/seodaegal/blog-sy/main/blog_example_data/sunburst_example.csv\")\ndata2 &lt;-data[, names(data) := lapply(.SD, function(x) gsub(\"\\\\+\", \"-\", x)), .SDcols = names(data)]%&gt;%\n  .[, .(size= personCount, Step.1=Step.1, Step.2=Step.2)]%&gt;%\n  .[, c(\"st1level2\", \"st1level1\") := tstrsplit(Step.1, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;%\n  .[is.na(st1level1), st1level1 := st1level2]%&gt;%\n  .[, Step.1:= NULL]%&gt;%\n  .[, c(\"st2level1\", \"st2level2\", \"st2level3\") := tstrsplit(Step.2, \" - \", fixed = TRUE, type.convert = FALSE)]%&gt;%\n  .[is.na(st2level2), st2level2 := st2level1]%&gt;%\n  .[is.na(st2level3), st2level3 := st2level2] %&gt;%\n  .[,Step.2:=NULL]%&gt;%\n  setcolorder(., c(\"st1level1\", \"st1level2\", \"st2level1\", \"st2level2\",\"st2level3\", \"size\"))%&gt;% .[]\n\n\nlibrary(d3r)\nhierarchy &lt;- d3_nest(data2, value_cols = \"size\")\n\n#colors\ncolors&lt;- c('#FFAA00', '#2D5F91','#91D4D2', '#E8655F')\nlabels &lt;- c(\"KYR_Aspirin_20230414\", \"KYR_Clopidogrel_20230414\", \"KYR_cilostazol_20230414\",  \"KYR_Triflusal_20230414\")\n\n3.2. Table panel\n우리가 보여주고 싶은 Web App은 Table Panel이 2 개이다: 데이터 테이블과 선버스트.\n\n우선 navbar을 이용해 두 개의 table panel을 생성해보자.\n\n# using navbar to create table panel\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\"),\n                 tabPanel(\"Sunburst\")\n)\n\n\nserver &lt;- function(input, output) {\n\n}     \n\nnavbarPage( ) 함수를 통해 Title name과 table panel 이름들을 지정해준다.\n▶️Run App을 실행하면 아래와 같이 탭 3 개가 생긴 것을 확인할 수 있다.\n\n3.3. Data Panel 설정\n\n생성한 Data Panel에서는 메인 패널에 기존 약을 복용한 환자들의 예제 data set을 보여주면 된다.\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\",\n                          mainPanel(\n                            DTOutput(\"data\")\n                          )\n                 ),\n                 tabPanel(\"Plot\")\n)\n\nserver &lt;- function(input, output) {\n  \n  output$data &lt;- renderDT({\n    DT::datatable(data, rownames = F,  caption = \"Pathway 리포트\")\n  })\n}\n\n▶️Run App을 실행하면 데이터 테이블이 나오는 것을 확인할 수 있다.\n\n3.4. Plot Panel 설정\nPlot panel에서는 선버스트 차트를 보여주면 된다. 마우스를 차트 위에 올렸을 때, 약의 종류, 퍼센트 그리고 count가 나오도록 설정해야 한다.\n\nui &lt;- navbarPage(\"SUNBURST\",\n                 tabPanel(\"Data\",\n                          mainPanel(\n                            DTOutput(\"data\")\n                          )\n                 ),\n                 tabPanel(\"Plot\",\n                          mainPanel(\n                            sunburstOutput(\"sunburst\"),\n                            textOutput(\"selection\")\n                          )\n                 )\n)\n\n선버스트 차트를 output으로 보여줄 것이기 때문에 sunburstOutput( ) 함수를 사용한다. textOutput( )은 selection을 문자열 인수로 설정한다.\n\nserver &lt;- function(input, output) {\n  \n  output$data &lt;- renderDT({\n    DT::datatable(a, rownames = F,  caption = \"Pathway 리포트\")\n  })\n  \n  output$sunburst &lt;- renderSunburst({\n    add_shiny(sunburst(hierarchy, legend = FALSE, width = \"100%\", colors= list(range = colors, domain = labels), height = 400, count= TRUE))\n  })\n  \n  selection &lt;- reactive({\n    input$sunburst_mouseover\n  })\n  output$selection &lt;- renderText(selection())\n  \n}\n\nshinyApp(ui = ui, server = server)\n\nadd_shiny( ) 함수를 통해 sunburst 차트를 shiny 앱에 추가한다. selection은 input$sunburst_mouseover 값에 반응하여 업데이트되도록 설정되어 있다. output$selection 은 sunburst 차트에서 유저가 선택한 항목의 이름을 출력한다.\n▶️Run App을 실행하면 선버스트 차트가 잘 나오는 것을 확인할 수 있다."
  },
  {
    "objectID": "posts/2023-04-21-sunburst/index.html#마치며",
    "href": "posts/2023-04-21-sunburst/index.html#마치며",
    "title": "sunburstr 패키지 소개",
    "section": "마치며",
    "text": "마치며\n지금까지 sunburstr 패키지 사용법을 알아보고 Shiny에서 직접 작동시켜 Web App을 제작해 보았다. 선버스트 차트는 계층적 데이터를 시각화하는데 강력한 도구이지만, 사용할 때 고려해야 할 부분들도 있다. 일례로, 선버스트의 세그먼트가 많아지거나 레이블이 길어지는 경우 가독성이 떨어질 수 있다. 또, 각도를 이용해 각 그룹의 크기를 나타내는 특성상, 작은 각도 차이도 큰 크기 차이를 나타낼 수 있어 그룹 간 비교가 다소 부정확할 수 있다. 선버스트 차트의 이러한 사항들을 고려하고 필요에 따라 대안적인 시각화 방법을 탐색하는 것도 필요 할 것 같다.\n\nReferences\n\n\n“Sunburst 2.0.0.” Sunburst 2.0.0 • sunburstR, 5 Feb. 2023, timelyportfolio.github.io/sunburstR/articles/sunburst-2-0-0.html.\n“Add Shiny Events — Add_Shiny.” Add Shiny Events — Add_Shiny • sunburstR, timelyportfolio.github.io/sunburstR/reference/add_shiny.html."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html",
    "href": "posts/2020-06-20-shinymed2020/index.html",
    "title": "2020년 만들었던 ShinyApps",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 6월 Shinykorea 밋업에 참석, 올해 만들었던 ShinyApps 를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#요약",
    "href": "posts/2020-06-20-shinymed2020/index.html#요약",
    "title": "2020년 만들었던 ShinyApps",
    "section": "요약",
    "text": "요약\n\n대한심혈관중재학회 COBIS III 레지스트리 분석: 추가계약\n서울성모병원 COREA-AMI II 레지스트리 분석: 10개 연구 계약\n삼성서울병원 공통데이터모델(CDM) 분석: 심평원 코로나데이터 분석 중\n강동성심병원 위암 위험인자 분석: 공단표본데이터 분석 중\n경기도감염병관리지원단 코로나 대시보드 with Shinykorea: 최종보고\n삼성서울병원 이식외과 육종(sarcoma) 데이터 분석: 5개 연구 계약\n해운대백병원 정신질환 네트워크분석: 논문 4편 게재\n성균관의대 환경역학연구실 미세먼지 대시보드\nShiny로 연구용 환자정보 입력웹(Electronic Case Report Forms, eCRF) 만들어 분석모듈 앞에 붙이고 싶습니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#slide",
    "href": "posts/2020-06-20-shinymed2020/index.html#slide",
    "title": "2020년 만들었던 ShinyApps",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/shinymed2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html",
    "href": "posts/2023-10-19-quarto-manuscript/index.html",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "",
    "text": "지난 9월 R Studio 2023.09.0 버전이 공개되었습니다. 새롭게 추가된 여러 기능 중에서 .qmd 파일을 이용해 학술 논문을 작성하고 웹페이지로 발행할 수 있는 Quarto Manuscript Project를 소개합니다. 본 게시글은 Quarto manuscript 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#개요",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#개요",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "",
    "text": "지난 9월 R Studio 2023.09.0 버전이 공개되었습니다. 새롭게 추가된 여러 기능 중에서 .qmd 파일을 이용해 학술 논문을 작성하고 웹페이지로 발행할 수 있는 Quarto Manuscript Project를 소개합니다. 본 게시글은 Quarto manuscript 공식 문서를 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript-소개",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript-소개",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "Quarto Manuscript 소개",
    "text": "Quarto Manuscript 소개\nQuarto Manuscript는 .qmd 파일을 통해 학술 논문을 작성할 수 있는 프로젝트로, 다음과 같은 장점을 갖습니다.\n\n그림, 표, 수식, 인용 정보, 코드 블록 등 학술 논문에 필요한 모든 요소를 포함한 원고를 작성할 수 있습니다.\n작성한 원고를 웹사이트로 발행할 수 있어 공유가 쉽습니다.\n원고를 PDF, docx, 압축 파일 등 다양한 형식으로 손쉽게 다운로드할 수 있습니다.\n\nQuarto Manuscript를 활용해 작성한 학술 논문 예시는 여기에서 확인할 수 있습니다. 이 예시를 참고해 Quarto manuscript 원고를 작성하고 웹사이트로 발행해보겠습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript로-학술-논문-작성하기",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#quarto-manuscript로-학술-논문-작성하기",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "Quarto Manuscript로 학술 논문 작성하기",
    "text": "Quarto Manuscript로 학술 논문 작성하기\n1. 준비하기\nQuarto Manuscript를 사용하기에 앞서 (1) 2023.09 버전 이상의 RStudio와 (2) 1.4 버전 이상의 Quarto를 설치해야 합니다.\n2. 원고 작성하기\nRStudio에서 New project &gt; New Directory &gt; Quarto Manuscript를 선택해 프로젝트를 생성합니다.\n학술 논문 원고는 index.qmd 파일에서 작성합니다. 우선 작성할 논문의 제목, 저자, 키워드, 초록과 같은 정보를 아래 예시와 같이 YAML header에 입력합니다.\n\n\n\nindex.qmd\n\n# YAML head 작성 예시\n\n---\ntitle: La Palma Earthquakes\nauthor:\n  - name: Steve Purves\n    orcid: 0000-0002-0760-5497\n    corresponding: true\n    email: steve@curvenote.com\n    roles:\n      - Investigation\n      - Project administration\n      - Software\n      - Visualization\n    affiliations:\n      - Curvenote\n  - name: Rowan Cockett\n    orcid: 0000-0002-7859-8394\n    corresponding: false\n    roles: []\n    affiliations:\n      - Curvenote\nkeywords:\n  - La Palma\n  - Earthquakes\nabstract: |\n  In September 2021, a significant jump in seismic activity on the island of La Palma (Canary Islands, Spain) signaled the start of a volcanic crisis that still continues at the time of writing. Earthquake data is continually collected and published by the Instituto Geográphico Nacional (IGN). ...\nplain-language-summary: |\n  Earthquake data for the island of La Palma from the September 2021 eruption is found ...\nkey-points:\n  - A web scraping script was developed to pull data from the Instituto Geogràphico Nacional into a machine-readable form for analysis\n  - Earthquake events on La Palma are consistent with the presence of both mantle and crustal reservoirs.\ndate: last-modified\nbibliography: references.bib\ncitation:\n  container-title: Earth and Space Science\nnumber-sections: true\n---\n\n\nYAML header 아래에 본문을 작성합니다. 본문은 Quarto markdown 형식으로 작성하며, 기본적인 사용법은 Quarto Markdown Basics 공식 문서 혹은 R markdown 기초를 다루었던 이전 게시글에서 참고할 수 있습니다. 본 게시글에서는 논문 작성에 필요한 몇 가지 기능을 중점적으로 살펴보겠습니다.\n2.1. figure 삽입하기\nr 코드 블록을 활용해 figure를 삽입할 수 있습니다. 아래 예시와 같이 figure을 생성하는 코드와 함께 figure의 label, caption, alt text, width, height 등을 입력합니다. 본문에는 figure와 캡션만 표시되고 코드는 Article Notebook에서 모아 볼 수 있습니다. 본문에서 @fig-timeline와 같이 @ 기호와 figure의 label을 입력하면 figure을 인용할 수 있습니다.\n\n\n\nindex.qmd\n\n# figure 작성 예시\n\n# ```{r}\neruptions &lt;- c(1492, 1585, 1646, 1677, 1712, 1949, 1971, 2021)\nn_eruptions &lt;- length(eruptions)\n# ```\n\n# ```{r}\n#| label: fig-timeline\n#| fig-cap: Timeline of recent earthquakes on La Palma\n#| fig-alt: An event plot of the years of the last 8 eruptions on La Palma.\n#| fig-height: 1.5\n#| fig-width: 6\npar(mar = c(3, 1, 1, 1) + 0.1)\nplot(eruptions, rep(0, n_eruptions), \n  pch = \"|\", axes = FALSE)\naxis(1)\nbox()\n# ```\n\n# ```{r}\n#| output: false\navg_years_between_eruptions &lt;- mean(diff(eruptions[-n_eruptions]))\navg_years_between_eruptions\n#```\n\n\n2.2. 다른 .qmd 파일에 작성한 plot 삽입하기\nSection 3.2.1 에서와 같이 간단한 그림이 아닌, 연구에서 사용된 데이터를 통해 plot을 그리고 이를 본문에 삽입하기 위해서는 디렉토리에 notebooks 폴더를 생성해야 합니다. 생성한 notebooks 폴더 안에 데이터 파일을 넣고 새로운 .qmd 파일을 만들어 plot을 그리는 r 코드 블록을 작성합니다.\n이후 index.qmd 파일로 돌아와 아래와 같은 코드를 작성하면 plot이 삽입됩니다.\n\n\n\nindex.qmd\n\n# explore-earthquakes.qmd 파일에 작성한 fig-spatial-plot 삽입 예시\n\n{{&lt; embed notebooks/explore-earthquakes.qmd#fig-spatial-plot &gt;}}\n\n\nnotebooks 폴더의 .qmd 파일에서 작성한 코드는 마찬가지로 Article Notebook에서 모아 볼 수 있습니다.\n2.3. 참고문헌 작성하기\n참고문헌은 아래 예시와 같이 references.bib 파일에 BibTeX 형태로 작성합니다. index.qmd 파일의 본문에서 @marrero2019와 같이 @ 기호와 참고문헌의 label을 입력하면 참고문헌을 인용할 수 있습니다.\n\n\n\nreferences.bib\n\n# 참고문헌 작성 예시\n\n@article{marrero2019,\n  author = {Marrero, Jos{\\' e} and Garc{\\' i}a, Alicia and Berrocoso, Manuel and Llinares, {\\' A}ngeles and Rodr{\\' i}guez-Losada, Antonio and Ortiz, R.},\n  journal = {Journal of Applied Volcanology},\n  year = {2019},\n  month = {7},\n  pages = {},\n  title = {Strategies for the development of volcanic hazard maps in monogenetic volcanic fields: the example of {La} {Palma} ({Canary} {Islands})},\n  volume = {8},\n  doi = {10.1186/s13617-019-0085-5},\n}\n\n\n2.4. journal template 적용하기\njournal template은 PDF에 적용됩니다. .qmd 파일을 PDF 파일로 내보내려면 tinytex 패키지가 설치되어 있어야 합니다. 아래 코드를 통해 패키지를 설치하겠습니다.\n\n\n\nconsole\n\n# tinytex 설치 후 로드\n\ninstalled.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nlibrary(tinytex)\n\n\n\n\n\nconsole\n\n# 패키지 설치 확인(True가 출력되면 성공적으로 설치된 것임)\n\ntinytex:::is_tinytex()\n\n\njournal format을 추가하기 위해 Quarto Extensions: Journal Articles를 참고해 터미널에 아래 코드를 실행시켜 익스텐션을 설치합니다.\n\n\n\nterminal\n\n# acs format 설치 예시\n\nquarto install extension quarto-journals/acs\n\n\n이후 _quarto.yml 파일의 format:에 acs-pdf: default를 추가하면 acs format이 적용됩니다.\n웹페이지를 발행한 뒤 PDF 파일을 다운로드하면 아래와 같이 journal format이 적용된 것을 확인할 수 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(좌) acs format, (우) elsevier format 적용 예시\n3. 깃허브 페이지를 통해 웹으로 발행하기\n원고를 깃허브 페이지를 통해 웹으로 발행하기 위해 _quarto.yml파일의 project:에 output-dir: docs 설정을 추가합니다. 이후 index.qmd 파일으로 돌아와 터미널에 quarto render을 입력합니다. 모든 변경사항을 깃허브에 commit 후 push 하면, 원고가 깃허브 페이지를 통해 웹으로 발행됩니다. 깃허브의 해당 레포지토리의 settings &gt; pages에서 웹페이지 링크를 확인할 수 있습니다."
  },
  {
    "objectID": "posts/2023-10-19-quarto-manuscript/index.html#마치며",
    "href": "posts/2023-10-19-quarto-manuscript/index.html#마치며",
    "title": "Quarto Manuscripts를 이용해 학술 논문 작성하기",
    "section": "마치며",
    "text": "마치며\n이번 게시글에서는 새롭게 공개된 Quarto Manuscript를 통해 학술 논문 원고를 작성하고 깃허브 페이지를 통해 웹으로 발행하는 방법을 알아보았습니다. Quarto Manuscript를 통해 논문 작성과 공유가 더욱 편리하게 이루어지기를 기대하며 글을 마칩니다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "",
    "text": "김진섭 대표는 11월 7일 동국대학교 의생명공학과 세미나에 참석, 의료 데이터분석가가 되기까지의 경험을 학생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유하며, 초청해주신 김진식 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 의대 진학.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립.\n\nR 과 shiny 이용, 맞춤형 분석웹 제공\n범용 통계분석 웹: http://app.zarathu.com/\nR 패키지 개발"
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/dongguk_mbt 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html",
    "href": "posts/2019-05-20-godelincompleteness/index.html",
    "title": "괴델의 불완전성 정리",
    "section": "",
    "text": "김진섭 대표는 5월 30일(목) 제주대학교 경영정보학과 산업·직무 특화 전문가 특강에 참석, 괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명할 예정입니다. 정리한 슬라이드와 강의록을 미리 공유합니다. 초청해주신 현정석 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "href": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "title": "괴델의 불완전성 정리",
    "section": "요약",
    "text": "요약\n\n18/19 세기 미적분학, 해석학의 발전으로, 수학은 점점 기존의 직관과 상식에서 벗어나 추상화되면서 많은 문제점들이 생겼다.\n특히 무한의 개념을 엄밀하게 다룰 필요성이 있었는데, 칸토어(Georg Cantor)는 집합론의 논법으로 무한을 엄밀하게 정의하고 그것들의 크기를 비교하였다.\n20세기 수학자들은 집합론을 이용, 수학의 기초를 구성하고 모순이 없는 수학체계를 만들 수 있다는 꿈으로 부풀어 있었으나, 러셀의 역설(Bertrand Russel’s paradox)로 대표되는 “자기언급의 역설”은 집합론의 기초를 위태롭게 하였다.\n힐베르트(David Hilbert)는 “자기언급의 역설” 은 수학의 명제와 메타수학(meta-mathematics)의 명제를 구분하지 않아 일어나는 것으로 판단하였으며, 이를 잘 구분하는 공리계를 세심하게 설계할 수만 있다면 모순없는 수학체계를 만들 수 있다고 보았다.\n그러나 괴델(Kurt Gödel)은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 즉, “자기언급의 역설” 을 피하기 위해 아무리 세심하게 공리계를 설계해도 그것을 피할 수는 없다는 것을 증명하였으며, 이것이 불완전성 정리이다.\n불완전성 정리로 인해 “참이지만 증명불가능한 명제가 존재” 하고 나아가 “수학의 무모순성을 수학 자체적으로는 증명할 수 없음” 이 증명되어 모순없는 완전한 수학체계의 꿈은 결국 산산조각이 난다.\n불완전성 정리를 인간 이성의 한계로만 해석하고 실의에 빠지지 말자. 어떤 기계보다도 더 복잡하고 정교한 인간의 정신구조와 능력을 긍정하며 창조적 이성의 힘을 인정할 때이다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "href": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "title": "괴델의 불완전성 정리",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureGodel/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "href": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "title": "괴델의 불완전성 정리",
    "section": "강의록",
    "text": "강의록\n본 강의록은 과거 https://jinseob2kim.github.io/godel.html 에 정리했던 내용을 약간만 수정하였습니다.\n서론\n괴델의 불완전성 정리에 대한 많은 책들이 시중에 출판되어 있다. 허나 대부분이 역사와 증명의 의미에 치중되어 있고 실제 증명의 아이디어를 설명한 책은 거의 없으며, 있다고 하더라도 외국 서적을 번역하면서 난해한 표현이 많이 나와 이해하기가 어렵다. 이에 본 글에서는 불완전성 정리 증명의 핵심적인 아이디어를 크게 수학(mathematics)과 메타수학(meta-mathematics), 괴델수(Gödel number), 메타수학의 수학화의 3가지로 나누어 설명하겠다. 본 글이 불완전성정리의 아름다움을 느끼는 데 도움이 되길 바란다.\n수학(mathematics)과 메타수학(meta-mathematics)\n보통 불완전성 정리에 대한 책들에서는 간략하게 증명의 개요만 언급하는데 그것은 아래와 같다.\n\\[G: G는 \\:증명불가능하다.\\]\n만약 \\(G\\)가 증명 가능하다면 그것은 참이고, 내용은 “\\(G\\)는 증명 불가능하다.” 므로 모순이다. 따라서 \\(G\\)는 증명 불가능하며, \\(G\\)의 내용은 “\\(G\\)가 증명 불가능하다”는 것이므로 참이다. 따라서 \\(G\\)는 참이지만 증명불가능한 명제라는 것이다. 이 설명을 보고 고개를 끄덕 인 후 증명이 끝난 것 아닌가? 라는 생각을 할 수도 있을 것이다. 그러나 명제 내부에 명제 자신을 언급한 순환논리부분을 해결하지 않으면 안되고 사실 불완전성정리의 핵심부분도 이 부분이며 수학의 명제와 메타수학의 명제를 구분하는 것이 이해의 시작이 된다.\n수학 명제와 메타수학 명제의 차이점\n아주 간단한 수학 명제 하나를 살펴보자.\n\\[1+1=2\\]\n이 표현은 수학에 속한 표현이다. 이제 다음 명제를 살펴보면\n\\[\"1+1=2\"는 \\:수학\\: 명제이다.\\]\n이 진술은 앞에 나온 수학명제에 대해 무엇인가를 주장하고 있으며 따라서 수학이 아니라 메타수학의 명제라고 할 수 있다. 비슷하게 “\\(x\\)는 변수이다”, “\\(x=1\\)은 방정식이다” 들도 수학 명제가 아니라 메타수학의 명제이다. 수학과 메타수학을 구별하는 것은 몇 번을 강조해도 지나치지 않은데 이 구별에 소홀한 것이 수많은 역설이 만들어지는 이유이기 때문이다. 힐베르트를 포함한 당시의 수학자들은 이를 잘 구별하면 역설을 제거할 수 있다고 보았다. 이제 다시 처음 식을 살펴보자. \\(G\\)가 수학명제라면 “\\(G\\)는 증명불가능하다” 는 수학의 명제가 아니라 메타수학의 명제이므로, 이것이 수학명제인 \\(G\\)가 될 수는 없다. 따라서 자기언급의 역설은 발생하지 않는다.\n그러나 괴델은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 괴델수부터 차근차근 알아보도록 하자.\n괴델수(Gödel number)\n괴델수는 모든 기호, 변수, 명제, 명제묶음(증명)들에 유일한 숫자를 부여하는 것인데 몇가지 예를 들어보면 다음과 같다.\n\n상항기호\n\n기호\n괴델수\n의미\n\n\n\n\\(\\sim\\)\n1\n아니다\n\n\n\\(\\vee\\)\n2\n또는\n\n\n\\(\\supset\\)\n3\n\n\\(\\cdots\\)라면 \\(\\cdots\\)다\n\n\n\\(\\exists\\)\n4\n\n\\(\\cdots\\)이 존재한다\n\n\n\\(=\\)\n5\n같다\n\n\n0\n6\n영(0)\n\n\ns\n7\n바로 다음 수\n\n\n(\n8\n왼쪽 괄호\n\n\n)\n9\n오른쪽 괄호\n\n\n,\n10\n쉼표\n\n\n\\(+\\)\n11\n더하기\n\n\n\\(\\times\\)\n12\n곱하기\n\n\n\n위의 표에 나와있는 기호들을 상항기호라고 하며, 이와 대응되는 개념은 변항(variable)기호인데 숫자 변항, 문장 변항, 술어 변항으로 나눌 수 있다. 숫자 변항에는 12보다 큰 소수, 문장 변항에는 12보다 큰 소수의 제곱수, 술어변항에는 12보다 큰 소수의 세제곱수를 부여하게 되며 예는 아래의 표와 같다.\n\n변항기호\n\n\n변항\n괴델수\n대입 예\n\n\n\n숫자변항\n\\(x\\)\n13\n0\n\n\n\n\\(y\\)\n17\n\\(s0\\)\n\n\n\n\\(z\\)\n19\n\\(y\\)\n\n\n문장변항\n\\(p\\)\n\\(13^2\\)\n\\(0=0\\)\n\n\n\n\\(q\\)\n\\(17^2\\)\n(\\(\\exists\\) x)(\\(x=sy\\)): \\(y\\)의 다음 수 \\(x\\)가 존재한다.\n\n\n\n\\(r\\)\n\\(19^2\\)\n\\(p\\supset q\\)\n\n\n술어변항\n\\(P\\)\n\\(13^3\\)\n\n\\(P(x)\\): \\(x\\)는 소수이다.\n\n\n\n\\(Q\\)\n\\(17^3\\)\n\n\n\n\n\\(R\\)\n\\(19^3\\)\n\n\n\n\n이제 문장 (\\(\\exists x\\))(\\(x=sy\\)) 를 살펴보자. 이것은 \\(y\\) 다음 수가 존재한다고 읽으며, 기본기호 하나하나의 괴델수를 살펴보면 8, 4, 13, 9, 8, 13, 5, 7, 17, 9이고 문장 전체의 괴델수는 다음과 같이 소수를 이용하여 표현한다.\n\\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n마찬가지로 증명에 대해서도 괴델수를 부여할 수 있는데 예를들어 증명이 두 서술로 되어 있고 각 서술의 괴델수가 \\(m\\)과 \\(n\\)이라면 증명의 괴델수는 \\(2^m \\times 3^n\\) 으로 표현한다. 이런식으로 모든 수학적 표현에 대해서 겹치지 않고 유일하게 괴델수를 부여할 수 있다. 한가지 예를 더 들면 괴델수 243,000,000은 \\(2^6 \\times 3^5 \\times 5^6\\)으로 소인수분해 되며 지수부분인 6,5,6은 각각 0, \\(=\\), 0에 대응되므로 결국 \\(0=0\\)이라는 수학명제를 나타내게 된다.\n메타수학의 수학화\n괴델은 괴델수를 이용하여 메타수학의 명제를 수학의 명제로 바꾸는 데 성공하였는데 간단한 예를 들어보겠다.\n\\[\\sim(0=0)\\]\n는 “0은 0이 아니다.” 라는 단순한 수학 명제인 반면\n\\[\\text{수학 명제} \"\\sim(0=0)\" \\text{의 첫 번째 기호는 틸드}(\\sim) \\text{이다}.\\]\n는 수학명제가 아니라 메타수학의 명제이다. 그런데 이 메타수학의 명제는 적당한 과정을 거치면 수학명제로 바꿀 수 있다. 편의상 \\(\"\\sim(0=0)\"\\)의 괴델수를 \\(a\\)라고 하고 위의 상항기호 표에서 \\(\"\\sim\"\\)의 괴델수가 1임을 기억하자. 그렇다면 첫 번째 기호가 \\(\"\\sim\"\\)이라는 것은 곧 \\(a\\)를 소인수분해했을 때 제일 작은 소수인 2의 지수가 1임을 의미하게 되고 메타수학의 명제는\n\\[2 \\text{는 } a \\text{의 인수이지만, } 2^2 \\text{ 은 } a \\text{의 인수가 아니다}.\\]\n와 같이 수학명제로 바뀌게 된다. 기호로 표현하기 위해 표현을 바꾸면 “\\(y=z\\times 2\\)인 \\(z\\)가 존재하고, \\(y=z\\times 2 \\times 2\\)인 \\(z\\)는 존재하지 않는다.” 가 되며 실제로 이를 기호로 표현하면 다음과 같다.\n\\[(\\exists z) (sss\\cdots sss0=z\\times ss0) \\cdot \\sim(\\exists z) (sss\\cdots sss0=z\\times (ss0 \\times ss0))\\]\n(\\(sss\\cdots sss0\\)에서 \\(s\\)는 정확히 \\(a\\)번 나타난다.) 이와 비슷한 방법으로 모든 메타수학의 명제를 괴델수를 이용하여 수학의 명제로 바꿀 수 있다.\n불완전성 정리\n이제 불완전성 정리를 천천히 이해해보자. Dem/dem과 Sub/sub의 개념을 정의한 후 증명의 핵심 아이디어로 들어가 보겠다.\nDem\ndem은 증명을 뜻하는 demonstration의 약자로 dem(\\(x\\), \\(z\\))은\n\\[\\text{괴델수 } x \\text{를 가진 문장묶음이 괴델수 } z \\text{를 가진 명제의 증명이다}.\\]\n의 축약표현으로 정의한다. 예를 들어 “피타고라스 정리 증명”의 괴델수가 \\(m\\)이고 “피타고라스 정리”의 괴델수가 \\(n\\)이라면 dem(\\(m\\), \\(n\\))이라고 쓸 수 있는 것이다. 한편, Dem(\\(x\\), \\(z\\))은 \\(x\\)와 \\(z\\)의 관계 dem을 형식적 표기법으로 표현하는 형식문으로 정의한다.\nSub\nSub은 치환 혹은 대입을 의미하며 소문자로 표현한 sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수 } x \\text{를 가진 문장에 등장하는} \\textbf{ 변수 } y \\textbf{들에 전부 숫자 } x \\textbf{를 대입} \\text{해서 만들어진 명제의 괴델수}\\]\n로 정의한다(17은 \\(y\\)의 괴델수임을 기억하자). 한 편, \\(s\\)를 대문자로 표시한 Sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수가 아닌 }\\textbf{명제 그 자체}\\]\n로 정의한다. 이해를 돕기 위해 \\(2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\) 를 다시 살펴보겠다. (\\(\\exists x\\))(\\(x=sy\\)) 는 앞서 설명했듯이 “\\(y\\) 다음 수가 존재한다” 고 읽을 수 있으며, 그 괴델수를 \\(k\\)라 하면\n\\[k=2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n가 된다. 이제 \\(y\\)대신 \\(k\\)를 대입한다면 수정된 명제는 (\\(\\exists x\\))(\\(x=sss \\cdots sss0\\))이 되고(\\(s\\)는 \\(k+1\\)번 연속됨), \\(s\\)의 괴델수가 7임을 고려하면 \\(k\\)를 대입한 명제의 괴델수는 \\(y\\)부분에 해당되는 \\(23^{17}\\)부터 \\(s\\)로 바뀌어 아래의 식과 같이 된다.\n\\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{7} \\times 29^7 \\times 31^7 \\times 37^7 \\times \\cdots (P_{k+10})^9\\]\n(\\(P_{k+10}\\): \\(k+10\\)번 째 소수)\n얼핏 보기에 문장의 괴델수를 문장 그 자체에 대입한다는 것이 순환논리같은 느낌을 주는데 이것이 바로 괴델의 핵심적인 아이디어 중 하나이다. 이제부터 본격적인 증명의 내용으로 들어가기로 하자.\n불완전성 정리\n불완전성 정리의 증명은 “괴델수 \\(z\\)를 가진 명제는 증명불가능하다”라는 메타수학의 명제를 수학의 명제로 바꾼 후, 이 명제의 특별한 경우가 증명될 수 없다는 것을 보이는것으로 이루어진다. 이제 다음의 형식문을 살펴보자.\n\\[\\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(y, 17, y))\\]\n이 수학의 형식문은 “Sub(\\(y\\) ,17, \\(y\\)) 의 증명은 존재하지 않는다, 즉 증명불가능하다”는 메타수학적 의미를 갖는다. 이 형식문의 괴델수를 \\(n\\)이라 하고, 형식문에 포함된 변수 \\(y\\)를 숫자 \\(n\\)으로 바꾼 형식문 \\(G\\)를 살펴보자.\n\\[\\text{G}: \\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(n, 17, n))\\]\n\\(G\\)는 변수가 포함되어 있지 않으므로 수학명제이며, 이것의 메타수학적 의미를 살펴보면 “괴델수 sub(\\(n\\), 17, \\(n\\))을 가진 명제는 증명 불가능하다.”이다. \\(G\\)의 괴델수를 \\(g\\)라고 하고 \\(g\\)는 어떤 숫자일지 생각해보자. 놀랍게도 \\(g=\\text{sub}(n, 17, n)\\)임을 알 수 있다. \\(G\\)는 괴델수 \\(n\\)을 가진 형식문에서 \\(y\\)에 숫자 \\(n\\)을 대입하여 만든 명제이므로, 이것의 괴델수 \\(g\\)는 정확히 sub(\\(n\\), 17, \\(n\\))의 정의와 일치하게 된다. 이제 \\(G\\)의 메타수학적 의미를 다시 살펴보면 “괴델 수 \\(g\\)를 가진 명제는 증명 불가능하다.”이고 즉, “\\(G\\)는 증명 불가능하다”는 의미가 된다. 맨 처음으로 돌아가면, \\(G\\)는 참이지만 증명 불가능한 명제가 되어 불완전성의 정리가 증명된다.\n마치며\n필자는 고등학생때부터 괴델의 불완전성정리를 이해해보려고 괴델의 논문원본도 찾아보고 여러 교양서적을 많이 읽어보았었다. 그러나 논문을 다 보는것은 이해하기가 어렵고 교양서적은 겉핥기식으로만 나와있어서 최근까지도 증명의 핵심을 이해하지 못했었는데 출판사 승산에서 나온 괴델의 증명을 읽으면서 한층 이해수준을 올릴 수 있었다. 이 책은 외국서적을 번역한 것으로 번역하면서 생소한 단어와 표현들이 많이나와 읽기가 어려운 부분이 있어 이번기회에 잘 풀어서 설명해볼 목적으로 글을 쓰게 되었다. 본 글이 논문과 교양서적 사이에서 가교 역할을 하여 불완전성 정리를 이해하는데 도움이 되길 기대한다.\n참고문헌\n\n괴델, 에셔, 바흐 : 영원한 황금 노끈 / 지은이: 더글러스 호프스태터 ; 옮긴이: 박여성, 안병서\n(호프스태터가 서문을 쓰고 개정한) 괴델의 증명 / 지은이: 어니스트 네이글, 제임스 뉴먼 ; 옮긴이: 곽강제, 고중숙\n괴델 불완전성 정리 / 지은이: 요시나가 요시마사 ; 옮긴이: 임승원"
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "",
    "text": "OTP는 One Time Password의 약자로, 일회용 비밀번호를 뜻합니다. 고정된 비밀번호와 달리, 필요 할 때마다 발급되어 한 번만 사용할 수 있습니다. 대부분의 경우, 6자리 숫자가 30초마다 갱신되는 형태로, 휴대폰 어플리케이션이나 실물 OTP 생성기 등으로 발급합니다.\n차라투에서는 여러 개의 RStudio Server를 구동하고 있습니다. 이 중 인턴십을 위해 사용하는 서버에 시범적으로 도입 해 보고자 하였고, 후기를 남깁니다.\n본 게시글은 R-bloggers 게시글을 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#개요",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#개요",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "",
    "text": "OTP는 One Time Password의 약자로, 일회용 비밀번호를 뜻합니다. 고정된 비밀번호와 달리, 필요 할 때마다 발급되어 한 번만 사용할 수 있습니다. 대부분의 경우, 6자리 숫자가 30초마다 갱신되는 형태로, 휴대폰 어플리케이션이나 실물 OTP 생성기 등으로 발급합니다.\n차라투에서는 여러 개의 RStudio Server를 구동하고 있습니다. 이 중 인턴십을 위해 사용하는 서버에 시범적으로 도입 해 보고자 하였고, 후기를 남깁니다.\n본 게시글은 R-bloggers 게시글을 참고해 작성되었습니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-1",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-1",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 서버 1",
    "text": "과정 - 서버 1\nOTP 도입을 위해서는 서버와 사용자의 설정이 필요합니다. 이 문단에서는 우선 서버측의 작업에 대해 다루겠습니다.\napt-get update\n명령어를 통해 패키지를 최신화합니다. 그 후,\napt-get install -y libpam-google-authenticator\n로 libpam-google-authenticator 패키지를 설치합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---사용자",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---사용자",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 사용자",
    "text": "과정 - 사용자\n다음으로, 사용자 측에서 수행할 작업입니다.\n\n첫째로, 우선 RStudio Server에 로그인합니다.\n\n\n\n둘째로, Terminal에서,\n\ngoogle-authenticator\n를 실행합니다.\n\n\n셋째로, y 를 입력합니다.\n\n\n\n넷째로, 아래와 같은 QR코드와 secret key가 나타납니다.\n\n\n휴대폰의 “Google Authenticator”나, 또는 이와 유사한 OTP 지원 어플리케이션으로, 화면에 제시된 QR코드를 입력(촬영)하거나, 아래의 secret key를 OTP 어플리케이션에 등록합니다.\n \n\n중요한 작업은 거의 끝났습니다. 간단한 설정을 수행합니다.\n\n\n관련 사항을 저장하는 옵션입니다. y로 설정합니다.\n\n\n\n하나의 코드로 한 번만 로그인을 허용할지 결정하는 옵션입니다. y로 설정합니다.\n\n\n\n기본적으로 현재의 코드, 이전 코드, 이후 코드의 3개의 코드로만 로그인이 허용됩니다. 이 옵션을 허용하면 현재의 코드, 앞 8개 코드, 뒤 8개 코드의 총 17개로 로그인이 가능하게 허용됩니다. 사용자와 서버의 시간 문제가 발생하면 y로 설정하면 되나, 현재는 필요 없어 n으로 설정했습니다.\n\n\n\n30초에 3번만 로그인을 시도할 수 있도록 하는 옵션입니다. y로 설정합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-2",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#과정---서버-2",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "과정 - 서버 2",
    "text": "과정 - 서버 2\n위 과정을 다 수행한 후, RStudio의 인증 프로파일을 수정하기 위하여 아래와 같이 서버 설정을 진행합니다.\nvi /etc/pam.d/rstudio 파일에,\nauth required pam_google_authenticator.so\n@include common-account\n@include common-session\n위 내용을 추가합니다.\nvi /etc/rstudio/rserver.conf 파일에,\n# Server Configuration File\nauth-pam-require-password-prompt=0\n위 내용을 추가합니다.\n이렇게 설정한 후, RStudio Server 서비스를 재시작합니다.\n우리 회사는 Docker Container 내에서 RStudio Server를 실행하고 있으므로, 아래와 같이 Docker Container를 재시작 하였습니다. 만약 다른 방법으로 사용하고 계신 경우, 적절한 방법으로 서비스를 재시작하시면 됩니다.\n# 컨테이너명: internship\ndocker restart internship\n이제, 휴대폰 OTP에 표시된 6자리 숫자로 로그인이 가능합니다."
  },
  {
    "objectID": "posts/2024-01-05-RStudio-Server-2FA/index.html#마치며",
    "href": "posts/2024-01-05-RStudio-Server-2FA/index.html#마치며",
    "title": "RStudio Server에 2FA(OTP) 도입하기",
    "section": "마치며",
    "text": "마치며\n기존에는 정적인 비밀번호를 사용하고 있었지만, OTP를 통해 비밀번호를 계속 변경하는 효과를 누릴 수 있습니다. 우선 하나의 서버에만 적용하였지만, 사용성이 우수하다고 판명 될 경우 다른 서버에도 확대 적용 계획입니다.\nRStudio Server를 사용하시는 분들께 도움이 되었으면 좋겠습니다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "title": "메타분석 웹 개발 후기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 8월 Shinykorea 밋업에 참석, 메타분석 ShinyApps 만든 후기를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "title": "메타분석 웹 개발 후기",
    "section": "요약",
    "text": "요약\nhttp://app.zarathu.com/meta-analysis 에 메타분석 ShinyApps 를 공개하였다.\n\n메타분석은 여러 연구결과를 합쳐서 보여주는 분석, meta 패키지를 ShinyApps 로 구현하였다.\nrhandsontable 로 연구결과를 직접 입력한다.\n서버에 한글폰트 설치 후 showtext 로 불러와 한글깨짐을 해결한다.\ncolourpicker 로 글자색 조절한다.\nEMF 포맷 다운로드를 지원, PPT에서 직접 그림수정할 수 있다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "title": "메타분석 웹 개발 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/meta-analysis 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html",
    "href": "posts/2020-02-11-tokyor81/index.html",
    "title": "TokyoR 81회 리뷰",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 3월 Shinykorea 밋업에 참석, 일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#요약",
    "href": "posts/2020-02-11-tokyor81/index.html#요약",
    "title": "TokyoR 81회 리뷰",
    "section": "요약",
    "text": "요약\n일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰하였다.\n\n초심자세션: R과 shiny 기초를 다루었다.\n응용세션1: 비동기 프로그래밍, 30분만에 적당히 사용할 shiny 앱 만들기 를 다루었다.\n응용세션2: leaflet 이용 지도앱 만들기, shiny앱을 지탱하는 엔지니어링 패키지 를 다루었다.\nLT: Shiny로 만드는 사진편집앱, Shiny로 사내용 범용 통계 앱 만들기, 오픈소스 약물동태 시뮬레이션, shiymeta로 재현가능한 R code 생성하기 등을 다루었다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#slide",
    "href": "posts/2020-02-11-tokyor81/index.html#slide",
    "title": "TokyoR 81회 리뷰",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/TokyoR81 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html",
    "href": "source_code/Custom_Search_Zarathu.html",
    "title": "Depedency",
    "section": "",
    "text": "!pip install simplejson\n\nRequirement already satisfied: simplejson in c:\\users\\zarathu09\\anaconda3\\envs\\zarathu\\lib\\site-packages (3.18.3)\nfrom datetime import datetime\nimport os\nimport sys\nimport urllib.request\nimport pandas as pd \nimport json\nimport re \nimport requests\nimport simplejson"
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "href": "source_code/Custom_Search_Zarathu.html#api-key-발급받는-방법은-링크를-참조해주세요",
    "title": "Depedency",
    "section": "API KEY 발급받는 방법은 링크를 참조해주세요",
    "text": "API KEY 발급받는 방법은 링크를 참조해주세요\n\n네이버 https://zerosecu.tistory.com/18\n\n일 허용 한도 25000건\n\n카카오 https://kadosholy.tistory.com/25\n구글 https://gomgomi.tistory.com/3\n\n일일 검색어 제한 10,000개\n\n# Naver_client_id = \n# Naver_client_secret = \n# Kakao_API_key= \n# Google_SEARCH_ENGINE_ID = \n# Google_API_KEY ="
  },
  {
    "objectID": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "href": "source_code/Custom_Search_Zarathu.html#지식인-블로그-동영상-pdf-파일-book-신문기사-제외-링크에-포함되어선-안될-도메인을-제거해줍니다",
    "title": "Depedency",
    "section": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!",
    "text": "지식인, 블로그, 동영상, pdf 파일, book, 신문기사 제외 링크에 포함되어선 안될 도메인을 제거해줍니다!\n추가하실 도메인을 넣어주세요\n\nTrash_Link = [\"tistory\", \"kin\", \"youtube\", \"blog\", \"book\", \"news\", \"dcinside\", \"fmkorea\", \"ruliweb\", \"theqoo\", \"clien\", \"mlbpark\", \"instiz\", \"todayhumor\"]"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html",
    "href": "posts/2024-05-13-rhub/index.html",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECKは、Rパッケージを開発した後に「正常に開発されているか」を検証するために、約50以上のチェックリストを実行するプロセスです。関数の使用方法が適切に記述されているか、関数のパラメータが適切に記述されているかなどが含まれます。\nもちろん、R CMD CHECKを厳密に実行せずにgithubを介してパッケージを配布して実行することに問題はありませんが、パッケージのエラーを最小限に抑え、安定したパッケージをユーザーに提供できることが証明された後に、CRANなどの公式リポジトリを介してのみパッケージを共有するべきです。\nこの投稿では具体的な内容には触れませんが、興味がある場合はHadley WickhamのR Packagesを参照することもできます。\nいずれにせよ、R CMD CHECKは、devtoolsパッケージを使用してRパッケージを作成した場合、devtools::check()関数またはRstudioのCheckボタンを使用して実行でき、警告、エラー、ノートなどで修正が推奨される問題を確認できます。\n\nしかし、R CMD CHECKの特徴の1つは、パッケージを開発しているPCの環境を基準にチェックを行うということです。つまり、下の画像のようにmacOS（Apple clang）環境でパッケージがテストされ、実行が保証されていますが、ユーザーのOSがmac以外のwindow、linuxなどの場合、パッケージが正常に動作しない可能性があります。\nCRANは基本OSを指定していませんが、Windows、macOS、linuxの少なくとも2つのOSでのテストに問題がないことを要求するため、Rパッケージの開発にはさまざまなOSでのテストが含まれることがよくあります。\nこのため、さまざまなOSハードウェア、つまりWindows PC、mac、Linuxサーバーがあればベストですが、このようなケースは多くなく、ほとんどの場合、Githubアクション、AppVeyor、Travis CIなどのCI/CDサービスを使用してさまざまなOSでのテストを実行します。"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "href": "posts/2024-05-13-rhub/index.html#r-cmd-check",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "",
    "text": "R CMD CHECKは、Rパッケージを開発した後に「正常に開発されているか」を検証するために、約50以上のチェックリストを実行するプロセスです。関数の使用方法が適切に記述されているか、関数のパラメータが適切に記述されているかなどが含まれます。\nもちろん、R CMD CHECKを厳密に実行せずにgithubを介してパッケージを配布して実行することに問題はありませんが、パッケージのエラーを最小限に抑え、安定したパッケージをユーザーに提供できることが証明された後に、CRANなどの公式リポジトリを介してのみパッケージを共有するべきです。\nこの投稿では具体的な内容には触れませんが、興味がある場合はHadley WickhamのR Packagesを参照することもできます。\nいずれにせよ、R CMD CHECKは、devtoolsパッケージを使用してRパッケージを作成した場合、devtools::check()関数またはRstudioのCheckボタンを使用して実行でき、警告、エラー、ノートなどで修正が推奨される問題を確認できます。\n\nしかし、R CMD CHECKの特徴の1つは、パッケージを開発しているPCの環境を基準にチェックを行うということです。つまり、下の画像のようにmacOS（Apple clang）環境でパッケージがテストされ、実行が保証されていますが、ユーザーのOSがmac以外のwindow、linuxなどの場合、パッケージが正常に動作しない可能性があります。\nCRANは基本OSを指定していませんが、Windows、macOS、linuxの少なくとも2つのOSでのテストに問題がないことを要求するため、Rパッケージの開発にはさまざまなOSでのテストが含まれることがよくあります。\nこのため、さまざまなOSハードウェア、つまりWindows PC、mac、Linuxサーバーがあればベストですが、このようなケースは多くなく、ほとんどの場合、Githubアクション、AppVeyor、Travis CIなどのCI/CDサービスを使用してさまざまなOSでのテストを実行します。"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#github-action",
    "href": "posts/2024-05-13-rhub/index.html#github-action",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Github action",
    "text": "Github action\nこの投稿では、参考としてGithubアクションを紹介しますが、他のサービスでもプロセスはほぼ同じです。\nGithubアクションは、Githubが提供するCI/CDサービスで、Githubが提供するさまざまなアクションを使用して（Githubが提供するサーバーでコマンドを実行することにより）、自動化されたテスト、ビルド、デプロイなどを実行できます。\n\nアクションは、サーバーにRをインストールしたり、Rパッケージをインストールしたり、R CMD CHECKを実行したりするなど、ymlファイルで構成されたコマンドのコレクションと考えることができます。\nR-hub actions, R-lib actions 参考までに。"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#r-hub2",
    "href": "posts/2024-05-13-rhub/index.html#r-hub2",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "r-hub2",
    "text": "r-hub2\nr-hubプロジェクトは、Rコンソーシアムのプロジェクトの1つであり、R開発者がRパッケージをより良く開発できるようにすることを目的としています。 さまざまなOSでのテストを実行することを支援することもその1つで、上記で言及したGithub Actionを開発したり、インフラを提供したり、コミュニティが問題を解決するのを支援したりする役割があります。\nしかし、R-hubプロジェクトは最近、このGHAを設定するのを助けるためのRパッケージ、rhubを開発してリリースしました。\n公式ブログで説明されているように、すでにGithub Actionを使用していなくても、さまざまなOSでR CMD CHECKを簡単に実行できるGithub Actionを設定できます。\n最初にやるべきことは、もちろんrhubパッケージをインストールすることです。 ここで、pakは、install.packagesやremotes::install_githubなどの従来のパッケージインストール方法を統合し、さまざまなソースからRパッケージをインストールするための関数を提供することを推奨しています。\npak::pkg_install(\"rhub\")\nrhubパッケージは、公式バージョンが2であるにもかかわらず、rhub2ではなくrhubとしてインストールおよび実行する必要があることに注意してください。\nこの投稿では、それをrhubと呼びます。このrhubパッケージを実行するには、以下の3つが必要ですが、Rパッケージを開発してgithubに共有した経験がある場合、新たに準備する必要はありません。\n\ngithubアカウント\nRパッケージをアップロードしたリポジトリ。CRANを目指す場合は、もちろんPublicである必要があります。\nGithub PAT（Personal Access Token）、githubから取得でき、gitcredsという別のRパッケージを使用してもかまいません。\n\n\nSetup\nrhubパッケージを終えた後に最初に行うべきことは、Rパッケージディレクトリでrhub_setup関数を実行することです。この関数はディレクトリ内のgitリポジトリを認識し、Github Action用のymlファイルを生成します。\n以下の画像では、以前に作成したgemini.Rパッケージを例にしています。\n\nすべてがうまくいけば、rhubパッケージは次のステップについても案内してくれます。つまり、追加されたymlファイルをgithubにコミットして更新を反映させた後、rhub_doctor関数を実行します。\n\n\nDoctor\nrhub_doctor関数は、Github PATが正しく設定されているかどうかを確認します。後で紹介するrhubのrhub_check関数は、Rstudioのコンソールで言及されたGithub PATを使用してGithub Actionを手動で実行するため、PATの設定を確認する必要があります。\nGithub PATは 「https://github.com/settings/tokens 」のリンクから作成できますが、repoとworkflowのパーミッションを付与して作成した場合に限ります。\n\nRstudioでGithub PATを設定する方法は、credentialパッケージのset_github_pat関数を使用します。 リンク\nrhub_doctor関数が正常に動作した場合、残りの作業はrhub_check関数を実行することです。\n\n\n\nCheck\n前のステップは、この関数のための準備作業であったと言っても過言ではありません。\n\nrhub_check関数は、githubリポジトリとPATを認識し、次にR CMD CHECKを実行するOSを入力値として受け取ります。\nこの時、Windows、macOS、Linuxに加えて、画像のように（rhubプロジェクトで提供される）さまざまなOSを数字とカンマで区切って選択することができます。\n関数を実行した後、GHAページに接続できるリンクを提供し、進行状況を確認できます。\n\n最終的に、rhubとGHAを使用してテストパスをリポジトリにバッジとして追加すると、以下のようになります。\n\nバッジアイコンをreadmeに追加するには、次のように書く必要があります。\n![example workflow](https://github.com/&lt;OWNER&gt;/&lt;REPOSITORY&gt;/actions/workflows/&lt;WORKFLOW_FILE&gt;/badge.svg)\n使用した例では、をjhk0530、をgemini.R、をrhub.yamlに置き換えます。\nこのGithub Actionを介したR CMD CHECKには時間がかかるため、開発中のPCでR CMD CHECKを完了した後に実行することをお勧めします。\nもちろん、rhubはgithubやpublicリポジトリ以外の場合についてのガイダンスも提供していますが、これはほとんどのRパッケージ、特にCRANとはあまり関係がないため、別途説明しません。"
  },
  {
    "objectID": "posts/2024-05-13-rhub/index.html#summary",
    "href": "posts/2024-05-13-rhub/index.html#summary",
    "title": "rhub와 Github action를 활용한 OS별 R 패키지 검증",
    "section": "Summary",
    "text": "Summary\nこの投稿では、Rパッケージ開発者が Github Actionを使用するためのrhubパッケージを紹介しました。これにより、Rパッケージ開発者はローカル環境だけでなく、さまざまなOSでのR CMD CHECKを通じてパッケージエラーを最小限に抑え、より良いパッケージを作成することができます。\n詳細な情報は、rhubブログでも確認できます。\nDeepL TranslatorおよびGithub Copilotで翻訳されたコンテンツ"
  }
]