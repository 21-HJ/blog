[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "차라투 블로그",
    "section": "",
    "text": "Python Streamlit 패키지를 이용한 대시보드 만들기\n\n\n\n\n\n\n\npython\n\n\nstreamlit\n\n\n\n\nstreamlit 패키지의 사용방법을 알아보자\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\n의료데이터분석가 성장기\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n\n의료데이터분석가 성장기를 다시 정리했습니다. 본 내용은 보건산업진흥원이 지원하고 성균관대학교 의과대학에서 주관하는 “융합형 의사과학자 심포지엄” 에서 발표 예정입니다.\n\n\n\n\n\n\nOct 17, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nGAM(Generalized Additive Model) 소개\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n비선형모델인 GAM(Generalized Additive Model) 을 소개합니다. 본 강의는 성균관대 바이오헬스 규제과학과 강의자료로 쓰일 예정입니다.\n\n\n\n\n\n\nSep 30, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nShiny for Python\n\n\n\n\n\n\n\nPython\n\n\nShiny\n\n\n\n\nPython으로 반응형 웹 어플리케이션을 만들수 있는 Shiny for Python에 대해 소개합니다.\n\n\n\n\n\n\nAug 19, 2022\n\n\nChaehee Lee\n\n\n\n\n\n\n  \n\n\n\n\ncollapse 패키지 소개\n\n\n\n\n\n\n\nR\n\n\ncollapse\n\n\ndata.table\n\n\n\n\ncollapse 패키지를 소개하고 data.table 패키지와 비교하여 파악해보겠습니다.\n\n\n\n\n\n\nJul 25, 2022\n\n\nbeomsu Park\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 패키지 기초\n\n\n\n\n\n\n\nR\n\n\ndata.table\n\n\nlecture\n\n\n\n\n데이터를 빠르게 가공할 수 있는 data.table에 대하여 패키지 설치부터, 기본 구조 및 데이터를 가공하여 재구조화 하는 방법에 대해서 소개합니다.\n\n\n\n\n\n\nJul 13, 2022\n\n\nKO JUN HYUK\n\n\n\n\n\n\n  \n\n\n\n\n데이터과학자가 갖춰야할 기술\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nstatistics\n\n\n\n\n의료분야 데이터과학자에 필요한 역량을 정리하였습니다. 본 내용은 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 발표할 예정입니다.\n\n\n\n\n\n\nJun 27, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n22년 지원사업 후기\n\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n\n22년 각종 지원사업 선정, 탈락 후기를 공유합니다.\n\n\n\n\n\n\nApr 12, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR로 논문용 그래프 그리기\n\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n\nR 기본 함수, ggplot2 패키지, ggpubr 패키지를 활용해 의학논문에 필요한 그래프를 만들어보자.\n\n\n\n\n\n\nMar 25, 2022\n\n\nYumin Kim\n\n\n\n\n\n\n  \n\n\n\n\nReviewer들을 위한 의학통계\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 로 발표할 슬라이드를 미리 공유합니다.\n\n\n\n\n\n\nMar 19, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\ndata.table 패키지 소개\n\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n\n대용량의 데이터를 분산 처리 시스템 없이 처리할 수 있는 data.table 데이터 구조와 이를 조작, 관리하는데 사용하는 data.table 패키지에 대해서 소개합니다.\n\n\n\n\n\n\nFeb 11, 2022\n\n\nYujin Lee\n\n\n\n\n\n\n  \n\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n\n\ndocker\n\n\nlecture\n\n\n\n\nDocker와 Traefik을 활용한 Reverse-Proxy 구현\n\n\n\n\n\n\nFeb 8, 2022\n\n\nSiYeol Jung\n\n\n\n\n\n\n  \n\n\n\n\ntableone 패키지 소개\n\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n\n효율적으로 의학 연구 논문에 들어갈 table1을 만들 수 있는 tableone 패키지에 대해 소개합니다.\n\n\n\n\n\n\nFeb 7, 2022\n\n\nYujin Lee\n\n\n\n\n\n\n  \n\n\n\n\ngtsummary 패키지 소개\n\n\n\n\n\n\n\nR\n\n\nRpackage\n\n\n\n\n데이터 셋의 변수를 하나의 테이블로 요약하여 효율적으로 논문에 들어갈 table1을 만들 수 있는 gtsummary 패키지에 대해 소개합니다.\n\n\n\n\n\n\nFeb 4, 2022\n\n\nYujin Lee\n\n\n\n\n\n\n  \n\n\n\n\n창업 경험 공유\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 창업 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다.\n\n\n\n\n\n\nJan 25, 2022\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용\n\n\n\n\n\n\n\ndocker\n\n\ngithub\n\n\n\n\nMySQL 기반 DockerContainer에서 GitHub-Repository로의 주기적인 DB 백업 구현\n\n\n\n\n\n\nJan 20, 2022\n\n\nSiyeol Jung\n\n\n\n\n\n\n  \n\n\n\n\n인턴십 - Django로 게시판 만들고 기능 추가하기\n\n\n\n\n\n\n\nDjango\n\n\n\n\n숭실대학교 인턴십 프로그램을 통해 인턴으로 활동하게 된 차라투에서 1주차 동안 학습한 내용에 대해 공유합니다.\n\n\n\n\n\n\nJan 5, 2022\n\n\nSiyeol Jung\n\n\n\n\n\n\n  \n\n\n\n\nNotion으로 홈페이지 제작후 oopy로 배포한 후기\n\n\n\n\n\n\n\nreview\n\n\n\n\nNotion과 Oopy를 사용해 개편한 당사 홈페이지의 구축 논의 사항과 장·단점을 설명하였습니다.\n\n\n\n\n\n\nOct 1, 2021\n\n\nChangwoo Lim\n\n\n\n\n\n\n  \n\n\n\n\nShiny 환자데이터 입력웹 개발(2)\n\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n\n4월에 이어 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이터 입력웹을 소개합니다. 본 내용은 차라투가 후원하는 Shinykorea 10월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nSep 28, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nGoogle Login\n\n\n\n\n\n\n\nreview\n\n\n\n\nZarathu 앱에 적용할 예정인 구글 로그인에 대해 소개합니다.\n\n\n\n\n\n\nSep 11, 2021\n\n\nLee Hyunki\n\n\n\n\n\n\n  \n\n\n\n\nR 활용 공공빅데이터 분석지원\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\n\nR 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유합니다. 본 내용은 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에서 발표할 예정입니다.\n\n\n\n\n\n\nAug 21, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR 활용 의학연구지원\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\n\nR 활용 의학연구지원경험을 공유합니다. 본 내용은 “Be a data scientist - major actor in the future research” 라는 제목으로 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에서 발표할 예정입니다.\n\n\n\n\n\n\nAug 19, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\ngoogleAuth\n\n\n\n\n\n\n\nlecture\n\n\n\n\nFollow up assignment of ShinyProxy lecture\n\n\n\n\n\n\nJul 25, 2021\n\n\nHyunJun Ko\n\n\n\n\n\n\n  \n\n\n\n\n창업지원사업 도전기\n\n\n\n\n\n\n\npresentation\n\n\nkstartup\n\n\n\n\n지금까지 창업지원사업 도전했던 경험을 공유합니다. 본 내용은 차라투가 후원하는 Shinykorea 7월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nJul 11, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n채널톡(channel.io) 설치 후기\n\n\n\n\n\n\n\nreview\n\n\n\n\n최근 Zarathu 공식 홈페이지에 추가한 channel.io 서비스 관련 적용 후기입니다.\n\n\n\n\n\n\nJul 5, 2021\n\n\nChangwoo Lim\n\n\n\n\n\n\n  \n\n\n\n\nShiny 환자데이터 입력웹 개발\n\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\n\n\n삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 4월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nApr 2, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n코로나 수리모델링: 서울시 감염병연구센터 자문\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n\n서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 정리했습니다. 본 내용은 2월 Shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nJan 22, 2021\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n생존분석 실습\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\n\nKaplan-meier curve, 비례위험가정 확인, Time-dependent analysis 그리고 모수적 생존분석을 중심으로 R 코드를 정리했습니다. 본 내용은 성균관의대 사회의학교실 특강에서 실습할 예정입니다.\n\n\n\n\n\n\nOct 31, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n의학통계지원 소개\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n\n차라투 업무 소개입니다. 본 내용은 영남대학교 “의사과학자 역량 배가 프로젝트” 에서 발표할 예정입니다.\n\n\n\n\n\n\nOct 5, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nRStudio & Shiny Docker 소개\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\ndocker\n\n\n\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앴습니다. 본 내용은 Shinykorea 10월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nOct 5, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n메타분석 웹 개발 후기\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\nshiny\n\n\n\n\n메타분석 ShinyApps 만든 후기를 정리하였습니다. 본 내용은 Shinykorea 8월 밋업에서 발표할 예정입니다..\n\n\n\n\n\n\nAug 22, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n회귀분석 in 의학연구\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n\n의학 연구에서 사용하는 선형/로지스틱 회귀분석과 Cox 비례위험모형을 소개합니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n\nJul 22, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n의학 연구에서의 기술통계\n\n\n\n\n\n\n\npresentation\n\n\nR\n\n\n\n\n의학 연구에서 Table 1 에 활용되는 기술통계를 정리하였습니다. 본 내용은 삼성서울병원 정신건강의학과 교육에 이용될 예정입니다.\n\n\n\n\n\n\nJul 8, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n2020년 만들었던 ShinyApps\n\n\n\n\n\n\n\npresentation\n\n\nshiny\n\n\nR\n\n\n\n\n올해 만들었던 ShinyApps 를 간단히 정리하였습니다. 본 내용은 6월 shinykorea 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nJun 20, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\n\n%>% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다.\n\n\n\n\n\n\nApr 14, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR 데이터 매니지먼트: 기초\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\n\n\nR 기본 문법과, 보험공단 샘플 데이터를 이용한 데이터 매니지먼트 방법을 정리하였습니다. 본 내용은 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다.\n\n\n\n\n\n\nMar 10, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nTokyoR 81회 리뷰\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n일본 R 밋업 중 하나인 TokyoR 중 shiny 특집을 리뷰하였습니다. 본 내용은 차라투가 후원하는 Shinykorea 3월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nFeb 14, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR로 만드는 웹 애플리케이션\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nR\n\n\nshiny\n\n\n\n\nR과 shiny로 웹 애플리케이션을 만든 경험을 소개합니다. 본 내용은 디시인사이드가 후원하는 프로그래밍 갤러리 컨퍼런스 2020 에서 발표할 예정입니다.\n\n\n\n\n\n\nJan 25, 2020\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nRSelenium 이용 팁\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\nRSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유합니다. 본 내용은 Zarathu가 후원하는 Shinykorea 1월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nNov 30, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n의료데이터 분석가가 되기까지의 경험을 슬라이드로 공유합니다. 본 내용은 동국대학교 의생명공학과 세미나에서 발표할 예정으로, 초청해주신 김진식 교수님께 감사드립니다.\n\n\n\n\n\n\nNov 4, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nShiny 워크샵: 서울IT직업전문학교 국비교육\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nshiny\n\n\nR\n\n\nworkshop\n\n\n\n\nShiny 기초학습을 위한 강의 슬라이드와 실습파일입니다. 본 내용은 “서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정” 에서 쓰일 예정입니다.\n\n\n\n\n\n\nOct 27, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nRUCK2019 발표: From ShinyApps to CRAN\n\n\n\n\n\n\n\npresentation\n\n\nRpackage\n\n\nR\n\n\nshiny\n\n\n\n\n맞춤형 의학연구 앱을 만들고, 그것을 패키지로 만들어 CRAN에 배포한 경험을 슬라이드로 정리하였습니다. 본 내용은 R User Conference in Korea 2019(RUCK 2019)에서 발표하였습니다.\n\n\n\n\n\n\nOct 25, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR 이용 공공빅데이터 분석 경험\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nRpackage\n\n\nR\n\n\n\n\nR을 이용, 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의대 학술원 특강에서 발표할 예정입니다.\n\n\n\n\n\n\nSep 20, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nShinyApps 에 로그인 기능 넣기\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nRpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n\nShiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명하였습니다. 본 내용은 Zarathu가 후원하는 Shinykorea 9월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nAug 25, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장(2): 허수축 도입\n\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nRpackage\n\n\n\n\n이전 글 “선형모형의 다차원공간으로의 확장” 의 추가 제안으로, 선형모형의 무대를 허수축(Imaginary Axis)을 포함한 휘어진 다차원공간으로 확장, Inverted U-shape 관계를 선형관계로 재해석하였습니다.\n\n\n\n\n\n\nAug 14, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n괴델의 불완전성 정리\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nmathematics\n\n\n\n\n괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명하였습니다. 본 내용은 “제주대학교 경영정보학과 산업·직무 특화 전문가 특강” 에서 발표할 예정입니다.\n\n\n\n\n\n\nMay 22, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR 활용 맞춤형 통계지원 소개\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nRpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n\n의학연구를 지원하면서 다양하게 R을 활용했던 경험을 슬라이드로 정리하였습니다. 본 내용은 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에서 발표할 예정입니다.\n\n\n\n\n\n\nMay 13, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nShiny 활용 의학연구지원 경험\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nRpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n\nShiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 슬라이드로 정리하였습니다. 본 내용은 차라투(주)가 후원하는 Shinykorea 5월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nMay 11, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n세무기장마법사 머니핀(MoneyPin) 리뷰\n\n\n\n\n\n\n\nreview\n\n\nfinance\n\n\n\n\n법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다.\n\n\n\n\n\n\nApr 4, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nShinyApps를 R 패키지로 만들기\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nRpackage\n\n\nshiny\n\n\nR\n\n\nshinykorea\n\n\n\n\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록, RStudio Addins을 포함한 R 패키지를 만들어 CRAN에 배포신청했으나 실패한 경험을 정리하였습니다. 본 내용은 Anpanman이 후원하는 Shinykorea 2월 밋업에서 발표할 예정입니다.\n\n\n\n\n\n\nFeb 23, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nSelf-controlled case series\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\nsccs\n\n\n\n\n성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다.\n\n\n\n\n\n\nFeb 6, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR Markdown 기초\n\n\n\n\n\n\n\nlecture\n\n\nmarkdown\n\n\n\n\nYAML Header, 마크다운(Markdown) 텍스트, R 코드 청크(chunk) 그리고 그림과 테이블을 중심으로, R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 정리하였습니다.\n\n\n\n\n\n\nJan 28, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\nR 데이터 매니지먼트: tidyverse\n\n\n\n\n\n\n\nlecture\n\n\ntidyverse\n\n\ndata.table\n\n\npurrr\n\n\nR\n\n\n\n\n파일을 읽는 readr, 읽기 쉬운 코드를 만드는 %>% 연산자, 데이터를 다루는 dplyr 그리고 반복문을 다루는 purrr 패키지를 중심으로 tidyverse 생태계에서 데이터를 다루는 방법을 정리하였습니다.\n\n\n\n\n\n\nJan 23, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n진료실 밖 의사로서의 경험\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\n\n\n성균관의대 “의사의 길” 학부 강의에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다.\n\n\n\n\n\n\nJan 23, 2019\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n의학 연구에서의 기술 통계 with R\n\n\n\n\n\n\n\nlecture\n\n\nstatistics\n\n\nR\n\n\nshiny\n\n\n\n\n중앙보훈병원 정신건강의학과에서 강의한 내용으로, 의학 연구에 필요한 기술 통계(descriptive statistics)를 정리하고 웹 애플리케이션과 Rstudio Addins을 이용하여 실습하였습니다.\n\n\n\n\n\n\nNov 28, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio\n\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\n\n유전율(heritability)은 어떤 형질의 유전적인 측면을 정량적으로 설명하는 유용한 지표이나 이분형 형질의 경우 해석이 어렵습니다. 이 때는 Sibling recurrence risk가 직관적인 지표이나, 유병률 정보가 필요하고 다른 변수의 보정이 어려운 문제가 있습니다. 본 연구에서는 흔히 쓰는 OR scale을 이용, 이분형 변수에서 직관적이고 다른 변수의 보정도 가능한 유전율 지표를 제안합니다.\n\n\n\n\n\n\nNov 8, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)\n\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\nrpackage\n\n\n\n\n아인슈타인의 일반상대성이론은 태양 근처에서 빛이 휘어지는 현상을 빛이 아닌 시공간이 휘어지는 것으로 해석합니다. 비슷한 아이디어를 통계학에 적용하여 U-shape 관계를 휘어진 다차원 공간에서의 선형모형으로 재해석하였습니다.\n\n\n\n\n\n\nNov 8, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\n  \n\n\n\n\n맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축\n\n\n\n\n\n\n\npresentation\n\n\nlecture\n\n\ndevOps\n\n\nR\n\n\ndocker\n\n\nshiny\n\n\n\n\nR User Conference in Korea 2018(RUCK 2018)에서 발표했던 내용입니다.\n\n\n\n\n\n\nNov 8, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRedefine Null Hypothesis\n\n\n\n\n\n\n\narticle\n\n\nstatistics\n\n\nR\n\n\n\n\nP value의 가장 큰 문제는 샘플 숫자만 늘리면 아무리 작은 차이라도 유의미한 결과로 만들 수 있다는 것입니다. 이는 대부분의 연구에서 차이가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 생기는 문제인데, 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못합니다. 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였습니다.\n\n\n\n\n\n\nNov 8, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\n\n\nradix\n\n\n\n\nWelcome to Anpanman!\n\n\n\n\n\n\nSep 25, 2018\n\n\nJinseob Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html",
    "href": "posts/2021-08-19-beadatascientist/index.html",
    "title": "R 활용 의학연구지원",
    "section": "",
    "text": "김진섭 대표는 사단법인 헬리코박터 마이크로바이옴 연구회 워크숍에 참석, “Be a data scientist - major actor in the future research” 라는 제목으로 R 활용 의학연구지원경험을 발표할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#요약",
    "href": "posts/2021-08-19-beadatascientist/index.html#요약",
    "title": "R 활용 의학연구지원",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\nShinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2021-08-19-beadatascientist/index.html#slide",
    "href": "posts/2021-08-19-beadatascientist/index.html#slide",
    "title": "R 활용 의학연구지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/microbiome 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html",
    "href": "posts/2022-03-19-statreview/index.html",
    "title": "Reviewer들을 위한 의학통계",
    "section": "",
    "text": "김진섭 대표는 4월 8일(금) 제18차 대한이식학회 춘계학술대회 심포지엄에서 “리뷰어들을 위한 의학통계” 주제로 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#요약",
    "href": "posts/2022-03-19-statreview/index.html#요약",
    "title": "Reviewer들을 위한 의학통계",
    "section": "요약",
    "text": "요약\nTable 1\n\n연속변수 2그룹: 정규분포 t-test, 아니면 Wilcox-test\n연속변수 3그룹이상: 정규분포 ANOVA, 아니면 Kruskal–Wallis ANOVA\n범주형 변수: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n\n회귀분석\n\nUnivariate, multivariate 같이 보여주기, Subgroup 분석 추천\nStepwise selection 비추천: 예측모형 목적 아님, 임상맥락 고려X\n\n생존분석\n\nKaplan-meier 그림선 겹치면 안됨: Time stratification 필요\n보정할 변수가 Index time 이후면 안됨: Time-dependent covariate 필요\nPropensity score 매칭 후 pair 고려한 stratified cox 는 필수아님\n\n국민건강영양조사\n\n표본추출정보를 고려한 통계분석: Survey table1/GLM/Cox"
  },
  {
    "objectID": "posts/2022-03-19-statreview/index.html#slide",
    "href": "posts/2022-03-19-statreview/index.html#slide",
    "title": "Reviewer들을 위한 의학통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/statreview 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html",
    "href": "posts/2018-11-08-medianorratio/index.html",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#abstract",
    "href": "posts/2018-11-08-medianorratio/index.html#abstract",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Abstract",
    "text": "Abstract\nHeritability는 trait의 유전적인 측면을 정량적으로 설명하는 지표로 genetic study에서 흔히 쓰이는 지표이다. \\(Y\\)가 continuous variable일 때는 Intraclass Correlation Coeffcient(ICC) 혹은 Variance Partition Coefficients(VPC)의 형태로 heritability를 간단히 표시할 수 있으나, binary trait에는 이를 그대로 적용할 수 없다. 이에 liability threshold model 등의 다양한 approximation 방법이 이용되고 있으나 결과 해석의 어려움이 문제로 지적되고 있다. 한편 binary trait의 경우 sibling recurrence risk ratio(\\(\\lambda_s\\))가 유전적인 부분을 표현하는 직관적인 지표로 쓰이고 있으나 prevalence값이 필요하고 covariate 보정이 안되는 한계점이 있다. 이에 저자는 binary multilevel study에서 이용되는 Median Odds Ratio(MOR)을 이용하여 binary trait의 유전적인 정도를 평가하는 편하고 직관적인 지표인 Median OR Ratio(\\(\\text{MORR}_{SU}\\))을 제안하였으며 Healthy Twin Study, Korea의 hypertriglycemia trait에 적용하여 간단하며 직관적으로 유전적인 정도를 측정할 수 있었다. 이 지표가 binary trait에서 유전적인 정도를 표현하는 새로운 지표로서 heritability나 \\(\\lambda_s\\)를 보완할 수 있을 것으로 확신한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#introduction",
    "href": "posts/2018-11-08-medianorratio/index.html#introduction",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Introduction",
    "text": "Introduction\nHeritability는 continuous trait에서 polygenic effect가 설명할 수 있는 정도를 측정하는 지표로 전체 분산 중 polygenic effect가 설명하는 비율(the portion of phenotypic variance in a population attributable to additive genetic factors)로 정의되며, trait의 정규분포 가정이 깨지지 않는다면 mixed effect model 등을 이용하여 쉽게 구할 수 있다.(Manolio et al. 2009; Vattikuti 2012). Mixed effect model의 결과에서 polygenic effect의 분산과 residual의 분산이 추정되고 이를 통해 전체분산과 polygenic effect가 설명하는 분율을 계산하는 것인데 이는 Intraclass Correlation Coeffcient(ICC) 또는 Variance Partition Coefficients(VPC)의 형태이며, mixed effect model을 이용함으로서 covariate(age, sex, etc..)들을 보정할 수 있는 장점이 있다.\n그러나 binary trait일 때는 이 방법으로 heritability를 계산할 수가 없다. 보통 질병에 걸릴 확률 \\(p\\)가 Logistic distribution을 따른다고 가정하고 logistic regression model을 이용하게 되는데 이 때, Continuous variable에서 한 것처럼 추정된 variance of polygenic effect의 분산은 probability의 logit function의 scale에서 계산된 것이며, 전체분산은 probability scale에서 계산이 되기 때문에 이를 단순하게 분자, 분모로 하여 계산할 수 없다(Browne et al. 2005). binary trait에서 heritability를 구하는 방법으로는 first order Taylor series expansion을 이용하여 logit함수에 대한 회귀식을 \\(Y\\)에 대한 회귀식으로 approximation하여 linearisation 하는 방법, simulation을 이용하는 방법, latent variable을 이용하는 방법 등이 제시되어 있는데 이들 방법은 근본적으로 근사값의 계산이라는 한계점이 있으며, 해석을 어떻게 해야 되는지도 문제가 된다(Browne et al. 2005; Bonnet 2016; Vigre et al. 2004; Davies et al. 2015; Tenesa and Haley 2013).\nbinary trait에서 유전적인 정도를 표현하는 또다른 방법으로 recurrence risk ratio라는 것이 있는데, 이 중 흔히 이용되는 것은 sibling recurrence risk ratio(\\(\\lambda_s\\))로 일반 인구집단의 prevalence에 비해 affected people의 sibling 집단에서의 prevalence가 몇배나 더 높은지를 표현하며 직관적으로 이해할 수 있는 지표라는 장점이 있다(Rybicki and Elston 2000). 그러나 이 값은 일반 인구 집단에서의 prevalence를 정확히 측정해야 한다는 부담을 갖고 있으며 ascertainment bias에 민감하며, polygenic effect의 변화없이 common environmental factor의 영향에 의해서도 변화할 수 있다는 문제점이 있다(S.-W. Guo 1998, 2002; S.-W. Guo 2000).\n한편 binary trait의 multilevel analysis에서 group level의 효과를 쉽게 이해하기 위해 Median Odds Ratio(MOR)라는 개념이 제시되었는데, 이것의 정의는 “the median value of the odds ratio between the group at highest risk and the group at lowest risk when randomly picking out two groups”이다(Merlo et al. 2006). MOR은 group변수의 분산만을 이용하여 쉽게 계산될 수 있으며 OR scale이기 때문에 이해하기도 쉬운데, 1보다 클수록 group effect가 크고 1에 가까울수록 group effect가 없다고 해석할 수 있다(Larsen et al. 2000; Larsen and Merlo 2005; Merlo et al. 2006).\n이에 저자는 multilevel logistic regression에서 이용하였던 MOR의 개념을 이용하여 binary trait의 유전적인 정도를 측정하는 새로운 지표인 Median OR Ratio between Sibling Pairs and Unrelated Pairs(\\(\\text{MORR}_{SU}\\))을 소개할 것이다. 본 지표는 sibling pairs끼리의 OR에 비해 unrelated pairs끼리의 OR의 값이 대략적으로(median) 얼마나 높은가를 나타내며, 유전적인 부분이 전혀 없는 질병이라면 sibling pair에서나 unrelated pair에서의 OR이 같게 되어 \\(\\text{MORR}_{SU}\\)의 값은 1이 된다. 반면에 이 값이 크면 클수록 sibling pair끼리의 질병발생 양상이 unrelated pair끼리의 그것보다 비슷해지고 이는 유전적인 부분이 큰 질병임을 뜻한다. 본 지표는 hierarchial generalized linear model(HGLM)를 통하여 계산할 수 있으며, covariates를 보정할 수 있다는 heritability의 장점과 쉽게 해석되는 \\(\\lambda_s\\)의 장점을 동시에 갖고 있다. 반대로 말하면 직관적인 해석이 어려운 heritability의 단점과 prevalence 정보가 필요하며 covariate보정이 어려운 \\(\\lambda_s\\)의 단점을 보완하였다고도 할 수 있다. (Lee and Nelder 1996). 본 연구에서는 \\(\\text{MORR}_{SU}\\)의 개념을 소개한 후 실제 데이터인 Healthy twin study, Korea의 hypertriglycemia trait에 적용하여 유전적인 부분을 해석해 볼 것이다(SUNG et al. 2006).\nMethod\nBreif review of Meidan OR\n먼저 MOR에 대해 간략하게 요약하여 설명하겠다(Larsen et al. 2000).\n\\(Y_{ij}\\)를 \\(j\\)th group의 \\(i\\)th individual의 health status라 하고(case: 1, control: 0), \\(X_{ij}\\)를 vector of covariates, \\(G_j\\)를 \\(j\\)th group의 effect라 정의하자. 이 때 multilevel logistic regression의 formula를 mixed effect model로 표시하면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\beta_0+X_{ij}'\\beta_1+G_j\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_j\\): random intercept \\(G_j\\sim \\text{iid  } N(0,V_g)\\))\n이 때 Conditional Odds는\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{ij}=1|X_{ij},G_j)]=\\exp{(\\beta_0)}\\exp{(X_{ij}'\\beta_1)}\\exp{(G_j)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 X가 고정되어 있을 때 임의로 뽑은 \\(j\\)th Group과 \\(k\\)th Group의 Odds ratio는\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{ij}=1|X,G_j)]}{{\\text{Odds}[Pr(Y_{ik}=1|X,G_k)]}}=\\exp{(G_j-G_k)}\n\\end{aligned}\n\\]\n가 되고 Odds가 큰그룹을 Odds가 작은 그룹과 비교한다면 \\(\\text{OR}=\\exp{|G_j-G_k|}\\)이다. 이제 \\((G_j-G_k)\\sim N(0,2V_g)\\) 임을 이용하여 이것의 중앙값(median)을 계산하면\n\\[\n\\begin{aligned}\n\\text{MOR} = \\exp{(\\sqrt{2V_g}\\times \\Phi^{-1}{(0.75)})}\\simeq \\exp{(0.95\\sqrt{V_g})}\n\\end{aligned}\n\\]\n이 되고 이를 MOR로 정의한다. MOR은 VPC와 다르게 오직 group variable의 분산인 \\(V_g\\)만 가지고 계산될 수 있어 그 자체값과 95% 신뢰구간을 간단히 계산할 수 있으며, MOR\\(=1\\) 이라면 group variable의 effect가 없다고 해석할 수 있고 MOR이 커질수록 group variable의 effect가 크다고 해석할 수 있다.\nFormula’s of \\(\\text{MORR}_{SU}\\)\n\n이제 \\(\\text{MORR}_{SU}\\)의 수식을 유도하여 보자. \\(Y_{i}\\)를 \\(i\\)th individual의 health status라 하고(case: 1, control: 0, \\(1\\le i \\le n\\)), \\(X_{i}\\)를 vector of covariates, \\(G_i\\)를 \\(i\\)th individual의 polygenic effect라 정의하자. 이제 polygenic model을 수식으로 나타내면 다음과 같다.\n\\[\n\\begin{aligned}\n\\text{Logit}[Pr(Y_{i}=1|X_{i},G_i)]=\\beta_0+X_{i}'\\beta_1+G_i\n\\end{aligned}\n\\]\n(\\(\\beta_0\\): intercept, \\(\\beta_1\\): vector of fixed regression coefficients, \\(G_i\\): polygenic effect of \\(i\\)th individual)\n한편 \\(G_i\\)들의 vector를 \\(G=(G_1,G_2,\\cdots,G_n)'\\)이라 하면, \\(G\\sim N(0,V_p\\Sigma)\\) 이 된다(\\(\\Sigma\\): genetic relationship matrix, \\(V_p\\): variance of polygenic effect).\n이제 Conditional Odds를 계산해보면\n\\[\n\\begin{aligned}\n\\text{Odds}[Pr(Y_{i}=1|X_{i},G_i)]=\\exp{(\\beta_0)}\\exp{(X_{i}'\\beta_1)}\\exp{(G_i)}\n\\end{aligned}\n\\]\n로 표현할 수 있으며 임의로 뽑은 \\(i\\)th individual과 \\(j\\)th individual의 Odds ratio는 \\(X\\)가 같을 때 다음과 같이 표현된다.\n\\[\n\\begin{aligned}\n\\frac{\\text{Odds}[Pr(Y_{i}=1|X,G_i)]}{{\\text{Odds}[Pr(Y_{j}=1|X,G_j)]}}=\\exp{(G_i-G_j)}\n\\end{aligned}\n\\]\n이제 두 가지 경우를 생각하자.\n\n\\(i\\)와 \\(j\\)를 unrelated individuals에서 뽑았을 경우이다. 이 때는 앞서 MOR과 마찬가지로 \\((G_i-G_j) \\sim N(0,2V_g)\\)가 된다.\n\\(i\\),\\(j\\)를 sibling pair에서 뽑았다면, 즉 \\(i\\)와 \\(j\\)가 항상 sibling이라면 \\(Cov(G_i,G_j)=\\frac{1}{2}V_g\\) 이므로, \\((G_i-G_j) \\sim N(0,V_g)\\)가 된다.\n\n이제 unrelated individual과 sibling을 한 쌍씩 뽑아 각각 \\(G_i, G_j\\)와 \\(G_k, G_l\\)이라고 하면 Odds Ratio의 비인 OR Ratio(ORR)를 다음과 같이 정의한다.\n\\[\n\\begin{aligned}\n\\text{ORR}_{SU}=\\frac{\\text{OR}_{unrelated}}{\\text{OR}_{sibling}}=\\frac{\\exp{|G_i-G_j|}}{\\exp{|G_k-G_l|}} =  \\exp{(|G_i-G_j|-|G_k-G_l|)}\n\\end{aligned}\n\\]\n\\(|G_i-G_j|-|G_k-G_l|\\)는 \\(|N(0,2V_g)| - |N(0, V_g)|\\) 의 분포를 따르는 것을 이용하여 median값을 계산하면 약 \\(0.2453\\times \\sqrt{V_p}\\)이고 최종지표인 Median OR Ratio(MORR)은 아래와 같다(Appendix).\n\\[\n\\begin{aligned}\n\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\n\\end{aligned}\n\\]\n즉 sibling pair들에서의 polygenic effect의 OR과 unrelated에서의 polygenic effect의 그것을 비교한 지표이며 범위는 1부터 무한대까지이다. \\(\\text{MORR}_{SU}\\)가 1이면 unrelated pair에서의 질병발생 양상의 차이가 sibling pair에서의 그것과 같게 되어 유전적인 부분이 전혀 없다고 해석할 수 있다. 값이 커질수록 sibling pair끼리는 unrelated pair끼리보다 질병발생 양상이 비슷하다고 볼 수 있으며 이는 곧 유전적인 부분이 큰 것으로 이해할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-medianorratio/index.html#apply-to-real-data",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Apply to Real Data",
    "text": "Apply to Real Data\nHealthy Twin Study, Korea의 데이터에 본 지표를 적용하였다(SUNG et al. 2006). 가족-쌍둥이 구조로 이루어진 3,461명의 사람 중 지질검사와 음주, 흡연 정보가 있는 2,729명을 대상으로 150이상을 case, 150미만을 control로 정의하였으며 아무것도 보정하지 않은 Null model(Model 1), 성별과 연령을 보정한 모형(Model 2), 그리고 성별, 연령, 음주, 흡연력을 보정한 모형(Model 3)에 대해서 각각의 \\(V_p\\)값과 그에 따른 \\(\\text{MORR}_{SU}\\)값을 제시하였다(Table 1). 분석은 R 3.5.1버전에서 hglm package를 이용하였다(Ronnegard, Shen, and Alam 2010).\n\n\n\nVariance parameter, heritablity and MORR of polygenic effect\n\n\n\n\n\n\n\n\nModel 1 (95%CI)\nModel 2 (95%CI)\nModel 3 (95%CI)\n\n\n\n\\(V_p\\)\n0.59 (0.49-0.71)\n0.68 (0.57-0.81)\n0.66 (0.55-0.79)\n\n\nHeritablity\n0.15 (0.13-0.18)\n0.17 (0.15-0.2)\n0.17 (0.14-0.19)\n\n\n\\(\\text{MORR}_{su}\\)\n1.16 (1.13-1.19)\n1.18 (1.15-1.22)\n1.18 (1.14-1.21)\n\n\n\n\n\n(Heritability: Intraclass correlation coefficients(\\(\\frac{V_p}{V_p+\\frac{\\pi^2}{3}}\\)), Model 1: no covariate, Model 2: age & sex as covariates, Model 3: age, sex and alcohol/smoking status as covariates)\nModel 2를 살펴보면 age와 sex의 effect를 보정하고 난 후의 \\(V_p\\)값은 0.59(95% CI: 0.49-0.71)이고 이에 해당하는 \\(\\text{MORR}_{su}\\)의 값은 1.18(95% CI: 1.15-1.22)였으며 이는 sibling pair의 OR에 비해 unrelated pair의 OR의 값이 대략적으로 18% 더 높다고 해석할 수 있다. 한편 Null model(Model 1)의 \\(\\text{MORR}_{su}\\)의 값은 1.16(95% CI: 1.13-1.19)이며 age, sex, smoling status를 보정했을 때(Model 3)는 1.18(95% CI: 1.14-1.21)로 세 Model의 결과는 비슷했다. 이는 age, sex, alcohol, smoking status 보정 여부에 크게 상관없이 hypertriglycemia에 일정한 유전적인 영향이 존재함을 의미한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "href": "posts/2018-11-08-medianorratio/index.html#conclusion",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Conclusion",
    "text": "Conclusion\n저자가 제시한 \\(\\text{MORR}_{su}\\)의 개념을 이용하여 binary trait의 유전적인 정도를 직관적으로 설명할 수 있었다. 이는 해석이 어려운 heritability의 단점을 극복하였다는 의미가 있다. 또한 age, sex 등 다양한 covariates의 효과를 보정한 후의 유전적인 부분을 설명할 수 있고, 인구집단의 prevalence를 측정할 필요가 없다는 점에서 \\(\\lambda_s\\)의 단점을 보완한 지표라 할 수 있다.\n향후 heritability와 \\(\\lambda_s\\) 각각의 장점은 살리고 단점은 보완한 이 지표가 binary trait의 polygenic effect를 직관적으로 설명하는 방법으로 널리 쓰이길 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-medianorratio/index.html#appendix",
    "href": "posts/2018-11-08-medianorratio/index.html#appendix",
    "title": "New Scale Measure of Heritability in Binary Trait Using Median Odds Ratio: Median OR Ratio",
    "section": "Appendix",
    "text": "Appendix\nCalculate MORR\n\\(|N(0,2V_p)| - |N(0, V_p)|\\)의 median을 \\(M\\)이라 하자. R의 distr package를 이용하면 아래와 같이 \\(V_g\\)값들에 따른 \\(M\\)값을 쉽게 계산할 수 있다(Ruckdeschel and Kohl 2014).\n\n\n\n\n\\(V_p, \\sqrt{V_p}\\) vs Median value of \\(|N(0,2V_p)| - |N(0, V_p)|\\)\n\n\n\n\n이 그래프의 직선의 기울기가 바로 0.2453이고 따라서 \\(\\text{MORR}_{SU}= \\exp{(0.2453\\times \\sqrt{V_p})}\\)가 된다.\nCompare to Other Measures\n\n\n\n\nRelationship between \\(h^2\\) and \\(\\text{MORR}_{SU}\\)"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html",
    "href": "posts/2018-11-08-mdlm/index.html",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "",
    "text": "본 연구는 김진섭 대표가 박사학위 논문으로 계획했던 연구로, 결과적으로 학술지 게재와 심사통과에 실패했다는 것을 미리 알려드립니다. 계산법은 R package 로 만들었습니다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#abstract",
    "href": "posts/2018-11-08-mdlm/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Abstract",
    "text": "Abstract\n선형모형을 적용하기 어려운 \\(J,U\\)-shape같은 curved linear relationship을 비선형모형으로 분석하면 선형모형에 비해 해석이 어려워진다. 이에 본 연구에서는 선형관계의 컨셉은 유지하면서 \\(J,U\\)-shape 같은 curved linear relationship을 표현할 수 있는 모형을 제안한다. 이것은 선형모형의 무대를 1차원에서 휘어진 다차원 공간으로 확장함으로서 가능하며, curved linear relationship을 다차원공간에서의 선형관계로 재해석할 수 있다. 시뮬레이션 결과 선형관계는 기존의 선형모형과 동등한 성능으로 추정하면서 더 우수한 성능으로 curved relationship를 추정할 수 있었고, 실제 \\(U\\)-shape을 보이는 관계를 다차원공간에서의 선형관계로 쉽게 설명할 수 있었으며 \\(U\\)-shape의 cut-off값도 쉽게 계산할 수 있었다. 선형모형을 완벽히 포함하여 확장한 본 연구의 제안이 건강연구의 새로운 표준으로 자리잡을 수 있으리라 자신한다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#introduction",
    "href": "posts/2018-11-08-mdlm/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Introduction",
    "text": "Introduction\nMultivariable Linear Model은 분석결과의 해석이 간단하면서도 여러 독립변수들을 동시에 고려할 수 있는 장점이 있어 Health Science에서 널리 이용된다(Schneider, Hommel, and Blettner 2010). 그러나 모든 관계가 선형관계인 것은 아니며 흔한 non-linear relationship으로 \\(J,U\\)-shape같은 curved linear relationship이 있다(Calabrese and Baldwin 2001; Power, Rodgers, and Hope 1998; de Wit et al. 2009; Knutson and Turek 2006). 이런 관계를 단순히 선형모형으로 분석하게 되면 간단하긴 하나 정확한 추정을 할 수 없으며 exponential, Log나 제곱, 루트를 이용해 변수를 치환하여 선형모형을 이용할 수 있다(Jagodzinski and Weede 1981). 그러나 치환으로 선형관계를 만들 수 있는 경우는 극히 일부분에 지나지 않아 많은 경우에 비선형모형(non-linear model)을 활용하는데, 대표적인 방법으로는 독립변수의 고차항을 모형에 추가하거나(Polynomial Model) 비모수적인 방법으로 곡선을 추정하는 Additive Model, 그리고 Multi-layer를 이용한 neural network이 있다(Jagodzinski and Weede 1981; Buja, Hastie, and Tibshirani 1989; Hornik, Stinchcombe, and White 1989). 그러나 이런 비선형모형들은 휘어진 모양을 해석하기 때문에 직선으로 해석하는 선형모형에 비해 해석이 복잡할 수 밖에 없다.\n이와 비슷한 문제가 20세기 초 물리학에서도 있었는데 태양 주위에서 빛이 휘는 문제가 바로 그것이다. 이 현상은 뉴턴의 물리학으로 설명되지 않았었는데, 아인슈타인(Albert Einstein)은 빛이 휘는 것이 아니라 태양 근처의 4차원 시공간(spacetime)이 휘어진 것이라는 발상의 전환을 통해 이 문제를 설명하였다(Coles 2001). 이것이 유명한 일반상대성이론으로 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 의미한다(Verlinde 2011).\n이에 저자는 아인슈타인의 아이디어와 비슷하게 선형모형의 무대를 휘어진 다차원 공간으로 확장함으로서 \\(J,U\\)-shape을 선형관계로 해석할 수 있는 Multi-dimensional Linear Model(MDLM)을 제안한다. 이것은 기존의 선형모형에 차원(dimension)의 개념을 추가하여 일반화한 것으로 모든 독립변수(independent variable)들이 같은 dimension의 정보라면 기존의 Linear Model과 일치한다. 먼저 개념을 수식으로 정리한 후 계수들을 추정하는 방법을 설명할 것이며 다양한 시나리오를 시뮬레이션하여 MDLM의 유용성을 살펴보겠다. 마지막으로 실제 \\(U\\)-shape을 갖는 데이터에 본 모형을 적용하여 유용성을 평가할 것이다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#formula",
    "href": "posts/2018-11-08-mdlm/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Formula",
    "text": "Formula\n음이 아닌 실수 \\(Y\\)를 종속변수로 실수 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\)들을 독립변수라 하자.\n2 independent variables, 2 dimensions\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 선형관계를 2차원 벡터공간에서 표현하면 아래와 같다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\n\\end{aligned}\n\\]\n\\(\\boldsymbol{\\vec{g}}_i\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며 Figure @ref(fig:fig1)에 그림으로 표현되어 있다.\n\n\n\n\nMDLM with 2 variables, 2 dimensions\n\n\n\n\n이것은 방향의 개념을 제외하면 기존의 선형모형과 같으며, 만일 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)이 같은 방향이라면 아래와 같이 기존 선형모형과 일치하게 된다.\n\\[\n\\begin{aligned}\nY &= (\\beta_{01} + \\beta_1X_1) + (\\beta_{02} + \\beta_2X_2) \\\\\n  &= \\beta_{0} + \\beta_1X_1 + \\beta_2X_2\n\\end{aligned}\n\\]\n(\\(\\beta_0 = \\beta_{01}+\\beta_{02}\\))\n해석은 기존의 선형모형과 같이 변화량을 이용하며 \\(Y\\)와 \\(X_1\\), \\(X_2\\)의 변화량에 대해서 식을 재구성하면 아래와 같다.\n\\[\n\\begin{aligned}\nd\\boldsymbol{\\vec{Y}} &= \\beta_1dX_1\\boldsymbol{\\vec{g}}_1 +\\beta_2dX_2\\boldsymbol{\\vec{g}}_2\n= \\beta_1d\\boldsymbol{\\vec{X}_1} + \\beta_2d\\boldsymbol{\\vec{X}_2}\n\\end{aligned}\n\\]\n즉, \\(X_2\\)가 고정되어 있을 때 \\(Y\\)는 \\(X_1\\)의 방향으로 \\(\\beta_1\\)만큼 증가한다고 할 수 있으며, \\(X_1\\)이 고정되어 있다면 \\(Y\\)는 \\(X_2\\)의 방향으로 \\(\\beta_2\\)만큼 증가한다고 볼 수 있다.\n벡터로 표현된 위 식을 \\(\\boldsymbol{\\vec{g}}_1\\)과 \\(\\boldsymbol{\\vec{g}}_2\\)의 내적값인 \\(g_{12}\\)를 이용해 스칼라로 표현하면 아래와 같다.\n\\[Y^2 = (\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)\\]\n만약 \\(g_{12}=0\\) 즉, \\(X_1, X_2\\)가 독립된 차원을 갖는다면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1=-\\frac{\\beta_{01}}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보이며 \\(X_2\\)와 \\(Y\\)의 관계도 마찬가지이다. 일반적으로 \\(Y^2 = (\\beta_{01} + \\beta_1X_1 + g_{12}(\\beta_{02} + \\beta_2X_2))^2+ (1-g_{12}^2)(\\beta_{02} + \\beta_2X_2)^2\\)로 식을 변형하면 \\(X_2\\)가 고정되었을 때 \\(X_1\\)과 \\(Y\\)의 관계는 \\(X_1 = -\\frac{\\beta_{01}+g_{12}(\\beta_{02} + \\beta_2X_2)}{\\beta_1}\\)에서 최소값을 갖는 \\(U\\)-shape을 보임을 확인할 수 있다.\n\n\\(p\\) independent variable, 2 dimensions\n일반적으로 독립변수가 \\(p\\)개인 경우 \\(X_1, \\cdot, X_l\\)이 같은 차원, \\(X_{l+1}, \\cdot, X_p\\)가 같은 차원에 있다고 가정하면 다음과 같이 벡터식과 스칼라식을 표현할 수 있다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p )\\boldsymbol{\\vec{g}}_2 \\\\\\\\\nY^2 &= (\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)^2 + (\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)^2 \\\\\n+ & 2g_{12}(\\beta_{01} + \\beta_1X_1 + \\cdots + \\beta_lX_l)(\\beta_{02} + \\beta_{l+1}X_{l+1} \\cdots + \\beta_pX_p)\n\\end{aligned}\n\\]\n\n\\(p\\) independent variable, \\(p\\) dimensions\n마지막으로 독립변수들이 전부 다른 방향을 갖고 있다고 가정한다면 아래와 같은 벡터식과 스칼라식을 얻는다.\n\\[\n\\begin{aligned}\n\\boldsymbol{\\vec{Y}} &= (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_{2}X_2 )\\boldsymbol{\\vec{g}}_2 + \\cdots (\\beta_{0p} + \\beta_pX_p)\\boldsymbol{\\vec{g}}_p \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\\\\\nY^2 &= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\cdot\n\\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)\\boldsymbol{\\vec{g}}_i} \\\\\n&= \\sum_{i=1}^{p}{(\\beta_{0i} + \\beta_iX_i)^2} + 2\\sum_{i < j}{g_{ij}(\\beta_{0i} + \\beta_iX_i)(\\beta_{0j} + \\beta_jX_j)}\n\\end{aligned}\n\\]\n여기서 \\(g_{ij}\\)는 \\(X_i\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_i\\)와 \\(X_j\\)방향의 단위벡터 \\(\\boldsymbol{\\vec{g}}_j\\)의 내적값으로 두 벡터의 dependency를 나타낸다. 위의 경우와 마찬가지로 모든 \\(g_{i}\\)들의 방향이 같다면 아래와 같이 기존의 선형모형과 같은 관계를 얻는다.\n\\[Y= \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_pX_p\\]\n(단, (\\(\\beta_0 = \\beta_{01}+\\beta_{02}+\\cdots + \\beta_{0p}\\)) )"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#estimation",
    "href": "posts/2018-11-08-mdlm/index.html#estimation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Estimation",
    "text": "Estimation\n\\(\\beta = (\\beta_1, \\beta_2, \\cdots, \\beta_p, \\beta_{01}, \\beta_{02}, \\cdots, \\beta_{0p})\\) estimation을 위해 최소화해야 할 cost function은 아래와 같이 오차제곱의 합(Sum of Squared Error: SSE)으로 하면 자연스럽게 기존 선형모형의 최소제곱 추정을 일반화 할 수 있다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i<j}g_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n위 식에서 \\(g_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\n\\(\\beta\\) Estimation\n기존 선형모형과 달리 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 직접 계산하기 어려워, optimization technique을 이용하며 Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다 (Nelder and Mead 1965; Fletcher 1964; Byrd et al. 1995). 위의 방법들을 활용하여 우리는 초기 \\(\\beta\\)값들부터 시작해서 반복적인 계산을 통해 수렴값을 얻게 된다. 한편 초기값이 바뀌면 \\(SSE(\\beta)\\)의 값은 같더라도 \\(\\beta\\)들의 부호가 다른 결과를 얻을 가능성이 있는데 이는 \\(SSE(\\beta)\\)가 \\(\\beta\\)들의 2차식으로만 구성되어 있기 때문이며, \\(\\beta\\)들의 부호가 바뀌더라도 \\(SSE(\\beta)\\)가 같다면 해석은 동일하다.\n추정된 \\(\\beta\\)값들의 standard error들은 \\(SSE(\\beta)\\)의 hessian matrix(\\(H\\))로 부터 구할 수 있다. 어떤 함수 \\(f(\\theta)\\)의 hessian matrix란 \\(f(\\theta)\\)를 2번 미분한 성분들로 이루어진 행렬인데, 일반적으로 \\(\\theta\\)의 variance와 반비례함이 알려져 있다(Dovi, Paladino, and Reverberi 1991). 여기에서는 1변수 함수의 예를 통해 직관적으로 이해해보도록 하자. \\(f(\\theta)\\)가 \\(\\theta_0\\)에서 최소값을 갖을 때 \\(f\\)를 1번 미분한 값은 0임을 이용, \\(f\\)를 \\(\\theta_0\\) 부근에서 2차항까지만 테일러 급수전개를 하면\n\\[\n\\begin{aligned}\nSSE(\\hat{\\theta} + d\\theta) &= SSE(\\hat{\\theta}) + H \\cdot\\dfrac{(d\\theta)^2}{2}\n\\end{aligned}\n\\]\n이고 \\(d\\theta\\)에 대해 정리하면\n\\[(d\\theta)^2 = 2\\cdot H^{-1}\\cdot (SSE(\\hat{\\theta}+d\\theta)-SSE(\\hat{\\theta}))\\]\n이 된다. 즉 hessian이 커질수록 \\(\\theta\\)의 분산에 해당하는 \\((d\\theta)^2\\) 이 감소함을 알 수 있다.\n일반적으로 \\(SSE(\\beta)\\)를 최소로 하는 \\(\\hat{\\beta}\\)들의 variance-covariance matrix는 아래와 같이 표현되며, 대각성분에 루트를 취하면 \\(\\beta\\)들의 standard error 값이 되어 p-value와 confidence interval(CI)을 계산할 수 있다(Dovi, Paladino, and Reverberi 1991).\n\\[\\text{vcov}(\\hat{\\beta}) = 2\\cdot H^{-1}\\cdot MSE(\\hat{\\beta})\\]\n(\\(MSE\\): Mean Squared Error)\nEstimation of \\(g_{ij}\\)\n\n위에 설명한 추정은 \\(g_{ij}\\)가 고정되었을 때를 가정한 것인데, \\(g_{ij}\\)를 데이터에서 직접 구할 수도 있으며 이것은 Generalized Estimating Equation(GEE)에서 working correlation matrix를 직접 계산할 수 있는 것과 마찬가지이다(Pan and Connett 2002). 이 때는 \\(g_{ij}\\)들은 \\(\\beta\\)들과 달리 -1에서 1까지의 값을 갖는다는 제한조건이 있어 constrained optimization technique를 이용해야 하며 나머지는 앞서와 동일하다(Rios and Sahinidis 2013)."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#simulation",
    "href": "posts/2018-11-08-mdlm/index.html#simulation",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Simulation",
    "text": "Simulation\n\\(Y\\)와 \\(X_1\\), \\(X_2\\)의 여러가지 관계에 대한 Simulation을 이용해 MDLM의 유용성을 살펴볼 것이며 구체적으로 다음의 5개 모형을 비교하겠다.\n\nLinear Model (LM)\nMDLM with fixed \\(g_{12}=0\\) (MDLM 1)\nMDLM with non-fixed \\(g_{12}\\) (MDLM 2)\nQuadratic Model: Polynomial model with 2nd degree (Quadratic)\nGeneralized Additive Model (GAM)\n\n모형비교는 Root Mean Square Error(RMSE)와 Akaike Information Criterion(AIC)을 이용하였으며 GAM의 경우는 effective degree of freedom(edf)를 AIC 계산에 활용하였다(Wood 2001).\n편의상 \\(X_1\\)과 \\(X_2\\)는 각각 1부터 10까지의 자연수를 갖는 것으로 가정하였으며 따라서 샘플수는 100이다. 모든 계산은 R 3.5.1의 optim, constrOptim 함수를 이용하였다.\n\n\n\nScenario 1: \\(Y = X_1 + X_2\\)\n\n\\(Y \\sim N(X_1 + X_2, 1)\\) 로 샘플링해 데이터를 생성하였으며 100회의 시뮬레이션을 수행해 RMSE와 AIC를 비교하였다(Table 1).\n\n\n\nResults: \\(Y = X_1 + X_2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1 ± 0\n1.2 ± 0\n1 ± 0\n1 ± 0\n0.9 ± 0\n\n\nDF\n4\n5\n6\n6\n7.2 ± 0.7\n\n\nAIC\n290 ± 2.8\n335.1 ± 4.9\n291.2 ± 5\n293.1 ± 3.3\n282.3 ± 7.9\n\n\n\n\n\n비교 결과 선형모형이 적은 parameter로 효율적인 추정을 하고 있음을 알 수 있었으며 \\(g_{12}\\)를 고정하지 않은 MDLM(2)가 선형모형과 비슷한 성능을 보이는데 이는 MDLM이 선형모형을 포함한 개념임을 생각했을 때 자연스러운 결과이다.\nScenario 2: \\(Y^2 = X_1^2 + X_2^2\\)\n\n이번엔 \\(Y \\sim N(\\sqrt{X_1^2 + X_2^2}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 2).\n\n\n\nResults: \\(Y^2 = X_1^2 + X_2^2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.2 ± 0.1\n1 ± 0\n1 ± 0\n1.1 ± 0.1\n1.1 ± 0.1\n\n\nDF\n4\n5\n6\n6\n5.1 ± 0.1\n\n\nAIC\n321.3 ± 9.7\n292.6 ± 7.1\n293.2 ± 6.9\n318.2 ± 11.9\n317.7 ± 11.9\n\n\n\n\n\n이번엔 MDLM(1)이 선형모형보다 확실히 우수한 추정값을 보이는것을 확인할 수 있으며, \\(g_{12}\\)를 고정하지 않은 MDLM(2) 또한 MDLM(1)과 비슷한 성능을 보이는 것을 확인할 수 있다.\nScenario 3: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n마지막으로 Scenario2를 일반화한 경우를 시뮬레이션하였다. \\(\\beta\\)들은 -5~5, \\(g_{12}\\)는 -1~1의 값을 임의로 선택하여 \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)를 만족하는 \\(Y\\)를 샘플링하였다. 즉, \\(Y \\sim N(\\sqrt{(\\beta_{01} + \\beta_1X_1)^2 + (\\beta_{02} + \\beta_2X_2)^2 + 2g_{12}(\\beta_{01} + \\beta_1X_1)(\\beta_{02} + \\beta_2X_2)}, 1)\\)로 샘플링해 데이터를 생성하여 마찬가지로 100회의 시뮬레이션을 수행하였다(Table 3).\n\n\n\nResults: \\(\\boldsymbol{\\vec{Y}} = (\\beta_{01} + \\beta_1X_1)\\boldsymbol{\\vec{g}}_1 + (\\beta_{02} + \\beta_2X_2)\\boldsymbol{\\vec{g}}_2\\)\n\n\n\n\n\n\n\n\n\n\n\nLM\nMDLM(1)\nMDLM(2)\nQuadratic\nGAM\n\n\n\nRMSE\n1.1 ± 0\n1.3 ± 0.4\n1 ± 0.1\n1.1 ± 0.1\n1.1 ± 0.1\n\n\nDF\n4\n5\n6\n6\n4.9 ± 1.6\n\n\nAIC\n311.6 ± 1.2\n347.2 ± 58.6\n294.7 ± 28.3\n306.8 ± 12.7\n304.8 ± 10.5\n\n\n\n\n\n시뮬레이션 결과 \\(g_{12}\\)를 추정할 수 있는 MDLM(2)가 다른 모형들보다 압도적으로 우수한 성능을 보이는 것을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "href": "posts/2018-11-08-mdlm/index.html#apply-to-real-data",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Apply to Real data",
    "text": "Apply to Real data\n이번에는 실제 \\(U\\)-shape을 보이는 데이터를 MDLM으로 분석해보겠다. http://biostat.mc.vanderbilt.edu/dupontwd/wddtext/data/3.25.2.SUPPORT.csv의 데이터에는 응급실 내원 당시 평균 동맥압(mean arterial pressure, MAP)과 재실기간(length of stay,LOS)의 정보가 있는데 MAP와 LOS의 natural logarithm값의 관계가 \\(U\\)-shape이다. 이제 log(LOS)와 MAP의 관계를 아래와 같이 모델링 하였다.\n\\[\\boldsymbol{\\vec{\\text{log(LOS)}}} = \\beta_{00}\\boldsymbol{\\vec{g_{1}}} + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})\\boldsymbol{\\vec{g_{2}}}\\]\n즉 log(LOS)를 intercept와 MAP의 독립된 2차원으로 바라보는 관점으로 스칼라로 표현한 모형은 아래와 같다.\n\\[(\\text{log(LOS)})^2 = \\beta_{00}^2 + (\\beta_{01}+ \\beta_1\\cdot \\text{MAP})^2\\]\n\n\n\n\n\n\nMDLM을 이용하여 Intercept와 MAP의 2차원공간으로 log(LOS)를 \\(\\text{log(LOS)}^2 = 2.3669^2 + (-2.4276 + 0.0295\\cdot\\text{MAP})^2\\)의 모형으로 추정할 수 있었고 AIC값은 \\(2413\\)였다. 이것은 선형모형으로 추정한 \\(\\text{log(LOS)} = 2.2624 + 0.0027 \\cdot \\text{MAP}\\)(AIC \\(2434\\)), quadratic항을 추가한 \\(\\text{log(LOS)} = 3.3742 -0.0246 \\cdot \\text{MAP} + 2\\times 10^{-4} \\cdot \\text{MAP}^2\\)(AIC \\(2414\\))보다 좋은 추정 결과이다(Figure @ref(fig:fig2)). 또한 U shape의 cutoff값을 \\(\\dfrac{2.4276}{0.0295} = 82.29\\)로 간단히 계산할 수 있었다.\n\n\n\n\nRelation between mean arterial pressure(mmHg) and length of stay day(log scale)"
  },
  {
    "objectID": "posts/2018-11-08-mdlm/index.html#discussion",
    "href": "posts/2018-11-08-mdlm/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장 (Linear Model in Multidimensional Space)",
    "section": "Discussion",
    "text": "Discussion\nMDLM을 이용해서 기존의 선형모형을 완벽히 포괄하면서 선형모형의 개념을 휘어진 다차원 공간으로 확장할 수 있었고, 이를 통해 \\(J,U\\)-shape같은 curved linear relationship을 잘 추정하고 cut-off값도 쉽게 확인할 수 있는 장점을 확인할 수 있었다. 기존 선형모형의 틀은 유지하면서 그것만이 진실은 아닐 수 있다는 것을 보여줬다는 점, 이를 통해 선형관계가 아닌 것을 선형관계로 해석할 수 있는 방법을 제시하였다는 점에서 큰 의미가 있다. 향후 연구자들이 평면에서의 선형관계에 국한되지 않고 가설을 검증할 수 있을 것이며, 이를 토대로 Health science 연구에서 MDLM이 기존 선형모형을 포괄하는 새로운 표준으로 자리잡을 수 있으리라 예상한다.\nMDLM의 추정식은 제곱근이 포함되어 있어 \\(Y\\)가 음수값을 갖고 있는 경우에는 적용하기 어렵다는 한계가 있다. 그러나 Health Science 분야에서 음수값을 갖는 지표는 별로 많지 않아 큰 문제는 아닐 것으로 생각하며, 변수변환을 통해 (+)로만 이루어진 새로운 변수를 만들어 해결할 수도 있다. 이 문제는 물리학자 Paul Dirac이 특수상대성이론을 고려한 양자역학의 방정식을 만들 때 겪었던 문제와 비슷한데, 그는 방정식의 계수가 꼭 숫자일 필요가 없고 행렬일 수도 있다는 기발한 아이디어로 이를 해결했다(Dirac 1928). 예를 들어 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 }\\) 일 때, \\(\\beta_0, \\beta_1, \\beta_2\\)가 숫자일 필요가 없다는 것이다. \\(\\beta\\)들을 행렬로 간주한다면 \\(Y = \\sqrt{\\beta_0^2 + \\beta_1^2x_1^2 + \\beta_2^2x_2^2 } = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\) 꼴이 되어 제곱근을 없애는 것이 가능하다. 이 행렬들은 최소 \\(4\\times 4\\) 이상의 정방행렬이어야 함이 알려져 있으며, 대표적인 예로 Cliford Algebra를 만족하는 Dirac matrices 있는데 \\(\\beta\\)들의 예를 하나 들면 아래와 같다(Traubenberg 2009).\n\\[\n\\beta_0 = \\alpha_0 \\times \\begin{pmatrix}\n  1 & 0 &  0 &  0 \\\\\n  0 & 1 &  0 &  0 \\\\\n  0 & 0 &  -1 &  0 \\\\\n  0 & 0 &  0 & -1\n\\end{pmatrix}\n\\]\n\\[\n\\beta_1 = \\alpha_1 \\times \\begin{pmatrix}\n   0 &  0 & 0 & 1 \\\\\n   0 &  0 & 1 & 0 \\\\\n   0 & -1 & 0 & 0 \\\\\n  -1 &  0 & 0 & 0\n\\end{pmatrix}\n\\]\n\\[\n\\beta_2 = \\alpha_2 \\times \\begin{pmatrix}\n   0 & 0 & 1 &  0 \\\\\n   0 & 0 & 0 & -1 \\\\\n  -1 & 0 & 0 &  0 \\\\\n   0 & 1 & 0 &  0\n\\end{pmatrix}\n\\]\n(\\(\\alpha_0, \\alpha_1, \\alpha_2\\): 실수)\n회귀계수가 숫자가 아닌 행렬이 가능하다는 이 아이디어가 향후 본 연구의 제곱근 문제를 극복하는 열쇠가 될 수 있을 것이라 생각한다.\n\\(SSE(\\beta)\\)를 최소화하는\\(\\beta\\)를 구하기 위해 optimization tequnique을 활용한 것도 문제가 될 수 있는데, 얻은 \\(SSE(\\beta)\\)값이 진짜 최소값(global minimum)인지 보장할 수 없기 때문이다. 이를 local minima problem이라 한다. 그러나 machine learning의 유행과 더불어 optimization technique도 빠르게 발전되고 있어 조만간 이 문제가 해결되리라 예상한다. 게다가 최근 연구에서 high-dimensional space인 경우 local minima problem은 매우 희귀한 것으로 나타났는데, 모든 차원에서 local minima일 가능성은 매우 낮기 때문으로 여겨진다(Dauphin et al. 2014).\nIntroduction에서 언급했듯이 아인슈타인(Albert Einstein)은 공간의 무대를 3차원이 아니라 휘어진 4차원으로 확장한다면 빛은 여전히 직선임을 설명하였는데, 수식으로 살펴보면 3차원 공간에서 기술된 뉴턴의 중력장 방정식 \\(\\nabla^2\\Phi = 4 \\pi G\\rho_0\\)를 휘어진 4차원에서의 방정식 \\(\\boldsymbol{R}_{uv}-\\dfrac{1}{2}\\boldsymbol{g}_{uv} = \\dfrac{8\\pi G}{c^4}\\boldsymbol{T}_{uv}\\)로 확장한 것이다(Verlinde 2011). 본 연구의 MDLM을 통해 선형공간의 무대를 다차원으로 확장하여 \\(U\\)-shape같은 curved linear relationship을 선형관계로 바라볼 수 있게 되었다는 점에서 물리학에서의 아인슈타인 방정식과 비슷한 의미를 가진다고 감히 주장해 본다. 실제로 \\(\\beta_i^2\\) 를 \\(g_{ii}\\), \\(g_{ij}\\beta_i\\beta_j\\)를 합쳐서 \\(g_{ij}\\)라 놓으면 \\(g_{ij}\\)는 아인슈타인 중력장 방정식을 표현하는데 쓰이는 계량텐서(metric tensor) \\(g_{uv}\\)와 같은 의미를 갖게 되는데, \\((dY)^2\\)를 표현하는 식은 \\(\\sum_{i,j}g_{ij}(dX_i)(dX_j)\\)로 휘어진 시공간에서 두 지점 사이의 거리를 나타내는 방법과 정확히 일치한다. 뉴턴의 방정식으로도 일상적인 운동을 잘 설명할 수 있으나 우주 공간같은 거시적인 스케일에서는 아인슈타인 방정식이 필요해지는데, 이와 마찬가지로 다차원 공간에서 기술된 본 연구의 MDLM이 population level에서 기존 선형모형보다 더 정확히 건강관련 현상을 설명할 수 있으리라 예상한다.\n한편 일반상대성이론은 원자 이하의 미시세계의 현상을 잘 설명하지 못한다는 문제점이 있으며,불확정성의 원리(uncertainty principle)와 슈뢰딩거 방정식(Schrödinger equation)으로 대표되는 양자역학(quantum mechanics)의 논리가 이곳을 지배한다. 슈뢰딩거 방정식은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용으로 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특이한 점이다. 복소수는 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현하게 되고 놀라운 정확도로 미시세계의 현상을 설명할 수 있다. 이것은 Health science에도 중요한 시사점이 될 수 있는데, Health science에서 가장 큰 문제점 중 하나가 population level의 연구결과가 개인의 건강상태를 잘 설명하지 못한다는 것이다. 상태를 확률로 기술한다는 점에서는 베이지안 접근법(bayesian approach)이 양자역학의 접근과 비슷하지만 복소수를 활용할 수 없다는 점에서 차이가 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 개인의 건강상태를 설명하는 새로운 방법이 될 것이라 과감히 추측해 본다."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 2월 Shinykorea 밋업에 참석, 서울시 감염병연구센터 자문으로 코로나 수리모델링을 수행한 경험을 공유할 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#요약",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "요약",
    "text": "요약\n\n대표적인 감염병 모형은 SIR/SEIR 이고, 초기값과 parameter(예: 감염률)가 주어진 미분방정식으로 표현.\n확진자수 등 실제 데이터를 활용, parameter 들을 추정.\nSEIR 에 서울시 확진자수를 적용, 시간에 따라 변화하는 감염률을 계산함.\nParameter와 그 파생지표의 범위를 제한하고 신뢰구간을 계산하기 위해, 베이지안통계 이용 예정."
  },
  {
    "objectID": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "href": "posts/2021-01-22-covidmodel-seoul/index.html#slide",
    "title": "코로나 수리모델링: 서울시 감염병연구센터 자문",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/covidmodel-seoul/modelling/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html",
    "href": "posts/2018-11-08-ruck2018/index.html",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "",
    "text": "김진섭 대표는 11월 26일(금) 서울특별시 시민청에서 열린 R User Conference in Korea 2018(RUCK 2018) 에서 맞춤형 의학연구 애플리케이션 개발 환경 구축 경험에 대해 발표하였습니다. 초록과 발표 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#abstract",
    "href": "posts/2018-11-08-ruck2018/index.html#abstract",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Abstract",
    "text": "Abstract\n맞춤형 의학통계 앱 제작을 위해\n\nDocker swarm 기반의 Rstudio & shiny server 를 구축하고\n의학통계 앱에 필요한 R 패키지와 Shiny Application 들을 만들었습니다.\n\n미리 Rstudio와 shiny server가 설치된 도커(docker) 이미지를 만들고 이것을 도커 스웜을 이용해 배포함으로써 서버의 종류와 갯수에 구애받지 않는 마이크로서비스 아키텍처(microservice architecture)를 구축하였으며, 동적 프록시 서버(dynamic proxy server) 프로그램인 Traefik 을 이용하여 서비스가 추가될 때 마다(ex: 홈페이지, Jupyter) 이에 맞추어 https 보안이 적용된 서브도메인(subdomain) 주소를 부여하였습니다. 흔히 이용되는 의학통계 방법들을 Shiny Application으로 만들어 위의 환경에 배포하였으며 DT, tableone, epiDisplay, svglite 등의 기존 패키지와 자체적으로 개발한 패키지를 이용, 데이터 라벨(label) 정보가 적용된 논문용 테이블과 그림을 보여줄 수 있었습니다. 이번 발표에서는 이러한 개발 환경 구축 경험을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-08-ruck2018/index.html#slide",
    "href": "posts/2018-11-08-ruck2018/index.html#slide",
    "title": "맞춤형 의학연구 애플리케이션을 위한 개발 환경 구축",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/swarm-setting/RUCK2018_JSKIM 를 클릭하면 볼 수 있으며 PC 환경에 최적화되었다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html",
    "href": "posts/2019-05-20-godelincompleteness/index.html",
    "title": "괴델의 불완전성 정리",
    "section": "",
    "text": "김진섭 대표는 5월 30일(목) 제주대학교 경영정보학과 산업·직무 특화 전문가 특강에 참석, 괴델(Kurt Gödel)의 불완전성 정리가 나온 배경을 소개하고 증명의 핵심 아이디어를 수학과 메타수학(meta-mathematics), 괴델수(Gödel number), 그리고 메타수학의 수학화 3가지로 나누어 설명할 예정입니다. 정리한 슬라이드와 강의록을 미리 공유합니다. 초청해주신 현정석 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "href": "posts/2019-05-20-godelincompleteness/index.html#요약",
    "title": "괴델의 불완전성 정리",
    "section": "요약",
    "text": "요약\n\n18/19 세기 미적분학, 해석학의 발전으로, 수학은 점점 기존의 직관과 상식에서 벗어나 추상화되면서 많은 문제점들이 생겼다.\n특히 무한의 개념을 엄밀하게 다룰 필요성이 있었는데, 칸토어(Georg Cantor)는 집합론의 논법으로 무한을 엄밀하게 정의하고 그것들의 크기를 비교하였다.\n20세기 수학자들은 집합론을 이용, 수학의 기초를 구성하고 모순이 없는 수학체계를 만들 수 있다는 꿈으로 부풀어 있었으나, 러셀의 역설(Bertrand Russel’s paradox)로 대표되는 “자기언급의 역설”은 집합론의 기초를 위태롭게 하였다.\n힐베르트(David Hilbert)는 “자기언급의 역설” 은 수학의 명제와 메타수학(meta-mathematics)의 명제를 구분하지 않아 일어나는 것으로 판단하였으며, 이를 잘 구분하는 공리계를 세심하게 설계할 수만 있다면 모순없는 수학체계를 만들 수 있다고 보았다.\n그러나 괴델(Kurt Gödel)은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 즉, “자기언급의 역설” 을 피하기 위해 아무리 세심하게 공리계를 설계해도 그것을 피할 수는 없다는 것을 증명하였으며, 이것이 불완전성 정리이다.\n불완전성 정리로 인해 “참이지만 증명불가능한 명제가 존재” 하고 나아가 “수학의 무모순성을 수학 자체적으로는 증명할 수 없음” 이 증명되어 모순없는 완전한 수학체계의 꿈은 결국 산산조각이 난다.\n불완전성 정리를 인간 이성의 한계로만 해석하고 실의에 빠지지 말자. 어떤 기계보다도 더 복잡하고 정교한 인간의 정신구조와 능력을 긍정하며 창조적 이성의 힘을 인정할 때이다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "href": "posts/2019-05-20-godelincompleteness/index.html#slide",
    "title": "괴델의 불완전성 정리",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureGodel/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "href": "posts/2019-05-20-godelincompleteness/index.html#강의록",
    "title": "괴델의 불완전성 정리",
    "section": "강의록",
    "text": "강의록\n본 강의록은 과거 https://jinseob2kim.github.io/godel.html 에 정리했던 내용을 약간만 수정하였습니다.\n서론\n괴델의 불완전성 정리에 대한 많은 책들이 시중에 출판되어 있다. 허나 대부분이 역사와 증명의 의미에 치중되어 있고 실제 증명의 아이디어를 설명한 책은 거의 없으며, 있다고 하더라도 외국 서적을 번역하면서 난해한 표현이 많이 나와 이해하기가 어렵다. 이에 본 글에서는 불완전성 정리 증명의 핵심적인 아이디어를 크게 수학(mathematics)과 메타수학(meta-mathematics), 괴델수(Gödel number), 메타수학의 수학화의 3가지로 나누어 설명하겠다. 본 글이 불완전성정리의 아름다움을 느끼는 데 도움이 되길 바란다.\n수학(mathematics)과 메타수학(meta-mathematics)\n보통 불완전성 정리에 대한 책들에서는 간략하게 증명의 개요만 언급하는데 그것은 아래와 같다.\n\\[G: G는 \\:증명불가능하다.\\]\n만약 \\(G\\)가 증명 가능하다면 그것은 참이고, 내용은 “\\(G\\)는 증명 불가능하다.” 므로 모순이다. 따라서 \\(G\\)는 증명 불가능하며, \\(G\\)의 내용은 “\\(G\\)가 증명 불가능하다”는 것이므로 참이다. 따라서 \\(G\\)는 참이지만 증명불가능한 명제라는 것이다. 이 설명을 보고 고개를 끄덕 인 후 증명이 끝난 것 아닌가? 라는 생각을 할 수도 있을 것이다. 그러나 명제 내부에 명제 자신을 언급한 순환논리부분을 해결하지 않으면 안되고 사실 불완전성정리의 핵심부분도 이 부분이며 수학의 명제와 메타수학의 명제를 구분하는 것이 이해의 시작이 된다.\n수학 명제와 메타수학 명제의 차이점\n아주 간단한 수학 명제 하나를 살펴보자.\n\\[1+1=2\\]\n이 표현은 수학에 속한 표현이다. 이제 다음 명제를 살펴보면\n\\[\"1+1=2\"는 \\:수학\\: 명제이다.\\]\n이 진술은 앞에 나온 수학명제에 대해 무엇인가를 주장하고 있으며 따라서 수학이 아니라 메타수학의 명제라고 할 수 있다. 비슷하게 “\\(x\\)는 변수이다”, “\\(x=1\\)은 방정식이다” 들도 수학 명제가 아니라 메타수학의 명제이다. 수학과 메타수학을 구별하는 것은 몇 번을 강조해도 지나치지 않은데 이 구별에 소홀한 것이 수많은 역설이 만들어지는 이유이기 때문이다. 힐베르트를 포함한 당시의 수학자들은 이를 잘 구별하면 역설을 제거할 수 있다고 보았다. 이제 다시 처음 식을 살펴보자. \\(G\\)가 수학명제라면 “\\(G\\)는 증명불가능하다” 는 수학의 명제가 아니라 메타수학의 명제이므로, 이것이 수학명제인 \\(G\\)가 될 수는 없다. 따라서 자기언급의 역설은 발생하지 않는다.\n그러나 괴델은 이 부분을 파고들어 메타수학의 명제를 수학의 명제로 바꾸는 괴델수(Gödel number) 라는 독창적인 아이디어를 제안, 메타수학의 명제를 수학 체계로 갖고 온 후 자기언급의 역설을 보여주었다. 괴델수부터 차근차근 알아보도록 하자.\n괴델수(Gödel number)\n괴델수는 모든 기호, 변수, 명제, 명제묶음(증명)들에 유일한 숫자를 부여하는 것인데 몇가지 예를 들어보면 다음과 같다.\n\n상항기호\n\n기호\n괴델수\n의미\n\n\n\n\\(\\sim\\)\n1\n아니다\n\n\n\\(\\vee\\)\n2\n또는\n\n\n\\(\\supset\\)\n3\n\n\\(\\cdots\\)라면 \\(\\cdots\\)다\n\n\n\\(\\exists\\)\n4\n\n\\(\\cdots\\)이 존재한다\n\n\n\\(=\\)\n5\n같다\n\n\n0\n6\n영(0)\n\n\ns\n7\n바로 다음 수\n\n\n(\n8\n왼쪽 괄호\n\n\n)\n9\n오른쪽 괄호\n\n\n,\n10\n쉼표\n\n\n\\(+\\)\n11\n더하기\n\n\n\\(\\times\\)\n12\n곱하기\n\n\n\n위의 표에 나와있는 기호들을 상항기호라고 하며, 이와 대응되는 개념은 변항(variable)기호인데 숫자 변항, 문장 변항, 술어 변항으로 나눌 수 있다. 숫자 변항에는 12보다 큰 소수, 문장 변항에는 12보다 큰 소수의 제곱수, 술어변항에는 12보다 큰 소수의 세제곱수를 부여하게 되며 예는 아래의 표와 같다.\n\n변항기호\n\n\n변항\n괴델수\n대입 예\n\n\n\n숫자변항\n\\(x\\)\n13\n0\n\n\n\n\\(y\\)\n17\n\\(s0\\)\n\n\n\n\\(z\\)\n19\n\\(y\\)\n\n\n문장변항\n\\(p\\)\n\\(13^2\\)\n\\(0=0\\)\n\n\n\n\\(q\\)\n\\(17^2\\)\n(\\(\\exists\\) x)(\\(x=sy\\)): \\(y\\)의 다음 수 \\(x\\)가 존재한다.\n\n\n\n\\(r\\)\n\\(19^2\\)\n\\(p\\supset q\\)\n\n\n술어변항\n\\(P\\)\n\\(13^3\\)\n\n\\(P(x)\\): \\(x\\)는 소수이다.\n\n\n\n\\(Q\\)\n\\(17^3\\)\n\n\n\n\n\\(R\\)\n\\(19^3\\)\n\n\n\n\n이제 문장 (\\(\\exists x\\))(\\(x=sy\\)) 를 살펴보자. 이것은 \\(y\\) 다음 수가 존재한다고 읽으며, 기본기호 하나하나의 괴델수를 살펴보면 8, 4, 13, 9, 8, 13, 5, 7, 17, 9이고 문장 전체의 괴델수는 다음과 같이 소수를 이용하여 표현한다.\n\\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n마찬가지로 증명에 대해서도 괴델수를 부여할 수 있는데 예를들어 증명이 두 서술로 되어 있고 각 서술의 괴델수가 \\(m\\)과 \\(n\\)이라면 증명의 괴델수는 \\(2^m \\times 3^n\\) 으로 표현한다. 이런식으로 모든 수학적 표현에 대해서 겹치지 않고 유일하게 괴델수를 부여할 수 있다. 한가지 예를 더 들면 괴델수 243,000,000은 \\(2^6 \\times 3^5 \\times 5^6\\)으로 소인수분해 되며 지수부분인 6,5,6은 각각 0, \\(=\\), 0에 대응되므로 결국 \\(0=0\\)이라는 수학명제를 나타내게 된다.\n메타수학의 수학화\n괴델은 괴델수를 이용하여 메타수학의 명제를 수학의 명제로 바꾸는 데 성공하였는데 간단한 예를 들어보겠다.\n\\[\\sim(0=0)\\]\n는 “0은 0이 아니다.” 라는 단순한 수학 명제인 반면\n\\[\\text{수학 명제} \"\\sim(0=0)\" \\text{의 첫 번째 기호는 틸드}(\\sim) \\text{이다}.\\]\n는 수학명제가 아니라 메타수학의 명제이다. 그런데 이 메타수학의 명제는 적당한 과정을 거치면 수학명제로 바꿀 수 있다. 편의상 \\(\"\\sim(0=0)\"\\)의 괴델수를 \\(a\\)라고 하고 위의 상항기호 표에서 \\(\"\\sim\"\\)의 괴델수가 1임을 기억하자. 그렇다면 첫 번째 기호가 \\(\"\\sim\"\\)이라는 것은 곧 \\(a\\)를 소인수분해했을 때 제일 작은 소수인 2의 지수가 1임을 의미하게 되고 메타수학의 명제는\n\\[2 \\text{는 } a \\text{의 인수이지만, } 2^2 \\text{ 은 } a \\text{의 인수가 아니다}.\\]\n와 같이 수학명제로 바뀌게 된다. 기호로 표현하기 위해 표현을 바꾸면 “\\(y=z\\times 2\\)인 \\(z\\)가 존재하고, \\(y=z\\times 2 \\times 2\\)인 \\(z\\)는 존재하지 않는다.” 가 되며 실제로 이를 기호로 표현하면 다음과 같다.\n\\[(\\exists z) (sss\\cdots sss0=z\\times ss0) \\cdot \\sim(\\exists z) (sss\\cdots sss0=z\\times (ss0 \\times ss0))\\]\n(\\(sss\\cdots sss0\\)에서 \\(s\\)는 정확히 \\(a\\)번 나타난다.) 이와 비슷한 방법으로 모든 메타수학의 명제를 괴델수를 이용하여 수학의 명제로 바꿀 수 있다.\n불완전성 정리\n이제 불완전성 정리를 천천히 이해해보자. Dem/dem과 Sub/sub의 개념을 정의한 후 증명의 핵심 아이디어로 들어가 보겠다.\nDem\ndem은 증명을 뜻하는 demonstration의 약자로 dem(\\(x\\), \\(z\\))은\n\\[\\text{괴델수 } x \\text{를 가진 문장묶음이 괴델수 } z \\text{를 가진 명제의 증명이다}.\\]\n의 축약표현으로 정의한다. 예를 들어 “피타고라스 정리 증명”의 괴델수가 \\(m\\)이고 “피타고라스 정리”의 괴델수가 \\(n\\)이라면 dem(\\(m\\), \\(n\\))이라고 쓸 수 있는 것이다. 한편, Dem(\\(x\\), \\(z\\))은 \\(x\\)와 \\(z\\)의 관계 dem을 형식적 표기법으로 표현하는 형식문으로 정의한다.\nSub\nSub은 치환 혹은 대입을 의미하며 소문자로 표현한 sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수 } x \\text{를 가진 문장에 등장하는} \\textbf{ 변수 } y \\textbf{들에 전부 숫자 } x \\textbf{를 대입} \\text{해서 만들어진 명제의 괴델수}\\]\n로 정의한다(17은 \\(y\\)의 괴델수임을 기억하자). 한 편, \\(s\\)를 대문자로 표시한 Sub(\\(x\\),17,\\(x\\))는\n\\[\\text{괴델수가 아닌 }\\textbf{명제 그 자체}\\]\n로 정의한다. 이해를 돕기 위해 \\(2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\) 를 다시 살펴보겠다. (\\(\\exists x\\))(\\(x=sy\\)) 는 앞서 설명했듯이 “\\(y\\) 다음 수가 존재한다” 고 읽을 수 있으며, 그 괴델수를 \\(k\\)라 하면\n\\[k=2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{17} \\times 29^9\\]\n가 된다. 이제 \\(y\\)대신 \\(k\\)를 대입한다면 수정된 명제는 (\\(\\exists x\\))(\\(x=sss \\cdots sss0\\))이 되고(\\(s\\)는 \\(k+1\\)번 연속됨), \\(s\\)의 괴델수가 7임을 고려하면 \\(k\\)를 대입한 명제의 괴델수는 \\(y\\)부분에 해당되는 \\(23^{17}\\)부터 \\(s\\)로 바뀌어 아래의 식과 같이 된다.\n\\[2^8 \\times 3^4 \\times 5^{13} \\times 7^9 \\times 11^8 \\times 13^{13} \\times 17^5 \\times 19^7 \\times 23^{7} \\times 29^7 \\times 31^7 \\times 37^7 \\times \\cdots (P_{k+10})^9\\]\n(\\(P_{k+10}\\): \\(k+10\\)번 째 소수)\n얼핏 보기에 문장의 괴델수를 문장 그 자체에 대입한다는 것이 순환논리같은 느낌을 주는데 이것이 바로 괴델의 핵심적인 아이디어 중 하나이다. 이제부터 본격적인 증명의 내용으로 들어가기로 하자.\n불완전성 정리\n불완전성 정리의 증명은 “괴델수 \\(z\\)를 가진 명제는 증명불가능하다”라는 메타수학의 명제를 수학의 명제로 바꾼 후, 이 명제의 특별한 경우가 증명될 수 없다는 것을 보이는것으로 이루어진다. 이제 다음의 형식문을 살펴보자.\n\\[\\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(y, 17, y))\\]\n이 수학의 형식문은 “Sub(\\(y\\) ,17, \\(y\\)) 의 증명은 존재하지 않는다, 즉 증명불가능하다”는 메타수학적 의미를 갖는다. 이 형식문의 괴델수를 \\(n\\)이라 하고, 형식문에 포함된 변수 \\(y\\)를 숫자 \\(n\\)으로 바꾼 형식문 \\(G\\)를 살펴보자.\n\\[\\text{G}: \\sim(\\exists x)\\text{Dem}(x,\\text{Sub}(n, 17, n))\\]\n\\(G\\)는 변수가 포함되어 있지 않으므로 수학명제이며, 이것의 메타수학적 의미를 살펴보면 “괴델수 sub(\\(n\\), 17, \\(n\\))을 가진 명제는 증명 불가능하다.”이다. \\(G\\)의 괴델수를 \\(g\\)라고 하고 \\(g\\)는 어떤 숫자일지 생각해보자. 놀랍게도 \\(g=\\text{sub}(n, 17, n)\\)임을 알 수 있다. \\(G\\)는 괴델수 \\(n\\)을 가진 형식문에서 \\(y\\)에 숫자 \\(n\\)을 대입하여 만든 명제이므로, 이것의 괴델수 \\(g\\)는 정확히 sub(\\(n\\), 17, \\(n\\))의 정의와 일치하게 된다. 이제 \\(G\\)의 메타수학적 의미를 다시 살펴보면 “괴델 수 \\(g\\)를 가진 명제는 증명 불가능하다.”이고 즉, “\\(G\\)는 증명 불가능하다”는 의미가 된다. 맨 처음으로 돌아가면, \\(G\\)는 참이지만 증명 불가능한 명제가 되어 불완전성의 정리가 증명된다.\n마치며\n필자는 고등학생때부터 괴델의 불완전성정리를 이해해보려고 괴델의 논문원본도 찾아보고 여러 교양서적을 많이 읽어보았었다. 그러나 논문을 다 보는것은 이해하기가 어렵고 교양서적은 겉핥기식으로만 나와있어서 최근까지도 증명의 핵심을 이해하지 못했었는데 출판사 승산에서 나온 괴델의 증명을 읽으면서 한층 이해수준을 올릴 수 있었다. 이 책은 외국서적을 번역한 것으로 번역하면서 생소한 단어와 표현들이 많이나와 읽기가 어려운 부분이 있어 이번기회에 잘 풀어서 설명해볼 목적으로 글을 쓰게 되었다. 본 글이 논문과 교양서적 사이에서 가교 역할을 하여 불완전성 정리를 이해하는데 도움이 되길 기대한다.\n참고문헌\n\n괴델, 에셔, 바흐 : 영원한 황금 노끈 / 지은이: 더글러스 호프스태터 ; 옮긴이: 박여성, 안병서\n(호프스태터가 서문을 쓰고 개정한) 괴델의 증명 / 지은이: 어니스트 네이글, 제임스 뉴먼 ; 옮긴이: 곽강제, 고중숙\n괴델 불완전성 정리 / 지은이: 요시나가 요시마사 ; 옮긴이: 임승원"
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html",
    "href": "posts/2021-07-11-kstartup/index.html",
    "title": "창업지원사업 도전기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 7월 Shinykorea 밋업에 참석, 창업지원사업 도전경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#요약",
    "href": "posts/2021-07-11-kstartup/index.html#요약",
    "title": "창업지원사업 도전기",
    "section": "요약",
    "text": "요약\n4전 5기만에 창업지원사업(비대면스타트업육성사업) 선정(1.4억)\n\n비대면 의료 분야: 블록체인 기반 의료데이터 입력, 관리, 분석 플랫폼\n\n18년초 창업전, 지도교수, 대학원동료와 함께 창업선도대학 선정(6천만)\n\n서울대학교: 블록체인 기반 유전체 빅데이터 플랫폼\n\n심평원 공모전 선정: 맞춤형 의학연구웹\n창업 후 주요사업 광탈, 작은 사업 선정\n\n20년 초기창업패키지, 추경(비대면), 추경2차, 21년 초기창업패키지 4연속 서류탈락\n비대면바우처(400만원), 클라우드 지원사업(720만원) 선정\n벤처기업인증: 혁신성장유형\n\n21년 공개소프트웨어 기반 창업기업 선정, 선릉역 오피스 6개월 지원."
  },
  {
    "objectID": "posts/2021-07-11-kstartup/index.html#slide",
    "href": "posts/2021-07-11-kstartup/index.html#slide",
    "title": "창업지원사업 도전기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/kstartup 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-09-25-welcome/index.html",
    "href": "posts/2018-09-25-welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Anpanman의 블로그를 만들었습니다.\n\n\n\n\nDiamond Prices\n\n\n\n\nCross reference 연습입니다.\nFootnote 연습입니다.1\n\n\nFootnotes\n\nThis will become a hover-able footnote↩︎\nCitationBibTeX citation:@online{kim2018,\n  author = {Jinseob Kim},\n  title = {Welcome},\n  date = {2018-09-25},\n  url = {https://blog.zarathu.com/posts/2018-09-25-welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJinseob Kim. 2018. “Welcome.” September 25, 2018. https://blog.zarathu.com/posts/2018-09-25-welcome."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html",
    "href": "posts/2022-02-07-tableone/index.html",
    "title": "tableone 패키지 소개",
    "section": "",
    "text": "본 자료는 데이터셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. tableone 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "href": "posts/2022-02-07-tableone/index.html#categorical-variable-conversion",
    "title": "tableone 패키지 소개",
    "section": "Categorical variable conversion",
    "text": "Categorical variable conversion\nfactorVars 인자를 사용하여 범주형 변수를 지정할 수 있다. 이때 vars 인자를 통해 전체 데이터 셋 중 테이블에 들어갈 변수를 설정할 수 있고, 지정하지 않을 시 데이터 셋의 모든 변수가 포함된다.\n\n# Variables\nmyVars <- c(\"HGHT\", \"WGHT\", \"BMI\", \"HDL\", \"LDL\", \"TG\", \"SGPT\", \n            \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\n# Categorical variables\ncatVars <- c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_HBV_AG\", \"Q_SMK_YN\")\nt1 <- CreateTableOne(vars = myVars, factorVars = catVars, data = dt)\nt1\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n범주형 변수로 설정한 컬럼의 요약값이 mean(sd)에서 n(percentage)로 바뀐 것을 볼 수 있다.\n두 개의 범주가 있는 범주형 변수의 경우, 두 번째 범주의 요약값만 출력된다. 예를 들어 0과 1의 범주가 있을 때, 범주1의 개수와 백분율이 출력된다. 이는 옵션 설정을 통해 전체 범주의 요약값을 출력하도록 변경할 수 있다.\n3개 이상의 범주가 있을 때에는 모든 범주의 값이 요약되며, 백분율은 누락된 값을 제외한 후 계산된다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "href": "posts/2022-02-07-tableone/index.html#multiple-group-summary",
    "title": "tableone 패키지 소개",
    "section": "Multiple group summary",
    "text": "Multiple group summary\nstrata 인자를 설정하여 그룹별 연산을 할 수 있다. strata는 dplyr 패키지의 group_by() 함수와 유사하며, 그룹 연산을 할 변수를 지정하여 사용할 수 있다.\n\nt2 <- CreateTableOne(data = dt,\n                     vars = myVars,\n                     strata = \"Q_SMK_YN\",\n                     factorVars = catVars,\n                     includeNA = F)\nt2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n<0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n<0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n<0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n<0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n<0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n<0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n<0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)\n\n\n\n\n\n\n\n\n연속형 변수의 경우, 기본적으로 one-way ANOVA test가 적용되며 nonnormal일 경우 옵션 설정을 통해 Kruskal–Wallis one-way ANOVA test를 적용할 수 있다.\n범주형 변수의 경우, 기본적으로 chisq-test가 적용되며 print 함수의 exact 옵션 설정을 통해 fisher-test를 적용할 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "href": "posts/2022-02-07-tableone/index.html#showing-all-levels",
    "title": "tableone 패키지 소개",
    "section": "Showing all levels",
    "text": "Showing all levels\n범주형 변수에서 모든 범주의 요약값을 확인하려면 ShowAllLevels 또는 cramVars 옵션을 사용한다. ShowAllLevels = T 를 설정하거나 cramVars 옵션에 원하는 변수명을 지정하여 사용할 수 있다.\n\n1.use showAllLevels\n\n\n\nprint(t1, showAllLevels = T)\n\n\n\n\n\n\nlevel\nOverall\n\n\n\nn\n\n1644\n\n\nHGHT (mean (SD))\n\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n\n65.10 (12.53)\n\n\nBMI (mean (SD))\n\n23.92 (3.38)\n\n\nHDL (mean (SD))\n\n55.90 (19.47)\n\n\nLDL (mean (SD))\n\n118.69 (201.99)\n\n\nTG (mean (SD))\n\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n\n25.98 (27.18)\n\n\nQ_PHX_DX_STK (%)\n0\n1059 (98.9)\n\n\n\n1\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ (%)\n0\n1052 (97.6)\n\n\n\n1\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n1\n77 ( 4.7)\n\n\n\n2\n1102 (67.1)\n\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n1\n995 (60.6)\n\n\n\n2\n256 (15.6)\n\n\n\n3\n391 (23.8)\n\n\n\n\n\n\n2.use cramVars\n\n\n\nprint(t1, cramVars=\"Q_PHX_DX_STK\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (mean (SD))\n118.69 (201.99)\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 0/1 (%)\n1059/12 (98.9/1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "href": "posts/2022-02-07-tableone/index.html#nonnormal-variables",
    "title": "tableone 패키지 소개",
    "section": "nonnormal variables",
    "text": "nonnormal variables\n비모수통계를 사용하는 연속형 변수에는 nonnormal 옵션을 설정한다. nonnormal 설정 시 mean(sd)에서 median(IQR)로 요약값이 변경된다.\n\nprint(t1, nonnormal=\"LDL\")\n\n\n\n\n\n\nOverall\n\n\n\nn\n1644\n\n\nHGHT (mean (SD))\n164.55 (9.19)\n\n\nWGHT (mean (SD))\n65.10 (12.53)\n\n\nBMI (mean (SD))\n23.92 (3.38)\n\n\nHDL (mean (SD))\n55.90 (19.47)\n\n\nLDL (median [IQR])\n112.00 [90.00, 134.00]\n\n\nTG (mean (SD))\n134.90 (104.75)\n\n\nSGPT (mean (SD))\n25.98 (27.18)\n\n\nQ_PHX_DX_STK = 1 (%)\n12 ( 1.1)\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n26 ( 2.4)\n\n\nQ_HBV_AG (%)\n\n\n\n1\n77 ( 4.7)\n\n\n2\n1102 (67.1)\n\n\n3\n463 (28.2)\n\n\nQ_SMK_YN (%)\n\n\n\n1\n995 (60.6)\n\n\n2\n256 (15.6)\n\n\n3\n391 (23.8)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#exact",
    "href": "posts/2022-02-07-tableone/index.html#exact",
    "title": "tableone 패키지 소개",
    "section": "exact",
    "text": "exact\nexact 옵션을 통해 fisher-test를 진행할 범주형 변수를 설정할 수 있다. 범주형 변수는 기본적으로 chisq-test가 적용되며, exact 옵션에 fisher-test를 적용할 변수를 지정하여 사용할 수 있다.\n\nprint(t2, exact=c(\"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\n\n\n\nn\n995\n256\n391\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n<0.001\n\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n<0.001\n\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n<0.001\n\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n<0.001\n\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n<0.001\n\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n<0.001\n\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.045\nexact\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.281\nexact\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n<0.001\n\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2022-02-07-tableone/index.html#smd",
    "href": "posts/2022-02-07-tableone/index.html#smd",
    "title": "tableone 패키지 소개",
    "section": "smd",
    "text": "smd\nsmd 옵션을 통해 smd(standardized mean difference)를 table1에 포함할 수 있다. default는 FALSE이고, smd=TRUE 설정 시 각 변수의 smd 값이 출력된다.\n\nprint(t2, smd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\np\ntest\nSMD\n\n\n\nn\n995\n256\n391\n\n\n\n\n\nHGHT (mean (SD))\n160.67 (8.34)\n168.83 (6.45)\n171.61 (7.09)\n<0.001\n\n0.972\n\n\nWGHT (mean (SD))\n61.17 (11.08)\n70.09 (10.72)\n71.76 (13.07)\n<0.001\n\n0.611\n\n\nBMI (mean (SD))\n23.63 (3.39)\n24.52 (2.93)\n24.27 (3.54)\n<0.001\n\n0.181\n\n\nHDL (mean (SD))\n57.83 (14.08)\n53.91 (36.73)\n52.37 (13.54)\n<0.001\n\n0.197\n\n\nLDL (mean (SD))\n112.26 (32.81)\n147.52 (505.27)\n116.34 (56.89)\n0.046\n\n0.091\n\n\nTG (mean (SD))\n114.05 (76.97)\n162.89 (126.51)\n169.24 (133.28)\n<0.001\n\n0.341\n\n\nSGPT (mean (SD))\n23.33 (28.42)\n28.61 (20.62)\n31.00 (26.96)\n<0.001\n\n0.196\n\n\nQ_PHX_DX_STK = 1 (%)\n11 ( 1.8)\n1 ( 0.5)\n0 ( 0.0)\n0.051\n\n0.137\n\n\nQ_PHX_DX_HTDZ = 1 (%)\n18 ( 2.9)\n5 ( 2.6)\n3 ( 1.1)\n0.287\n\n0.084\n\n\nQ_HBV_AG (%)\n\n\n\n0.193\n\n0.109\n\n\n1\n40 ( 4.0)\n19 ( 7.5)\n17 ( 4.3)\n\n\n\n\n\n2\n679 ( 68.3)\n164 ( 64.3)\n259 ( 66.2)\n\n\n\n\n\n3\n275 ( 27.7)\n72 ( 28.2)\n115 ( 29.4)\n\n\n\n\n\nQ_SMK_YN (%)\n\n\n\n<0.001\n\nNaN\n\n\n1\n995 (100.0)\n0 ( 0.0)\n0 ( 0.0)\n\n\n\n\n\n2\n0 ( 0.0)\n256 (100.0)\n0 ( 0.0)\n\n\n\n\n\n3\n0 ( 0.0)\n0 ( 0.0)\n391 (100.0)"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "",
    "text": "김진섭 대표는 7월 15일(금) 성균관대학교 바이오헬스규제과학과 단기 교육 프로그램에서 “데이터과학자가 갖춰야할 기술” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#요약",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "요약",
    "text": "요약\n분석: 의학 + 연구는 R 추천\n\n데이터 전처리는 data.table 과 %>%.\nR로 ppt/xlsx 만들기(분석결과 반출)\n일반, 반복측정, 표본추출, Propensity, meta, 시계열, ML\n코드관리는 Github\n\n서비스: 리포트(Rmarkdown), 웹 애플리케이션(Shiny)\n서버: 분석환경 or 웹 서비스\n\n리눅스, 클라우드, Docker"
  },
  {
    "objectID": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "href": "posts/2022-06-26-biohrs-data-scientist/index.html#slide",
    "title": "데이터과학자가 갖춰야할 기술",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/short-2022summer 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html",
    "href": "posts/2022-09-30-GAM/index.html",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "",
    "text": "김진섭 대표는 성균관대학교 바이오헬스 규제과학과 강의에서 비선형모델인 GAM(Generalized Additive Model) 을 소개할 예정입니다. 전체 강의자료는 https://github.com/jinseob2kim/R-skku-biohrs 에 있습니다."
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#요약",
    "href": "posts/2022-09-30-GAM/index.html#요약",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "요약",
    "text": "요약\nGAM 은 비선형관계를 다루는 통계방법이다\n\nLOWESS: 구간 촘촘하게 나눈 후 평균값\nCubic spline(cs): 구간 몇개로 나눈 후 각각 3차함수 fitting\nNatural cubic spline(ns): cs 맨 처음과 끝구간만 선형 fitting\nSmoothing spline(GAM default): 최적화때 smoothing penalty(λ) 부여\n\n종속변수 형태따라 여러종류\n\nContinuous: normal\nBinary: logistic\nCount: poisson, quasipoisson(평균 ≠ 분산 일 때)\nSurvival: coxph"
  },
  {
    "objectID": "posts/2022-09-30-GAM/index.html#slide",
    "href": "posts/2022-09-30-GAM/index.html#slide",
    "title": "GAM(Generalized Additive Model) 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/R-skku-biohrs/gam 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html",
    "href": "posts/2022-01-06-internship-django-1/index.html",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 1주차 동안 학습한 내용에 대해 공유합니다.\n차라투에서는 사용자에게 여러가지 통계 웹앱을 제공하는 Openstat 서비스를 준비중입니다. 서비스를 이용하는 사용자 본인이 불필요한 기능을 제거하고, 본인에게 필요한 기능만을 제공받을 수 있는 기능 도입 준비 중입니다. Django를 사용하여 사용자에게 보여질 기능을 DB로 유지하며 해당 DB기반으로 사용자에게 서비스를 제공해야합니다. 이에 사용자가 직접 DB의 필드값을 수정하며 서비스를 제공받는 기능을 구현했습니다."
  },
  {
    "objectID": "posts/2022-01-06-internship-django-1/index.html#목차",
    "href": "posts/2022-01-06-internship-django-1/index.html#목차",
    "title": "인턴십 - Django로 게시판 만들고 기능 추가하기",
    "section": "목차",
    "text": "목차\n\nModel.Boolean 값으로 화면 출력 통제\n\nCheckBox를 활용하여 Model.BooleanField 핸들링\n\n\n\nModel.Boolean 값으로 화면 출력 통제\n\n\n[models.py]\nClass Post(models.Model):\n  user_id = models.CharField(max_length = 50, default=\"\")\n  postname = models.CharField(max_length =50)\n  content = models.TextField()\n  important = models.BooleanField(default = True)  \n\n Model 필드에 BooleanField 형식의 필드를 추가합니다.\n이 예제에서는 important라는 필드명을 사용했지만, 필드명은 다른 이름으로 사용해도 괜찮습니다.\n(기존 Model에 새로운 필드를 추가하는 경우, 기존 Model 속 데이터들의 필드들도 일괄적으로 변경됩니다. 새로운 필드에 값을 넣지 않을 경우, 에러가 발생하므로 Model에 새로운 필드를 추가할때는 default 값을 넣어주는것이 좋습니다.)\n\n\n\n[views.py]\nfrom .models import Post\n\ndef show_important_post(request):\n    postlist = Post.objects.all()\n    return render(request,'blog/ImportantPost_Posting.html',{'postlist':postlist})\n\n\npostlist = Post.objects.all()\n\n현재 생성된 Models의 Post를 import하고 현재 모델 Post 속에 담겨있는 모든 objects들을 postlist에 담습니다.  \n\nretrun render(request,‘blog/ImportantPost_Posting.html’,{‘postlist’:postlist})\n\n{‘postlist’:postlist}로 {key,value} 값을 맞춰서 blog/ImportantPost_Posting.html에 넘깁니다.\n(예시에서는 blog/ImportantPost_Posting.html에 넘겼지만 본인의 상황에 적합한 templates에 넘기면 됩니다.)   \n\n\n[templates-(ImportantPost_Posting.html)]\n{% block contents %}\n<h1>중요한 게시판 </h1>\n    {% for list in postlist %}\n        {% if list.important is True %}\n        <ul>\n            <li>\n                작성자: {{list.user_id}} \n                <a href=\"/blog/showImportant/{{list.pk}}/\">{{list.postname}}</a>\n            </li>\n        </ul>\n        {% endif %}\n    {%endfor%}\n    <button><a href=\"/blog/showImportant/new_post/\">글쓰기</a></button>\n    <input type=\"button\" value = \"돌아가기\" onclick = \"back()\">\n{% endblock %}\n\n\n{{%for list in postlist%}}\n　　　{%if list.important is True%}\n{% endfor %}\n\n앞서 views에서 넘겨 받은 postlist를 순회합니다. 만약 postlist에 해당하는 부분을 전부 출력하면 현재 가지고 있는 POST DB에 존재하는 모든 내용을 출력하게 됩니다. 하지만 저희는 if 문을 활용하여 DB 속 모든 객체들 중 important 필드의 값이 True 인 경우만 <li> 태그에 넣어 화면에 출력하도록 합니다. \n\n\n{{list.postname}}\n\n위 예제에서는 list의 postname 필드를 출력하였지만, list의 다른 필드를 출력해도 상관없습니다.\n\n\n\n\n\nCheckBox활용 Model.BooleanField 핸들링\n\n\n[Views.py]\ndef edit_post(request,pk):\n    post = Post.objects.get(pk=pk)\nif request.method == 'POST':\n        if len(request.POST.getlist('important')) == 0:\n            important = False\n        else: important = True\n        post.postname = request.POST['postname']\n        post.content = request.POST['contents']\n        post.important = important\n        post.save()\n\n\npost = Post.objects.get(pk=pk)\n\nedit_post함수를 통해 request 메시지와 (pk:primary key)를 받으면 Model class인 Post에서 pk가 동일한 객체를 찾아 post에 넘겨줍니다.  \n\nif len(request.POST.getlist(‘important’))==0\n　　important = False\nelse: important = True\n\n’important’값은 체크박스를 통하여 값을 넘깁니다. 만약 위 예제 코드 속 다른 코드에서 쓰이는 것처럼 post.important = request.POST[‘important’] 형식을 사용하여 important 값을 넣으려고 시도한다면 문제가 생깁니다. 체크 박스를 체크하여 request 요청했다면 문제가 되지 않지만, 체크 박스를 체크하지 않고 값을 넘기면 MultiValueDictKeyError 문제가 발생하기에 코드를 이와 같이 수정해야 합니다.\nMultiValueDictKeyError를 피하기 위하여 우리는 important의 값을 list 형태로 가지고 옵니다. 만약 체크 박스를 체크했다면 list에 값이 들어있을 것이고 체크박스를 체크하지 않았다면 list에 값이 없을 것입니다. 그렇게 list의 길이를 len을 통해 측정하여 길이가 0이면 False를, 길이가 0이 아니면 True를 important 값에 넣어 post 필드 값을 수정하여 새롭게 save() 해줍니다.\n \n\n\n[templates-(editPost.html)]\n{% if Post.important is True %}\n  <input type = \"checkbox\" name= \"important\" value= \"True\" checked> important <br>\n{% else %}\n  <input type = \"checkbox\" name = \"important\" value = \"False\"> important <br>\n{% endif %}\n Post의 필드 중 important 부분을 체크박스를 통하여 수정하는 templates의 일부를 가지고 왔습니다. 현재 Post의 important 부분이 True 이면 체크된 상태로 화면에 출력되도록 하고 False이면 체크되지 않은 상태로 화면에 출력되도록 했습니다. \n \n\n\n\n출력\n\n\n\n\n\nimportant = True\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimportant = False\n\n\n\n\n\n\n\n\n\n\n\n상기 이미지에 보이는 것처럼 체크박스의 체크 유무가 필드값인 important로 대입되어 important의 값에 따라 True 인 경우에는 화면에 출력되고 False인 경우에는 출력되지 않는 모습을 확인할 수 있습니다.\n\n\n\n결론\nModel의 booleanField 값을 CheckBox를 통해 제어하며 화면에 출력을 통제하는 방법에 대해 알아보았습니다.\n해당 방법을 활용하여 User 맞춤 서비스 제공 등 다양한 방식으로 DB의 내용을 기준하여 서비스 제공에 기여할 것으로 기대됩니다."
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html",
    "href": "posts/2022-04-12-status2022/index.html",
    "title": "22년 지원사업 후기",
    "section": "",
    "text": "김진섭 대표는 4월 20일(수) Shinykorea 밋업에서 “22년 지원사업 후기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#요약",
    "href": "posts/2022-04-12-status2022/index.html#요약",
    "title": "22년 지원사업 후기",
    "section": "요약",
    "text": "요약\n사무실\n\n송파ICT청년창업지원센터 입주, 기업부설연구소 설립\n공개SW 창업기업 6개월 연장(선릉 저스트코타워)\n\nR&D 과제\n\n창업성장기술개발사업(디딤돌 첫걸음) 서류탈락\n정보통신·방송 기술개발사업 앤틀러과 같이 지원, 6:1 경쟁률 결과 기다리는중\n\n창업지원사업\n\n혁신분야 창업패키지(BIG3), SW고성장기업 서류탈락\n창업도약패키지 심사중, 클라우드서비스 이용지원, Datastars 심사중\n\n특허지원사업\n\n국제 지재권분쟁 대응전략 지원사업(특허) 심사중\n\n멘토링\n\nSW마에스트로 기술멘토 선정, 한이음멘토링\n오픈소스 컨트리뷰톤 심사중\n\n고용지원, 인턴십, 기타\n\n`21 청년디지털일자리, 미래청년인재육성사업 TO 5명\n’22년 일경험프로그램 TO 1명, 송파구 중소기업 청년취업인턴제 TO 1명\n숭실대 스타트업 인턴십 2명(6주), ICT 학점연계 프로젝트 인턴십 TO 1명\n인공지능고성능컴퓨팅지원 3년 연속 선정"
  },
  {
    "objectID": "posts/2022-04-12-status2022/index.html#slide",
    "href": "posts/2022-04-12-status2022/index.html#slide",
    "title": "22년 지원사업 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/status2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 2주차에는 %>% 연산자와 dplyr 패키지를 중심으로, 최근 R 문법 트렌드인 tidyverse 스타일을 정리했습니다. 본 슬라이드는 서울대병원 진단검사의학과 선생님들의 교육에 쓰일 예정입니다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nProject 생성\n\n\n\n\n강의록은 과거 글 https://blog.zarathu.com/posts/2019-01-03-rdatamanagement/ 을 참고하자."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#요약",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "요약",
    "text": "요약\ntidyverse는 직관적인 코드를 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\n\nmagrittr 패키지의 %>% 연산자로 의식의 흐름대로 코딩한다.\ndplyr 패키지의 select, mutate, filter, group_by, summarize 함수는 %>% 와 찰떡궁합이다."
  },
  {
    "objectID": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "href": "posts/2020-04-06-rdatamanagementtidyverse/index.html#slide",
    "title": "R 데이터 매니지먼트 최근: tidyverse",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-snuhlab/tidyverse 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html",
    "href": "posts/2020-07-08-table1inmed/index.html",
    "title": "의학 연구에서의 기술통계",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 1주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#요약",
    "href": "posts/2020-07-08-table1inmed/index.html#요약",
    "title": "의학 연구에서의 기술통계",
    "section": "요약",
    "text": "요약\n\n연속변수의 2그룹 비교: 정규분포 인정하면 t-test, 아니면 Wilcox-test\n연속변수의 3그룹 이상 비교: 정규분포 인정하면 one-way ANOVA, 아니면 Kruskal–Wallis one-way ANOVA\n범주형 변수의 그룹 비교: 샘플수 충분하면 Chisq-test, 아니면 Fisher-test\n본사가 개발한 웹 과 R 패키지 에서 바로 Table 1 을 얻을 수 있다."
  },
  {
    "objectID": "posts/2020-07-08-table1inmed/index.html#slide",
    "href": "posts/2020-07-08-table1inmed/index.html#slide",
    "title": "의학 연구에서의 기술통계",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/table1 를 클릭하면 볼 수 있다. 강의록은 과거 내용인 https://blog.zarathu.com/posts/2018-11-24-basic-biostatistics 를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html",
    "href": "posts/2019-10-25-ruck2019/index.html",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "",
    "text": "김진섭 대표는 10월 25일(금) 광화문 한국마이크로소프트 11층에서 열린 R User Conference in Korea 2019(RUCK 2019) 참석, 맞춤형 의학연구 앱을 만들고 그것을 패키지로 만들어 CRAN에 배포한 경험을 발표하였습니다. 초록과 슬라이드를 공유합니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#abstract",
    "href": "posts/2019-10-25-ruck2019/index.html#abstract",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Abstract",
    "text": "Abstract\n의학연구자들에게 제공한 맞춤형 ShinyApps 중, 범용으로 쓰일만한 것들을 Shiny module 로 만들고 웹으로 공개하였습니다. 큰 용량의 데이터는 개인 PC에서 직접 다룰 수 있도록 Rstudio Addins 을 포함한 R 패키지 를 만들어 github 에 배포하였습니다. 패키지 관리를 위해 (1) testthat, covr 로 코드 테스트를 수행한 결과 리포트를, (2) pkgdown 으로 패키지를 소개하는 웹사이트를 만들었고, (3) Travis CI 와 appveyor 로 앞의 과정과 여러 운영체제에서의 테스트를 자동화하였습니다. 최종적으로 CRAN 에 패키지를 배포하였고, 1.01 버전까지 업데이트하였습니다."
  },
  {
    "objectID": "posts/2019-10-25-ruck2019/index.html#slide",
    "href": "posts/2019-10-25-ruck2019/index.html#slide",
    "title": "RUCK2019 발표: From ShinyApps to CRAN",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RUCK2019/ 를 클릭하면 볼 수 있으며, Chrome 에 최적화되었습니다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html",
    "href": "posts/2020-01-25-pgconference2020/index.html",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "",
    "text": "김진섭 대표는 2월 15일(토) 디시인사이드 가 후원하는 프로그래밍 갤러리 컨퍼런스 2020에 참석, R과 shiny로 웹 애플리케이션을 만든 경험을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#요약",
    "href": "posts/2020-01-25-pgconference2020/index.html#요약",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포.\n\n모임 소개\n\nshinykorea 밋업 후원: R 웹만들기 지식 공유\n카카오 오픈채팅: 프로그래밍 갤러리 R사용자 모임"
  },
  {
    "objectID": "posts/2020-01-25-pgconference2020/index.html#slide",
    "href": "posts/2020-01-25-pgconference2020/index.html#slide",
    "title": "R로 만드는 웹 애플리케이션",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/pgconference2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html",
    "href": "posts/2020-10-05-yu-seminar/index.html",
    "title": "의학통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 영남대학교 “의사과학자 역량 배가 프로젝트” 에 참석, 그동안 해온 지원업무에 대해 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#요약",
    "href": "posts/2020-10-05-yu-seminar/index.html#요약",
    "title": "의학통계지원 소개",
    "section": "요약",
    "text": "요약\n다양한 의료데이터를 다뤄본 경험으로, 의학연구 활성화에 기여하겠습니다.\n\n의과대학, 유전체연구 박사과정, 삼성전자 무선사업부를 거치며 임상, 유전체, 모바일헬스 등 다양한 데이터를 다루었습니다.\n현재 연구지원법인 차라투 를 운영 중입니다.\nShiny 밋업 의 진행과 후원을 맡아, 의료/축산/게임/반도체/신용평가/IPTV 등 다양한 분야의 사람들과 함께 Shiny 를 알아가는 중입니다."
  },
  {
    "objectID": "posts/2020-10-05-yu-seminar/index.html#slide",
    "href": "posts/2020-10-05-yu-seminar/index.html#slide",
    "title": "의학통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/yu/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html",
    "href": "posts/2022-07-25-collapse/index.html",
    "title": "collapse 패키지 소개",
    "section": "",
    "text": "R의 고급 데이터 변환 및 통계 컴퓨팅을 위한 C/C++ 기반 패키지입니다.\n\n유연하고 간결한 구문을 통해 매우 빠르고 클래스에 구애받지 않습니다,\n기본 R, ‘dplyr’, ‘tibble’, ‘data.table’, ‘sf’, ‘plm’ 과 잘 통합됩니다.\n\n\n\n##setup\n\n#install.packages(\"collapse\")\n\nlibrary(magrittr)\nlibrary(data.table) \nlibrary(dplyr)\nlibrary(collapse)\nlibrary(microbenchmark)\n\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하겠습니다.\n\n# Exam data: 09-15\n\ndt <- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndf <- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#load",
    "href": "posts/2022-07-25-collapse/index.html#load",
    "title": "collapse 패키지 소개",
    "section": "load",
    "text": "load\n\ncollapse는 데이터를 불러오는 함수가 존재하지 않기 때문에, data.table를 이용하여 읽습니다.\nfselect()는 컬럼명을 명시하거나 인덱스를 전달하면 원하는 컬럼을 불러올 수 있습니다.\n\n\n## data.table(Only specific column)\ndt1 <- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\",select = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"))\ndt2 <- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", select = 1:5)\ndt3 <- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\", drop = 6:10)\n\n## collapse(Only specific column)\ndt4 <- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM)\ndt5 <- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),1:5)\ndt6 <- fselect(fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"),-(6:10)) \n\n\n예시로 dt6를 확인해보면, dt3와 마찬가지로 인덱스 6부터 10까지 제외되서 출력된 것을 볼 수 있습니다.\n\n\ndt3\n\n\n\n\n\n  \n\n\n\n\ndt6"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#row",
    "href": "posts/2022-07-25-collapse/index.html#row",
    "title": "collapse 패키지 소개",
    "section": "row",
    "text": "row\n\ncollapse::fselect와 dplyr::select는 유사하지만 fselect가 x100배 정도 빠릅니다.\nss()함수는 컬럼명이 아닌 인덱스로 행,열을 출력을 할 때 사용합니다. fsubset() 보다 빠르지만 기능이 제한적이기 떄문에 간단한 행,열 출력할 때 사용가능합니다.\n\n\n## data.table(row)\ndt[1:10]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)]\ndt[order(HME_YYYYMM)]\ndt[order(HME_YYYYMM, -HGHT)]\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)][order(HGHT)]        \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)] %>% .[order(HGHT)]  #same\n\n## collapse(row)                                       \nfsubset(dt, 1:10)                                                   #ss(dt,1:10)\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25 )               \nroworder(dt, HME_YYYYMM)                                            \nroworder(dt, HME_YYYYMM, -HGHT)\nroworder(dt, HGHT) %>% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25)\n\n\n예시로 두번째 코드와 다섯번째 코드를 확인해보겠습니다.\n\n두번째\n\nfsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25 ) \n\n\n\n\n\n  \n\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)]\n\n\n\n\n\n  \n\n\n\n다섯번째\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)][order(HGHT)] \n\n\n\n\n\n  \n\n\n\n\nroworder(dt, HGHT) %>% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column",
    "href": "posts/2022-07-25-collapse/index.html#column",
    "title": "collapse 패키지 소개",
    "section": "column",
    "text": "column\n\n열 이름을 정규식으로 선택하기 위해서는 fselect()가 아니라 get_vars() 함수를 이용해야합니다. regex = TRUE 정규식을 사용하겠다는 의미이고 return = “names” 컬럼명을 출력하겠다는 의미입니다. get_var()는 fselect()함수와 유사하지만, 수행속도가 좀 더 빠르며 벡터, 정수 형태로 값을 전달합니다.\n유의사항은 컬럼명을 리스트로 전달하면 ERROR 발생합니다.\nfsubset는 빠르고 부분적인 작업을 위해 base::subset 패키지 의 C 함수를 사용하는 향상된 버전입니다.\n\n\n## data.table(column)\ndt[, 1:10]\ndt[, c(\"HGHT\", \"WGHT\")]\ndt[, .(HGHT, WGHT)]\ndt[, .(Height = HGHT, Weight = WGHT)]   \ndt[, .(HGHT)] \ndt[, \"HGHT\"]\ncolvars1 <- grep(\"Q_\", names(dt), value = T)\ndt[, ..colvars1]\ndt[, colvars1, with = FALSE]      \ndt[, .SD, .SDcols = colvars1]     \ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25), ..colvars]\ndt[, !..colvars1]\ndt[, -..colvars1]\ndt[, .SD, .SDcols = -colvars1]\n\n## collapse(column)\nfselect(dt, 1:10)                 \nfselect(dt, c(\"HGHT\", \"WGHT\"))    #get_vars(dt, 1:10)\nfselect(dt, HGHT, WGHT)           #get_vars(dt, c(\"HGHT\", \"WGHT\"))\nfselect(dt, Height = HGHT, Weight = WGHT)\nfselect(dt, .(HGHT))              #ERROR\nfselect(dt, \"HGHT\")\ncolvars2 <-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")    #regex = TRUE 정규식 사용/ return = \"names\" 컬럼명 출력\nfselect(dt, colvars2)                                            #fselect(dt, c(colvars))\nget_vars(dt, colvars2)                                           #get_var(dt, c(colvars))\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25, colvars2)\nfselect(dt, -(4:12))\nfselect(dt, -(Q_PHX_DX_STK:Q_DRK_FRQ_V09N))\n\n\n예시로 같은 값을 출력하는지 확인해 보겠습니다.\n\n\ncolvars1 <- grep(\"Q_\", names(dt), value = T)\n\nqDT(colvars1)\n\n         colvars1\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ncolvars2 <-get_vars(dt, \"Q_\", regex = TRUE, return = \"names\")\n\nqDT(colvars2)\n\n         colvars2\n1:   Q_PHX_DX_STK\n2:  Q_PHX_DX_HTDZ\n3:   Q_PHX_DX_HTN\n4:    Q_PHX_DX_DM\n5:   Q_PHX_DX_DLD\n6:   Q_PHX_DX_PTB\n7:       Q_HBV_AG\n8:       Q_SMK_YN\n9: Q_DRK_FRQ_V09N\n\n\n\ndt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25), ..colvars1]\n\n\n\n\n\n  \n\n\n\n\nfsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25, colvars2)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#column-summary",
    "href": "posts/2022-07-25-collapse/index.html#column-summary",
    "title": "collapse 패키지 소개",
    "section": "Column summary",
    "text": "Column summary\n\nfmean는 (열별) 평균을 계산하고, (선택적으로) 그룹화 및 가중치를 계산하는 일반적인 함수입니다.\ndapply는 데이터에 대한 정보(속성)를 잃거나 데이터의 클래스 또는 형식을 변경하지 않고 데이터의 행이나 열에 함수를 적용하는 효율적인 함수입니다.\n\n\n## data.tanble(Column summary)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))] \n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n#collapse(Column summary)\nfselect(dt,HGHT,WGHT,BMI) %>% fmean()     #fmean(fselect(dt,HGHT,WGHT,BMI))\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n두번째 코드를 비교해보겠습니다.\n\n\ndt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)]\n\n\n\n\n\nHGHT\nWGHT\nBMI\n\n\n164.5487\n65.09672\n23.92257\n\n\n\n\n\ndapply(fselect(dt,HGHT,WGHT,BMI),fmean)\n\n\n\n\n\n\nfmean\n\n\n\nHGHT\n164.54866\n\n\nWGHT\n65.09672\n\n\nBMI\n23.92257"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#by",
    "href": "posts/2022-07-25-collapse/index.html#by",
    "title": "collapse 패키지 소개",
    "section": "By",
    "text": "By\n\ncollap()는 ‘Fast Statistical Functions’ 사용하여 각 컬럼에 여러 함수를 적용할 수 있습니다.(#Fast Statistical Functions: fsum, fprod, fmean, fmedian, fmode, fvar, fsd, fmin, fmax, fnth, ffirst, flast, fnobs, fndistinct)\nadd_stub() 연산을 통해 열을 추가할 수 있는 명령입니다. ““를 통해 열 이름을 설정할 수 있습니다.\n\n\n##data.table(By)\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY]\ndt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = \"EXMD_BZ_YYYY\"]  #same\ndt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY]       #same\ndt[HGHT >= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)]        \ndt[HGHT >= 175, .N, by= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                               #same\ndt[HGHT >= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]                            #same(정렬)\n\n#collapse(By)                                                  \ncollap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16))                                # ~ ≒ by\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY)                       #same\nadd_stub(count(fsubset(dt, HGHT >= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")  #dplyr::count()\n\n\n마지막 코드를 비교해보겠습니다. 결측치 값의 정렬 순서의 차이가 있지만, 같은 기능을 수행할 수 있습니다.\n\n\ndt[HGHT >= 175, .N, keyby= c(\"EXMD_BZ_YYYY\", \"Q_SMK_YN\")]\n\n\n\n\n\n  \n\n\n\n\nadd_stub(count(fsubset(dt, HGHT >= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\")"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#new-variable",
    "href": "posts/2022-07-25-collapse/index.html#new-variable",
    "title": "collapse 패키지 소개",
    "section": "New variable",
    "text": "New variable\n\nftransform 새 열을 계산하거나 기존 열을 수정 및 삭제하는 데 사용할 수 있으며 항상 전체 데이터 프레임을 반환합니다.\nftransform은 base::transform 데이터 프레임 및 목록 의 향상된 버전입니다.\n\n\n## data.table(New variable)\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\ndt[, `:=`(BP_SYS140 = factor(as.integer(BP_SYS >= 140)), BMI25 = factor(as.integer(BMI >= 25)))]\n\ndt[, BMI2 := NULL]\n\n## collapse(New variable)\nftransform(dt, BMI2 = round(WGHT/(HGHT/100)^2, 1))\n\nftransform(dt,BP_SYS140 = factor(as.integer(BP_SYS >= 140)),BMI25 = factor(as.integer(BMI >= 25)))\n\nftransform(dt, BMI2 = NULL)\n\n\n첫번째와 두번째 코드를 확인해보겠습니다.\ndata.table\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n\ncollapse"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "href": "posts/2022-07-25-collapse/index.html#specific-symbol-.n-.sd-.sdcols",
    "title": "collapse 패키지 소개",
    "section": "Specific symbol .N, .SD, .SDcols",
    "text": "Specific symbol .N, .SD, .SDcols\n\n## data.table(Specific symbol .N, .SD, .SDcols)\ndt[, .SD]\n\ndt[, lapply(.SD, class)]\n\ndt[, .N, keyby = \"RN_INDI\"]\n\n## collapse\nfselect(dt,1:32)\n\ndapply(dt,class)\n\nadd_stub(count(dt,RN_INDI),\"N\")\n\n\n두번째 코드를 비교해보겠습니다. 비슷하지만 출력하는 형태가 다릅니다. class()를 사용하여 확인해보면 형태가 다른 것을 알 수 있습니다.\n\n\ndt[, lapply(.SD, class)]\n\n\n\n\n\n  \n\n\n\n\ndapply(dt,class)\n\n\n\n\n\n  \n\n\n\n\nclass(dt[, lapply(.SD, class)])\n\n[1] \"data.table\" \"data.frame\"\n\nclass(dapply(dt,class))\n\n[1] \"character\""
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#order",
    "href": "posts/2022-07-25-collapse/index.html#order",
    "title": "collapse 패키지 소개",
    "section": "order",
    "text": "order\n\n#data.table(order)\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n#collapse(order)\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %>% roworder(BMI)\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %>% roworder(-BMI)\n\n\n첫번째 코드를 통해 data.table과 collapse 출력 결과를 확인해보겠습니다.\n\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n\n  \n\n\n\n\nfmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %>% roworder(BMI)"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-행-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 행 연산",
    "text": "기본 행 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[1:10] ,\n               collapse = fsubset(dt, 1:10))\n\nWarning in microbenchmark(data.table = dt[1:10], collapse = fsubset(dt, : less\naccurate nanosecond times to avoid potential integer overflows\n\n\nUnit: microseconds\n       expr    min      lq     mean  median     uq     max neval cld\n data.table 43.091 43.8905 46.85316 44.4645 45.756 173.512   100   b\n   collapse  5.084  5.6990  6.12294  5.9655  6.232  15.703   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)],\n               collapse = fsubset(dt, EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25 ))\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval cld\n data.table 67.732 70.6225 74.87338 72.2830 74.8455 196.636   100   b\n   collapse 25.994 29.4995 31.62248 30.4835 32.1235  48.339   100  a \n\nmicrobenchmark(data.table = dt[order(HME_YYYYMM, -HGHT)],\n               collapse = roworder(dt, HME_YYYYMM, -HGHT))\n\nUnit: microseconds\n       expr     min       lq      mean  median       uq      max neval cld\n data.table 136.653 142.5365 190.75250 148.543 154.8775 3990.366   100   b\n   collapse  56.744  62.3405  66.85255  65.764  68.7570  109.429   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25)][order(HGHT)],\n               collapse = roworder(dt, HGHT) %>% fsubset(EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25))\n\nUnit: microseconds\n       expr     min       lq     mean   median       uq      max neval cld\n data.table 131.200 135.2795 144.9817 139.8920 142.9875  347.475   100   a\n   collapse  62.402  68.2445 110.8259  75.7065  78.4330 3636.495   100   a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "href": "posts/2022-07-25-collapse/index.html#기본-열-연산",
    "title": "collapse 패키지 소개",
    "section": "기본 열 연산",
    "text": "기본 열 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, 1:10],\n               collapse = fselect(dt, 1:10))\n\nUnit: microseconds\n       expr    min      lq     mean  median     uq     max neval cld\n data.table 32.472 33.3945 37.27023 34.8705 37.720 112.258   100   b\n   collapse  3.649  4.0590  4.56576  4.3460  4.674  18.286   100  a \n\nmicrobenchmark(data.table = dt[, .(Height = HGHT, Weight = WGHT)],\n               collapse = fselect(dt, Height = HGHT, Weight = WGHT))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 145.837 149.6705 156.56383 152.2945 156.333 334.888   100   b\n   collapse   4.510   5.1455   6.05652   5.8630   6.560  18.040   100  a \n\n#base::grep() more faster \nmicrobenchmark(data.table = colvars <- grep(\"Q_\", names(dt), value = T),\n               collapse = colvars <-get_vars(dt, \"Q_\",regex = TRUE, return = \"names\"))\n\nUnit: microseconds\n       expr   min    lq    mean median     uq    max neval cld\n data.table 4.633 4.715 4.82078  4.715 4.8380  8.405   100  a \n   collapse 5.658 5.781 6.05119  5.863 5.9655 22.058   100   b\n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = get_vars(dt, colvars))\n\nUnit: microseconds\n       expr    min      lq     mean  median      uq     max neval cld\n data.table 31.570 32.3695 34.59252 33.1895 34.0505 121.401   100   b\n   collapse  2.501  2.9520  4.11599  3.2390  3.4645  85.813   100  a \n\nmicrobenchmark(data.table = dt[, ..colvars],\n               collapse = fselect(dt, colvars) )\n\nUnit: microseconds\n       expr    min     lq     mean median      uq     max neval cld\n data.table 31.980 32.636 34.10544 33.087 33.7020 101.844   100   b\n   collapse  4.141  4.879  5.43537  5.248  5.5555  21.566   100  a \n\nmicrobenchmark(data.table = dt[(EXMD_BZ_YYYY %in% 2009:2012) & (BMI >= 25), ..colvars],\n               collapse = fsubset(dt,EXMD_BZ_YYYY %in% 2009:2012 & BMI >= 25, colvars))\n\nUnit: microseconds\n       expr    min     lq     mean median      uq     max neval cld\n data.table 72.775 74.333 78.30959 75.809 78.7200 204.959   100   b\n   collapse 23.575 24.764 25.98047 25.133 26.2605  46.125   100  a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#평균-연산",
    "href": "posts/2022-07-25-collapse/index.html#평균-연산",
    "title": "collapse 패키지 소개",
    "section": "평균 연산",
    "text": "평균 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(mean(HGHT), mean(WGHT), mean(BMI))],\n               collapse = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 174.414 179.0265 184.97683 181.1175 184.336 386.015   100   b\n   collapse   7.216   7.9335   8.94415   8.6920   9.266  35.014   100  a \n\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI))],\n               collaspe = fmean(fselect(dt,HGHT,WGHT,BMI)))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq     max neval cld\n data.table 170.027 172.1180 178.54762 174.1475 177.3045 343.211   100   b\n   collaspe   7.380   8.0975   9.17539   8.9585   9.4915  31.816   100  a \n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(13, 14, 16)],\n               collapse = dapply(fselect(dt,HGHT,WGHT,BMI),fmean))\n\nUnit: microseconds\n       expr     min       lq      mean   median       uq     max neval cld\n data.table 176.710 179.8260 185.93008 181.5275 185.0740 321.686   100   b\n   collapse  13.858  14.6575  16.01911  15.4775  16.6255  35.178   100  a"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "href": "posts/2022-07-25-collapse/index.html#그룹별-통계-연산",
    "title": "collapse 패키지 소개",
    "section": "그룹별 통계 연산",
    "text": "그룹별 통계 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT = mean(HGHT), WGHT = mean(WGHT), BMI = mean(BMI)), by = EXMD_BZ_YYYY],\n               collapse.collap = collap(dt, ~ EXMD_BZ_YYYY, fmean, cols = c(13,14,16)),\n               collapse.fmean = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n            expr     min       lq      mean   median       uq      max neval\n      data.table 203.442 210.6580 217.08270 214.5325 218.3660  366.704   100\n collapse.collap  57.728  62.7505  76.49821  65.6000  68.6340 1098.431   100\n  collapse.fmean  36.367  37.3510  38.55107  38.2120  39.0935   55.350   100\n cld\n   c\n  b \n a  \n\nmicrobenchmark(data.table = dt[, lapply(.SD, mean), .SDcols = c(\"HGHT\", \"WGHT\", \"BMI\"), by = EXMD_BZ_YYYY],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 205.984 209.0180 214.88469 210.9245 215.783 404.096   100   b\n   collapse  35.834  36.8795  38.00495  37.4740  38.294  62.853   100  a \n\n#data.table more faster\nmicrobenchmark(data.table = dt[HGHT >= 175, .N, by= .(EXMD_BZ_YYYY, Q_SMK_YN)],\n               collapse = add_stub(count(fsubset(dt, HGHT >= 175, EXMD_BZ_YYYY,Q_SMK_YN),EXMD_BZ_YYYY,Q_SMK_YN),\"N\") )\n\nUnit: microseconds\n       expr      min        lq      mean    median       uq      max neval cld\n data.table  243.868  269.1855  316.4114  303.0925  348.951  543.619   100  a \n   collapse 2144.587 2261.2935 2491.8049 2338.5375 2538.556 5655.335   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "href": "posts/2022-07-25-collapse/index.html#특수-심볼",
    "title": "collapse 패키지 소개",
    "section": "특수 심볼",
    "text": "특수 심볼\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .SD],\n               collapse = fselect(dt,1:32))\n\nUnit: microseconds\n       expr     min       lq      mean   median      uq     max neval cld\n data.table 211.888 216.0290 230.11783 223.3270 231.158 388.516   100   b\n   collapse   4.100   4.5715   5.47678   5.1455   5.576  23.124   100  a \n\nmicrobenchmark(data.table = dt[, lapply(.SD, class)],\n               collapse = dapply(dt,class))\n\nUnit: microseconds\n       expr     min       lq      mean  median       uq     max neval cld\n data.table 450.303 461.5165 489.46415 483.144 499.6875 656.574   100   b\n   collapse   7.421   8.1385   9.17088   9.020   9.5325  16.441   100  a \n\n#data.table more faster\nmicrobenchmark(data.table = dt[, .N, keyby = \"RN_INDI\"],\n               collapse = add_stub(count(dt,RN_INDI),\"N\"))\n\nUnit: microseconds\n       expr      min        lq      mean   median        uq      max neval cld\n data.table  195.365  217.2385  253.8142  251.863  277.6725  382.120   100  a \n   collapse 3653.633 3710.2540 3957.8870 3780.897 3938.3780 6161.275   100   b"
  },
  {
    "objectID": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "href": "posts/2022-07-25-collapse/index.html#정렬-연산",
    "title": "collapse 패키지 소개",
    "section": "정렬 연산",
    "text": "정렬 연산\n\n## benchmarking\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %>% roworder(BMI))\n\nUnit: microseconds\n       expr     min      lq      mean   median       uq     max neval cld\n data.table 273.470 279.825 288.99301 284.1505 289.3165 523.980   100   b\n   collapse  42.025  44.403  47.12991  46.7810  47.9085  92.865   100  a \n\nmicrobenchmark(data.table = dt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)],\n               collapse = fmean(fselect(dt,EXMD_BZ_YYYY,HGHT,WGHT,BMI), dt$EXMD_BZ_YYYY) %>% roworder(-BMI))\n\nUnit: microseconds\n       expr     min      lq      mean   median      uq     max neval cld\n data.table 276.135 282.695 289.30994 285.7085 289.378 488.474   100   b\n   collapse  44.854  47.150  50.61163  49.3640  51.045 115.169   100  a"
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html",
    "href": "posts/2019-09-19-nhiswithr/index.html",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "",
    "text": "김진섭 대표는 9월 25일 을지의대 학술원 특강에 참석, R 을 이용하여 공단/심평원 빅데이터와 국건영 자료를 분석한 경험을 발표할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#요약",
    "href": "posts/2019-09-19-nhiswithr/index.html#요약",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "요약",
    "text": "요약\nR 로 심평원, 공단 빅데이터의 데이터 정리와 통계분석을 수행하였다.\n\nhaven 패키지로 SAS 파일을 직접 읽을 수 있다.\ndplyr, data.table 을 활용, 기존 R 문법보다 빠르게 데이터 를 정리할 수 있다.\ndbplyr 를 활용, R 코드를 PROC SQL 문으로 바꿔 복잡한 SAS 작업을 수행할 수 있다.\n자체 개발한 jsmodule 패키지를 이용, GUI 환경에서 기술통계와 회귀/생존분석을 수행하고 테이블과 그림을 바로 다운받는다.\n\njsmodule 의 표본조사 데이터 분석 기능을 활용, GUI 환경에서 국건영 데이터 분석을 수행하였다."
  },
  {
    "objectID": "posts/2019-09-19-nhiswithr/index.html#slide",
    "href": "posts/2019-09-19-nhiswithr/index.html#slide",
    "title": "R 이용 공공빅데이터 분석 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/NHIS_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html",
    "href": "posts/2022-03-25-graph/index.html",
    "title": "R로 논문용 그래프 그리기",
    "section": "",
    "text": "이번 시간에는 R을 이용해서 데이터와 통계 분석 결과를 한 눈에 전달할 수 있는 그래프를 만들 것이다. 예제 데이터로 지난 시간에 사용한 건강검진 데이터를 이용한다."
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram",
    "href": "posts/2022-03-25-graph/index.html#histogram",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\n연속형 데이터를 히스토그램으로 나타내보자.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\")\n\n\n\n\nbreaks=n 옵션을 이용해서 계급구간의 수를 설정하고, freq=F 옵션을 이용하면 y축을 빈도수가 아닌 확률밀도로 표시할 수 있다. 그래프의 색도 간단하게 설정할 수 있다.\n\nhist(data$HGHT, main=\"Distribution of height\", xlab=\"height(cm)\",\n     breaks = 30, freq=F, col=\"grey\", border=\"white\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot",
    "href": "posts/2022-03-25-graph/index.html#barplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n히스토그램과 유사하지만, X축에 표현하고자 하는 변수가 이산형 변수일 때는 빈도수를 바 그래프로 나타낼 수 있다. table() 함수를 이용해 빈도표를 만들고, 바 그래프로 나타낸다.\n\ntable <- table(data$Q_SMK_YN)\nprint(table)\n\n\n  1   2   3 \n995 256 391 \n\nbarplot(table, main=\"Distribution of smoking\", names.arg=c(\"Never\", \"Ex-smoker\", \"Current\"), ylab=\"frequency\")\n\n\n\n\n연도에 따른 흡연 여부의 분포를 하나의 그래프로 나타낼 수 있다. beside=T 옵션을 사용하면 적층형 그래프가 그룹형 그래프로 바뀐다.\n\ntable <- table(data$Q_SMK_YN, data$EXMD_BZ_YYYY)\nprint(table)\n\n   \n    2009 2010 2011 2012 2013 2014 2015\n  1  125  132  140  146  141  157  154\n  2   34   42   35   36   35   38   36\n  3   53   62   48   52   67   59   50\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n\nbarplot(table, main=\"Distribution of smoking by year\", ylab=\"frequency\",\n        legend=c(\"Never\", \"Ex-smoker\", \"Current\"), beside=T)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot",
    "href": "posts/2022-03-25-graph/index.html#boxplot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n범주형 변수(흡연 여부, X축)에 따른 연속형 변수(수축기 혈압, Y축)의 분포를 나타내는 데는 박스 그래프를 이용할 수 있다.\n\nboxplot(BP_SYS ~ Q_SMK_YN, data = data, names=c(\"Never\", \"Ex-smoker\", \"Current\"), \n        main=\"SBP average by smoking\", ylab=\"SBP(mmHg)\", xlab=\"Smoking\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n두 연속형 변수 간의 관계는 산점도로 한 눈에 보여줄 수 있다. pch=n 옵션은 점의 모양, cex=n 옵션은 점의 크기를 지정한다.\n\nplot(HGHT ~ WGHT, data=data,\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\n\n\n\n\n범주형 변수에 따른 점의 분포를 표현하고자 할 때는 점의 색깔(col= 옵션)로 구분해서 표현할 수 있다. 2009년과 2015년에 실시한 검사에서 수검자의 신장, 체중 분포에 차이가 있는지 확인해보자.\n또, legend() 함수를 이용하면 범례에 사용될 옵션을 따로 설정할 수 있다.\n\ndata2 <- data %>% filter(EXMD_BZ_YYYY %in% c(2009, 2015))\nplot(HGHT ~ WGHT, data=data2, col=factor(EXMD_BZ_YYYY),\n     ylab=\"Height(cm)\", xlab=\"Weight(kg)\",\n     pch=16, cex=0.5)\nlegend(x=\"bottomright\", legend=c(\"2009\", \"2015\"), col=1:2, pch = 19)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#line-plot",
    "href": "posts/2022-03-25-graph/index.html#line-plot",
    "title": "R로 논문용 그래프 그리기",
    "section": "Line plot",
    "text": "Line plot\nplot() 함수에 type=“l” 옵션을 사용하면 선 그래프를 그릴 수 있다.summarize 함수를 이용해 연도에 따른 흡연자 비율(Q_SMK_YN=3)을 계산한 뒤, 선 그래프로 표현해보자.\n\ntable <- data %>% group_by(EXMD_BZ_YYYY) %>% \n  summarize(smoker= mean(Q_SMK_YN==3, na.rm=T))\nprint(table)\n\n# A tibble: 7 × 2\n  EXMD_BZ_YYYY smoker\n         <int>  <dbl>\n1         2009  0.25 \n2         2010  0.263\n3         2011  0.215\n4         2012  0.222\n5         2013  0.276\n6         2014  0.232\n7         2015  0.208\n\nplot(table$EXMD_BZ_YYYY, table$smoker, type=\"l\",\n     xlab=\"Year\", ylab=\"prop of current smoker\")"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n앞에서 만들었던 산점도를 ggplot2를 이용해 다시 만들어보며 ggplot의 기본 문법을 이해해 보자.\nggplot 문법의 첫번째 요소는 시각화할 데이터, x축과 y축 변수, 기하학적 object의 모양, 색, 크기를 지정하고 변수의 스케일을 결정하는 aesthetic mapping이다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY)))\n\n\n\n\n위 코드를 통해 기본적인 그래프의 배경이 그려진다. 여기에 + 연산자를 이용해 기하학적 object를 추가한다. + 연산자는 magrittr에서의 %>%와 같이 ggplot2 함수들을 연결해주는 역할을 한다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point()\n\n\n\n\nggtitle(), xlab(), ylab() 함수를 이용해 각각 그래프 제목, X축 라벨과 Y축 라벨을 추가할 수 있다. scale_color_manual() 함수를 이용하면 범례에 사용될 옵션들을 지정할 수 있다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT, col=factor(EXMD_BZ_YYYY))) +\n  geom_point() +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\")\n\n\n\n\nggplot 문법의 장점은 + 연산자를 이용해서 기존 그래프 위에 새로운 요소를 추가해서 덧씌우는 것이 용이하다는 점이다.\n위의 산점도에 geom_smooth() 함수를 추가하면 그래프 위에 추세선을 덧씌울 수 있다. 이때, aes(col=) 옵션을 ggplot() 함수에서 제외하고 geom_point() 함수 내로 이동시키면, aes(col=) 옵션은 geom_point object에만 영향을 미치고 geom_smooth object는 영향을 받지 않게 된다. 마찬가지로 각 object 함수 내에 지정된 alpha(투명도), size, color 옵션은 해당 object에만 영향을 미친다.\n\nggplot(data=data2, aes(x=HGHT, y=WGHT)) +\n  geom_point(aes(col=factor(EXMD_BZ_YYYY)), alpha=0.5) +\n  ggtitle(\"Height and weight in year 2009 and 2015\") + xlab(\"Height(cm)\") + ylab(\"Weight(cm)\") +\n  scale_color_manual(\n      values = c(\"orange\", \"skyblue\"),\n      labels = c(\"Year 2009\", \"Year 2015\"),\n      name = \"Exam year\") +\n  geom_smooth(color=\"brown\", size=0.8)\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-1",
    "href": "posts/2022-03-25-graph/index.html#boxplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\nggplot의 문법을 이해하고 나면 이를 응용해서 다양한 그래프를 그릴 수 있다.\n앞서 만들었던 흡연 여부에 따른 수축기 혈압의 분포 그래프를 다시 만들어 보자.\n\ndata2 <- data %>% filter(!is.na(Q_SMK_YN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n여기에 하나의 변수를 더 추가해서, facet으로 구분된 그래프를 만들 수 있다.\n\ndata2 <- data2 %>% filter(!is.na(Q_PHX_DX_HTN))\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_wrap(~Q_PHX_DX_HTN, labeller=label_both)\n\n\n\n\nfacet_grid를 이용해 2X2 grid 형태로 그래프를 그릴 수 있다. 고혈압 과거력 변수에 더해, 당뇨 과거력 변수에 따라서도 구분해서 그래프가 나타난다. labeller 함수를 사용해 facet의 label도 원하는 대로 설정할 수 있다.\n\ndata2 <- data2 %>% filter(!is.na(Q_PHX_DX_DM))\n\nHTN.labs <- c(\"No HTN\", \"HTN\")\nnames(HTN.labs) <- c(\"0\", \"1\")\nDM.labs <- c(\"No DM\", \"DM\")\nnames(DM.labs) <- c(\"0\", \"1\")\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN), y=BP_SYS)) +\n  geom_boxplot() +\n  ggtitle(\"SBP average by smoking\") + ylab(\"SBP(mmHg)\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\")) +\n  facet_grid(Q_PHX_DX_DM~Q_PHX_DX_HTN,\n             labeller = labeller(Q_PHX_DX_HTN = HTN.labs, Q_PHX_DX_DM = DM.labs))"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#barplot-1",
    "href": "posts/2022-03-25-graph/index.html#barplot-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Barplot",
    "text": "Barplot\n앞서 만들었던 바 그래프도 ggplot 패키지를 이용하면 table 변환 과정 없이 바로 그릴 수 있다.\n\nggplot(data=data2, aes(x=factor(Q_SMK_YN))) +\n  geom_bar(fill=\"grey\", color=\"black\") +\n  ggtitle(\"Distribution of smoking\") + xlab(\"Smoking\") +\n  scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\n\n\n\n연도에 따른 흡연 여부의 분포도 table 변환 과정 없이 그릴 수 있다. geom_bar()의 position=‘fill’ 옵션을 설정하면 100% 누적 비율 바 그래프가 그려진다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"fill\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"proportion\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015)\n\n\n\n\n누적 비율이 아닌 count를 나타내고 싶다면 geom_bar()의 옵션을 position=‘stack’으로 변경한다. 적층형 그래프가 아닌 그룹형 그래프로 나타내고 싶다면 position=‘dodge’로 변경한다.\n그래프의 X축과 Y축 위치를 뒤집고 싶을 때는 coord_flip() 함수를 이용한다. X축과 Y축의 위치가 서로 바뀌는데, 축의 scale과 label을 다시 설정하지 않아도 되기 때문에 편리하다.\n\nggplot(data=data2, aes(x=EXMD_BZ_YYYY, fill=factor(Q_SMK_YN))) +\n  geom_bar(position=\"dodge\", color=\"grey\") +\n  ggtitle(\"Distribution of smoking by year\") + xlab(\"Year\") + ylab(\"count\") +\n  scale_fill_manual(\n      values = c(\"orange\", \"skyblue\", \"navy\"),\n      labels = c(\"Never\", \"Ex-smoker\", \"Current\"),\n      name = \"Smoking\") +\n  scale_x_continuous(breaks=2009:2015) +\n  coord_flip()"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#histogram-1",
    "href": "posts/2022-03-25-graph/index.html#histogram-1",
    "title": "R로 논문용 그래프 그리기",
    "section": "Histogram",
    "text": "Histogram\nggpubr 패키지의 기본 문법을 활용해서 히스토그램을 그려보자. 고혈압 병력이 있는 군과 없는 군 간에 체중 분포에 차이가 있을지 확인해본다.\n\ndata3 <- data2 %>% mutate(HTN = as.factor(ifelse(Q_PHX_DX_HTN==1, \"Yes\", \"No\")))\np <- gghistogram(data=data3, x=\"WGHT\",\n                     color=\"HTN\", fill = \"HTN\", add=\"mean\")\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\nplot1 <- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               xlab=\"Weight(kg)\",\n               legend.title=\"HTN Dx history\")\nprint(plot1)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#boxplot-2",
    "href": "posts/2022-03-25-graph/index.html#boxplot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Boxplot",
    "text": "Boxplot\n같은 분포를 박스 그래프로도 나타낼 수 있다. 여기에 stat_compare_means() 함수를 활용하면, 고혈압 병력군 간 체중 평균에 통계적으로 유의한 차이가 있는지 확인할 수 있다.\n\np <- ggboxplot(data=data3, x=\"HTN\", y=\"WGHT\", color=\"HTN\") +\n        stat_compare_means(method = \"t.test\", label.x.npc = \"middle\")\n\nplot2 <- ggpar(p,\n               main=\"Weight distrubution by HTN history\",\n               ylab=\"Weight(kg)\",\n               xlab=\"HTN Dx history\",\n               legend=\"none\")\n\nprint(plot2)\n\n\n\n\n세 개 이상의 범주로 나누어졌을 때도 마찬가지로 통계적 유의성을 검정할 수 있다. scale_x_discrete() 함수의 사용에서 확인할 수 있듯이, ggpubr 패키지는 ggplot의 문법을 기반으로 하고 있다.\n\nmy_comparisons <- list(c(\"1\", \"2\"), c(\"2\", \"3\"), c(\"1\", \"3\"))\np <- ggboxplot(data=data3, x=\"Q_SMK_YN\", y=\"WGHT\", color=\"Q_SMK_YN\") +\n        stat_compare_means(comparisons = my_comparisons) +\n        stat_compare_means(label.y = 150) +\n        scale_x_discrete(labels=c(\"Never\", \"Ex-smoker\", \"Current\"))\n\nplot3 <- ggpar(p,\n               main=\"Weight distrubution by smoking\",\n               ylab=\"Weight(kg)\",\n               xlab=\"Smoking\",\n               legend=\"none\")\n\nprint(plot3)"
  },
  {
    "objectID": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "href": "posts/2022-03-25-graph/index.html#scatter-plot-2",
    "title": "R로 논문용 그래프 그리기",
    "section": "Scatter plot",
    "text": "Scatter plot\n위에서 여러번 만들었던 신장과 체중의 산점도를 다시 그려보자. add = “reg.line” 옵션을 이용해 그래프 위에 추세선을 그린 뒤, stat_cor() 함수로 두 변수 간의 상관계수와 p value를 구할 수 있다.\n\np <- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", \n               add = \"reg.line\", conf.int = TRUE,\n               add.params = list(color = \"navy\", fill = \"lightgray\")) +\n        stat_cor(method = \"pearson\")\n\nplot4 <- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot4)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n같은 산점도를 고혈압 병력에 따른 두 그룹으로 나누어서 다른 색으로 표현하고, 상관계수와 p-value를 따로 계산할 수도 있다.\n\np <- ggscatter(data=data3, x=\"HGHT\", y=\"WGHT\", color=\"HTN\", alpha=0.5,\n               add = \"reg.line\", conf.int = TRUE) +\n        stat_cor(aes(color = HTN))\n\nplot5 <- ggpar(p,\n               ylab=\"Weight(kg)\",\n               xlab=\"Height(cm)\")\n\nprint(plot5)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nggarange 함수를 사용하면 여러 개의 그래프를 한 페이지에 배열해서 보여줄 수 있다.\n\nggarrange(plot2, plot3,\n          labels = c(\"A\", \"B\"),\n          ncol = 2, nrow = 1)"
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html",
    "href": "posts/2019-02-06-sccs/index.html",
    "title": "Self-controlled case series",
    "section": "",
    "text": "김진섭 대표는 2월 18일(월) 성균관의대 사회의학교실 주관 가습기 살균제 연구 세미나에 참석, 자기 자신을 대조군으로 이용하는 연구 방법론 중 하나인 self-controlled case series (SCCS)를 리뷰하고 R로 실습을 진행할 예정입니다. 강의 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#요약",
    "href": "posts/2019-02-06-sccs/index.html#요약",
    "title": "Self-controlled case series",
    "section": "요약",
    "text": "요약\n\nSelf-controlled methods는 자기 자신을 대조군으로 비교, time-invariant confounders의 영향을 최소화할 수 있다.\nSelf-controlled case series (SCCS), case-crossover (CCO) design, sequence symmetry analysis (SSA)가 대표적이다.\nSCCS는 위험에 노출된 기간과 그렇지 않은 기간의 상대위험도 (RR) 를 구한다.\n한 사람에게 일어나는 각 사건(ex: 노출, 발생, 나이)이 변화할 때 마다 데이터를 만든다 (Long format data).\nMatched case-control study와 유사, Conditional logistic regression으로도 분석할 수 있다."
  },
  {
    "objectID": "posts/2019-02-06-sccs/index.html#slide",
    "href": "posts/2019-02-06-sccs/index.html#slide",
    "title": "Self-controlled case series",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureSCCS/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html",
    "title": "Redefine Null Hypothesis",
    "section": "",
    "text": "본 연구는 김진섭 대표가 계획했던 연구로, 결과적으로 학술지 게재에 실패했다는 것을 미리 알려드립니다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#abstract",
    "title": "Redefine Null Hypothesis",
    "section": "Abstract",
    "text": "Abstract\n통계적 가설검정에서 광범위하게 이용되는 \\(p\\)-value는 샘플 숫자만 늘리면, 아무리 작은 차이라도 유의미한 결과로 해석되는 문제가 있다. 이것은 대부분의 연구에서 차이 또는 효과가 정확히 0이라는 비현실적인 귀무가설을 사용하기 때문에 발생하는 문제이며, 실제 차이가 0.0000001이라도 정확히 0만 아니라면 샘플 수만 늘려도 결국 \\(p <0.05\\)인 유의한 결과를 얻을 수 있다. 그러나 실제 차이가 정확히 0이라고 생각하는 사람은 아무도 없으며, 아무도 주장하지 않는 것을 반박해 봐야 유용한 결론을 얻지 못한다. 이에 본 연구에서는 귀무가설에 uncertainty 개념을 추가하여 가설검정방법을 재정의하였고, 이 방법에 따른 새로운 \\(p\\)-value가 기존의 \\(p\\)-value와 effect size를 종합한 지표로서 지나치게 작은 차이를 유의한 결과로 해석하는 문제를 해결할 수 있음을 보였다. 한편 다중비교에서 검정할 가설의 갯수가 증가하는 것을 각 가설의 uncertainty가 증가하는 것으로 재해석할 수 있었고, 이 접근법이 기존의 family-wise error rate(FWER), false discovery rate(FDR) control이 지나치게 큰 sample size를 필요로 하는 단점을 보완할 수 있었다. 이 새로운 가설검정방법이 기존 가설검정의 단점을 극복한 새로운 표준이 될 것으로 기대한다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#introduction",
    "title": "Redefine Null Hypothesis",
    "section": "Introduction",
    "text": "Introduction\n통계적 가설검정은 물리학, 생물학, 의학 등의 자연과학뿐만 아니라 경제학, 심리학 등의 사회과학에서도 어떤 주장을 하기위한 필수적인 도구로 이용되며 핵심 개념은 가설검정(hypothesis test)이다(Anderson, Burnham, and Thompson 2000). R.A Fisher에 의해 처음으로 사용된 귀무가설(null hypothesis)은 반박하려는 가설로 이용되며, 이 가설 하에서 일어날 확률이 낮은 사건을 제시함으로서 이것이 틀렸음을 설명하는데, 이는 수학의 증명법 중 하나인 귀류법과 비슷하다(Yates 1964). 여기서 일어날 확률이 낮은 사건을 정량적으로 표현하는 것이 \\(p\\)-value이며 흔히 기준이 되는 \\(p <0.05\\)는 연구의 결과보다 더 극단적인 사건이 발생할 확률이 5%미만임을 의미한다(Wasserstein and Lazar 2016).\n\\(p <0.05\\)는 지금까지 과학 연구에서 새로운 발견의 절대적인 기준으로 이용되었는데, 이에 대한 비판도 꾸준히 제시되어 왔으며 최근에는 유의성의 기준을 0.05에서 0.005로 바꿔야한다는 주장까지 나왔다(JA 2018; Anderson, Burnham, and Thompson 2000; Benjamin et al. 2018; Wasserstein and Lazar 2016). 그러나 \\(p\\)-value의 개념은 그대로 두고 유의수준만 낮추는 것은 지금부터 언급할 가설검정법의 근본적인 문제를 해결하지 못한 임시방편에 불과하다.\n본 연구자들은 비현실적인 귀무가설을 가설검정법의 근본적인 문제로 판단한다. 철학자 쇼펜하우어는 논쟁에서 이기는 38가지 비법이라는 책에서 상대방의 주장을 단순화하라고 주장했는데, 가설검정에서도 귀무가설은 명확하게 제시하는 것이 원칙이며(예: \\(\\mu =0\\)), 가설이 명확해야 그것을 반박하기도 쉽다(Goffey 2005). 그러나 명확한 귀무가설은 현실에서는 존재하지 않는다. 예를 들어 어떤 값이 정확히 0이라고 주장하는 사람이 있을까? 대부분은 어느 정도 uncertainty를 마음속에 갖고 있으며 실제 측정값이 0.0000001이기 때문에 값이 0이라는 가설은 틀렸다고 주장한다면 설득력을 얻기 어렵다. 그러나 현재의 가설검정방법은 uncertainty가 전혀 없는 명확한 귀무가설을 사용하기 때문에 실제 측정값과 가정의 차이가 아무리 작아도 샘플수만 커지면 결국은 \\(p < 0.05\\)인 결과를 얻을 수 있고, 이는 유의수준을 아무리 올려도 마찬가지이다. 예를 들어 Two-sided \\(z\\)-test의 검정통계량은 \\(z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\) 꼴인데 \\(\\bar{X}-\\mu\\)가 0만 아니라면 \\(n\\)을 늘려서 \\(p\\)값을 무한히 0에 가깝게 만들 수 있다. 이에 \\(p\\)-value와 별개로 실제 차이값도 살펴봐야 한다는 주장이 제기되었는데, 이 차이를 표준화한 개념이 effect size로 원래도 required sample size를 계산할 때 이용하는 지표이며, 흔히 차이를 표준편차로 나눈 값을 사용한다(Sullivan and Feinn 2012). 이처럼 현실적인 귀무가설을 설정하는 것은 그 자체로 중요한 것 뿐만 아니라 effect size를 고려하는 것과도 연결되어 있지만 이것을 개념화한 이론이 없어 실제 연구설계에 반영되기 어렵다.\n이에 저자는 uncertainty를 포함한 귀무가설과 이것을 이용한 가설검정방법을 제안할 것이며 이를 통해 계산된 새로운 \\(p\\)-value가 기존 \\(p\\)-value와 effect size를 종합적으로 고려할 수 있는 지표임을 보일 것이다. 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 직접적으로 연결되어 있음을 보임으로서 가설검정 시 effect size를 고려해야 하는 이유에 대한 이론적 배경을 탄탄히 할 수 있으며, 새로운 \\(p\\)를 기준으로 유의수준을 설정하면 샘플수의 힘만으로 작은 차이를 유의한 결과로 해석하는 것을 방지할 수 있다. 간단한 Two-sided \\(z\\)-test를 예로 들어 개념을 정리 한 후 다른 test로 확장할 것이며, 마지막으로 이 개념을 다중비교에 활용하여 새로운 다중비교법을 제안하고 FWER, FDR control방법과 비교해 보겠다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#basic-concept",
    "title": "Redefine Null Hypothesis",
    "section": "Basic Concept",
    "text": "Basic Concept\nBrief Review of Hypothesis Test\n평균은 모르고 분산이 \\(\\sigma^2\\)인 모집단에서 \\(n\\)개의 샘플 \\(X_1, X_2, \\cdots, X_n\\)을 뽑아 모평균이 \\(\\mu_0\\)인지 가설검정을 한다고 하자. 기존에는 귀무가설 \\(H_0\\)를 \\(\\mu = \\mu_0\\)로 설정한 후, 이 가정 하에서 \\(\\bar{X}\\sim N(\\mu_0, \\frac{\\sigma^2}{n})\\)임을 이용해서(\\(n\\)이 적당히 클 때 중심극한정리) \\((\\bar{X}-\\mu_0)\\sim N(0,\\frac{\\sigma^2}{n})\\)을 얻을 수 있으며 검정통계량은 아래와 같이 표현된다.\n\\[ z= \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]\nTwo-sided test라면 \\(z\\)값이 1.96보다 크거나 -1.96보다 작을 때 \\(p <0.05\\)로 유의한 결과로 판단한다. 한편, \\(\\bar{X}-\\mu_0\\)이 딱 0만 아니라면 \\(n\\)이 무한히 커질 때 \\(z\\)도 무한히 커져 어떠한 \\(p\\)-value 기준으로도 유의한 결과가 되는 문제가 있다.\nNull hypothesis with uncertainty\n위의 귀무가설에 uncertainty 개념을 추가해 보자. \\(H_0: \\mu \\sim N(\\mu_0,\\tau^2)\\)는 모평균이 \\(\\mu_0\\)이라고 생각하지만 \\(\\tau\\)정도의 uncertainty를 갖고 있다는 뜻으로 \\(\\tau=0\\)이면 기존의 귀무가설과 일치한다(Figure @ref(fig:fig1)).\n\n\n\n\nNull hypothesis with uncertainty: \\(\\mu_0=0\\)\n\n\n\n\n앞에서는 \\(\\bar{X}-\\mu_0\\)의 분산이 \\(\\frac{\\sigma^2}{n}\\)이었지만 이번에는 모평균 \\(\\mu_0\\)이 uncertainty를 갖고 있으므로 \\(\\bar{X}-\\mu_0\\)의 분산은 \\(\\frac{\\sigma^2}{n}\\)이 아니고 \\(\\frac{\\sigma^2}{n}+ \\tau^2\\)이 된다. 이를 토대로 검정통계량을 새로 표현하면\n\\[z_{\\tau}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + \\tau^2}} \\sim N(0,1)\\]\n이고 이 때의 two-sided \\(p\\)-value를 \\(p_{\\tau} = 2\\times P(Z\\ge |z_{\\tau}|)\\)로 정의한다. \\(p_0\\)은 기존의 \\(p\\)-value와 같으며 \\(\\tau\\)가 0이 아니라면 \\(n\\)이 아무리 커져도 \\(|z_{\\tau}| < |\\frac{\\bar{X}-\\mu_0}{\\tau}|\\) 이 되어 \\(1.96\\times \\tau\\) 이하의 차이는 \\(p_{\\tau}<0.05\\)를 얻을 수 없다.\nUncertainty interpretation\nUncertainty \\(\\tau\\)의 의미는 그것을 모표준편차 \\(\\sigma\\)와 비교했을 때 더 잘 드러난다. \\(\\tau = c\\sigma\\)로 치환한 후 이를 \\(z_{\\tau}\\)에 적용하면 아래와 같다.\n\\[z_{\\tau}=z_{c\\sigma}=\\dfrac{\\bar{X}-\\mu_0}{\\sqrt{\\sigma^2/n + c^2\\sigma^2}} = \\dfrac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{z}{\\sqrt{1+nc^2}}\\]\n이제 통계적으로 유의한결과를 얻으려면 기존 가설검정 대비 \\(\\sqrt{1+nc^2}\\)배의 \\(z\\)값이 필요함을 알 수 있으며, 기존 \\(p\\)-value와 \\(z_{c\\sigma}\\)로 계산한 \\(p_{c\\sigma}\\)의 관계를 Figure @ref(fig:fig2)에 그래프로 나타내었다.\n\n\n\n\n\\(p\\) and \\(p_{c\\sigma}\\)-values according uncertainty and sample size\n\n\n\n\n한편 \\(|z_{c\\sigma}|\\)를 effect size \\(d=|\\frac{\\bar{X}-\\mu_0}{\\sigma}|\\)에 대해 표현하면\n\\[|z_{c\\sigma}|= |\\dfrac{\\bar{X}-\\mu_0}{\\sigma}| \\cdot \\dfrac{1}{\\sqrt{1/n + c^2}} = \\dfrac{d}{\\sqrt{1/n+c^2}}\\]\n의 관계를 얻는다(Figure @ref(fig:fig3)).\n\n\n\n\n\\(p_{c\\sigma}\\) according to effect size, sample size and uncertainty of null hypothesis\n\n\n\n\n만약 \\(d\\)가 \\(1.96\\times c\\) 이하라면\n\\[|z_{c\\sigma}| = \\dfrac{d}{\\sqrt{1/n+c^2}} <  \\dfrac{d}{c} \\le \\dfrac{1.96\\times c}{c}  = 1.96\\]\n이 되어 샘플수 \\(n\\)을 아무리 늘리더라도 \\(p_{c\\sigma} <0.05\\)를 얻을 수는 없다. 연구자는 이 사실을 이용하여 역으로 \\(c\\)를 조절함으로서 연구자가 생각하기에 너무 작은 effect size가 유의한 결과가 되는 것을 막을 수 있는데, \\(c=0.01\\) 즉, 모집단 표준편차 대비 1%의 불확실성을 설정한다면 \\(d\\)가 0.0196 이하인 매우 작은 effect가 유의한 결과를 얻는 것을 막을 수 있다. \\(n\\)이 작을 때는 1%의 uncertainty가 기존의 \\(p\\)-value에 거의 영향을 끼치지 않지만, \\(n=10,000\\)일 때는 유의한 \\(p\\)-value 기준을 0.005로 강화한 것과 같은 효과를 갖는다(Figure 2).\nSample Size Calculation\nUncertainty의 개념이 추가되면 Required sample size도 바뀌게 되는데, 먼저 기존 two-sided \\(z\\)-test의 샘플수 구하는 공식을 standard error에 대해 정리하면 아래와 같다(\\(\\alpha\\): Type 1 error rate, \\(\\beta\\): Type 2 error rate)(Noordzij et al. 2011).\n\\[\n\\begin{aligned}\nn &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{(\\mu-\\mu_0)/\\sigma})^2 \\\\\n\\dfrac{n}{\\sigma^2} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{\\sigma^2\\cdot(1/n)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2\n\\end{aligned}\n\\]\n이제 uncertainty parameter \\(c\\)를 추가하려면 좌변의 분모에 \\(c^2\\sigma^2\\)을 더해주면 되고 새로운 샘플수 구하는 공식은 아래와 같이 바뀌게 된다.\n\\[\n\\begin{aligned}\n\\dfrac{1}{\\sigma^2\\cdot(1/n_{c\\sigma}+c^2)} &= (\\dfrac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\mu-\\mu_0})^2 \\\\\n\\dfrac{1}{n_{c\\sigma}} &= (\\dfrac{(\\mu-\\mu_0)/\\sigma}{Z_{1-\\alpha/2} + Z_{1-\\beta}})^2 -c^2 \\\\\n               &= \\dfrac{1}{n}-c^2 \\\\\nn_{c\\sigma} &= \\dfrac{1}{1/n-c^2}\n\\end{aligned}\n\\]\n따라서 \\(\\frac{1}{n} -c^2 \\le 0\\) 즉, 기존 가설검정방법으로도 \\(1/c^2\\) 이상의 샘플수가 필요했었다면, uncertainty를 포함한 가설검정으로는 아무리 샘플수를 늘려도 Type 1,2 error를 컨트롤할 수 없다. Figure @ref(fig:fig4)에 \\(\\alpha = 0.05\\), \\(1-\\beta=0.8\\)인 경우의 effect size와 required sample size의 관계가 그래프로 표현되어 있으며, uncertainty가 존재한다면 아무리 샘플수를 늘려도 어떤 effect size이하는 검정할 수 없음을 확인할 수 있다.\n\n\n\n\nRequired sample size according effect size in some uncertainty scenarios: \\(\\alpha=0.05\\), \\(1-\\beta=0.8\\)"
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#extend-to-other-statistics",
    "title": "Redefine Null Hypothesis",
    "section": "Extend to Other Statistics",
    "text": "Extend to Other Statistics\n이번엔 \\(Z_{c\\sigma}\\)의 논리를 다른 통계량으로 확장해보자.\n\n\\(t\\)-test: One or paired 2 sample\nOne sample \\(t\\)-test의 귀무가설은 위의 \\(z\\)-test와 일치하며, 모분산 \\(\\sigma^2\\)을 표본분산인 \\(s^2\\)으로만 바꾸면 통계량은 자유도 \\(n-1\\)인 \\(t\\)-distribution을 따르는 것이 알려져 있다(Kim 2015). 이제 귀무가설에 \\(cs\\)만큼의 uncertainty를 추가하면 아래와 같다.\n\\[t_{cs} = \\dfrac{\\bar{X}-\\mu_0}{\\sqrt{s^2/n + c^2s^2}} = \\dfrac{\\bar{X}-\\mu_0}{s/\\sqrt{n}}\\cdot \\dfrac{1}{\\sqrt{1+nc^2}}=\\dfrac{t}{\\sqrt{1+nc^2}}\\]\nPaired \\(t\\)-test의 경우도 \\(\\bar{X}\\)를 두 그룹의 차이의 평균인 \\(\\bar{d}\\), \\(s\\)를 차이의 표준편차인인 \\(s_d\\)로 바꾸면 위와 동일하다.\nIndependent 2 sample \\(t\\)-test\n독립된 2집단(equal variance)의 평균이 같은지를 비교할 때 귀무가설은 \\(H_0: \\mu_1-\\mu_2=0\\)의 형태가 되고 통계량은 아래와 같다(Kim 2015).\n\\[t=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\]\n(\\(s_p\\): pooled variance, \\(n_1, n_2\\): sample numbers of group 1, 2 )\n이 \\(t\\)값이 자유도 \\(n_1+n_2-2\\)인 \\(t\\)-distritution을 따른다고 알려져 있으며, 귀무가설 \\(H_0: \\mu_1-\\mu_2=0\\) \\(s_p\\)의 \\(c\\)배만큼 uncertainty를 추가하면 통계량은\n\\[t_{cs_p}=\\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})+c^2s_p^2)}}= \\dfrac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_p^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\cdot \\dfrac{1}{\\sqrt{1+\\frac{1}{\\frac{1}{n_1}+\\frac{1}{n_2}}}c^2} = \\dfrac{t}{\\sqrt{1+\\frac{n_1n_2}{n_1+n_2}c^2}}\\]\n이 된다.\nRegression coefficient\n종속변수 \\(Y\\)와 독립변수 \\(X_1\\)의 선형관계를 살펴보는 \\(Y=\\beta_0 + \\beta_1X_1\\)에서 \\(H_0: \\beta_1=0\\)을 검정하는 아래 통계량은 자유도 \\(n-2\\)인 \\(t\\)-distribution을 따른다는 것이 알려져 있다(@ Altman and Krzywinski 2015).\n\\[t= \\dfrac{\\hat{\\beta}_1}{se({\\hat{\\beta}_1})}=\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/ \\sqrt{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}} \\]\n(\\(\\hat\\sigma_{\\varepsilon}\\): Mean squared error(MSE), \\(s_{x_1}\\): standard deviance of \\(X_1\\))\n이제 \\(\\dfrac{\\hat\\sigma_{\\varepsilon}}{{s_{x_1}}}\\)의 \\(c\\)배 만큼 귀무가설에 uncertainty를 추가하면 통계량은 아래와 같다.\n\\[t_{c\\hat\\sigma_{\\varepsilon}/s_{x_1}}= \\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n+c^2}} = (\\dfrac{\\hat{\\beta}_1}{\\hat\\sigma_{\\varepsilon}/{s_{x_1}}}\\cdot \\frac{1}{\\sqrt{1/n}}) \\cdot \\dfrac{1}{\\sqrt{1+nc^2}}= \\dfrac{t}{\\sqrt{1+nc^2}}\\]\n일반적으로 \\(Y\\)와 \\(X_1, X_2, \\cdots, X_p\\)들의 선형모형은 행렬을 이용해서 \\(\\mathbf{Y}= \\mathbf{X\\beta}\\)로 표현할 수 있으며, \\(\\beta_i=0\\)을 검정하는 경우 앞서 \\(\\frac{1}{\\sum\\limits_{i=1}^n (X_{1i}-\\bar{X}_1)^2}\\)대신 \\((\\mathbf{X'X})^{-1}\\)의 \\(i+1\\)번째 diagonal인 \\(g_{ii}\\)을 대입한 통계량을 사용하며 자유도는 \\(n-p-1\\)이다(Alexopoulos 2010). 이제 uncertainty를 추가한 통계량을 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)로 정의하면 simple linear model의 경우를 자연스럽게 확장한 셈이 된다. 선형모형을 \\(Y\\)가 정규분포가 아닐 때로 확장한 Generalized linear model(GLM)의 경우에는 \\(z=\\frac{\\hat{\\beta}_i}{se{(\\hat{\\beta}_i)}}\\)가 asymptotically normal distribution을 따르는 것을 이용하며, 앞서와 마찬가지로 uncertainty를 추가한 통계량을 \\(\\dfrac{z}{\\sqrt{1+nc^2}}\\)로 정의하면 일관성을 유지할 수 있다(Nelder and Wedderburn 1972; Wedderburn 1974). .\nCorrelation coefficients\n두 변수 \\(X\\), \\(Y\\)의 상관관계를 볼 때 흔히 사용되는 pearson correlation coefficient(\\(r\\))은 아래 통계량을 이용한다.\n\\[t= \\dfrac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\\]\n이것이 자유도 \\(n-2\\)인 \\(t\\)-distribuition을 따른다는 것이 알려져 있으며 실제로 이 값을 계산해보면 앞서 simple linear model의 \\(\\dfrac{\\hat{\\beta}_1}{se(\\hat{\\beta}_1)}\\)과 정확히 일치한다. 따라서 uncertainty를 추가한 \\(t\\)를 \\(\\dfrac{t}{\\sqrt{1+nc^2}}\\)으로 정의하면 일관성을 유지할 수 있으며, 이는 \\(H_0: r'=\\dfrac{r}{\\sqrt{1-r^2}}=0\\)을 검정할 때 \\(\\dfrac{n}{n-2}\\)의 \\(c\\)배 만큼 uncertainty를 준 것으로 해석할 수 있다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#new-approach-for-multiple-comparison-uncertainty-control",
    "title": "Redefine Null Hypothesis",
    "section": "New Approach for Multiple Comparison: Uncertainty control",
    "text": "New Approach for Multiple Comparison: Uncertainty control\n마지막으로 귀무가설의 uncertainty를 조절하여 multiple comparison 문제를 다루는 uncertainty control을 제안한다.\nDefinition of uncertainty control\n여러 개의 귀무가설을 동시에 검정할 때 원래의 유의수준을 그대로 적용하면 위양성이 늘어나는 문제가 있는데, 각 가설마다의 유의수준이 \\(\\alpha\\)라면 전체 \\(m\\)개의 가설에서 하나라도 위양성결과를 얻을 확률은 \\(1-(1-\\alpha)^m \\simeq m\\alpha\\) 이 되어 원래 위양성 확률의 \\(m\\)배가 된다. 이를 해결하기 위해 흔히 이용되는 방법은 family-wise error rate(FWER) control로 bonferroni correction이 대표적이다. Bonferroni correction에서는 가설이 \\(m\\)개라면 각 가설의 유의수준을 \\(\\alpha_m=\\frac{\\alpha}{m}\\)로 낮춰 family-wise error rate를 \\(\\alpha\\)이하로 control 할 수 있으며 식으로 표현하면 아래와 같다(Armstrong 2014).\n\\[-\\text{log}\\alpha_m = -\\text{log}\\alpha + \\text{log}m\\]\n한편, 여러 개의 귀무가설을 동시에 검정한다는 것을 각 가설의 uncertainty가 늘어나는 것으로 바라볼 수도 있다. Genome-Wide Association Study(GWAS)가 대표적인 예로 이것은 어떤 유전자가 질병과 관계있는지 아무런 사전정보도 없지만 일단 갖고있는 유전자들을 전부 각각 검정해보는 방법이며, 각 검정의 유의수준은 보통 100만개의 유전자를 동시검정할 때의 bonferroni correction에 해당하는 \\(5\\times 10^{-8}\\)를 사용한다(Jannot, Ehret, and Perneger 2015). 그런데 어떤 유전자가 질병과 관련있는지 사전정보가 없음에도 불구하고 각 효과가 정확히 0이라고 생각하는 사람이 과연 있을까? 검정하려는 귀무가설의 갯수가 증가할수록 가설의 uncertainty도 증가한다고 간주하는 것이 자연스러우며 가설이 \\(m\\)개일 때의 uncertainty \\(c_m\\)을 아래와 같이 정의하겠다.\n\\[\\text{log}c_m = \\text{log}c + b\\cdot\\text{log}m\\]\n이것은 아까의 bonferroni correction과 비슷한 형태로 원래의 \\(c\\)(log scale)에 \\(\\text{log}m\\)에 비례하는 penalty를 추가로 부여하며, \\(1.96\\times c_m\\)보다 작은 effect size는 \\(n\\)이 아무리 커도 통계적으로 유의한 결과를 얻을 수 없다. 연구자는 \\(c\\)를 조절하여 initial uncertainty를, 상수 \\(b\\)를 조정하여 uncertainty control의 정도를 조절할 수 있으며, \\(c=0.01\\), \\(b=0.069\\)으로 정하면 \\(n=10000\\)에서 bonferroni correction과 같은 정도의 control을 줄 수 있다 (Figure @ref(fig:fig5)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and required \\(p\\)-values(\\(p_{\\text{req}}\\)): bonferroni correction vs uncertainty control(\\(b=0.069\\), \\(c=0.01\\)\n\n\n\n\nFigure @ref(fig:fig5)를 보면 uncertainty control도 bonferroni correction과 마찬가지로 \\(m\\)이 증가함에 따라 \\(p\\)-value의 기준을 강화함을 알 수 있다. 차이점은 샘플 수에 따라 기준을 강화하는 정도가 다르다는 것인데, \\(m=10^6\\) 기준으로 \\(N=1500\\)일 때는 \\(5.46\\times 10^{-3}\\), \\(N=3000\\)일 때는 \\(6.61\\times 10^{-4}\\), \\(N=5000\\)일 때는 \\(4.22\\times 10^{-5}\\)의 \\(p\\)-value 기준을 갖는다. \\(N=10000\\)일 때는 아까 언급했듯이 \\(p\\)-value 기준 \\(5.06\\times 10^{-8}\\)로 bonferroni correction과 비슷한 정도의 control을 주게 된다.\nWe can look at the change in significance levels in terms of the effect size. Unlike the Bonferroni correction, the degree of significance correction can be changed by the effect sizes even if they have the same \\(p_{c_{m\\sigma}}\\)-values of the Uncertainty control (Figure @ref(fig:fig6)).\n\n\n\n\nNumber of hypotheses(\\(m\\)) and \\(p_{c_m}\\) by effect size(\\(d\\)): uncertainty control\n\n\n\n\nIn Figure @ref(fig:fig6), when applying \\(c=0.01\\) and \\(b=0.069\\), the result with an effect size of \\(0.5\\) and \\(p_{c_{m\\sigma}}=5\\times 10^{-5}\\) is hardly affected by the increase in the number of hypotheses; however, the result with an the effect size of \\(0.05\\) cannot gain significance when the number of hypotheses increases to \\(10^6\\) even if the initial \\(p_{c_{m\\sigma}}\\)-value is same as above.\n\n\n\nSome scenarios: compare with bonferroni correction, false discovery rate(FDR)\n몇 가지 시나리오를 설정하여 uncertainty correction 방법을 흔히 이용하는 multiple comparison 방법인 bonferroni correction, FDR control과 비교해보겠다(Benjamini and Hochberg 1995). \\(10^6\\)개의 귀무가설 중 1% 즉, 10000개가 실제로 참인 가설이고 그 effect size는 \\(d\\)라고 가정하자. 통계적 유의성은 \\(d\\)와 sample size \\(N\\), 그리고 multiple comparison 방법에 따라 달라지게 되며 이론상의 결과와 데이터를 생성해서 simulation한 결과를 Table 1,2에 정리하였다. FWER와 FDR control의 기준은 모두 0.05이며 simulation 때 FDR control은 Benjamini-Hochberg Procedure를 이용하였다(Benjamini and Hochberg 1995).\n\n\n\nThe number of false positive/negative results among \\(10^6\\) hypotheses(1% true) with some scenarios\n\n\n\n\n\n\n\n\n\n\n\nSample size\nd\nBonferroni: False (+)\nBonferroni: True (+)\nFDR: False (+)\nFDR: True (+)\nUncertainty: False (+)\nUncertainty: True (+)\n\n\n\n2000\n0.02\n0\n0\n0\n0\n2655\n176\n\n\n2000\n0.04\n0\n1\n0\n2\n2655\n1125\n\n\n2000\n0.06\n0\n28\n41\n788\n2655\n3750\n\n\n2000\n0.08\n0\n305\n245\n4652\n2655\n7176\n\n\n2000\n0.10\n0\n1637\n437\n8311\n2655\n9292\n\n\n3000\n0.02\n0\n0\n0\n0\n654\n104\n\n\n3000\n0.04\n0\n6\n4\n81\n654\n1123\n\n\n3000\n0.06\n0\n152\n167\n3172\n654\n4526\n\n\n3000\n0.08\n0\n1424\n424\n8050\n654\n8355\n\n\n3000\n0.10\n0\n5103\n515\n9776\n654\n9809\n\n\n4000\n0.02\n0\n0\n0\n0\n164\n62\n\n\n4000\n0.04\n0\n17\n24\n450\n164\n1082\n\n\n4000\n0.06\n0\n488\n301\n5727\n164\n5115\n\n\n4000\n0.08\n0\n3477\n496\n9429\n164\n9021\n\n\n4000\n0.10\n0\n8087\n525\n9979\n164\n9947\n\n\n5000\n0.02\n0\n0\n0\n0\n42\n37\n\n\n5000\n0.04\n0\n44\n64\n1217\n42\n1027\n\n\n5000\n0.06\n0\n1134\n400\n7596\n42\n5588\n\n\n5000\n0.08\n0\n5814\n519\n9857\n42\n9409\n\n\n5000\n0.10\n0\n9474\n526\n9998\n42\n9985\n\n\n10000\n0.02\n0\n3\n1\n20\n0\n3\n\n\n10000\n0.04\n0\n733\n350\n6656\n0\n736\n\n\n10000\n0.06\n0\n7084\n523\n9944\n0\n7091\n\n\n10000\n0.08\n0\n9946\n526\n10000\n0\n9946\n\n\n10000\n0.10\n0\n10000\n526\n10000\n0\n10000\n\n\n\n\n\nTable 1을 보면 uncertainty control는 나머지 둘과는 달리 sample size가 작을 때 유의한 결과를 많이 얻을 수 있음을 알 수 있다. 구체적으로 \\(d\\)가 0.02일 때 bonferroni correction, FDR control로는 \\(n\\)이 5000이하일 때 유의한 결과를 하나도 얻을 수 없지만 uncertainty control로는 \\(n=1500,3000,5000\\)일 때 각각 5175, 646, 62개의 유의한 결과를 얻을 수 있다. \\(d=0.05\\)인 경우 \\(n=1500, 3000\\)이면 uncertainty control > FDR control > bonferroni correction, \\(n=5000\\)이면 FDR control > uncertainty control > bonferroni correction 순으로 유의한 결과를 많이 얻을 수 있다. 한편 \\(n=10000\\)인 모든 시나리오에서 uncertainty control은 bonferroni correction보다도 강한 기준으로 나타나고 sample size의 힘만으로 작은 effect size를 유의한 결과로 판단하는 것에 penalty를 주게 된다."
  },
  {
    "objectID": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "href": "posts/2018-11-08-redefinenullhypothesis/index.html#discussion",
    "title": "Redefine Null Hypothesis",
    "section": "Discussion",
    "text": "Discussion\n본 연구에서는 uncertainty의 개념을 포함한 현실적인 귀무가설을 만들고 그것을 검정할 수 있는 가설검정법을 제안하였으며, 이를 통해 계산된 \\(p_{c\\sigma}\\)-value가 기존의 \\(p\\)-value와 effect size를 종합적으로 고려한 지표로 작은 effect size가 sample size의 힘만으로 유의한 결과를 얻는 것에 penalty를 줄 수 있음을 보였다. 이는 현실적인 귀무가설을 설정하는 것이 effect size를 고려하는 것과 어떻게 연결되는지를 개념화했다는 의의가 있으며, 이제 연구자들은 차이가 정확히 0이라는 비현실적인 귀무가설에서 벗어나 현실적인 가설검정을 수행하면서 effect size를 가설검정의 틀 안에서 고려할 수 있을 것이다.\n얼마만큼의 uncertainty를 고려해야 하는지는 검증가능한 최소 effect size가 어느정도인지에 달려 있는데, 이는 연구분야와 주제에 따라서 달라질 수 있기 때문에 각 분야의 연구자들의 공감대가 형성되어야 할 것으로 생각된다. 앞서 언급한 \\(c=0.01\\)이 기존의 \\(p\\)-value에 큰 영향을 끼치지 않으면서 10000이상의 매우 큰 sample size에 0.005정도의 유의수준을 부여할 수 있는 적절한 기준이라 판단한다. 한편 effect size로 표준화하지 않더라도 차이값 자체를 기준으로 uncertainty를 고려할 수도 있는데, 사실 어떤 지표든지 그것을 측정하는 순간 측정의 최소단위로 인한 uncertainty가 존재할 수 밖에 없다. 예를 들어 수축기 혈압을 측정해서 120이 나왔을 때 그것의 의미는 실제값이 정확히 120이라는 것이 아니라 실제값이 119.5~120.5 사이에 존재한다는 것이다. 따라서 차이가 1이하인 것을 가설검정하는 것은 그 자체로 비현실적이라 할 수 있으며 \\(\\tau= \\frac{1}{1.96}\\)로 두면 \\(|z_{\\tau}| < |\\frac{\\bar{X}-\\mu_0}{\\tau}| \\le \\frac{1}{\\tau} = 1.96\\)이 되어 1이하의 차이를 유의한 결과로 판단하는 것을 방지할 수 있다. 꼭 측정의 최소단위가 아니더라도 연구자가 생각하는 차이의 최소 단위를 \\(\\tau\\)에 반영할 수도 있다.\n베이지안 통계에서도 prior를 통해 사전지식의 uncertainty를 다룰 수 있는데, prior의 정보가 데이터의 정보가 합쳐져서 posterior가 된다는 것에서 본 연구와는 차이가 있다. 예를 들어 분산이 \\(\\sigma\\)인 정규분포 에서 \\(n\\)개의 sample을 뽑아 베이지안 방법으로 모평균을 추정할 때, prior(conjugate prior)의 분산이 \\(\\tau^2\\)이라면 posterior의 분산은 \\(\\dfrac{1}{\\frac{1}{\\sigma^2/n}+\\frac{1}{\\tau^2}}\\)가 된다(Murphy 2007). 이 때 \\(\\tau\\)가 무한히 커지면 분산은 \\(\\sigma^2/n\\)이 되는데, \\(\\tau\\)가 커질수록 prior의 정보가 없어 data의 정보가 posterior로 그대로 반영되는 것으로 볼 수 있다. 반면 본 연구에서는 귀무가설의 uncertainty값인 \\(\\tau\\)가 penalty로 작용하여 이것이 커질수록 data의 정보가 희석된다고 생각할 수 있다.\n3개 이상의 그룹을 비교할 수 있는 ANOVA와 Chi-square test에 대해서는 uncertainty의 개념을 적용하기 힘들었다. ANOVA의 귀무가설은 \\(H_0: \\mu_1= \\mu_2 = \\cdots = \\mu_k\\)꼴로 여러 그룹들을 동시에 비교하기 때문에 본 연구의 uncertainty의 개념을 그대로 적용할 수 없지만 실용적으로 \\(F_c= \\dfrac{F}{1+nc^2}\\)로 정의하면 다른 test들과 일관성을 유지할 수 있을 것이다. 범주형 변수끼리의 연관성을 비교할 때 쓰이는 카이제곱검정의 경우에도 \\(\\chi_c= \\dfrac{\\chi}{1+nc^2}\\)가 실용적인 지표로 활용될 수 있으리라 생각한다.\nUncertainty의 개념은 multiple comparison에서도 활용될 수 있는데, 검정할 귀무가설의 갯수가 늘어나는 것을 각 가설에 대한 uncertainty가 늘어나는 것으로 생각할 수 있기 때문이다. 이 기준을 이용하면 sample size가 적을 때 상대적으로 유의한 결과를 많이 얻을 수 있어 반대의 특징을 갖는 FWER, FDR control의 한계를 보완할 수 있다. 다중비교가 흔히 쓰이는 GWAS의 경우 \\(p<5 \\times 10^{-8}\\)라는 강한 기준이 gold standard로 사용되어 실제 효과가 있는 유전자는 통계적 유의성을 얻지 못하고, 아주 미미한 효과가 sample size의 힘만으로 의미있는 유전자가 되는 문제가 있었는데, uncertainty control을 같이 활용함으로서 이를 극복할 수 있으리라 생각한다.\n귀무가설에 uncertainty를 추가한 본 연구의 가설검정 방법이 과학연구에서 기존 가설 검정방법을 포괄하는 새로운 표준이 될 수 있으리라 확신한다."
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html",
    "href": "posts/2022-01-25-doctorskku2022/index.html",
    "title": "창업 경험 공유",
    "section": "",
    "text": "김진섭 대표는 1월 28일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정으로, 발표 슬라이드를 미리 공유합니다. 자세한 내용은 메디게이트뉴스 http://medigatenews.com/news/3852895813 참고 부탁드립니다."
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "href": "posts/2022-01-25-doctorskku2022/index.html#요약",
    "title": "창업 경험 공유",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n데이터 입력/관리/분석 통합서비스\n공동연구에 코인 인센티브 = 공동연구플랫폼\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n연구지원전문가 새로운 직업 창출"
  },
  {
    "objectID": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "href": "posts/2022-01-25-doctorskku2022/index.html#slide",
    "title": "창업 경험 공유",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/doctorskku2022 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "",
    "text": "김진섭 대표는 4월 2일(목) 부터 6회에 걸쳐, 서울대병원 진단검사의학과 의국원들의 통계분석 능력 함양을 위한 맞춤 교육 이라는 주제로 R 교육을 진행할 예정입니다. 1주차 강의록을 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다. 본 블로그에 정리 내용이 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 첫 번째에 해당하며 2주차에 tidyverse 를 다룰 것이다. data.table 은 이번 교육에는 포함시키지 않았는데, R에 익숙해지면서 느린 속도가 점점 거슬린다면 data.table 을 시작할 때이다.\n실습은 클라우드 환경인 RStudio cloud 를 이용하여 진행한다. 회원가입 후, 아래를 따라 강의자료가 포함된 실습환경을 생성하자.\n\n\nhttps://rstudio.cloud 회원 가입\n\n\n\n\nhttps://rstudio.cloud/spaces/53975/join?access_code=kuFNlbt%2FbSj6DH%2FuppMdXzvU4e1EPrQNgNsFAQBf 들어가서 “Join Space” 클릭\n\n\n\n\n위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 를 선택 후, Repo 주소 https://github.com/jinseob2kim/lecture-snuhlab 입력.\n\n\n\n\n\n\nproject 생성\n\n\n\n\n개인 PC에서 실습을 원한다면 http://www.r-project.org 와 https://rstudio.com/products/rstudio/download/#download 에서 R과 RStudio 를 설치하자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#전체-강의-일정",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "전체 강의 일정",
    "text": "전체 강의 일정\n\n\n회차\n일시\n주제\n\n\n\n1\n4월 2일(목) 11-13시\nR 데이터 매니지먼트 기초\n\n\n2\n4월 14일(화) 11-13시\nR 데이터 매니지먼트 최근: tidyverse\n\n\n\n3\n4월 28일(화) 11-13시\nR 데이터 시각화: ggplot2\n\n\n\n4\n5월 12일(화) 11-13시\n의학연구에서의 기술통계\n\n\n5\n5월 26일(화) 11-13시\n회귀분석, 생존분석\n\n\n6\n6월 9일(화) 11-13시\nR로 논문쓰기: rmarkdown"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#r-기초연산-벡터vector",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "\nR 기초연산 : 벡터(vector)",
    "text": "R 기초연산 : 벡터(vector)\nR 의 기본 연산단위는 벡터이며, x <- c(1, 2, 3) 은 1,2,3 으로 이루어진 길이 3인 벡터를 x 에 저장한다. 대입연산자는 = 와 <- 둘 다 가능하지만 함수의 인자로도 쓰이는 = 와 구별하기 위해 <- 를 권장한다. 자주 쓰는 연산을 실습하자.\n\nx <- c(1, 2, 3, 4, 5, 6)            ## vector of variable\ny <- c(7, 8, 9, 10, 11, 12)\nx + y                                  \n\n[1]  8 10 12 14 16 18\n\nx * y\n\n[1]  7 16 27 40 55 72\n\nsqrt(x)                            ## root\n\n[1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490\n\nsum(x)                                \n\n[1] 21\n\ndiff(x)                            ## difference\n\n[1] 1 1 1 1 1\n\nmean(x)                            ## mean  \n\n[1] 3.5\n\nvar(x)                             ## variance\n\n[1] 3.5\n\nsd(x)                              ## standard deviation\n\n[1] 1.870829\n\nmedian(x)                          ## median\n\n[1] 3.5\n\nIQR(x)                             ## inter-quantile range\n\n[1] 2.5\n\nmax(x)                             ## max value\n\n[1] 6\n\nwhich.max(x)                       ## order of max value\n\n[1] 6\n\nmax(x, y)                          ## max value among x & y\n\n[1] 12\n\nlength(x)                          \n\n[1] 6\n\n\nmax(x, y) 는 x, y 각각의 최대값이 아닌, 전체에서 최대인 값 1개를 보여줌을 기억하자. 잠시 후 각각의 최대값 구하는 연습문제가 나온다.\n벡터에서 특정 항목을 골라내려면 그것의 위치 혹은 조건문을 이용한다.\n\nx[2]                               ## 2 번째\n\n[1] 2\n\nx[-2]                              ## 2 번째만 빼고\n\n[1] 1 3 4 5 6\n\nx[1:3]                             ## 1-3 번째\n\n[1] 1 2 3\n\nx[c(1, 2, 3)]                      ## 동일 \n\n[1] 1 2 3\n\nx[c(1, 3, 4, 5, 6)]                ## 1, 3, 4, 5, 6  번째\n\n[1] 1 3 4 5 6\n\nx >= 4                             ## 각 항목이 4 이상인지 TRUE/FALSE\n\n[1] FALSE FALSE FALSE  TRUE  TRUE  TRUE\n\nsum(x >= 4)                        ## TRUE 1, FALSE 0 인식 \n\n[1] 3\n\nx[x >= 4]                          ## TRUE 인 것들만, 즉 4 이상인 것들         \n\n[1] 4 5 6\n\nsum(x[x >= 4])                     ## 4 이상인 것들만 더하기. \n\n[1] 15\n\nx %in% c(1, 3, 5)                  ## 1, 3, 5 중 하나에 속하는지 TRUE/FALSE\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nx[x %in% c(1, 3, 5)]               \n\n[1] 1 3 5\n\n\n벡터만들기\nseq 로 일정 간격인, rep 로 항목들이 반복되는 벡터를 만들 수 있다.\n\nv1 <- seq(-5, 5, by = .2); v1             ## Sequence\n\n [1] -5.0 -4.8 -4.6 -4.4 -4.2 -4.0 -3.8 -3.6 -3.4 -3.2 -3.0 -2.8 -2.6 -2.4 -2.2\n[16] -2.0 -1.8 -1.6 -1.4 -1.2 -1.0 -0.8 -0.6 -0.4 -0.2  0.0  0.2  0.4  0.6  0.8\n[31]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[46]  4.0  4.2  4.4  4.6  4.8  5.0\n\nv2 <- rep(1, 3); v2                       ## Repeat\n\n[1] 1 1 1\n\nv3 <- rep(c(1, 2, 3), 2); v3              ## Repeat for vector\n\n[1] 1 2 3 1 2 3\n\nv4 <- rep(c(1, 2, 3), each = 2); v4       ## Repeat for vector : each\n\n[1] 1 1 2 2 3 3\n\n\n\nfor, if/else, ifelse 문\nfor loop 는 같은 작업을 반복할 때 이용하며 while 도 비슷한 의미이다. 예시를 통해 배워보자.\n\nfor (i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\ni <- 0\nfor (j in c(1, 2, 4, 5, 6)){\n  i <- i + j\n}\ni\n\n[1] 18\n\n\nif 와 else 는 조건문을 다룬다. else 나 else if 문은 선행 조건문의 마지막과 같은 줄이어야 함을 기억하자.\n\nx <- 5\nif (x >= 3 ){\n  x <- x + 3\n}\nx\n\n[1] 8\n\nx <- 5\nif (x >= 10){\n  print(\"High\")\n} else if (x >= 5){\n  print(\"Medium\")\n} else {\n  print(\"Low\")\n}                                          ## else if, else 주의: 반드시 } 와 같은 줄에 위치하도록.\n\n[1] \"Medium\"\n\n\nifelse 는 벡터화된 if/else 문으로 벡터의 각 항목마다 조건문을 적용하는데, 엑셀의 if 문과 비슷하다.\n\nx <- 1:6\ny <- ifelse(x >= 4, \"Yes\", \"No\")           ## ifelse (조건,참일때,거짓일때)\ny\n\n[1] \"No\"  \"No\"  \"No\"  \"Yes\" \"Yes\" \"Yes\"\n\n\n함수 만들기\n막 R을 배우는 단계에서는 함수를 만들어 쓸 일이 거의 없겠지만, 결측치 포함된 데이터에서 평균이나 분산을 구할 때 귀찮을 수 있다. R은 결측치가 하나라도 포함되면 평균값, 분산값으로 NA를 출력하기 때문이다. 이를 해결하기 위해서라도 아래처럼 기초 함수 만드는 법은 알고 있는 것이 좋다.\n\nx <- c(1:10, 12, 13, NA, NA, 15, 17)      ## 결측치가 포함되어 있다면..\nmean(x)                                           \n\n[1] NA\n\nmean0 <- function(x){\n  mean(x, na.rm = T)\n}                                         ## mean함수의 na.rm 옵션을 TRUE로 바꿈. default는 F\n\nmean0 <- function(x){mean(x, na.rm = T)}  ## 한줄에 쓸 수도 있다. \nmean0(x)\n\n[1] 8\n\n\n둘 이상의 변수를 포함한 함수도 다음과 같이 만들 수 있다.\n\ntwomean <- function(x1, x2){\n  a <- (x1 + x2)/2\n  a\n}\ntwomean(4, 6)\n\n[1] 5\n\n\nApply 문 : apply, sapply, lapply\n\n벡터를 다루는 연산을 잘 활용하면, 벡터의 각 항목에 대해 for loop 을 쓰는 것보다 간편하게 코드를 작성할 수 있다. 행렬에서 행마다 평균을 구하는 예를 살펴보자.\n\nmat <- matrix(1:20, nrow = 4, byrow = T)   ## 4행 5열, byrow = T : 행부터 채운다. \nmat\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n[3,]   11   12   13   14   15\n[4,]   16   17   18   19   20\n\n\n모든 행에 대해 for loop 을 이용, 평균을 구하여 저장하는 코드는 아래와 같다.\n\nout <- NULL                                ## 빈 벡터, 여기에 하나씩 붙여넣는다.\nfor (i in 1:nrow(mat)){\n  out <- c(out, mean(mat[i, ]))\n}\nout\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고 여기에 결과를 붙여가는 모습이 번거로워 보인다. sapply 또는 lapply 를 사용하면 행 또는 열 단위 연산을 간단히 수행할 수 있다.\n\nsapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return vector\n\n[1]  3  8 13 18\n\nlapply(1:nrow(mat), function(x){mean(mat[x, ])})             ## Return list type\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 8\n\n[[3]]\n[1] 13\n\n[[4]]\n[1] 18\n\nunlist(lapply(1:nrow(mat), function(x){mean(mat[x, ])}))     ## Same to sapply\n\n[1]  3  8 13 18\n\n\n처음에 빈 벡터를 만들고, 이어붙이는 과정이 생략되어 간단한 코드가 되었다. list 는 벡터보다 상위개념으로 모든 것을 담을 수 있는 큰 그릇에 비유할 수 있는데, 본 강의에서는 unlist 를 취하면 벡터나 행렬을 얻게 된다는 정도만 언급하고 넘어가겠다. 사실 행렬의 행/열 단위 연산은 apply 혹은 row***, col*** 시리즈의 함수가 따로 있어, 더 간단히 이용할 수 있다.\n\napply(mat, 1, mean)                                          ## 1: 행\n\n[1]  3  8 13 18\n\nrowMeans(mat)                                                ## 동일\n\n[1]  3  8 13 18\n\nrowSums(mat)                                                 ## 행별로 합\n\n[1] 15 40 65 90\n\napply(mat, 2, mean)                                          ## 2: 열\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\ncolMeans(mat)                                                ## 열별로 합\n\n[1]  8.5  9.5 10.5 11.5 12.5\n\n\n연습문제 1\nsapply나 lapply를 이용하여, 아래 두 벡터의 최대값을 각각 구하여라.\n\nx <- 1:6\ny <- 7:12\n\n\n정답 보기\n\nlapply(list(x, y), max)\n\n[[1]]\n[1] 6\n\n[[2]]\n[1] 12\n\n  sapply(list(x, y), max)\n\n[1]  6 12\n\n\n\n멀티코어 병렬연산으로 apply 를 빠르게 수행할 수도 있는데 본 강의에서는 생략한다. 궁금하신 분은 과거 정리 내용 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#데이터-불러와서-작업하기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "데이터 불러와서 작업하기",
    "text": "데이터 불러와서 작업하기\n이제부터는 실제 데이터를 읽어서 그 데이터를 매니징 하는 방법을 배워보도록 하겠다.\n데이터 불러오기, 저장하기\n데이터를 불러오기 전에 미리 디렉토리를 지정하면 그 다음부터는 편하게 읽고 쓸 수 있다.\n\ngetwd()                                                     ## 현재 디렉토리 \nsetwd(\"data\")                                               ## 디렉토리 설정\n## 동일\nsetwd(\"/home/js/Homepage/blog/_posts/2020-02-16-rdatamanagement-basic/data\")\ngetwd()\n\n폴더 구분을 / 로 해야 한다는 점을 명심하자 (\\\\ 도 가능). R 은 유닉스 기반이기 때문이다. 이제 실습 데이터를 읽어볼텐데, 가급적이면 데이터 포맷은 csv로 만드는 것을 추천한다. 콤마로 분리된 가장 간단한 형태로, 용량도 작고 어떤 소프트웨어 에서도 읽을 수 있기 때문이다. 물론 Excel, SPSS, SAS 파일도 읽을 수 있는데, 변수명이나 값에 한글이 있으면 encoding 에러가 생길 수 있으므로 미리 처리하자.\n\nex <- read.csv(\"example_g1e.csv\")\nhead(ex)\n\nURL 링크를 이용할 수도 있다.\n\nex <- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n\n\n\n\n  \n\n\n\nExcel 파일은 readxl, SAS나 SPSS는 haven 패키지를 이용한다.\n\n#install.packages(c(\"readxl\", \"haven\"))                    ## install packages    \nlibrary(readxl)                                            ## for xlsx\nex.excel <- read_excel(\"example_g1e.xlsx\", sheet = 1)      ## 1st sheet\n\nlibrary(haven)                                             ## for SAS/SPSS/STATA   \nex.sas <- read_sas(\"example_g1e.sas7bdat\")                 ## SAS\nex.spss <- read_sav(\"example_g1e.sav\")                     ## SPSS\nhead(ex.spss)\n\n아래와 같이 Excel, SAS, SPSS 데이터는 read.csv 와 형태가 좀 달라보인다. 이것은 최근 R에서 인기있는 tidyverse 스타일의 데이터인데, 자세한 내용은 다음 강의에서 다룰 예정이니 일단 넘어가자.\n\n\n\n\n  \n\n\n\n파일 저장은 write.csv 를 이용하며, 맨 왼쪽에 나타나는 행 넘버링을 빼려면 row.names = F 옵션을 추가한다.\n\nwrite.csv(ex, \"example_g1e_ex.csv\", row.names = F)\n\nhaven 패키지에서 write_sas 나 write_sav 도 가능하다.\n\nwrite_sas(ex.sas, \"example_g1e_ex.sas7bdat\")\nwrite_sav(ex.spss, \"example_g1e_ex.sav\")"
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#읽은-데이터-살펴보기",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "읽은 데이터 살펴보기",
    "text": "읽은 데이터 살펴보기\n본격적으로 데이터를 살펴보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n데이터 살펴보기\nhead 로 처음 6줄, tail 로 마지막 6줄을 볼 수 있다. 데이터 간단히 확인하려고 쓰인다.\n\nhead(ex)                                                   ## 처음 6행\ntail(ex)                                                   ## 마지막 6행\nhead(ex, 10)                                               ## 처음 10행\n\n\n\n\n\n  \n\n\n\nstr 은 head 와는 다른 방식으로 데이터를 확인한다. int 는 정수, num 은 실수형을 의미한다.\n\nstr(ex)\n\n'data.frame':   1644 obs. of  32 variables:\n $ EXMD_BZ_YYYY  : int  2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 ...\n $ RN_INDI       : int  562083 334536 911867 183321 942671 979358 554112 487160 793017 219397 ...\n $ HME_YYYYMM    : int  200909 200911 200903 200908 200909 200912 200911 200908 200906 200912 ...\n $ Q_PHX_DX_STK  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTDZ : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_HTN  : int  1 0 0 NA NA NA NA NA NA 1 ...\n $ Q_PHX_DX_DM   : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_DLD  : int  0 0 0 NA NA NA NA NA NA 0 ...\n $ Q_PHX_DX_PTB  : int  NA NA NA NA NA NA NA NA NA NA ...\n $ Q_HBV_AG      : int  3 2 3 3 3 2 2 3 3 3 ...\n $ Q_SMK_YN      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Q_DRK_FRQ_V09N: int  0 0 0 0 0 0 0 0 0 0 ...\n $ HGHT          : int  144 162 163 152 159 157 160 159 156 146 ...\n $ WGHT          : int  61 51 65 51 50 55 56 54 53 48 ...\n $ WSTC          : int  90 63 82 70 73 73 67 66 67 78 ...\n $ BMI           : num  29.4 19.4 24.5 22.1 19.8 22.3 21.9 21.4 21.8 22.5 ...\n $ VA_LT         : num  0.7 0.8 0.7 0.8 0.7 1.5 1.5 1.2 1.2 1.5 ...\n $ VA_RT         : num  0.8 1 0.6 0.9 0.8 1.5 1.5 1.5 1 1.5 ...\n $ BP_SYS        : int  120 120 130 101 132 110 119 111 138 138 ...\n $ BP_DIA        : int  80 80 80 62 78 70 78 60 72 84 ...\n $ URN_PROT      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ HGB           : num  12.6 13.8 15 13.1 13 11.9 11.2 12.2 11 12.8 ...\n $ FBS           : int  117 96 118 90 92 100 84 88 74 107 ...\n $ TOT_CHOL      : int  264 169 216 199 162 192 152 166 155 178 ...\n $ TG            : int  128 92 132 100 58 109 38 42 86 87 ...\n $ HDL           : int  60 70 55 65 40 53 43 58 52 35 ...\n $ LDL           : int  179 80 134 114 111 117 101 99 85 125 ...\n $ CRTN          : num  0.9 0.9 0.8 0.9 0.9 0.7 0.8 1 0.6 0.7 ...\n $ SGOT          : int  25 18 26 18 24 15 8 16 15 21 ...\n $ SGPT          : int  20 15 30 14 23 12 6 11 13 21 ...\n $ GGT           : int  25 28 30 11 15 14 10 12 13 23 ...\n $ GFR           : int  59 74 79 61 49 83 97 65 96 70 ...\n\n\nnames 로 변수들 이름을 확인할 수 있다. 공백이나 특수문자는 “.” 로 바뀌고, 이름이 같은 변수들은 뒤에 숫자가 추가되어 구별된다. read.csv(..., check.names = F) 옵션으로 원래 이름을 유지할 수 있으나 에러의 원인이 되므로 추천하지 않는다.\n\nnames(ex)\n\n [1] \"EXMD_BZ_YYYY\"   \"RN_INDI\"        \"HME_YYYYMM\"     \"Q_PHX_DX_STK\"  \n [5] \"Q_PHX_DX_HTDZ\"  \"Q_PHX_DX_HTN\"   \"Q_PHX_DX_DM\"    \"Q_PHX_DX_DLD\"  \n [9] \"Q_PHX_DX_PTB\"   \"Q_HBV_AG\"       \"Q_SMK_YN\"       \"Q_DRK_FRQ_V09N\"\n[13] \"HGHT\"           \"WGHT\"           \"WSTC\"           \"BMI\"           \n[17] \"VA_LT\"          \"VA_RT\"          \"BP_SYS\"         \"BP_DIA\"        \n[21] \"URN_PROT\"       \"HGB\"            \"FBS\"            \"TOT_CHOL\"      \n[25] \"TG\"             \"HDL\"            \"LDL\"            \"CRTN\"          \n[29] \"SGOT\"           \"SGPT\"           \"GGT\"            \"GFR\"           \n\n\n샘플수, 변수 갯수는 dim, nrow, ncol 로 확인한다.\n\ndim(ex)                                                    ## row, column\n\n[1] 1644   32\n\nnrow(ex)                                                   ## row\n\n[1] 1644\n\nncol(ex)                                                   ## column\n\n[1] 32\n\n\n클래스는 class로 확인한다. read.csv 는 data.frame, Excel/SAS/SPSS 는 tibble & `data.frame 인데, data.frame 은 행렬이면서 데이터에 특화된 list, tibble 은 앞서 언급했던 tidyverse 스타일의 data.frame 인 정도만 알고 넘어가자.\n\nclass(ex)\n\n[1] \"data.frame\"\n\nclass(ex.spss)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nsummary 로 모든 변수들의 평균, 중위수, 결측치 등을 한 번에 확인할 수 있다. R은 결측치를 NA 로 표시하며, 안타깝지만 분산은 나오지 않는다.\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI          HME_YYYYMM      Q_PHX_DX_STK   \n Min.   :2009   Min.   :   2270   Min.   :200901   Min.   :0.0000  \n 1st Qu.:2010   1st Qu.: 230726   1st Qu.:201011   1st Qu.:0.0000  \n Median :2012   Median : 487160   Median :201210   Median :0.0000  \n Mean   :2012   Mean   : 490782   Mean   :201216   Mean   :0.0112  \n 3rd Qu.:2014   3rd Qu.: 726101   3rd Qu.:201406   3rd Qu.:0.0000  \n Max.   :2015   Max.   :1010623   Max.   :201512   Max.   :1.0000  \n                                                   NA's   :573     \n Q_PHX_DX_HTDZ     Q_PHX_DX_HTN   Q_PHX_DX_DM      Q_PHX_DX_DLD   \n Min.   :0.0000   Min.   :0.00   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.00   Median :0.0000   Median :0.0000  \n Mean   :0.0241   Mean   :0.25   Mean   :0.0693   Mean   :0.0399  \n 3rd Qu.:0.0000   3rd Qu.:0.25   3rd Qu.:0.0000   3rd Qu.:0.0000  \n Max.   :1.0000   Max.   :1.00   Max.   :1.0000   Max.   :1.0000  \n NA's   :566      NA's   :492    NA's   :547      NA's   :566     \n  Q_PHX_DX_PTB       Q_HBV_AG        Q_SMK_YN     Q_DRK_FRQ_V09N \n Min.   :0.0000   Min.   :1.000   Min.   :1.000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:2.000   1st Qu.:1.000   1st Qu.:0.000  \n Median :0.0000   Median :2.000   Median :1.000   Median :1.000  \n Mean   :0.0276   Mean   :2.235   Mean   :1.632   Mean   :1.026  \n 3rd Qu.:0.0000   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :1.0000   Max.   :3.000   Max.   :3.000   Max.   :7.000  \n NA's   :703      NA's   :2       NA's   :2       NA's   :6      \n      HGHT            WGHT            WSTC             BMI       \n Min.   :134.0   Min.   : 31.0   Min.   : 57.00   Min.   :12.30  \n 1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00   1st Qu.:21.50  \n Median :165.0   Median : 64.0   Median : 81.00   Median :23.70  \n Mean   :164.5   Mean   : 65.1   Mean   : 80.69   Mean   :23.92  \n 3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00   3rd Qu.:26.20  \n Max.   :188.0   Max.   :118.0   Max.   :114.00   Max.   :37.20  \n                                                                 \n     VA_LT           VA_RT            BP_SYS          BP_DIA     \n Min.   :0.100   Min.   :0.1000   Min.   : 81.0   Min.   : 49.0  \n 1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0   1st Qu.: 70.0  \n Median :1.000   Median :1.0000   Median :120.0   Median : 78.0  \n Mean   :0.984   Mean   :0.9792   Mean   :122.3   Mean   : 76.6  \n 3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0   3rd Qu.: 82.0  \n Max.   :9.900   Max.   :9.9000   Max.   :180.0   Max.   :120.0  \n                                                                 \n    URN_PROT          HGB             FBS            TOT_CHOL    \n Min.   :1.000   Min.   : 5.90   Min.   : 61.00   Min.   : 68.0  \n 1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00   1st Qu.:170.0  \n Median :1.000   Median :14.10   Median : 94.00   Median :193.0  \n Mean   :1.078   Mean   :14.11   Mean   : 97.23   Mean   :194.9  \n 3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00   3rd Qu.:218.0  \n Max.   :5.000   Max.   :18.30   Max.   :290.00   Max.   :363.0  \n NA's   :4                                                       \n       TG              HDL             LDL              CRTN        \n Min.   :  13.0   Min.   : 23.0   Min.   :  19.0   Min.   : 0.4000  \n 1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0   1st Qu.: 0.8000  \n Median : 106.0   Median : 54.0   Median : 112.0   Median : 0.9000  \n Mean   : 134.9   Mean   : 55.9   Mean   : 118.7   Mean   : 0.9891  \n 3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0   3rd Qu.: 1.0000  \n Max.   :1210.0   Max.   :593.0   Max.   :8100.0   Max.   :16.5000  \n                                  NA's   :16                        \n      SGOT            SGPT             GGT              GFR        \n Min.   :  6.0   Min.   :  3.00   Min.   :  6.00   Min.   :  3.00  \n 1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00   1st Qu.: 76.00  \n Median : 23.0   Median : 20.00   Median : 24.50   Median : 87.00  \n Mean   : 25.6   Mean   : 25.98   Mean   : 36.34   Mean   : 89.74  \n 3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00   3rd Qu.:101.00  \n Max.   :459.0   Max.   :779.00   Max.   :408.00   Max.   :196.00  \n                                                   NA's   :467     \n\n\n특정 변수 보기\ndata.frame 에서 특정변수는 $ 를 이용, 데이터이름$변수이름 로 확인할 수 있다. 앞서 언급했듯이 data.frame 은 행렬과 list의 성질도 갖고 있어 해당 스타일로도 가능하다.\n\nex$EXMD_BZ_YYYY                                            ## data.frame style\nex[, \"EXMD_BZ_YYYY\"]                                       ## matrix style\nex[[\"EXMD_BZ_YYYY\"]]                                       ## list style\nex[, 1]                                                    ## matrix style with order\nex[[1]]                                                    ## list style with order\n\n2개 이상 변수선택은 행렬 스타일을 이용한다.\n\nex[, c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"BMI\")]                  ## matrix syle with names\nex[, c(1, 2, 16)]                                          ## matrix syle with names\nex[, names(ex)[c(1, 2, 16)]]                               ## same\n\n\n\n\n\n  \n\n\n\n특정 변수는 벡터형태로 나타나므로 처음에 다루었던 벡터다루기를 그대로 활용할 수 있다. 예를 들어 년도 변수인 EXMD_BZ_YYYY의 첫 50개만 확인하면 아래와 같다.\n\nex$EXMD_BZ_YYYY[1:50]                                      ## data.frame style\nex[1:50, 1]                                                ## matrix style\nex[[1]][1:50]                                              ## list style\n\n\n\n [1] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[16] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[31] 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009 2009\n[46] 2009 2009 2009 2009 2009\n\n\nunique 로 변수가 어떤 값들로 이루어져 있는지, table 로 해당 값들이 몇개씩 있는지 확인한다.\n\nunique(ex$EXMD_BZ_YYYY)                                   ## unique value\n\n[1] 2009 2010 2011 2012 2013 2014 2015\n\nlength(unique(ex$EXMD_BZ_YYYY))                           ## number of unique value\n\n[1] 7\n\ntable(ex$EXMD_BZ_YYYY)                                    ## table\n\n\n2009 2010 2011 2012 2013 2014 2015 \n 214  236  223  234  243  254  240 \n\n\n새로운 변수 만들기\n연속형 변수인 BMI 에서 원하는 조건에 맞는 정보를 뽑아내는 연습을 해 보자.\n\nmean(ex$BMI)                                              ## mean\n\n[1] 23.92257\n\nBMI_cat <- (ex$BMI >= 25)                                 ## TRUE of FALSE\ntable(BMI_cat)                         \n\nBMI_cat\nFALSE  TRUE \n 1077   567 \n\nrows <- which(ex$BMI >= 25)                               ## row numbers\nhead(rows)                                      \n\n[1]  1 14 18 21 23 24\n\nvalues <- ex$BMI[ex$BMI >= 25]                            ## values\nhead(values)\n\n[1] 29.4 27.5 27.7 28.0 30.7 25.6\n\nlength(values)\n\n[1] 567\n\nBMI_HGHT_and <- (ex$BMI >= 25 & ex$HGHT >= 175)              ## and\nBMI_HGHT_or <- (ex$BMI >= 25 | ex$HGHT >= 175)               ## or\n\n데이터에 새로운 변수로 추가하는 방법은 간단하다.\n\nex$zero <- 0                                              ## variable with 0\nex$BMI_cat <- (ex$BMI >= 25)                              ## T/F\nex$BMI_cat <- as.integer(ex$BMI >= 25)                    ## 0, 1\nex$BMI_cat <- as.character(ex$BMI >= 25)                  ## \"0\", \"1\"\nex$BMI_cat <- ifelse(ex$BMI >= 25, \"1\", \"0\")              ## same\ntable(ex$BMI_cat)\n\n\n   0    1 \n1077  567 \n\nex[, \"BMI_cat\"] <- (ex$BMI >= 25)                         ## matrix style\nex[[\"BMI_cat\"]] <- (ex$BMI >= 25)                         ## list style\n\n변수 클래스 설정: 데이터 읽은 후 가장 먼저 해야할 것.\n앞서 데이터의 클래스가 data.frame 임을 언급했었는데, 각 변수들도 자신의 클래스를 갖으며 대표적인 것이 숫자형(numeric), 문자형(character), 팩터(factor) 이다. 그 외 T/F 로 나타내는 논리(logical), 날짜를 나타내는 Date 클래스가 있다. 숫자는 integer(정수), numeric(실수) 이 있는데, 전부 실수형(numeric)으로 해도 상관없어 설명은 생략한다. 범주형은 character 와 factor 두 종류가 있는데, 전자는 단순 문자인 반면 후자는 레벨(level) 이 있어 reference 나 순서를 설정할 수 있다. read.csv 로 읽으면 숫자는 int/num, 문자는 전부 factor 가 기본값이므로, 숫자 변수 중 0/1 같은 것들은 직접 factor 로 바꿔줘야 한다. ID와 설문조사 변수를 범주형으로 바꿔보자.\n\nvars.cat <- c(\"RN_INDI\", \"Q_PHX_DX_STK\", \"Q_PHX_DX_HTDZ\", \"Q_PHX_DX_HTN\", \"Q_PHX_DX_DM\", \"Q_PHX_DX_DLD\", \"Q_PHX_DX_PTB\", \n              \"Q_HBV_AG\", \"Q_SMK_YN\", \"Q_DRK_FRQ_V09N\")\nvars.cat <- names(ex)[c(2, 4:12)]                              ## same\nvars.cat <- c(\"RN_INDI\", grep(\"Q_\", names(ex), value = T))     ## same: extract variables starting with \"Q_\"\n\nvars.conti <- setdiff(names(ex), vars.cat)                     ## Exclude categorical variables\nvars.conti <- names(ex)[!(names(ex) %in% vars.cat)]            ## same: !- not, %in%- including\n\nfor (vn in vars.cat){                                          ## for loop: as.factor\n  ex[, vn] <- as.factor(ex[, vn])\n}\n\nfor (vn in vars.conti){                                        ## for loop: as.numeric\n  ex[, vn] <- as.numeric(ex[, vn])\n}\n\nsummary(ex)\n\n  EXMD_BZ_YYYY     RN_INDI       HME_YYYYMM     Q_PHX_DX_STK Q_PHX_DX_HTDZ\n Min.   :2009   4263   :   7   Min.   :200901   0   :1059    0   :1052    \n 1st Qu.:2010   38967  :   7   1st Qu.:201011   1   :  12    1   :  26    \n Median :2012   56250  :   7   Median :201210   NA's: 573    NA's: 566    \n Mean   :2012   84322  :   7   Mean   :201216                             \n 3rd Qu.:2014   99917  :   7   3rd Qu.:201406                             \n Max.   :2015   115809 :   7   Max.   :201512                             \n                (Other):1602                                              \n Q_PHX_DX_HTN Q_PHX_DX_DM Q_PHX_DX_DLD Q_PHX_DX_PTB Q_HBV_AG    Q_SMK_YN  \n 0   :864     0   :1021   0   :1035    0   :915     1   :  77   1   :995  \n 1   :288     1   :  76   1   :  43    1   : 26     2   :1102   2   :256  \n NA's:492     NA's: 547   NA's: 566    NA's:703     3   : 463   3   :391  \n                                                    NA's:   2   NA's:  2  \n                                                                          \n                                                                          \n                                                                          \n Q_DRK_FRQ_V09N      HGHT            WGHT            WSTC       \n 0      :805    Min.   :134.0   Min.   : 31.0   Min.   : 57.00  \n 1      :379    1st Qu.:158.0   1st Qu.: 56.0   1st Qu.: 74.00  \n 2      :249    Median :165.0   Median : 64.0   Median : 81.00  \n 3      :121    Mean   :164.5   Mean   : 65.1   Mean   : 80.69  \n 4      : 28    3rd Qu.:171.0   3rd Qu.: 73.0   3rd Qu.: 87.00  \n (Other): 56    Max.   :188.0   Max.   :118.0   Max.   :114.00  \n NA's   :  6                                                    \n      BMI            VA_LT           VA_RT            BP_SYS     \n Min.   :12.30   Min.   :0.100   Min.   :0.1000   Min.   : 81.0  \n 1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000   1st Qu.:110.0  \n Median :23.70   Median :1.000   Median :1.0000   Median :120.0  \n Mean   :23.92   Mean   :0.984   Mean   :0.9792   Mean   :122.3  \n 3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000   3rd Qu.:130.0  \n Max.   :37.20   Max.   :9.900   Max.   :9.9000   Max.   :180.0  \n                                                                 \n     BP_DIA         URN_PROT          HGB             FBS        \n Min.   : 49.0   Min.   :1.000   Min.   : 5.90   Min.   : 61.00  \n 1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90   1st Qu.: 86.00  \n Median : 78.0   Median :1.000   Median :14.10   Median : 94.00  \n Mean   : 76.6   Mean   :1.078   Mean   :14.11   Mean   : 97.23  \n 3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40   3rd Qu.:103.00  \n Max.   :120.0   Max.   :5.000   Max.   :18.30   Max.   :290.00  \n                 NA's   :4                                       \n    TOT_CHOL           TG              HDL             LDL        \n Min.   : 68.0   Min.   :  13.0   Min.   : 23.0   Min.   :  19.0  \n 1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0   1st Qu.:  90.0  \n Median :193.0   Median : 106.0   Median : 54.0   Median : 112.0  \n Mean   :194.9   Mean   : 134.9   Mean   : 55.9   Mean   : 118.7  \n 3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0   3rd Qu.: 134.0  \n Max.   :363.0   Max.   :1210.0   Max.   :593.0   Max.   :8100.0  \n                                                  NA's   :16      \n      CRTN              SGOT            SGPT             GGT        \n Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00   Min.   :  6.00  \n 1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00   1st Qu.: 16.00  \n Median : 0.9000   Median : 23.0   Median : 20.00   Median : 24.50  \n Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98   Mean   : 36.34  \n 3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00   3rd Qu.: 41.00  \n Max.   :16.5000   Max.   :459.0   Max.   :779.00   Max.   :408.00  \n                                                                    \n      GFR              zero      BMI_cat      \n Min.   :  3.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 76.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 87.00   Median :0   Median :0.0000  \n Mean   : 89.74   Mean   :0   Mean   :0.3449  \n 3rd Qu.:101.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :196.00   Max.   :0   Max.   :1.0000  \n NA's   :467                                  \n\n\nsummary 를 보면 설문조사 변수들이 처음과 달리 빈도로 요약됨을 알 수 있다. 한 가지 주의할 점은 factor 를 numeric 으로 바로 바꾸면 안된다는 것이다. 방금 factor 로 바꾼 Q_PHX_DX_STK 를 numeric 으로 바꿔서 테이블로 요약하면, 원래의 0/1 이 아닌 1/2로 바뀐다.\n\ntable(\n  as.numeric(ex$Q_PHX_DX_STK)\n  )\n\n\n   1    2 \n1059   12 \n\n\nfactor를 바로 바꾸면 원래 값이 아닌, factor에 내장된 레벨(순서값) 로 바뀌기 때문이다. 제대로 바꾸려면 아래처럼 character 로 먼저 바꿔준 후 숫자형을 적용해야 한다.\n\ntable(\n  as.numeric(as.character(ex$Q_PHX_DX_STK))\n  )\n\n\n   0    1 \n1059   12 \n\n\n마지막으로 Date 클래스를 살펴보자. 검진년월 변수인 HME_YYYYMM 를 Date 로 바꿔 볼텐데, Date는 년/월/일 이 모두 필요하므로 일은 1로 통일하고 paste 로 붙이겠다.\n\naddDate <- paste(ex$HME_YYYYMM, \"01\", sep = \"\")                ## add day- use `paste`\nex$HME_YYYYMM <- as.Date(addDate, format = \"%Y%m%d\")           ## set format                  \nhead(ex$HME_YYYYMM)\n\n[1] \"2009-09-01\" \"2009-11-01\" \"2009-03-01\" \"2009-08-01\" \"2009-09-01\"\n[6] \"2009-12-01\"\n\nclass(ex$HME_YYYYMM)\n\n[1] \"Date\"\n\n\n결측치 다루기\n변수 클래스만큼 중요한 것이 결측치 처리이다. 앞서 “함수만들기” 에서 봤듯이 결측치가 있으면 평균같은 기본적인 계산도 na.rm = T 옵션이 필요하다. 결측치가 있는 LDL 변수의 평균을 연도별로 구해보자. 그룹별 통계는 tapply 를 이용한다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, mean)                          ## measure/group/function\n\n\n\n\n\n2009\n2010\n2011\n2012\n2013\n2014\n2015\n\n\n150.9486\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n2009년만 결측치가 없고, 나머지는 결측치가 있어 평균값이 NA 로 나온다.na.rm = T 옵션으로 결측치를 제외하면 원하는 결과를 얻는다.\n\ntapply(ex$LDL, ex$EXMD_BZ_YYYY, \n       function(x){\n         mean(x, na.rm = T)\n         })    \n\n    2009     2010     2011     2012     2013     2014     2015 \n150.9486 112.9914 112.9450 117.5259 111.1577 116.5455 111.5294 \n\n\n더 큰 문제는, 대부분의 R 통계분석이 결측치를 갖는 샘플을 분석에서 제외한다는 점이다. 그래서 결측치를 신경쓰지 않고 분석하다보면, 원래 샘플 수와 분석에 이용된 샘플 수가 달라지는 문제가 생길 수 있다. LDL과 HDL 의 회귀분석 결과를 예로 살펴보자.\n\nsummary(lm(LDL ~ HDL, data = ex))\n\n\nCall:\nlm(formula = LDL ~ HDL, data = ex)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-103.8  -28.2   -6.6   15.4 7974.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 138.2747    15.2318   9.078   <2e-16 ***\nHDL          -0.3499     0.2570  -1.362    0.174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 201.9 on 1626 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.001139,  Adjusted R-squared:  0.0005244 \nF-statistic: 1.854 on 1 and 1626 DF,  p-value: 0.1735\n\n\n“16 observations deleted due to missingness” 라는 글자가 보일 것이다. LDL 이 결측인 16명은 분석에서 제외했다는 뜻이다.\n연습문제 2: 결측치 처리\n결측치를 처리하는 제일 간단한 방법은 “하나라도 결측치 있는 샘플은 제외” 로, na.omit 함수를 이용하면 된다.\n\nex.naomit <- na.omit(ex)\nnrow(ex.naomit)\n\n[1] 620\n\n\n1644 명에서 620 명으로 샘플 수가 줄어든 것을 확인할 수 있다. 필자는 보통 결측치 처리에 다음의 3가지 원칙을 적용한다.\n\n결측치 너무 많으면(예: 10% 이상) 그 변수는 삭제\n연속변수는 중간값(median)\n범주형변수는 최빈값(mode)\n\n이제 문제이다. 아까 변수형을 정리한 ex 데이터에 위 3가지 원칙을 적용, 새로운 데이터 ex.impute 을 만들어 보아라. 단 최빈값 함수는 아래와 같이 getmode 로 주어진다.\n\ngetmode <- function(v){\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ngetmode(ex$Q_PHX_DX_STK)\n\n[1] 0\nLevels: 0 1\n\n\n\n정답 보기\n\nvars.ok <- sapply(names(ex), function(v){sum(is.na(ex[, v])) < nrow(ex)/10})\nex.impute <- ex[, vars.ok]                                     ## only missing < 10%\n\nfor (v in names(ex.impute)){\n  if (is.factor(ex.impute[, v])){                              ## or class(ex[, v]) == \"factor\"\n    ex.impute[, v] <- ifelse(is.na(ex.impute[, v]), \n                             getmode(ex.impute[, v]), \n                             ex.impute[, v])\n  } else if (is.numeric(ex[, v])){                             ## or class(ex[, v]) %in% c(\"integer\", \"numeric\")\n    ex.impute[, v] <- ifelse(is.na(ex.impute[, v]), \n                             median(ex.impute[, v], na.rm = T), \n                             ex.impute[, v])\n  } else{                                                      ## when date\n    ex.impute[, v]\n  }\n}\n\nsummary(ex.impute)\n\n  EXMD_BZ_YYYY     RN_INDI        HME_YYYYMM            Q_HBV_AG    \n Min.   :2009   Min.   :  1.0   Min.   :2009-01-01   Min.   :1.000  \n 1st Qu.:2010   1st Qu.:133.8   1st Qu.:2010-11-01   1st Qu.:2.000  \n Median :2012   Median :275.0   Median :2012-10-01   Median :2.000  \n Mean   :2012   Mean   :272.7   Mean   :2012-08-31   Mean   :2.235  \n 3rd Qu.:2014   3rd Qu.:405.2   3rd Qu.:2014-06-01   3rd Qu.:3.000  \n Max.   :2015   Max.   :547.0   Max.   :2015-12-01   Max.   :3.000  \n    Q_SMK_YN     Q_DRK_FRQ_V09N       HGHT            WGHT      \n Min.   :1.000   Min.   :1.000   Min.   :134.0   Min.   : 31.0  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:158.0   1st Qu.: 56.0  \n Median :1.000   Median :2.000   Median :165.0   Median : 64.0  \n Mean   :1.631   Mean   :2.023   Mean   :164.5   Mean   : 65.1  \n 3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:171.0   3rd Qu.: 73.0  \n Max.   :3.000   Max.   :8.000   Max.   :188.0   Max.   :118.0  \n      WSTC             BMI            VA_LT           VA_RT       \n Min.   : 57.00   Min.   :12.30   Min.   :0.100   Min.   :0.1000  \n 1st Qu.: 74.00   1st Qu.:21.50   1st Qu.:0.800   1st Qu.:0.7000  \n Median : 81.00   Median :23.70   Median :1.000   Median :1.0000  \n Mean   : 80.69   Mean   :23.92   Mean   :0.984   Mean   :0.9792  \n 3rd Qu.: 87.00   3rd Qu.:26.20   3rd Qu.:1.200   3rd Qu.:1.2000  \n Max.   :114.00   Max.   :37.20   Max.   :9.900   Max.   :9.9000  \n     BP_SYS          BP_DIA         URN_PROT          HGB       \n Min.   : 81.0   Min.   : 49.0   Min.   :1.000   Min.   : 5.90  \n 1st Qu.:110.0   1st Qu.: 70.0   1st Qu.:1.000   1st Qu.:12.90  \n Median :120.0   Median : 78.0   Median :1.000   Median :14.10  \n Mean   :122.3   Mean   : 76.6   Mean   :1.078   Mean   :14.11  \n 3rd Qu.:130.0   3rd Qu.: 82.0   3rd Qu.:1.000   3rd Qu.:15.40  \n Max.   :180.0   Max.   :120.0   Max.   :5.000   Max.   :18.30  \n      FBS            TOT_CHOL           TG              HDL       \n Min.   : 61.00   Min.   : 68.0   Min.   :  13.0   Min.   : 23.0  \n 1st Qu.: 86.00   1st Qu.:170.0   1st Qu.:  72.0   1st Qu.: 46.0  \n Median : 94.00   Median :193.0   Median : 106.0   Median : 54.0  \n Mean   : 97.23   Mean   :194.9   Mean   : 134.9   Mean   : 55.9  \n 3rd Qu.:103.00   3rd Qu.:218.0   3rd Qu.: 163.0   3rd Qu.: 64.0  \n Max.   :290.00   Max.   :363.0   Max.   :1210.0   Max.   :593.0  \n      LDL              CRTN              SGOT            SGPT       \n Min.   :  19.0   Min.   : 0.4000   Min.   :  6.0   Min.   :  3.00  \n 1st Qu.:  90.0   1st Qu.: 0.8000   1st Qu.: 19.0   1st Qu.: 15.00  \n Median : 112.0   Median : 0.9000   Median : 23.0   Median : 20.00  \n Mean   : 118.6   Mean   : 0.9891   Mean   : 25.6   Mean   : 25.98  \n 3rd Qu.: 134.0   3rd Qu.: 1.0000   3rd Qu.: 28.0   3rd Qu.: 30.00  \n Max.   :8100.0   Max.   :16.5000   Max.   :459.0   Max.   :779.00  \n      GGT              zero      BMI_cat      \n Min.   :  6.00   Min.   :0   Min.   :0.0000  \n 1st Qu.: 16.00   1st Qu.:0   1st Qu.:0.0000  \n Median : 24.50   Median :0   Median :0.0000  \n Mean   : 36.34   Mean   :0   Mean   :0.3449  \n 3rd Qu.: 41.00   3rd Qu.:0   3rd Qu.:1.0000  \n Max.   :408.00   Max.   :0   Max.   :1.0000  \n\n\n\nSubset\n특정 조건을 만족하는 서브데이터는 지금까지 배웠던 것을 응용해 만들 수도 있지만, subset 함수가 편하다. 아래는 2012 이후의 자료만 뽑는 예시이다. 이제부터는 결측치를 전부 제외한 ex.naomit 데이터를 이용하겠다.\n\nex1 <- ex.naomit                                               ## simple name\nex1.2012 <- ex1[ex1$EXMD_BZ_YYYY >= 2012, ]\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\nex1.2012 <- subset(ex1, EXMD_BZ_YYYY >= 2012)                  ## subset\ntable(ex1.2012$EXMD_BZ_YYYY)\n\n\n2012 2013 2014 2015 \n 151  162  154  153 \n\n\n그룹별 통계\n결측치 다루기에서 그룹별 통계를 구할 때 tapply 를 이용했었다. tapply 를 여러 변수, 여러 그룹을 동시에 고려도록 확장할 수 있는 함수가 aggregate 로, 허리둘레와 BMI의 평균을 고혈압 또는 당뇨 여부에 따라 살펴보자.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN), mean)\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex1, mean)   ## same\n\n\n\n\n\nGroup.1\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.35687\n23.85592\n\n\n1\n84.48958\n25.11771\n\n\n\n\n\n결측치가 있어도 잘 적용된다는 장점이 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN, data = ex, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nWSTC\nBMI\n\n\n\n0\n80.23958\n23.70961\n\n\n1\n83.87847\n24.99861\n\n\n\n\n\n당뇨여부도 그룹으로 다루려면 list 에 추가하면 된다.\n\naggregate(ex1[, c(\"WSTC\", \"BMI\")], list(ex1$Q_PHX_DX_HTN, ex1$Q_PHX_DX_DM), mean)\n\n\n\n\n\nGroup.1\nGroup.2\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\nGroup.1 이 첫번째 그룹은 고혈압 여부, Group.2 가 두번째 그룹인 당뇨 여부이다. 위와 마찬가지로 formula 형태를 이용할 수도 있다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, mean)\n\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.23107\n23.82990\n\n\n1\n0\n83.93976\n25.17952\n\n\n0\n1\n87.55556\n25.34444\n\n\n1\n1\n88.00000\n24.72308\n\n\n\n\n\n표준편차를 같이 보려면 function(x){c(mean = mean(x), sd = sd(x))} 와 같이 원하는 함수들을 벡터로 모으면 된다.\n\naggregate(cbind(WSTC, BMI) ~ Q_PHX_DX_HTN + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})\n\n\n\nWarning in `[<-.data.frame`(`*tmp*`, , isn, value = structure(list(WSTC.mean =\nc(\"80.231068\", : provided 4 variables to replace 2 variables\n\n\n\n\nQ_PHX_DX_HTN\nQ_PHX_DX_DM\nWSTC\nBMI\n\n\n\n0\n0\n80.231068\n9.546884\n\n\n1\n0\n83.939759\n9.124277\n\n\n0\n1\n87.555556\n7.551674\n\n\n1\n1\n88.000000\n6.177918\n\n\n\n\n\n아예 데이터의 모든 변수의 평균을 다 볼순 없을까? 아래처럼 “.” 으로 전체 데이터를 지정할 수 있다.\n\naggregate(. ~ Q_PHX_DX_HTN  + Q_PHX_DX_DM, data = ex1, function(x){c(mean = mean(x), sd = sd(x))})    \n\n  Q_PHX_DX_HTN Q_PHX_DX_DM EXMD_BZ_YYYY.mean EXMD_BZ_YYYY.sd RN_INDI.mean\n1            0           0       2013.493204        1.109498    269.30680\n2            1           0       2013.578313        1.105645    251.78313\n3            0           1       2013.333333        1.414214    269.77778\n4            1           1       2013.307692        1.031553    303.53846\n  RN_INDI.sd HME_YYYYMM.mean HME_YYYYMM.sd Q_PHX_DX_STK.mean Q_PHX_DX_STK.sd\n1  159.12594      16102.3184      422.8574        1.00776699      0.08787296\n2  154.03951      16121.8072      413.1641        1.01204819      0.10976426\n3   92.88807      16036.3333      551.2248        1.00000000      0.00000000\n4  142.18686      16018.6923      417.4666        1.07692308      0.27735010\n  Q_PHX_DX_HTDZ.mean Q_PHX_DX_HTDZ.sd Q_PHX_DX_DLD.mean Q_PHX_DX_DLD.sd\n1         1.00194175       0.04406526         1.0174757       0.1311630\n2         1.06024096       0.23937916         1.0722892       0.2605404\n3         1.00000000       0.00000000         1.0000000       0.0000000\n4         1.07692308       0.27735010         1.0769231       0.2773501\n  Q_PHX_DX_PTB.mean Q_PHX_DX_PTB.sd Q_HBV_AG.mean Q_HBV_AG.sd Q_SMK_YN.mean\n1         1.0271845       0.1627787     2.2291262   0.5236863     1.6970874\n2         1.0000000       0.0000000     2.1927711   0.5512255     1.3855422\n3         1.0000000       0.0000000     2.0000000   0.0000000     1.6666667\n4         1.0769231       0.2773501     2.2307692   0.4385290     1.5384615\n  Q_SMK_YN.sd Q_DRK_FRQ_V09N.mean Q_DRK_FRQ_V09N.sd  HGHT.mean    HGHT.sd\n1   0.8674234           2.0388350         1.3329287 166.613592   9.116636\n2   0.6777172           1.9759036         1.3612754 160.506024   9.254364\n3   0.8660254           1.8888889         0.3333333 168.333333  10.185774\n4   0.6602253           1.9230769         1.1151636 162.384615   9.639662\n  WGHT.mean   WGHT.sd WSTC.mean   WSTC.sd  BMI.mean    BMI.sd VA_LT.mean\n1 66.582524 13.211630 80.231068  9.546884 23.829903  3.276315  1.0190291\n2 65.313253 13.155661 83.939759  9.124277 25.179518  3.693922  0.8469880\n3 71.777778  8.913161 87.555556  7.551674 25.344444  2.711140  0.9111111\n4 65.076923  6.211032 88.000000  6.177918 24.723077  2.057164  0.7769231\n   VA_LT.sd VA_RT.mean  VA_RT.sd BP_SYS.mean  BP_SYS.sd BP_DIA.mean BP_DIA.sd\n1 0.5248189  1.0079612 0.3503677  119.889320  13.378266   75.452427  9.464616\n2 0.3201895  0.8638554 0.3444962  132.879518  14.344539   81.481928 11.015910\n3 0.1691482  0.8111111 0.2368778  128.555556   8.647415   83.333333 11.842719\n4 0.2350668  0.9000000 0.1080123  129.461538  12.149180   79.307692  7.846280\n  URN_PROT.mean URN_PROT.sd   HGB.mean     HGB.sd  FBS.mean    FBS.sd\n1     1.0543689   0.3430173 14.3749515  1.5952305  94.75534  12.71807\n2     1.2168675   0.6634756 14.1048193  1.6682036 103.60241  14.34330\n3     1.0000000   0.0000000 15.2555556  1.0284832 131.11111  19.62425\n4     1.0769231   0.2773501 13.4153846  0.9711215 125.30769  34.11838\n  TOT_CHOL.mean TOT_CHOL.sd   TG.mean     TG.sd  HDL.mean    HDL.sd  LDL.mean\n1     196.96505    34.20684 132.80777 107.56421 54.943689 12.881333 118.49903\n2     191.30120    32.64769 138.09639  81.93106 55.903614 16.123468 108.22892\n3     169.77778    47.79325 164.66667  68.40870 46.333333  9.394147  92.77778\n4     179.61538    39.92397 154.76923 139.23072 48.769231 10.288779 102.30769\n     LDL.sd CRTN.mean   CRTN.sd SGOT.mean   SGOT.sd SGPT.mean   SGPT.sd\n1  50.86475 0.8871845 0.1867988 24.151456  9.426161 24.609709 16.616090\n2  29.09167 0.9168675 0.2483208 25.289157  6.400324 22.963855  9.671993\n3  38.88694 0.9666667 0.2549510 35.777778 25.849457 47.666667 44.235167\n4  28.39420 0.9153846 0.1675617 31.769231 19.689904 36.307692 27.417709\n  GGT.mean   GGT.sd GFR.mean   GFR.sd zero.mean zero.sd BMI_cat.mean BMI_cat.sd\n1 34.47573 31.35216 92.01359 19.14246         0       0    0.3223301  0.4678230\n2 35.77108 31.18799 81.16867 17.75430         0       0    0.4337349  0.4986022\n3 44.22222 21.01058 87.55556 22.20423         0       0    0.5555556  0.5270463\n4 48.46154 66.83265 80.69231 14.34332         0       0    0.1538462  0.3755338\n\n\nSort\n정렬은 순위함수인 order 를 이용한다. 기본은 오름차순이며, 내림차순을 원한다면 (-) 붙인 값의 순위를 구하면 된다.\n\nord <- order(ex1$HGHT)                                        ## 작은 순서대로 순위\nhead(ord)\n\n[1] 500 168   3 328 473 177\n\nhead(ex1$HGHT[ord])                                           ## Sort\n\n[1] 138 139 140 140 141 143\n\nord.desc <- order(-ex1$HGHT)                                  ## descending\nhead(ex1$HGHT[ord.desc])\n\n[1] 188 186 185 185 184 183\n\n\n\nex1.sort <- ex1[ord, ]\nhead(ex1.sort)\n\n\n\n\n\n  \n\n\n\nWide to long, long to wide format\n받은 데이터가 원하는 형태가 아닌 경우가 있다. 수축기 혈압을 10번 측정해서 각각 SBP1, SBP2, …, SBP10 변수에 기록된 데이터를 본다면, 이것들을 쫙 아래로 내려 측정시기, 측정값 2개의 변수로 정리하고 싶다는 마음이 들 것이다. 이럴 때 쓰는 함수가 melt, 반대로 데이터를 옆으로 늘릴 때 쓰는 함수가 dcast 이다(Figure @ref(fig:melt)3).\n\n\n\n\nmelt and dcast\n\n\n\n\n실습으로 수축기/이완기 혈압 변수를 합쳐서 아래로 내려보자.\n\nlibrary(reshape2)\nlong <- melt(ex1, id = c(\"EXMD_BZ_YYYY\", \"RN_INDI\"), measure.vars = c(\"BP_SYS\", \"BP_DIA\"), variable.name = \"BP_type\", value.name = \"BP\")\nlong\n\n\n\n\n\n  \n\n\n\nid 는 유지할 변수, measure.vars 는 내릴 변수를 의미하고, variable.name, value.name 은 각각 그룹, 값의 변수이름을 의미한다. 이를 원래대로 되돌리려면 dcast 를 이용하는데, “유지할 변수 ~ 펼칠 변수” 형태로 formula 를 입력한다.\n\nwide <- dcast(long, EXMD_BZ_YYYY + RN_INDI ~ BP_type, value.var = \"BP\")\nhead(wide)\n\n\n\n\n\n  \n\n\n\nMerge\nmerge 함수를 이용한다. “by” 옵션으로 기준이 되는 공통 컬럼을 설정하며, 기준 컬럼의 이름이 두 데이터 셋에서 다른 경우는 “by.x” 와 “by.y” 로 따로 설정한다. 실습을 위해 ex1 데이터를 2개로 나눈 후 merge 를 적용하겠다.\n\nex1.Q <- ex1[, c(1:3, 4:12)]\nex1.measure <- ex1[, c(1:3, 13:ncol(ex1))]\nhead(ex1.Q)\nhead(ex1.measure)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n전자는 설문조사 결과를, 후자는 측정값을 포함했고 “년도, ID, 검진년월” 은 공통변수이다. 이 공통변수로 merge 를 적용하면\n\nex1.merge <- merge(ex1.Q, ex1.measure, by = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all = T)\nhead(ex1.merge)\n\n\n\n\n\n  \n\n\n\n합쳐진 원래 데이터를 얻을 수 있다. all = T 는 한 쪽에만 있는 샘플을 유지하는 옵션이며 빈 변수는 NA 로 채워진다. 공통인 샘플만 취하려면 all = F 로 바꾸자."
  },
  {
    "objectID": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "href": "posts/2020-02-16-rdatamanagement-basic/index.html#마치며",
    "title": "R 데이터 매니지먼트: 기초",
    "section": "마치며",
    "text": "마치며\n이번 강의를 정리하자.\n\nRStudio cloud 로 클라우드 환경에서 실습을 진행했으며\n기초 벡터연산과 for, if, ifelse, 함수만들기, apply 문을 통해 기본 문법을 익혔고\n\n공단 검진 데이터를 실습자료를 읽어와 데이터를 살펴보는 법을 배웠다.\n\n변수 생성, 클래스 설정, 결측치 처리, 서브데이터, 그룹별 통계, 정렬\n\n\n마지막으로 Long/wide type 데이터 변환과 merge 를 다루었다.\n\n기타 기본적으로 알아야 할 R 명령어는 아래의 Base R Cheat Sheet 에서 확인할 수 있다.\n 다음 강의에서는 쉬운 문법으로 R 의 대세가 된 tidyverse 를 다룰 예정인데, 오늘 배운 기본 문법과 많은 비교가 될 것이다. 미리 알아보고 싶은 분은 본 블로그의 이전 글4 을 참고하기 바란다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html",
    "href": "posts/2022-02-07-gtsummary/index.html",
    "title": "gtsummary 패키지 소개",
    "section": "",
    "text": "본 자료에서는 데이터 셋의 변수를 하나의 테이블로 요약하는 방법에 대해 알아볼 것이다. gtsummary 패키지를 이용하면 효율적으로 논문에 들어갈 table1을 만들 수 있다. gtsummary 패키지에 관한 기본 개념 및 함수들을 예제를 통해 다루어 보자."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#setup",
    "href": "posts/2022-02-07-gtsummary/index.html#setup",
    "title": "gtsummary 패키지 소개",
    "section": "Setup",
    "text": "Setup\n\n## Setup\n\n# install.packages(\"tidyverse\")\n# install.packages(\"data.table\")\n# install.packages(\"gtsummary\")\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(gtsummary)"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#road-file",
    "href": "posts/2022-02-07-gtsummary/index.html#road-file",
    "title": "gtsummary 패키지 소개",
    "section": "Road file",
    "text": "Road file\n예제에 사용할 데이터를 fread함수를 통해 불러오자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32 명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\n\n## Load file\n\nurl <- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndt <- fread(url, header=T)\ndt"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "href": "posts/2022-02-07-gtsummary/index.html#tbl_summary",
    "title": "gtsummary 패키지 소개",
    "section": "tbl_summary",
    "text": "tbl_summary\ntbl_summary 함수를 사용하여 기본 테이블을 작성할 수 있다. 출력값으로 데이터 셋의 각 열에 대한 기술 통계량을 반환한다. 데이터 셋에 섞인 범주형 변수와 연속형 변수를 자동적으로 인식해 그에 맞는 값을 반환하며, 범주형 변수의 기본 출력값은 n(%)이고 연속형 변수의 기본 출력값은 median(IQR)이다. 결측값은 테이블에 Unknown으로 출력된다.\nfread를 통해 불러온 데이터에서 몇 개의 변수를 추출해 요약 테이블을 만들어보자.\n\n# select variables\n\ndt2 <- dt %>% select(\"EXMD_BZ_YYYY\", \"Q_PHX_DX_STK\", \"Q_SMK_YN\",\n                     \"HGHT\", \"WGHT\" ,\"TOT_CHOL\", \"TG\")\ndt2\n\n\n\n\n\n  \n\n\n\ntbl_summary 함수를 통해 간단한 요약 테이블을 만들어보자.\n\n# create table\n\ndt2 %>% tbl_summary()\n\n\n\n\n\n\nCharacteristic\n      \nN = 1,6441\n\n    \n\n\nEXMD_BZ_YYYY\n\n\n\n2009\n214 (13%)\n\n\n2010\n236 (14%)\n\n\n2011\n223 (14%)\n\n\n2012\n234 (14%)\n\n\n2013\n243 (15%)\n\n\n2014\n254 (15%)\n\n\n2015\n240 (15%)\n\n\nQ_PHX_DX_STK\n12 (1.1%)\n\n\nUnknown\n573\n\n\nQ_SMK_YN\n\n\n\n1\n995 (61%)\n\n\n2\n256 (16%)\n\n\n3\n391 (24%)\n\n\nUnknown\n2\n\n\nHGHT\n165 (158, 171)\n\n\nWGHT\n64 (56, 73)\n\n\nTOT_CHOL\n193 (170, 218)\n\n\nTG\n106 (72, 163)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\n\n변수 유형이 자동으로 구분되어 연속형 변수는 median(IQR), 범주형 변수는 n(%)의 형태로 출력된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "href": "posts/2022-02-07-gtsummary/index.html#그룹별-통계",
    "title": "gtsummary 패키지 소개",
    "section": "그룹별 통계",
    "text": "그룹별 통계\nby\ntbl_summary 함수에는 다양한 옵션이 존재한다. by 옵션을 이용하여 그룹별 통계량을 계산할 수 있다. by 옵션에 그룹별 통계를 수행할 변수를 지정하여 사용 가능하다. 다음 예시에서 연도 변수인 EXMD_BZ_YYYY를 기준으로 그룹별 통계량을 출력해보자.\n\ndt2 %>% tbl_summary(by = EXMD_BZ_YYYY)\n\n\n\n\n\n\nCharacteristic\n      \n2009, N = 2141\n\n      \n2010, N = 2361\n\n      \n2011, N = 2231\n\n      \n2012, N = 2341\n\n      \n2013, N = 2431\n\n      \n2014, N = 2541\n\n      \n2015, N = 2401\n\n    \n\n\nQ_PHX_DX_STK\n2 (1.5%)\n2 (1.2%)\n1 (0.7%)\n2 (1.3%)\n2 (1.2%)\n2 (1.3%)\n1 (0.6%)\n\n\nUnknown\n82\n74\n85\n78\n75\n95\n84\n\n\nQ_SMK_YN\n\n\n\n\n\n\n\n\n\n1\n125 (59%)\n132 (56%)\n140 (63%)\n146 (62%)\n141 (58%)\n157 (62%)\n154 (64%)\n\n\n2\n34 (16%)\n42 (18%)\n35 (16%)\n36 (15%)\n35 (14%)\n38 (15%)\n36 (15%)\n\n\n3\n53 (25%)\n62 (26%)\n48 (22%)\n52 (22%)\n67 (28%)\n59 (23%)\n50 (21%)\n\n\nUnknown\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n165 (159, 171)\n165 (159, 171)\n165 (157, 171)\n164 (159, 172)\n165 (159, 171)\n164 (158, 172)\n164 (158, 172)\n\n\nWGHT\n64 (55, 72)\n64 (56, 73)\n63 (56, 72)\n64 (57, 74)\n64 (57, 72)\n63 (56, 72)\n64 (57, 74)\n\n\nTOT_CHOL\n192 (170, 216)\n193 (168, 220)\n190 (168, 214)\n196 (173, 224)\n190 (168, 218)\n193 (171, 216)\n194 (171, 217)\n\n\nTG\n105 (71, 148)\n107 (70, 158)\n104 (74, 164)\n108 (69, 164)\n107 (76, 160)\n108 (75, 162)\n111 (71, 167)\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n\n\ntbl_strata\nby 옵션을 통해 그룹별 통계량을 계산한 것처럼 tbl_strata 함수를 이용하면 여러 계층으로 그룹을 묶을 수 있다. tbl_strata(data, strata, .tbl_fun, …) 형식을 사용하며 strata에 그룹화할 칼럼, .tbl_fun 인자에는 출력할 tbl_summary formula를 지정한다.\n\ntbl_strata(data = dt2,\n           strata = EXMD_BZ_YYYY,\n           .tbl_fun =\n             ~ .x %>%\n             tbl_summary(by = Q_SMK_YN) %>%\n             add_p() %>%\n             add_n(),\n           .header = \"**{strata}**, N={n}\")\n\n\n\n\n\n\n\nCharacteristic\n      \n        2009, N=214\n      \n      \n        2010, N=236\n      \n      \n        2011, N=223\n      \n      \n        2012, N=234\n      \n      \n        2013, N=243\n      \n      \n        2014, N=254\n      \n      \n        2015, N=240\n      \n    \n\nN\n      \n1, N = 1251\n\n      \n2, N = 341\n\n      \n3, N = 531\n\n      \np-value2\n\n      N\n      \n1, N = 1321\n\n      \n2, N = 421\n\n      \n3, N = 621\n\n      \np-value2\n\n      N\n      \n1, N = 1401\n\n      \n2, N = 351\n\n      \n3, N = 481\n\n      \np-value2\n\n      N\n      \n1, N = 1461\n\n      \n2, N = 361\n\n      \n3, N = 521\n\n      \np-value2\n\n      N\n      \n1, N = 1411\n\n      \n2, N = 351\n\n      \n3, N = 671\n\n      \np-value2\n\n      N\n      \n1, N = 1571\n\n      \n2, N = 381\n\n      \n3, N = 591\n\n      \np-value2\n\n      N\n      \n1, N = 1541\n\n      \n2, N = 361\n\n      \n3, N = 501\n\n      \np-value2\n\n    \n\n\n\nQ_PHX_DX_STK\n130\n1 (1.5%)\n1 (3.6%)\n0 (0%)\n0.5\n162\n2 (2.3%)\n0 (0%)\n0 (0%)\n>0.9\n138\n1 (1.1%)\n0 (0%)\n0 (0%)\n>0.9\n156\n2 (2.1%)\n0 (0%)\n0 (0%)\n>0.9\n168\n2 (2.1%)\n0 (0%)\n0 (0%)\n>0.9\n159\n2 (2.2%)\n0 (0%)\n0 (0%)\n>0.9\n156\n1 (1.0%)\n0 (0%)\n0 (0%)\n>0.9\n\n\nUnknown\n\n58\n6\n18\n\n\n44\n10\n20\n\n\n53\n14\n18\n\n\n52\n12\n14\n\n\n44\n7\n24\n\n\n67\n8\n20\n\n\n58\n12\n14\n\n\n\nHGHT\n212\n160 (155, 166)\n166 (164, 174)\n170 (165, 175)\n<0.001\n236\n160 (156, 166)\n168 (164, 173)\n172 (165, 176)\n<0.001\n223\n159 (155, 166)\n170 (166, 174)\n172 (169, 177)\n<0.001\n234\n161 (156, 167)\n169 (165, 172)\n172 (166, 177)\n<0.001\n243\n160 (156, 165)\n170 (166, 172)\n171 (167, 174)\n<0.001\n254\n160 (155, 165)\n169 (165, 174)\n173 (168, 177)\n<0.001\n240\n161 (155, 166)\n170 (164, 173)\n173 (169, 177)\n<0.001\n\n\nWGHT\n212\n59 (52, 67)\n68 (61, 74)\n71 (62, 77)\n<0.001\n236\n60 (54, 67)\n70 (66, 77)\n70 (62, 78)\n<0.001\n223\n60 (53, 67)\n69 (64, 75)\n72 (64, 79)\n<0.001\n234\n61 (54, 69)\n70 (63, 77)\n72 (64, 80)\n<0.001\n243\n59 (53, 69)\n68 (63, 73)\n69 (62, 75)\n<0.001\n254\n59 (53, 66)\n69 (62, 76)\n72 (64, 79)\n<0.001\n240\n61 (54, 69)\n70 (63, 79)\n74 (62, 84)\n<0.001\n\n\nTOT_CHOL\n212\n192 (166, 215)\n195 (177, 224)\n198 (174, 217)\n0.6\n236\n193 (166, 220)\n200 (181, 219)\n186 (168, 218)\n0.5\n223\n186 (165, 212)\n195 (172, 222)\n198 (176, 228)\n0.2\n234\n198 (173, 224)\n200 (171, 238)\n192 (175, 214)\n0.8\n243\n187 (165, 214)\n191 (176, 227)\n196 (170, 220)\n0.3\n254\n193 (170, 216)\n194 (174, 220)\n193 (174, 214)\n>0.9\n240\n189 (170, 216)\n194 (170, 216)\n204 (180, 222)\n0.12\n\n\nTG\n212\n87 (59, 125)\n130 (79, 189)\n139 (100, 173)\n<0.001\n236\n86 (64, 133)\n113 (78, 176)\n142 (101, 230)\n<0.001\n223\n93 (67, 129)\n129 (78, 194)\n148 (91, 200)\n<0.001\n234\n94 (65, 133)\n130 (91, 188)\n132 (94, 200)\n0.001\n243\n96 (72, 140)\n105 (75, 162)\n127 (84, 217)\n0.009\n254\n98 (73, 140)\n118 (87, 165)\n134 (82, 202)\n0.005\n240\n98 (64, 145)\n125 (97, 255)\n150 (90, 248)\n<0.001\n\n\n\n\n\n1 n (%); Median (IQR)\n    \n\n\n2 Fisher's exact test; Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "href": "posts/2022-02-07-gtsummary/index.html#modifying-function-arguments",
    "title": "gtsummary 패키지 소개",
    "section": "Modifying function arguments",
    "text": "Modifying function arguments\ntbl_summary 함수에는 다양한 옵션이 존재하며, 이러한 옵션 조정을 통해 원하는 테이블을 작성할 수 있다. 다음은 tbl_summary 함수의 주요 옵션에 대한 설명이다.\n\nlabel : 테이블에 출력되는 변수명 지정\ntype : 변수 유형 지정 (ex. 연속형, 범주형)\nstatistic : 요약 통계량 지정\ndigits : 자릿수 지정\nmissing : 결측값이 있는 행을 표시할지 여부\nmissing_text : 결측행의 변수명 지정\nsort : 빈도에 따라 범주형 변수의 level 정렬\npercent : 열/행의 백분율 출력\ninclude : 테이블에 포함할 변수 지정\n\n\n\n\n\n\n\n\n\n다음은 옵션을 활용한 예시이다. 연도 변수 EXMD_BZ_YYYY의 그룹별 통계량을 출력하고, Q_SMK_YN 변수를 “smoking y/n”로 바꾸어보자. 이때 연속형 변수의 출력값을 {mean}({sd})으로, 범주형 변수의 출력값을 {n}/{N} ({p}%) 형태로 바꾸어보자. 결측값의 변수명은 “Missing”으로 수정한다.\n\ndt2 %>%\n  tbl_summary(\n    by = EXMD_BZ_YYYY,\n    statistic = list(all_continuous() ~ \"{mean} ({sd})\",\n                     all_categorical() ~ \"{n} / {N} ({p}%)\"),\n    label = Q_SMK_YN ~ \"smoking y/n\",\n    missing_text = \"Missing\"\n  )\n\n\n\n\n\n\nCharacteristic\n      \n2009, N = 2141\n\n      \n2010, N = 2361\n\n      \n2011, N = 2231\n\n      \n2012, N = 2341\n\n      \n2013, N = 2431\n\n      \n2014, N = 2541\n\n      \n2015, N = 2401\n\n    \n\n\nQ_PHX_DX_STK\n2 / 132 (1.5%)\n2 / 162 (1.2%)\n1 / 138 (0.7%)\n2 / 156 (1.3%)\n2 / 168 (1.2%)\n2 / 159 (1.3%)\n1 / 156 (0.6%)\n\n\nMissing\n82\n74\n85\n78\n75\n95\n84\n\n\nsmoking y/n\n\n\n\n\n\n\n\n\n\n1\n125 / 212 (59%)\n132 / 236 (56%)\n140 / 223 (63%)\n146 / 234 (62%)\n141 / 243 (58%)\n157 / 254 (62%)\n154 / 240 (64%)\n\n\n2\n34 / 212 (16%)\n42 / 236 (18%)\n35 / 223 (16%)\n36 / 234 (15%)\n35 / 243 (14%)\n38 / 254 (15%)\n36 / 240 (15%)\n\n\n3\n53 / 212 (25%)\n62 / 236 (26%)\n48 / 223 (22%)\n52 / 234 (22%)\n67 / 243 (28%)\n59 / 254 (23%)\n50 / 240 (21%)\n\n\nMissing\n2\n0\n0\n0\n0\n0\n0\n\n\nHGHT\n164 (9)\n165 (9)\n164 (10)\n165 (9)\n165 (9)\n164 (9)\n164 (9)\n\n\nWGHT\n64 (13)\n65 (12)\n65 (13)\n66 (12)\n65 (12)\n64 (12)\n66 (13)\n\n\nTOT_CHOL\n195 (37)\n195 (39)\n194 (38)\n199 (35)\n192 (36)\n195 (36)\n195 (36)\n\n\nTG\n129 (90)\n136 (101)\n138 (108)\n129 (89)\n132 (98)\n138 (127)\n141 (113)\n\n\n\n\n1 n / N (%); Mean (SD)"
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html",
    "href": "posts/2019-04-03-reviewmoneypin/index.html",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "",
    "text": "법인 설립 후 세무기장 앱 머니핀(MoneyPin)을 활용, 직접 세무/회계를 처리하였습니다. 3월말 법인세까지 납부하면서 한 사이클을 경험했다고 생각하여 후기를 공유합니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#법인-설립-후-세무회계-고민",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "법인 설립 후 세무/회계 고민",
    "text": "법인 설립 후 세무/회계 고민\n법인을 설립하고 느낀 가장 큰 문제가 세무와 회계였습니다. 세무사에게 맡기자니 매출도 없는데 비용이 부담되었고, 평소 회계에 관심이 많아 이 기회에 회계를 배우고 싶었습니다. 그러던 중 인터넷 검색을 통해 머니핀과 자비스를 알게되어 둘다 이용하기 시작했습니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#머니핀으로-결정한-계기",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n머니핀으로 결정한 계기",
    "text": "머니핀으로 결정한 계기\n두 서비스 모두 국세청, 법인카드, 기업통장을 연계하여 자동으로 수입, 지출을 기록할 수 있습니다. 계정과목을 지정하면 바로 재무제표에 반영되어 확인할 수 있고, 이것을 부가세 등 세금을 낼 때 이용할 수 있습니다. 작년 말부터는 머니핀만 사용하기 시작했는데 이유는 다음과 같습니다.\n\n\n앱 하나로 다 됩니다. 회계관리, 부가세/법인세 납부까지 앱에서 해결할 수 있습니다(Figure @ref(fig:screen)).\n\n\n자비스는 영수증 등록만 앱으로 등록하고 나머지 업무들은 전부 웹에서 해야 합니다.\n\n\n\n\n\n\n\n앱 화면(출처: 머니핀 홈페이지)\n\n\n\n\n\n\n운영진 피드백이 매우 좋습니다. 회계를 처음 겪는 입장에서 큰 도움이 되었습니다(Figure @ref(fig:talk)).\n\n앱 내 메신저를 통해 운영진에게 질문을 자주 하는데, 항상 빠르고 친절하게 설명해 주십니다. 질문을 할 때마다 미안한 마음이 들 정도입니다.\n\n\n\n\n\n\n\n운영진 피드백\n\n\n\n\n\n\n쌉니다. 작년에는 무료였고 지금은 기본요금제가 월 9,900원입니다(Figure @ref(fig:bill)).\n\n아직 거래가 별로 없는 1인기업이 세무대행을 이용하는 것이 부담이었습니다.\n\n\n\n\n\n\n\n요금제"
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#차라투의-머니핀-이용법",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "\n차라투의 머니핀 이용법",
    "text": "차라투의 머니핀 이용법\n가장 많이 쓰는 기능은 국세청 세금계산서, 법인카드, 통장내역 자동 반영입니다. 거래가 일어난 후 앱 새로고침을 누르면 바로 거래내역이 반영되고, 어떤 회계항목에 해당되는지 메뉴에서 선택할 수 있습니다(Figure @ref(fig:content)). 어느 계정에 넣을지 헷갈릴때는 운영진께 질문하면 친절하게 답변해 주십니다.\n\n\n\n\n거래내역 고르기(출저: 머니핀)\n\n\n\n\n부득이하게 현금이나 개인카드를 이용한 경우 영수증을 카메라로 찍어 업로드하면 해당 내역이 앱에 반영됩니다. 저의 경우는 법인 설립 전 지출했거나 깜빡 잊고 개인카드를 이용한 내역을 반영하는 데 이용했습니다(Figure @ref(fig:bill2)).\n\n\n\n\n영수증 업로드\n\n\n\n\n부가세를 낼 때는 그동안의 내역을 바탕으로 바로 신고서와 전자파일을 만든 후, 국세청에 업로드하면 됩니다(유료, Figure @ref(fig:tax)). 지난 3월에는 법인세를 납부했는데 이 또한 머니핀을 통해 쉽게 마무리하였습니다(유료).\n\n\n\n\n18년 부가세 신고\n\n\n\n\n차라투는 아직 1인기업이라 직원급여와 관련된 기능은 이용하지 못했습니다. 올해 말에는 꼭 같이할 팀원이 생겨서 이 기능도 이용해보고 싶네요."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#아쉬운-점",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "아쉬운 점",
    "text": "아쉬운 점\n차라투의 회계는 100% 머니핀에 의존하다보니, 앱에서 작은 문제만 생겨도 큰 어려움을 겪습니다. 다음은 앱을 이용하면서 아쉬웠거나 개선이 필요한 내용입니다.\n\n\n앱 안정성 문제\n\n앱이 갑자기 멈추거나, 홈택스/법인카드 거래내역 반영이 안될 때가 있습니다. 운영진께 문의하면 바로 처리해 주십니다.\n\n\n\n버그, 오류 문제.\n\n앱 안정성보다 이것이 더 문제인데요, 같은 내역이 2번 반영되는 등 기본 숫자가 잘못되는 경우가 있습니다. 이번에 법인세를 내면서 잘못된 숫자들이 있다는 것을 알게 되었고, 운영진의 도움으로 무사히 수정하였습니다.\n\n\n\n회계 항목 추가\n\n일반적인 항목들은 다 있어 별 문제점은 없습니다만, 거래처 경조사같은 몇 가지 세부 항목이 추가되었으면 좋겠습니다. 현재 운영진께 건의드린 내용입니다.\n\n\n\n머니핀도 창업한지 얼마 안된 스타트업인 만큼, 서비스 운영을 하며 여러 시행착오를 겪는 것으로 느껴집니다. 저희가 겪는 시행착오랑 비슷한 경우도 많아 감정이입이 됩니다."
  },
  {
    "objectID": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "href": "posts/2019-04-03-reviewmoneypin/index.html#마치며",
    "title": "세무기장마법사 머니핀(MoneyPin) 리뷰",
    "section": "마치며",
    "text": "마치며\n창업 후 7개월간 머니핀을 사용한 느낌을 적어 보았습니다. 간단히 3줄 요약하자면 다음과 같습니다.\n\n앱 하나로 모든 세무, 회계를 처리할 수 있다.\n싸다.\n앱이 아직 불안정하다. 그러나 운영진의 피드백이 매우 빠르다.\n\n이제 막 사업을 시작하셨거나, 1인 법인 등 작은 규모의 업체를 운영하는 분께 적극 추천합니다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html",
    "href": "posts/2019-01-03-rmarkdown/index.html",
    "title": "R Markdown 기초",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, R Markdown으로 R 코드와 분석 결과가 포함된 문서를 작성하는 방법을 강의할 예정입니다. 강의 내용을 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rmarkdown/index.html#시작하기-전에",
    "title": "R Markdown 기초",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR Markdown은 R 코드와 분석을 포함한 컨텐츠를 만드는 툴이며 크게 3가지 활용법이 있다.\n\n문서(pdf, html, docx): 글쓰기, 분석 결과, 참고문헌 등 논문의 모든 작업을 R Markdown으로 수행한다.\n프리젠테이션(pdf, html, pptx): R 코드나 분석결과가 포함된 프리젠테이션을 만든다. 기본 템플릿1 외에 xaringan2 패키지가 최근 인기를 끌고 있다.\n웹(html): 웹사이트나 블로그를 만든다. blogdown3 이나 distill4 패키지가 대표적이다. 이 글의 블로그도 distill로 만들었으며, 과거 차라투 홈페이지는 blogdown을 이용하였다.\n\n본 강의는 1의 가장 기초에 해당하는 강의로 간단한 문서를 작성하는 것을 목표로 한다. pdf 문서를 만들기 위해서는 추가로 LaTeX 문서작성 프로그램인 Tex Live를 설치해야 하며 본 강의에서는 생략한다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "href": "posts/2019-01-03-rmarkdown/index.html#rmd-문서-시작하기",
    "title": "R Markdown 기초",
    "section": "Rmd 문서 시작하기",
    "text": "Rmd 문서 시작하기\nR Markdown은 Rmd 파일로 작성되며 rmarkdown5 패키지를 설치한 후, Rstudio에서 File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R markdown… 의 순서로 클릭하여 시작할 수 있다(Figure @ref(fig:rmdfilemenu), @ref(fig:rmdstart)).\n\n\n\n\nRstudio File 메뉴6\n\n\n\n\n\n\n\n\nR markdown 시작 메뉴7\n\n\n\n\n문서의 제목과 저자 이름을 적은 후 파일 형태를 아무거나 고르면(나중에도 쉽게 수정 가능) Figure @ref(fig:rmdfile)처럼 확장자가 Rmd인 문서가 생성될 것이다.\n\n\n\n\nR markdown 기본 문서8\n\n\n\n\n파일 내용을 보면 맨 먼저 제목을 쓰는 부분이 있고 글과 코드를 작성하는 부분도 있다. 일단 이 파일을 문서로 만들어보자. 문서 이름이 있는 바로 아래의 knit 탭을 누르거나, 그 옆의 아래방향 화살표를 누르고 원하는 파일 형태를 클릭하면 된다(Figure @ref(fig:knittab)). 처음에 언급했듯이 pdf는 Tex Live를 설치한 후 이용할 수 있다.\n\n\n\n\nknit 탭9\n\n\n\n\n다음은 각각 html, pdf, docx로 생성된 문서이다.\n\n\n\n\nhtml 문서10\n\n\n\n\n\n\n\n\npdf 문서11\n\n\n\n\n\n\n\n\nword 문서12\n\n\n\n\n생각보다 간단하지 않은가? 이제 본격적으로 Rmd 파일의 내용을 살펴보면서 어떻게 글과 R 코드를 작성하는지 알아보자. Rmd는 크게 제목을 적는 YAML Header, 글을 쓰는 Markdown Text와 코드를 적는 Code Chunk로 나눌 수 있다(Figure @ref(fig:rmdcontents)).\n\n\n\n\nRmd 파일 구성13"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "href": "posts/2019-01-03-rmarkdown/index.html#yaml-header",
    "title": "R Markdown 기초",
    "section": "YAML Header",
    "text": "YAML Header\nYAML은 YAML Ain’t Markup Language의 재귀형식의 이름을 갖고 있는 언어로 가독성에 초점을 두고 개발되었다. R Markdown은 Rmd의 시작 부분에 문서 형식을 설정하는 용도로 이 포맷을 이용한다. 다음은 기초 정보만 포함된 YAML이다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2022-08-23\"\noutput: html_document\n---\nKnit 버튼 오른쪽의 설정() \\(\\rightarrow\\) Output Options…를 클릭하여 html, pdf, word 포맷 각각에 대한 기본 설정을 할 수 있다(Figure @ref(fig:outputoption), @ref(fig:outputhtml)).\n\n\n\n\nOutput Options14\n\n\n\n\n\n\n\n\nHTML Option15\n\n\n\n\n설정을 마치면 업데이트 된 YAML을 볼 수 있다. 모든 포맷 공통인 설정값은\n\ntoc(yes or no): 목차 포함 여부\n그림의 높이(fig_height) 와 넓이(fig_width): R 코드로 만든 그림에는 해당되지 않는다. Figures 에서 다시 설명하겠다.\n\n\n\n\n이며, 자동으로 현재 날짜를 입력하려면 아래와 같이 `r format(Sys.Date())`를 이용하면 된다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\" \n---\n아래는 필자가 Rmd 문서를 만들 때 흔히 쓰는 YAML 설정이다.\n---\ntitle: \"R Markdown 기초\"\nsubtitle: \"성균관의대 강의 2019\"\nauthor: \"김진섭\"\ndate: \"`r format(Sys.Date())`\"\noutput:\n  html_document:\n    fig_height: 6\n    fig_width: 10\n    highlight: textmate\n    theme: cosmo\n    toc: yes\n    toc_depth: 3\n    toc_float: yes\n  pdf_document:\n    fig_height: 6\n    fig_width: 10\n    toc: no\n  word_document:\n    fig_height: 6\n    fig_width: 9\n    toc: no\n---\nhtml은 theme16에서 테마, highlight17에서 글씨 강조 스타일을 설정할 수 있으며, toc_float 옵션으로 움직이는 목차를 만들 수 있다(@ref(fig:tocfloat)).\n\n\n\n\ntoc_float- 움직이는 목차18\n\n\n\n\ndocx는 미리 설정을 마친 docx 문서를 아래와 같이 YAML에 추가하여 템플릿으로 이용할 수 있다.\n---\ntitle: \"R Markdown 기초\"\nauthor: \"김진섭\"\ndate: \"2022-08-23\"\noutput: \n    word_document:\n      reference_docx: mystyles.docx\n---\ndocx에 대한 자세한 내용은 Rstudio 블로그19를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "href": "posts/2019-01-03-rmarkdown/index.html#markdown-글쓰기",
    "title": "R Markdown 기초",
    "section": "Markdown 글쓰기",
    "text": "Markdown 글쓰기\nR Markdown은 이름에서 알 수 있듯이 마크다운(Markdown) 을 기반으로 만들어졌다. 마크다운은 문법이 매우 간단한 것이 특징으로 깃허브의 README.md가 대표적인 마크다운 문서이다. 아래의 [R markdown reference]20에 흔히 쓰는 문법이 정리되어 있다.\n2 가지만 따로 살펴보겠다.\nInline R code\n\n\n\n문장 안에 분석 결과값을 적을 때, 분석이 바뀔 때마다 바뀐 숫자를 직접 수정해야 한다. 그러나 숫자 대신 `r <코드>` 꼴로 R 코드를 넣는다면 재분석시 그 숫자를 자동으로 업데이트 시킬 수 있다.\nThere were  `r nrow(cars)` cars studied\n\nThere were 50 cars studied\n\n수식\nLaTeX 문법을 사용하며 hwp 문서의 수식 편집과 비슷하다. inline 삽입은 $...$, 새로운 줄은 $$...$$ 안에 식을 적으면 된다.\nThis summation expression $\\sum_{i=1}^n X_i$ appears inline.\n\nThis summation expression \\(\\sum_{i=1}^n X_i\\) appears inline.\n\n$$\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n$$\n\\[\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\]\n수식 전반은 LaTeX math and equations21을 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "href": "posts/2019-01-03-rmarkdown/index.html#r-chunk",
    "title": "R Markdown 기초",
    "section": "R chunk",
    "text": "R chunk\nRmd 문서에서 R 코드가 들어가는 방식은 4가지이다.\n\n몰래 실행. 코드와 결과는 다 숨긴다 - 최초 설정 때 쓰임.\n실행. 코드와 결과를 모두 보여준다.\n실행. 코드는 숨기고 결과만 보여준다.\n실행하지 않음. 코드 보여주기만 한다.\n\n하나씩 살펴보도록 하자.\n최초 설정\n문서를 처음 생성했을 때 최초로 보이는 R 코드는 다음과 같다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\ninclude=FALSE 옵션으로 문서에는 포함시키지 않고 몰래 실행할 수 있으며, 주로 최초 설정에 이용된다. setup은 이 코드에 해당하는 라벨로 생략 가능하다. knitr::opts_chunk$set 에서 디폴트 옵션을 설정할 수 있으며 echo = TRUE는 코드를 보여준다는 뜻이다. 흔히 쓰는 옵션들은 아래와 같다.\n\n\neval=F - 코드를 실행하지 않는다.\n\necho=F - 코드를 보여주지 않는다.\n\ninclude=F - 실행 결과를 보여주지 않는다.\n\nmessage=F - 실행 때 나오는 메세지를 보여주지 않는다.\n\nwarning=F - 실행 때 나오는 경고를 보여주지 않는다.\n\nerror=T - 에러가 있어도 실행하고 에러코드를 보여준다.\n\nfig.height = 7 - 그림 높이, R로 그린 그림에만 해당한다.\n\nfig.width = 7 - 그림 너비, R로 그린 그림에만 해당한다.\n\nfig.align = 'center' - 그림 위치, R로 그린 그림에만 해당한다.\n\n다음은 필자가 논문을 Rmd로 쓸 때 흔히 쓰는 디폴트 옵션이다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=F, fig.align = \"center\", message=F, warning=F, fig.height = 8, cache=T, dpi = 300, dev = \"jpg\")\n```\n전체 옵션은 knitr::opts_chunk$get 함수로 확인할 수 있다.\nknitr::opts_chunk$get()\n\n\n$eval\n[1] TRUE\n\n$echo\n[1] FALSE\n\n$results\n[1] \"markup\"\n\n$tidy\n[1] FALSE\n\n$tidy.opts\nNULL\n\n$collapse\n[1] FALSE\n\n$prompt\n[1] FALSE\n\n$comment\n[1] NA\n\n$highlight\n[1] TRUE\n\n$size\n[1] \"normalsize\"\n\n$background\n[1] \"#F7F7F7\"\n\n$strip.white\n[1] TRUE\n\n$cache\n[1] FALSE\n\n$cache.path\n[1] \"index_cache/html/\"\n\n$cache.vars\nNULL\n\n$cache.lazy\n[1] TRUE\n\n$dependson\nNULL\n\n$autodep\n[1] FALSE\n\n$cache.rebuild\n[1] FALSE\n\n$fig.keep\n[1] \"high\"\n\n$fig.show\n[1] \"asis\"\n\n$fig.align\n[1] \"center\"\n\n$fig.path\n[1] \"index_files/figure-html/\"\n\n$dev\n[1] \"png\"\n\n$dev.args\nNULL\n\n$dpi\n[1] 96\n\n$fig.ext\nNULL\n\n$fig.width\n[1] 7\n\n$fig.height\n[1] 5\n\n$fig.env\n[1] \"figure\"\n\n$fig.cap\nNULL\n\n$fig.scap\nNULL\n\n$fig.lp\n[1] \"fig:\"\n\n$fig.subcap\nNULL\n\n$fig.pos\n[1] \"\"\n\n$out.width\nNULL\n\n$out.height\nNULL\n\n$out.extra\nNULL\n\n$fig.retina\n[1] 2\n\n$external\n[1] TRUE\n\n$sanitize\n[1] FALSE\n\n$interval\n[1] 1\n\n$aniopts\n[1] \"controls,loop\"\n\n$warning\n[1] TRUE\n\n$error\n[1] FALSE\n\n$message\n[1] TRUE\n\n$render\nNULL\n\n$ref.label\nNULL\n\n$child\nNULL\n\n$engine\n[1] \"R\"\n\n$split\n[1] FALSE\n\n$include\n[1] TRUE\n\n$purl\n[1] TRUE\n\n$fenced.echo\n[1] FALSE\n\n$ft.shadow\n[1] FALSE\n\n\nChunk 별 설정\n최초 설정 이후부터는 아래와 같이 간단하게 코드를 보여주거나 실행할 수 있다.\n```{r}\nhead(mtcars)\n```\nhead(mtcars)\n\n\n\n\n  \n\n\n\n기본 설정과 다른 옵션을 적용하려면 chunk에 옵션을 따로 적으면 된다. 예를 들어 코드는 숨기고 결과만 보여주려면 echo=F 를 추가하면 된다.\n```{r, echo=F}\nhead(mtcars)\n```\n\n\n\n\n  \n\n\n\n반대로 실행은 안하고 코드만 보여주려면 eval=F를 추가하면 된다.\n```{r, eval=F}\nhead(mtcars)\n```\nhead(mtcars)"
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#figures",
    "href": "posts/2019-01-03-rmarkdown/index.html#figures",
    "title": "R Markdown 기초",
    "section": "Figures",
    "text": "Figures\nRmd 문서에 그림이 들어가는 방법은 2가지가 있다.\n\nR 코드로 생성 : plot 함수, ggplot2 패키지 등\n외부 그림 삽입\n\n앞서도 언급했듯이 주의할 점은 그림이 만들어지는 방법에 따라 서로 다른 옵션이 적용된다는 것이다. 일단 전자부터 살펴보자.\nFigures with R\n\nR 코드에서 자체적으로 만든 그림은 전부 chunk 옵션의 지배를 받아 간단하다.\n```{r, fig.cap = \"scatterplot: cars\", fig.width = 8, fig.height = 6}\nplot(cars, pch = 18)\n```\nplot(cars, pch = 18)\n\n\n\n\nscatterplot - cars\n\n\n\n\nExternal figures\n외부 그림은 R 코드로도 삽입할 수 있고 마크다운 문법을 쓸 수도 있는데, 어떤 방법을 쓰느냐에 따라 다른 옵션을 적용받는다는 것을 주의해야 한다. R에서는 knitr::include_graphics 함수를 이용하여 그림을 넣을 수 있고 이 때는 chunk 내부의 옵션이 적용된다.\n```{r, fig.cap = \"tidyverse logo\", fig.align = \"center\"}\nlibrary(knitr)\ninclude_graphics(\"https://www.tidyverse.org/images/tidyverse-default.png\")\n```\n\n\n\n\ntidyverse logo\n\n\n\n\n같은 그림을 chunk없이 바로 마크다운에서 삽입할 수도 있다. 이 때는 YAML의 옵션이 적용된다.\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n\n\ntidyverse logo\n\n\n{ width=50% } 는 그림의 크기를 조절하는 옵션이며 R chunk에서도 같은 옵션 out.width=\"50%\"이 있다. 위치를 가운데로 조절하려면 <center>...</center> 를 포함시키자.\n<center>\n![tidyverse logo](https://www.tidyverse.org/images/tidyverse-default.png){ width=50% }\n</center>\n\n\n\ntidyverse logo\n\n\n\n개인적으로는 외부 이미지도 chunk 내부에서 읽는 것을 추천한다. chunk 내부의 옵션들이 마크다운의 그것보다 훨씬 체계적이고 쉬운 느낌이다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#tables",
    "href": "posts/2019-01-03-rmarkdown/index.html#tables",
    "title": "R Markdown 기초",
    "section": "Tables",
    "text": "Tables\n논문을 쓸 때 가장 귀찮은 부분 중 하나가 분석 결과를 테이블로 만드는 것으로, knitr::kable() 함수를 쓰면 문서 형태에 상관없이 Rmd에서 바로 테이블을 만들 수 있다. 아래는 데이터를 살펴보는 가장 간단한 예시이다.\n```{r}\nkable(iris[1:5, ], caption = \"A caption\")\n```\n\n\n\nA caption\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\nepiDisplay 패키지의 regress.display, logistic.display 함수를 활용하면 회귀분석의 결과를 바로 테이블로 나타낼 수 있다(Table @ref(tab:regtable)).\n```{r}\nmtcars$vs <- as.factor(mtcars$vs)\nmtcars$cyl <- as.factor(mtcars$cyl)\n\nmodel <- glm(mpg ~ disp + vs + cyl, data = mtcars)\nmodel.display <- epiDisplay::regress.display(model, crude = T, crude.p.value = T)\nmodel.table <- model.display$table[rownames(model.display$table)!=\"\", ]\nkable(model.table, caption = model.display$first.line)\n```\n\n\n\nLinear regression predicting mpg\n\n\n\n\n\n\n\n\n\n\ncrude coeff.(95%CI)\ncrude P value\nadj. coeff.(95%CI)\nP(t-test)\nP(F-test)\n\n\n\ndisp (cont. var.)\n-0.04 (-0.05,-0.03)\n< 0.001\n-0.03 (-0.05,0)\n0.019\n< 0.001\n\n\nvs: 1 vs 0\n7.94 (4.6,11.28)\n< 0.001\n0.04 (-3.81,3.89)\n0.984\n0.334\n\n\ncyl: ref.=4\n\n\n\n\n0.041\n\n\n6\n-6.92 (-10.11,-3.73)\n< 0.001\n-4.77 (-8.56,-0.98)\n0.016\n\n\n\n8\n-11.56 (-14.22,-8.91)\n< 0.001\n-4.75 (-12.19,2.7)\n0.202\n\n\n\n\n\n\n테이블을 좀 더 다듬으려면 kableExtra 패키지가 필요하며, 자세한 내용은 cran 설명서22를 참고하기 바란다. html 문서의 경우 kable()외에도 다양한 함수들을 이용할 수 있는데, 대표적인 것이 rmarkdown::paged_table() 함수와 DT 패키지이다. 전자는 아래와 같이 YAML에서 테이블 보기의 기본 옵션으로 설정할 수도 있다.\n---\ntitle: \"Motor Trend Car Road Tests\"\noutput:\n  html_document:\n    df_print: paged\n---\nDT 패키지에 대한 설명은 Rstudio DT 홈페이지23를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "href": "posts/2019-01-03-rmarkdown/index.html#마치며",
    "title": "R Markdown 기초",
    "section": "마치며",
    "text": "마치며\n본 강의를 통해 R Markdown으로 기본적인 문서를 만드는 법을 알아보았다. 본 강의에서는 시간 관계상 참고문헌 다는 법을 언급하지 않았는데 궁금하다면 Bibliographies and Citations24을 참고하자. 이 내용까지 숙지한다면 R Markdown으로 논문을 쓸 준비가 된 것이다. R Markdown에 대한 전반적인 내용은 아래의 R Markdown Cheet Sheet25에 잘 요약되어 있으니 그때그떄 확인하면 좋다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html",
    "href": "posts/2020-07-22-regressionbasic/index.html",
    "title": "회귀분석 in 의학연구",
    "section": "",
    "text": "김진섭 대표는 삼성서울병원 정신건강의학과 를 방문, 2회에 걸쳐 의학 연구에서 쓰이는 통계에 대해 강의할 예정입니다. 2주차 주제를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#요약",
    "href": "posts/2020-07-22-regressionbasic/index.html#요약",
    "title": "회귀분석 in 의학연구",
    "section": "요약",
    "text": "요약\n\n\n\n\n\n\n\n\n\nDafault\nRepeated measure\nSurvey\n\n\n\nContinuous\nlinear regression\nGEE\nSurvey GLM\n\n\nEvent\nGLM (logistic)\nGEE\nSurvey GLM\n\n\nTime & Event\nCox\nmarginal Cox\nSurvey Cox\n\n\n0,1,2,3 (rare event)\nGLM (poisson)\nGEE\nSurvey GLM"
  },
  {
    "objectID": "posts/2020-07-22-regressionbasic/index.html#slide",
    "href": "posts/2020-07-22-regressionbasic/index.html#slide",
    "title": "회귀분석 in 의학연구",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-smc-psychiatry/regression 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html",
    "href": "posts/2021-10-01-notionoopy/index.html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "",
    "text": "기존 Hugo로 작성하고 Netlify로 배포하던 홈페이지의 한계점로, 새롭게 홈페이지를 구축하기로 결정.\n당사 홈페이지가 정적 웹이고, 최근 트렌드를 고려하여, Notion으로 홈페이지를 구축하기로 결정.\nNotion을 홈페이지화 해 주는 배포 서비스를 oopy로 결정.\n구축 후 oopy를 통해 커스텀 URL, 추가 JS, CSS 등 설정."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "href": "posts/2021-10-01-notionoopy/index.html#jekyll-프레임워크와-github-pages",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Jekyll 프레임워크와 GitHub Pages",
    "text": "Jekyll 프레임워크와 GitHub Pages\n정적 웹 프레임워크에서 가장 유명한 두 개의 프레임워크 중 하나는 Jekyll입니다. 당사는 소스코드를 GitHub로 관리하기에, GitHub Pages로 바로 배포가 가능한 Jekyll도 고려하였으나, 근본적으로 Hugo와 큰 차이가 없어 기존의 문제를 다 해결할 수는 없을 것이라고 생각해 포기하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "href": "posts/2021-10-01-notionoopy/index.html#새롭게-웹-작성",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "새롭게 웹 작성",
    "text": "새롭게 웹 작성\n새롭게 홈페이지을 작성하고 당사 서버의 남은 자원으로 호스팅 하는 방법도 있었습니다. 다만 빠른 제작이 필요했던 탓에 단기간에 만들기 힘든 부분이 있어 배제하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "href": "posts/2021-10-01-notionoopy/index.html#notion으로-페이지를-작성하고-oopy로-노션-페이지를-홈페이지로-가공-후-배포",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포",
    "text": "Notion으로 페이지를 작성하고 oopy로 노션 페이지를 홈페이지로 가공 후 배포\n최근 많은 기업체에서 채용 등 일부 페이지를 Notion으로 구축하는 사례가 늘고 있습니다. 예를 들어 왓챠나 클라썸같은 유명 스타트업이 있습니다. 다만 이들 또한 주 홈페이지까지 Notion을 사용하지는 않았습니다. 각 회사에서 서비스하는 제품이 정적 웹으로는 해결되지 않았기 때문입니다. 그러나 당사의 경우 주 홈페이지는 정적 웹으로 충분하고, 실제 서비스는 Zarathu App을 사용하기 때문에 큰 문제가 되지 않습니다.\n또한 노션의 기본 기능 뿐 아니라, oopy에서 간단한 SEO, 스타일(테마), HTML(CSS, JS포함), 클린 URL, Google Analytics나 channel.io 등 플러그인까지 지원하기에 자유도와 편리함이 적절하다 판단했습니다. oopy와 비슷한 서비스 - 대표적으로 Super - 도 존재하나, 지원의 편리성과 가격을 고려하여 Notion과 oopy를 사용하기로 최종 결정하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML",
    "text": "커스텀 HTML\n저희가 Notion과 oopy조합을 사용하게 된 가장 큰 이유는 커스텀 HTML(이하 CSS, JS 포함)입니다. 당사 홈페이지 하단의 ‘연구지원 신청하기’ 버튼은 oopy의 페이지별 HTML기능을 사용한 것입니다. oopy는 모든 페이지에 커스텀 HTML을 적용시킬 수도 있고, 페이지마다 개별적으로 적용할 수도 있습니다.  차라투는 위 사진처럼 media쿼리를 사용해 사용자의 환경에 따라 컨텐츠를 다르게 적용하기도 하였으며,\n\n\n\n전역적으로 적용하는 HTML\n\n\n위 사진처럼 모든 페이지 하단에 연구지원 신청하기 버튼을 만들기도 하였습니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "href": "posts/2021-10-01-notionoopy/index.html#쉽게-적용하는-플러그인",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "쉽게 적용하는 플러그인",
    "text": "쉽게 적용하는 플러그인\n\n\n\n플러그인\n\n\n대부분 JS를 통해 직접 설치하는 플러그인(Google Analytics나 channel.io)또한 위 사진처럼 쉽게 추가가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "href": "posts/2021-10-01-notionoopy/index.html#클린-url",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "클린 URL",
    "text": "클린 URL\n 노션 주소를 공유할 때에는 위 사진의 복잡한 부분처럼 uuid형식으로 된 값을 사용합니다. 이는 효율적인 방법일 수 있으나, 심미적인 관점에서는 그렇지 않습니다. 또 링크를 누군가에게 공유할 때에도 링크만을 보고 대략적 내용을 유추할 수 없어 비 효율적인 면이 있습니다. oopy는 이러한 Notion의 URL을 쉽게 바꿀 수 있게 해 줍니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "href": "posts/2021-10-01-notionoopy/index.html#커스텀-html의-편의성-부재",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "커스텀 HTML의 편의성 부재",
    "text": "커스텀 HTML의 편의성 부재\nMicrosoft의 Visual Studio Code(VSCode)나 JetBrains사의 WebStorm같은 전문 프로그램은 물론이고, 최근에는 가벼원 편집기들 또한 자동 완성이나 괄호 자동 닫음 기능을 지원합니다. oopy의 커스텀 html 편집의 경우 아래의 사진과 같은 창에서 수행하게 되는데, 외부 편집기를 이용할 수 없어 Intellisence같은 기능을 사용할 수 없는 등의 단점은 있습니다. 다행히, 기초적인 오류는 경고 표시로 탐지가 가능합니다."
  },
  {
    "objectID": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "href": "posts/2021-10-01-notionoopy/index.html#부족한-seo",
    "title": "Notion으로 홈페이지 제작후 oopy로 배포한 후기",
    "section": "부족한 SEO",
    "text": "부족한 SEO\n우피는 자체적으로 SEO를 지원한다고 밝혔으나, 실제로는 Favicon과 og:image 설정, 메타태그 설정 등만이 가능하고, robots.txt는 일괄 변경만이 가능해 보여 해당 기능들의 추가가 필요합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html",
    "href": "posts/2021-09-11-googlelogin/index.html",
    "title": "Google Login",
    "section": "",
    "text": "Django allauth와 google OAuth를 이용해 zarathu 앱에 구글 로그인을 구현합니다.\nDjango admin 페이지에서 회원별 접근 권한을 관리합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "href": "posts/2021-09-11-googlelogin/index.html#allauth-설치",
    "title": "Google Login",
    "section": "allauth 설치",
    "text": "allauth 설치\npython Django에서는 쉽게 사용할 수 있는 로그인 모듈을 제공합니다. 로그인 모듈 사용을 위해 Django allauth를 설치합니다.\n\npip install Django  \npip install django-allauth \n\nDjango allauth 사용을 위해서 settings.py에 다음과 같이 추가합니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "href": "posts/2021-09-11-googlelogin/index.html#구글-클라이언트-발급",
    "title": "Google Login",
    "section": "구글 클라이언트 발급",
    "text": "구글 클라이언트 발급\nGoogle cloud platform (https://console.cloud.google.com)에서 웹 애플리케이션의 클라이언트를 발급합니다.\n자세한 방법은 https://cloud.google.com/endpoints/docs/frameworks/java/creating-client-ids?hl=ko#web-client 를 참고하여 주시기 바랍니다.\n클라이언트를 발급하면 클라이언트 ID와 보안 비밀번호를 얻게 되는데,\n이 ID와 비밀번호를 장고 admin 페이지의 소셜 어플리케이션 탭에 입력합니다.\n장고 admin 페이지는 장고 사이트 주소 뒤에 /admin 을 입력하여 접근할 수 있습니다.\n\n이제 장고 사이트에서 구글 로그인 링크를 누르면 400 오류:redirect_uri_mismatch 메시지가 뜹니다.\nGoogle cloud platform의 웹 애플리케이션 클라이언트에서 ’승인된 리디렉션 URI’에 https://domain.name/accounts/google/login/callback/ 을 입력해 주면 오류를 해결할 수 있습니다.\n\n리디렉션 uri에는 반드시 도메인 네임을 포함하여야 합니다. IP주소만으로는 실행할 수 없습니다."
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "href": "posts/2021-09-11-googlelogin/index.html#접근-권한-관리",
    "title": "Google Login",
    "section": "접근 권한 관리",
    "text": "접근 권한 관리\n장고로 배포한 페이지에서 shinyproxy로 shiny app을 배포한 주소로 링크를 연결할 것입니다.\n저희의 목표는 회원분들 개인마다 접근할 수 있는 앱을 지정해 주는 것입니다.\n이는 장고 admin 페이지와 html 파일에서 쉽게 구현할 수 있습니다.\n먼저, 장고 admin에서 다음과 같이 개인 사용자에게 그룹을 지정해줍니다.\n\n그리고 다음과 같이 링크를 연결할 페이지에서 해당 유저가 링크를 보기 위한 그룹에 속해있는지 확인하는 코드를 입력합니다.\n제 코드에서는 main.html에 입력하였습니다.\nmain.html 파일의 if 다음의 class가 해당 그룹에 속한 유저들에게만 노출됩니다.\n\n{% if request.user | has_group:\"<group>\" %}\n\n\nhas_group 함수의 내용은 다음과 같습니다.\n\nhas_group 함수는 제 코드에서는 goologin/templatetags/user_tags에 정의되어 있고 main.html에 다음 코드를 입력하여 불러옵니다.\n\n{% load user_tags %}"
  },
  {
    "objectID": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "href": "posts/2021-09-11-googlelogin/index.html#참고사항",
    "title": "Google Login",
    "section": "참고사항",
    "text": "참고사항\n포트 포워딩\n장고의 기본 포트가 8000이기 때문에 브라우저 도메인 뒤에 8000포트가 붙게 됩니다.\n다양한 해결 방법이 있지만, 여기서는 프로그램 iptables를 이용하여 포트 포워딩을 수행하는 방법을 소개합니다.\n다음 명령어를 이용하면 80포트로 들어온 신호를 8000포트로 리다이렉트할 수 있습니다.\n\niptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8000 \n\nshinyproxy 배포\n장고 사이트에서 링크로 연결할 샤이니 앱을 배포하기 위해 shinyproxy를 이용합니다.\n먼저, rstudio server에서 도커 컨테이너를 빌드합니다.\n\nsudo docker build -t <container image>\n\napplication.yml 파일의 specs: 에 컨테이너 이미지를 입력하고 shinyproxy를 구동합니다.\n\njava -jar shinyproxy-2.5.0.jar\n\napplication.yml 파일에서 port, authentication, template-path, landing-page를 건드리면 배포할 포트를 결정하거나, 사용자 인증을 만들거나, 템플릿으로 이용할 html 파일을 지정하거나, 기본 페이지를 설정할 수 있습니다.\nshinyproxy에 관한 자세한 내용은 https://shinyproxy.io 를 참고하여 주시기 바랍니다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html",
    "href": "posts/2021-04-02-shinyecrf/index.html",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 4월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실과 개발 중인 shiny 환자데이티 입력웹 개발 현황을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#요약",
    "href": "posts/2021-04-02-shinyecrf/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "요약",
    "text": "요약\n삼성서울병원 심혈관중재실 의뢰: 환자데이터 입력웹(eCRF).\n\nTychobra의 Shiny CRUD 참고해 용병 1인과 개발 중.\nshinymanager 로 로그인 모듈: 어떤 ID가 생성, 수정했는지 기록.\nDB: RSQLite 이용, 파일로 관리.\nDT 사용: proxy 기능으로 빠른 업데이트 가능. 테이블 안에 클릭(수정)버튼 삽입.\n버튼 1개 당 shiny module 1개.\n의료데이터 입력/관리/분석 통합서비스 목표."
  },
  {
    "objectID": "posts/2021-04-02-shinyecrf/index.html#slide",
    "href": "posts/2021-04-02-shinyecrf/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html",
    "title": "메타분석 웹 개발 후기",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 8월 Shinykorea 밋업에 참석, 메타분석 ShinyApps 만든 후기를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#요약",
    "title": "메타분석 웹 개발 후기",
    "section": "요약",
    "text": "요약\nhttp://app.zarathu.com/meta-analysis 에 메타분석 ShinyApps 를 공개하였다.\n\n메타분석은 여러 연구결과를 합쳐서 보여주는 분석, meta 패키지를 ShinyApps 로 구현하였다.\nrhandsontable 로 연구결과를 직접 입력한다.\n서버에 한글폰트 설치 후 showtext 로 불러와 한글깨짐을 해결한다.\ncolourpicker 로 글자색 조절한다.\nEMF 포맷 다운로드를 지원, PPT에서 직접 그림수정할 수 있다."
  },
  {
    "objectID": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "href": "posts/2020-08-22-meta-analysis-shiny/index.html#slide",
    "title": "메타분석 웹 개발 후기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/meta-analysis 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "",
    "text": "김진섭 대표는 11월 7일 동국대학교 의생명공학과 세미나에 참석, 의료 데이터분석가가 되기까지의 경험을 학생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유하며, 초청해주신 김진식 교수님께 감사드립니다."
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#요약",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 의대 진학.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립.\n\nR 과 shiny 이용, 맞춤형 분석웹 제공\n범용 통계분석 웹: http://app.zarathu.com/\nR 패키지 개발"
  },
  {
    "objectID": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "href": "posts/2019-10-29-donggukmbtlectureintro/index.html#slide",
    "title": "의료 데이터분석가 성장기: 동국대학교 의생명공학과 세미나",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/dongguk_mbt 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "",
    "text": "본 내용은 필자가 준비하던 박사논문 선형모형의 다차원공간으로의 확장 의 후속으로 계획했던 내용으로, 선형모형 벡터공간에 허수축(Imaginary Axis) 을 도입, Inverted U-shape 을 선형관계로 재해석하였습니다. 아인슈타인 일반상대성이론의 4차원 시공간 중, 시간을 허수축으로 해석하는 것에서 아이디어를 얻었습니다. 이전 내용 의 요약은 아래 슬라이드를 참고해 주세요."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#abstract",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Abstract",
    "text": "Abstract\n이전 연구 에서 저자는 선형모형을 휘어진 다차원공간으로 확장하여 \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제시하였는데, Inverted \\(U\\)-shape은 이 방법으로 표현할 수 없었다. 이에 본 연구에서는 선형모형의 무대를 Imaginary Axis를 포함한 다차원공간까지 확장하여 Inverted \\(U\\)-shaped relationship을 선형모형으로 해석하는 방법을 제안한다. 본 연구를 활용하여 Health science 연구자들이 새로운 관점에서 연구데이터를 해석할 수 있을 것이다.\nkeywords : Multidimension, Linear Model, Vector Space, Metric Tensor"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#introduction",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Introduction",
    "text": "Introduction\n이전 연구에서 선형모형을 다차원 벡터공간으로 확장한 Multidimensional Vectorized Linear Model(MDVLM)을 제안하였는데, 변수들간의 dependency를 적절하게 조절하여 기존 선형모형에서 정확히 표현하기 어려웠던 관계를 표현할 수 있었다(Jinseob Kim 2017). 특히 MDVLM을 통해 점점 느리게 감소하는 혹은 점점 빠르게 증가하는 모양을 표현할 수 있는데, 이는 MDVLM의 표현식 \\(Y^2= \\beta_1^2X_1^2 + 2r\\beta_1\\beta_2X_1X_2+ \\beta_2^2X_2^2 = (\\beta_1X_1+r\\beta_2X_2)^2 + (1-r^2)\\beta_2^2X_2^2\\)에서 \\(X_1\\)과 \\(Y\\)의 관계가 쌍곡선 모양을 보이기 때문이다(Figure 1).\n\n\n\n\nExpressable relationship in conventional linear model and MDVLM\n\n\n\n\n이제 자연스러운 질문이 생긴다. “MDVLM은 점점 빠르게 감소하는 혹은 느리게 증가하는 관계를 표현할 수 있을까?” \\(Y^2=X_1^2 - X_2^2\\)의 간단한 예를 통해 이를 살펴보도록 하자.\nLinear relationship can’t explained via MDVLM\n\\(Y^2 = X_1^2 - X_2^2\\)의 그래프(\\(X, Y > 0\\))를 살펴보면 \\(Y\\)와 \\(X_2\\)는 타원모양(이 예시에서는 원)으로 감소하는 관계를 보이고 MDVLM으로 잘 표현할 수 없다(Figure 2(a)). MDVLM은 직선 혹은 쌍곡선 모양만 표현할 수 있기 때문이다.\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n\n즉, \\(Y^2 = X_1^2 - X_1^2\\) 간단한 표현임에도 불구하고, 선형모형을 벡터공간으로 일반화한 MDVLM으로도 잘 표현될 수 없다. 그렇다면 이것은 선형모형이 아닌 것일까? 분명히 점점 빠르게 감소하는 타원 모양은 선형관계가 아니다. 그러나 \\(X_2^2\\)이 1 증가할 때 마다 \\(Y^2\\)가 정확히 1이 감소하는 관계가 있는 것도 사실이다(Figure 2(b)). 이 또한 선형관계라고 할 수 있는 것 아닐까?\n이를 확인해 보기 위해 MDVLM에서와 마찬가지로 다차원 벡터공간에서 \\(Y^2 = X_1^2 - X_2^2\\)을 표현해보자.\n\\[\\vec{Y} = \\vec{X_1} + \\vec{X_2}\\]\n\\(\\vec{X_1}\\)와 \\(\\vec{X_2}\\)가 독립된 axis를 갖고 있다면 \\(Y^2 = \\vec{Y}\\cdot\\vec{Y} = X_1^2 + X_2^2\\)을 얻는다. 한편 \\(\\vec{X_2}\\)가 허수축(imaginar axis)를 갖고 있다고 생각하면 아래와 같이 \\(Y^2 = X_1^2 - X_2^2\\)을 얻는다.\n\\[Y^2 = \\vec{Y}\\cdot\\vec{Y} =  X_1^2 + (iX_2)^2 = X_1^2 - X_2^2\\]\n이 경우에도 \\(X_1\\)이 고정된 상태에서는 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}= d\\vec{X_2}\\)가 성립하며, 방향과 허수축을 고려했을 때 \\(dY\\)와 \\(dX_2\\)은 선형관계가 있다고 할 수 있다.\n허수 \\(i\\)는 실제로 존재하는 것이 아니지만 \\(i^2 = -1\\)임을 이용해서 내적이 음수인 허수축을 상상할 수 있고, 선형관계를 허수축을 포함한 벡터공간으로 확장할 수 있다. 허수축의 활용은 물리학에서 많이 볼 수 있는데, 대표적으로 아인슈타인의 특수상대성이론에서는 time coordinate를 허수축으로 두고 4차원 시공간(Minkowski space)에서의 거리 \\(ds\\)를 아래와 같이 정의한다(Corry 1997).\n\\[(ds)^2 = (dx)^2 + (dy)^2 + (dz)^2 + (idt)^2 = (dx)^2 + (dy)^2 + (dz)^2 - (dt)^2\\]\n일반적으로 \\(p\\)개의 실수축과 \\(q\\)개의 허수축으로 구성된 manifold를 pseudo-Riemannian manifolds라 하고 거리 \\(g\\)는 아래와 같이 정의한다(Kulkarni 1981).\n\\[g = dx_1^2 + \\cdots + dx_p^2 - dx_{p+1}^2 - \\cdots - dx_{p+q}^2\\]\n점점 느리게 증가하는 타원모양도 마찬가지로 MDVLM으로는 표현하기 어려우며 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 원을 예로 들 수 있다(Figure 3(a), 3(b)).\n\n\n\n\nRelation between \\(X_2\\) and \\(Y\\) by \\(X_1\\): \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n제안: MDVLM with Imaginary Axis\n지금까지의 고찰을 토대로 본 연구에서는 허수축을 포함한 다차원 벡터공간에서 선형관계를 표현하는 MDVLM with Imaginary Axis(MDVLM-IA)를 제안한다. 이것은 기존의 MDVLM에 Imaginary Axis의 개념을 추가하여 일반화한 것인데, MDVLM의 개념을 간단하게 리뷰한 후 여기에 허수축을 추가하여 본 연구의 모형과 추정방법을 설명하겠다. 그 후 몇 가지 example을 통해 이것이 어떻게 활용되는지 살펴볼 것이다."
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#formula",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Formula",
    "text": "Formula\n이전에 연구했던 MDVLM에 대해 간단히 리뷰를 한 후, 이것을 확장하여 본 연구의 모형과 추정을 설명하도록 하겠다.\nBrief Review of MDVLM\n음이 아닌 실수 \\(Y\\)와 \\(X_1\\), \\(X_2\\),\\(\\cdots\\), \\(X_n\\) 들의 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ (\\beta_2X_2+\\beta_{02})\\vec{e_2} + \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p}\\]\n이 때, \\(\\vec{e_i}\\)들은 \\(X_i\\)방향으로의 단위벡터로서 크기는 모두 1이며, \\(X_i\\)와 \\(X_j\\)가 완전히 독립적인 정보라면 \\(\\vec{e_i}\\cdot\\vec{e_j} = 0\\)이고 일반적으로 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)의 값은 0에서 1까지의 값을 갖는다.\n한편 \\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\beta_2dX_2\\vec{e_2} + \\cdots + \\beta_pdX_p\\vec{e_p}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고, 기존의 선형모형의 해석에 vector의 개념만 추가된 것이다.\n\\(\\beta\\)값들의 추정은 다음과 같은 스칼라식을 토대로 이루어진다.\n\\[\n\\begin{aligned}\nY^2 = (\\vec{Y})\\cdot(\\vec{Y}) &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{i0})\\vec{e_i}) \\\\\n&=\\sum_{i=1}^p(\\beta_iX_i+\\beta_{i0})^2 + 2\\sum_{i<j}r_{ij}(\\beta_iX_i+\\beta_{i0})(\\beta_jX_j+\\beta_{j0})\\\\\n\\end{aligned}\n\\]\n이제 최소제곱추정을 위한 오차제곱의 합(Sum of Squared Error: SSE)을 다음과 같이 정의하면, 기존 선형모형의 최소제곱추정을 자연스럽게 확장한 것이 된다.\n\\[SSE(\\beta) = \\sum_{k=1}^N (Y_k - \\sqrt{\\sum_{i=1}^n(\\beta_iX_{ki}+\\beta_{i0})^2 + 2\\sum_{i<j}r_{ij}(\\beta_iX_{ki}+\\beta_{i0})(\\beta_jX_{kj}+\\beta_{j0})})^2\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value)\n예를 들어 \\(r_{ij}\\)가 전부 1이라면 \\(SSE(\\beta) = \\sum_{k=1}^N (Y_k- \\beta_0 -\\beta_1X_{k1} - \\beta_2X_{k2} - \\cdots - \\beta_pX_{kp})^2\\)로 기존 선형모형의 최소제곱추정과 동일한 것을 확인할 수 있다.\n\\(SSE(\\beta)\\)를 최소로 하는 \\(\\beta\\)값은 optimization technique를 이용하며, Nelder-Mead, BFGS, CG, L-BFGS-B 등 다양한 방법이 있다Nelder and Mead (1965).\nMDVLM with Imaginary Axis\nMDVLM에서 허수축을 갖는 \\(X_{p+1}\\),\\(\\cdots\\),\\(X_{p+q}\\)를 추가하여 선형관계를 벡터공간에서 표현하면 아래와 같다.\n\\[\\vec{Y} = (\\beta_1X_1+\\beta_{01})\\vec{e_1}+ \\cdots + (\\beta_pX_p+\\beta_{0p})\\vec{e_p} + (\\beta_{p+1}X_{p+1}+\\beta_{0(p+1)})\\vec{e_{p+1}} + \\cdots + (\\beta_{p+q}X_{p+q}+\\beta_{0(p+q)})\\vec{e_{p+q}}\\]\n\\(\\vec{e_1},\\cdots,\\vec{e_p}\\)들은 실수축을 가진 단위벡터로서 자기자신과의 내적값인 \\(\\vec{e_i}\\cdot\\vec{e_i}\\)의 값이 1 이다. 반면 \\(\\vec{e_{p+1}},\\cdots,\\vec{e_{p+q}}\\)는 허수축을 가진 단위벡터로서 자기자신과의 내적값은 -1이다. \\(1\\le i, j \\le p\\)일 때는 \\(r_{ij} = r_{ji} = \\vec{e_i}\\cdot\\vec{e_j}\\)가 0에서 1까지의 값을 갖으며, \\(p+1\\le i, j \\le p+q\\)라면 -1에서 0까지의 값을 갖고, 그 외에는 \\(r_{ij}=0\\)이다. 즉 Axis들의 dependency는 실수축끼리, 혹은 허수축끼리만 정의한다.\n\\(\\vec{Y}\\)의 변화량 \\(d\\vec{Y}\\)는\n\\[d\\vec{Y} = \\beta_1dX_1\\vec{e_1} + \\cdots + \\beta_pdX_p\\vec{e_p} + \\beta_{p+1}dX_{p+1}\\vec{e_{p+1}} + \\cdots + \\beta_{p+q}dX_{p+q}\\vec{e_{p+q}}\\]\n이며 \\(\\dfrac{\\partial\\vec{Y}}{\\partial X_i} = \\beta_i\\vec{e_i}\\)가 된다. 이는 \\(X_i\\)만 변하고 나머지는 고정되어 있을 때 \\(\\vec{Y}\\)는 \\(X_i\\)의 방향(\\(\\vec{e_i}\\))으로 \\(\\beta_i\\)만큼 증가한다고 해석할 수 있고 이는 MDVLM에서의 해석과 동일하다.\n추정을 위한 스칼라 관계식은 MDVLM때와 비슷하게 아래와 같이 표현할 수 있다.\n$$\n\\[\\begin{aligned}\nY^2 &= (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i})\\cdot (\\sum_{i=1}^{p}(\\beta_iX_i+\\beta_{0i})\\vec{e_i} + \\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{1\\le i<j \\le p}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_i+\\beta_{0i})^2 + \\sum_{p< i<j\\le p+q}2r_{ij}(\\beta_iX_i+\\beta_{0i})(\\beta_jX_j+\\beta_{0j}))\\\\\n\\end{aligned}\\]\n\\[ $Y$의 값의 변화량 $dY$는 \\]\n\\[\\begin{aligned}\n(dY)^2 &= (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i})\\cdot (\\sum_{i=1}^{p}\\beta_idX_i\\vec{e_i} + \\sum_{i=p+1}^{p+q}\\beta_idX_i\\vec{e_i}) \\\\\n&=(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 + \\sum_{1 \\le i<j \\le p}2r_{ij}\\beta_i\\beta_jdX_idX_j) - (\\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2 + \\sum_{p < i<j \\le p+q}2r_{ij}\\beta_i\\beta_jdX_idX_j)\\\\\n\\end{aligned}\\]\n$$\n이고, 모든 \\(r_{ij}\\)들이 0이라면 \\(\\sum_{i=1}^p\\beta_i^2(dX_i)^2 - \\sum_{i=p+1}^{p+q}\\beta_i^2(dX_i)^2\\)로 간단히 표현할 수 있다.\n최소제곱 추정을 위한 \\(SSE(\\beta)\\)값도 비슷하게 정의할 수 있으며 추정방법은 MDVLM의 경우와 같이 optimization technique를 이용한다.\n\\[\n\\begin{aligned}\nf(\\beta,X_k) &= (\\sum_{i=1}^p(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{1\\le i<j \\le p}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) - (\\sum_{i=p+1}^{p+q}(\\beta_iX_{ki}+\\beta_{0i})^2 + \\sum_{p< i<j\\le p+q}2r_{ij}(\\beta_iX_{ki}+\\beta_{0i})(\\beta_jX_{kj}+\\beta_{0j})) \\\\\nSSE(\\beta) &= \\sum_{k=1}^N (Y_k - \\sqrt{f(\\beta,X_k)})^2\n\\end{aligned}\n\\]\n(\\(Y_k, X_{ki}\\): \\(k\\)th individual’s \\(Y, X_{i}\\) value, \\(X_k= (X_{k1},\\cdots,X_{k(p+q)})\\))"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#apply-to-data",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Apply to Data",
    "text": "Apply to Data\n앞서 언급했던 \\(Y^2 = X_1^2 - X_2^2\\)과 \\(Y^2 = X_1^2 - (5-X_2)^2\\)인 경우의 데이터에 적용해보도록 하겠다. 모든 계산은 R 3.3.3의 optim 함수를 이용하였다.\nExample 1: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\\(Y^2 = X_1^2 - X_2^2\\)인 양수 \\(Y, X_1, X_2\\)의 쌍을 100번 random sampling 해서 MDVLM과 본 연구의 모형을 비교해 보았다.\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - X_2^2\\): Dependency and MSE\n\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - X_2^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-0.224 + 1.163X_1 -0.634X_2)^2\\)\n\\(Y^2 = X_1^2 -X_2^2\\)\n\n\nMSE\n0.176\n0\n\n\n\n\n\n실제로 \\(Y^2\\)과 \\(X_1^2, X_2^2\\)의 값을 이용해서 선형모형으로 추정하면 정확한 추정 결과를 얻는다. 그러나 제곱한 값이 아닌 원래값을 이용해서 선형모형에 적용하면 \\(Y =\\) \\(-0.224\\) \\(+\\) \\(1.163\\) \\(X_1 +\\) \\(-0.634\\) \\(X_2\\)이 되어 정확한 추정을 얻지 못하고, MDVLM으로 확장해도 이보다 정확한 추정은 얻을 수 없다.\nExample 2: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\nMDVLM in \\(Y^2 = X_1^2 - (5-X_2)^2\\): Dependency and MSE\n\n\n\n\n\n\n\nResult comparison: \\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\n\n\n\n\n\n\nBest Scenario of MDVLM\nMDVLM-IA\n\n\n\nFormula\n\\(Y^2 = (-2.78 + 1.079X_1 + 0.567X_2)^2\\)\n\\(Y^2 = X_1^2 - (5-X_2)^2\\)\n\n\nMSE\n0.047\n0"
  },
  {
    "objectID": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "href": "posts/2019-08-14-mdlmwithimaginaryaxes/index.html#discussion",
    "title": "선형모형의 다차원공간으로의 확장(2): 허수축 도입",
    "section": "Discussion",
    "text": "Discussion\n예상대로 \\(Y\\)가 빨리 감소하거나 천천히 증가하는 타원모양의 관계는 MDVLM으로 잘 표현할 수 없었으며, 허수축을 활용했을 때 정확히 표현할 수 있었다.\n본 연구가 Health Status를 설명하기 위해 처음으로 허수 \\(i\\)의 개념을 이용하였다는 의의가 있다. 본 연구에서는 \\(i^2 = -1\\)이라는 특징을 이용해서 타원모양을 허수축에서의 선형관계로 재해석하여 MDVLM보다 더 확장된 선형모형을 제시하였는데, 이를 통해 연구자들이 선형관계라는 직관적 해석을 잃지 않으면서 지금보다 훨씬 다양하게 건강현상을 설명할 수 있을 것이라 확신한다.\n허수 \\(i\\)의 또하나의 큰 특징은 복소수 표현을 통해 실수체계를 확장할 수 있다는 것인데, 이것을 적극적으로 활용한 분야 중 하나가 양자역학이다. 양자역학의 대표적인 방정식인 슈뢰딩거 방정식(Schrödinger equation)은 입자의 운동은 확률로 기술되고 그 확률은 파동처럼 행동한다는 내용인데 파동을 기술하는 함수가 복소수로 표현되어 있다는 것이 특징이며, 복소수가 포함된 파동함수 그 자체로는 실제 세계를 해석하기 어렵지만 켤레복소수와의 곱을 통해 확률을 표현할 수 있다. 양자역학이 미시세계의 현상을 설명하는 새로운 방법이 된 것과 마찬가지로 확률을 복소수를 포함한 파동함수로 표현하는 방법이 향후 Health science에서 건강상태를 설명하는 새로운 방법이 될 수 있을 것이라 예상한다.\n본 연구를 시작으로 향후 Health science에서 복소수를 활용한 모형이 활발히 제안되길 기대한다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html",
    "href": "posts/2021-07-25-googleauth/index.html",
    "title": "googleAuth",
    "section": "",
    "text": "7월 17일 Virtual Box를 이용해 ShinyProxy 환경을 구축하는 강의가 있었습니다. 강의 시간에 직접 해보지 못한 Social Login 중 Google Authentication 방법에 대해 알아보겠습니다."
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#요약",
    "href": "posts/2021-07-25-googleauth/index.html#요약",
    "title": "googleAuth",
    "section": "요약",
    "text": "요약\n\n지난 Lecture에서 구축한 환경설정 간단하게 리뷰\nApplication.yml을 project folder에 생성하여 Social login을 사용하도록 설정\nGoogle Developer Console에서 OAuth 설정을 통해 Client Id, Key 발급과 Redirection URL 설정"
  },
  {
    "objectID": "posts/2021-07-25-googleauth/index.html#contents",
    "href": "posts/2021-07-25-googleauth/index.html#contents",
    "title": "googleAuth",
    "section": "Contents",
    "text": "Contents\nReview of previous lecture\n\n\nShinyManager (https://datastorm-open.github.io/shinymanager/)\n\nShiny App의 Authentication service 제공\nSigle thread 기반으로 구성되기 때문에 동시에 여러 유저가 로그인 불가\n\n\n\nShinyProxy (https://www.shinyproxy.io/)\n\nShinyManager를 사용할 때 동시에 하나의 유저만 로그인할 수 있는 한계를 극복하기 위해 도입\n여러 사용자의 동시 접근을 가능한 Shiny App deploy\n\n\n\nGoogle OAuth 설정으로 넘어가기 전에…\n\n강의시간에 만든 Virtual box의 가상이미지 실행 후 shinyproxy docker 이미지가 실행되고 있는지 확인\n\n\nsudo docker images | grep shinyproxy\n\n\n\nRstudio server 로그인 후 Terminal에서 shinyproxy 실행\n\n브라우저에서 127.0.0.1:8787로 접속 후 Virtual Box 이미지 생성 시 입력했던 아이디와 패스워드로 로그인\nRstudio Terminal에서 shinyproxy 실행\n\n\njava -jar shinyproxy-2.5.0.jar\n\n\n\n127.0.0.1:8080으로 접속하여 admin 계정을 통해 login 기능 작동 확인\n\n\n\n\n\n\nGoogle OAuth\n\nFacebook, Github 등 다양한 소셜 로그인 중 하나\nGoogle Developer Console https://console.cloud.google.com/apis/에서 프로젝트 생성 후 Client Id와 Client Key 발급\n발급받은 Id, Key를 application.yml 파일에 입력\nGoogle Devloper Console에서 로그인 후 redirection할 URL을 입력하면 끝!\n구체적인 과정은 네이버 블로그를 따라하세요\nShinyProxy Setting\n\napplication.yml : Authentication 방법을 정의하는 파일로 simple, LDAP, openid, social 등의 방법 선택 가능 (authentication: {원하는 로그인 방식})\n\nRStudio server에서 홈 디렉토리에 application.yml 파일 생성(아래 코드를 복사하세요)\n\nproxy:\n  title: Shiny App\n  logo-url: https://www.openanalytics.eu/shinyproxy/logo.png\n  landing-page: /\n  heartbeat-rate: 10000\n  heartbeat-timeout: 60000\n  port: 8080\n  authentication: social\n  admin-groups: scientists\n  openid:\n    auth-url: https://accounts.google.com/o/oauth2/v2/auth\n    token-url: https://www.googleapis.com/oauth2/v4/token\n    jwks-url: https://www.googleapis.com/oauth2/v3/certs\n    client-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n    client-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n    # Docker configuration\n  social:\n    google:\n      app-id:     528600301937-aic4b7n55ka3ac9he6g4d67fb6cdrkc6.apps.googleusercontent.com\n      app-secret: x7KIiNvJcwh4cl2eouPJ3IiS\n  docker:\n    url: http://localhost:2375\n    port-range-start: 20000\n  specs:\n  - id: 01_hello\n    display-name: Hello Application\n    description: Application which demonstrates the basics of a Shiny app\n    container-cmd: [\"R\", \"-e\", \"shinyproxy::run_01_hello()\"]\n    container-image: openanalytics/shinyproxy-demo\n    access-groups: [scientists, mathematicians]\n  - id: 06_tabsets\n    container-cmd: [\"R\", \"-e\", \"shinyproxy::run_06_tabsets()\"]\n    container-image: openanalytics/shinyproxy-demo\n    access-groups: scientists\n\nlogging:\n  file:\n    shinyproxy.log\n\n\n\n\nauthenticaiton을 openid로 설정할 경우\n\n\n위 코드의 openid 부분과 동일하게 auth, token, jws url 작성\nclient-id와 client-secret을 Google Developer Console에서 확인 후 입력\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/login/oauth2/code/shinyproxy\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행 (localhost:8080로 접근하기 위해서는 redirect url에 http://localhost:8080/login/oauth2/code/shinyproxy 등록)\n\n\nauthentication을 social로 설정할 경우\n\n\n위 코드의 social 부분과 동일하게 작성\nGoogle Developer Console에서 승인된 redirect url에 다음을 입력 http://127.0.0.1:8080/signin/google\nRStudio server Terminal에서 실행중인 shinyproxy 종료(Ctrl + C) 후 재실행\n브라우저에 127.0.0.1:8080을 입력 후 google login 수행\n마무리\n\nShinyProxy는 Spring framework 기반으로 작성되어 application.yml 수정을 통해 여러 인증방법을 적용할 수 있습니다.\nFacebook, Github 등 여러 API provider들이 구글과 비슷한 인증 서비스를 제공하고 있어 손쉽게 여러 social 인증을 추가할 수 있습니다.\n발급받은 client id와 secret은 외부로 노출되어서는 안됩니다(Github Repo에 commit 금지!)\nsocial과 openid 모두 적용할 수 있으나 redirect url에 차이가 있음에 주의 (정확한 차이는 연구 필요)"
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "",
    "text": "김진섭 대표는 2월 27일(수) Anpanman이 후원하는 Shinykorea 밋업에 참석, ShinyApps를 Rstudio Addins을 포함한 패키지로 만들어 CRAN에 배포신청한 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#요약",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "요약",
    "text": "요약\n개인 PC에서 직접 ShinyApps를 이용할 수 있도록\n\nShinyApps을 Rstudio Addins으로 만든 후, 이를 패키지로 만들어 github에 배포하였다.\ntestthat, covr로 코드 테스트를 수행하고 결과 리포트를 만들었으며, pkgdown으로 패키지를 소개하는 웹사이트를 만들었다.\nTravis CI와 appveyor로 2의 과정과 여러 운영체제에서의 테스트를 자동화하였다.\n최종적으로 CRAN에 패키지를 배포 신청했으나 거절되었다. 코멘트를 반영하여 재심사 중이다."
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#package",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Package",
    "text": "Package\nhttps://github.com/jinseob2kim/jsmodule"
  },
  {
    "objectID": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "href": "posts/2019-02-10-from-shiny-to-rpackage/index.html#slide",
    "title": "ShinyApps를 R 패키지로 만들기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html",
    "href": "posts/2021-08-19-pubicdatawithr/index.html",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "",
    "text": "김진섭 대표는 9월 12일 “대한상부위장관 · 헬리코박터학회 주관 2021 위원회 워크숍” 에 참석, R 활용 웹기반으로 공공빅데이터 분석지원한 경험을 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#요약",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "요약",
    "text": "요약\nR 로 보험공단/심평원 빅데이터 분석가능.\n\n공단표본코호트 V1 은 내부 분석환경 구축\n공단표본코호트 V2, 심평원 데이터는 원격 분석환경 이용\n대용량데이터 위한 R 패키지: data.table, fst, parallel\n\n자체개발 R 패키지 CRAN 배포, 원격분석환경에서도 이용가능.\n\nKaplan-meier 그림: jskm\n논문용 테이블: jstable\nGUI 분석: jsmodule\n\nShiny 로 웹기반 실시간 분석서비스.\n\n내부 분석환경: 웹에서 실시간 분석수행\n원격 분석환경: 모든 분석결과 반출 후 웹기반 시각화\nExcel/PPT 다운로드\n\nCDM 다기관 메타분석 서비스\n\nTable 1 합치기, Forest/Funnel plot 등"
  },
  {
    "objectID": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "href": "posts/2021-08-19-pubicdatawithr/index.html#slide",
    "title": "R 활용 공공빅데이터 분석지원",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/publicdata_with_R 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html",
    "href": "posts/2020-10-29-survivalpractice/index.html",
    "title": "생존분석 실습",
    "section": "",
    "text": "김진섭 대표는 성균관의대 사회의학교실 김종헌 교수님 수업에 참가, Kaplan-meier curve, 비례위험가정 및 모형적합도, Time-dependent covariate 그리고 모수적 생존분석을 중심으로 R 코드를 실습할 예정입니다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#요약",
    "href": "posts/2020-10-29-survivalpractice/index.html#요약",
    "title": "생존분석 실습",
    "section": "요약",
    "text": "요약\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "href": "posts/2020-10-29-survivalpractice/index.html#kaplan-meier-plot",
    "title": "생존분석 실습",
    "section": "Kaplan-meier plot",
    "text": "Kaplan-meier plot\nKaplan-meier plot 은 R 기본 plot에서도 제공하지만, survminer 패키지의 ggsurvplot 함수에서 다양한 옵션을 제공한다. 본 실습에서는 본사가 개발한 jskm 패키지의 jskm 함수를 survival 패키지 내장 데이터 veteran 에 적용하겠다. 우선 패키지를 불러온 후 survfit 으로 구간별 생존율을 구하자.\n\nlibrary(DT);library(survival);library(jskm)\ndatatable(veteran, rownames = F, caption = \"Example data\", options = list(scrollX = T))\n\n\n\n\n\nsfit <- survfit(Surv(time, status) ~ trt, data = veteran)\nsummary(sfit, times = c(100, 200, 300, 365), extend = T)\n\nCall: survfit(formula = Surv(time, status) ~ trt, data = veteran)\n\n                trt=1 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     34      34   0.5020  0.0606       0.3962        0.636\n  200     12      19   0.1947  0.0501       0.1176        0.322\n  300      5       6   0.0885  0.0371       0.0390        0.201\n  365      4       1   0.0708  0.0336       0.0279        0.180\n\n                trt=2 \n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  100     21      45    0.333  0.0578       0.2367        0.467\n  200     13       7    0.216  0.0517       0.1354        0.345\n  300      8       4    0.146  0.0454       0.0797        0.269\n  365      6       2    0.110  0.0407       0.0530        0.227\n\n\ntrt 1 은 “Standard”, 2 는 “Test” 이며 jskm 을 적용하면 아래와 같다.\n\njskm(sfit)\n\n\n\n\n\n\n\n라벨을 수정하고, risk table 과 log-rank p-value 를 추가하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T)\n\n\n\n\n\n\n\n십자가 무늬는 실제 censoring 이 발생한 부분이며 mark = F 로 숨길 수 있다. 생존율이 아닌 누적발생률을 %로 보는 코드는 아래와 같다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cumhaz = T, surv.scale = \"percent\" )\n\n\n\n\n\n\n\np-value 위치는 pval.coord legend 위치는 legendposition 옵션을 이용한다. 선을 흑백으로 바꾸려면 linecols = \"black\" 을 추가한다. legendposition 은 x,y 값 모두 0~1 scale 임을 주의하자.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, pval.coord = c(100, 0.1), legendposition = c(0.85, 0.6), linecols = \"black\")\n\n\n\n\n\n\n\n마지막으로 특정 시간을 기준으로 나누어보는 landmark analysis 옵션을 소개한다.\n\njskm(sfit, ystrataname = \"Treat\", ystratalabs = c(\"Standard\", \"Test\"), table = T, pval = T, \n     marks = F, cut.landmark = 365)\n\nWarning: Removed 2 row(s) containing missing values (geom_path)."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "href": "posts/2020-10-29-survivalpractice/index.html#연속변수의-최적-cut-off-구하기",
    "title": "생존분석 실습",
    "section": "연속변수의 최적 cut-off 구하기",
    "text": "연속변수의 최적 cut-off 구하기\nmaxstat 패키지를 이용한다.\n\nlibrary(maxstat)\nmtest <- maxstat.test(Surv(time, status) ~ karno, data = veteran, smethod = \"LogRank\")\nmtest\n\n\nMaximally selected LogRank statistics using none\n\ndata:  Surv(time, status) by karno\nM = 4.6181, p-value = NA\nsample estimates:\nestimated cutpoint \n                40 \n\ncut <- mtest$estimate\nveteran$karno_cat <- factor(as.integer(veteran$karno >= cut))\n\nsfit2 <- survfit(Surv(time, status) ~ karno_cat, data = veteran)\njskm(sfit2, ystrataname = \"Karno\", ystratalabs = paste(c(\"<\", \"≥\"), cut), table = T, pval = T)"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "href": "posts/2020-10-29-survivalpractice/index.html#비례위험가정-확인",
    "title": "생존분석 실습",
    "section": "비례위험가정 확인",
    "text": "비례위험가정 확인\nLogrank test, Cox model 로 추정할 때 비례위험을 가정하므로 이것이 깨지면 큰일이다. 본 글에서는 비례위험가정을 확인하는 그림 2개와 테스트를 소개한다. 자세한 내용은 https://3months.tistory.com/357?category=743476 를 참고하기 바란다.\nLog-log plot\n\\(\\log(t)\\) 와 \\(\\log(-\\log(S(t)))\\) 관계를 그림으로 보는 방법이다. 왜 로그를 이용하는지는 모수적 생존분석에서 이야기하겠다.\n\nplot(sfit, fun=\"cloglog\", lty=1:2, col=c(\"Black\", \"Grey50\"), lwd=2, font.lab=2, main=\"Log-log KM curves by Treat\", \n     ylab=\"log-log survival\", xlab=\"Time (log scale)\")\nlegend(\"bottomright\",lty=1:2,legend=c(\"Standard\", \"Test\"), bty=\"n\", lwd=2, col=c(\"Black\", \"Grey50\"))\n\n\n\n\n\n\n\n두 선이 평행한지 확인하면 되고 직선인지 곡선인지는 상관없다. 모수적 생존분석에서 다룰 weibull 모형에서는 직선인지도 확인해야 한다.\nObserved-expected plot\n비례위험을 가정하는 cox model 예상과 비교하는 방법이다.\n\nplot(sfit, lty=\"dashed\", col=c(\"Black\", \"Grey50\"), lwd=2, font=2, font.lab=2, main=\"Observed Versus Expected Plots by Treat\", \n     ylab=\"Survival probability\", xlab=\"Time\")\npar(new = T)\n\n#expected\nexp <- coxph(Surv(time, status) ~ trt, data = veteran)\nnew_df <- data.frame(trt = c(1, 2))\nkmfit.exp <- survfit(exp, newdata = new_df)\nplot(kmfit.exp, lty = \"solid\", col=c(\"Blue\", \"Red\"), lwd=2, font.lab=2)\n\n\n\n\n\n\n\nGoodness of fit\ncox.zph 함수로 통계검정을 수행한다.\n\ncox.zph(exp)\n\n       chisq df    p\ntrt     3.54  1 0.06\nGLOBAL  3.54  1 0.06\n\nplot(cox.zph(exp), var = \"trt\")\nabline(h = 0, lty = 3)\n\n\n\n\n\n\n\n선이 시간 상관없이 일정할수록, 즉 x축과 평행할수록 비례위험가정을 만족한다고 판단한다. 위 그림은 x축과 평행은 아니지만 경향성이 있다고 볼수도 없는 애매한 느낌이며 p 는 0.06 이다."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "href": "posts/2020-10-29-survivalpractice/index.html#모형-비교",
    "title": "생존분석 실습",
    "section": "모형 비교",
    "text": "모형 비교\nCox 모형에서 얻은 log-likelihood 값으로 여러 모형을 비교할 수 있다. 모형들은 n수가 전부 동일 해야 비교 가능하므로, 에러 나올땐 먼저 결측치를 확인하자.\n\nexp$loglik\n\n[1] -505.4491 -505.4442\n\nexp2 <- coxph(Surv(time, status) ~ trt + age, data = veteran)\nexp3 <- coxph(Surv(time, status) ~ trt + age + celltype, data = veteran)\n\nanova(exp, exp2, exp3)\n\nAnalysis of Deviance Table\n Cox model: response is  Surv(time, status)\n Model 1: ~ trt\n Model 2: ~ trt + age\n Model 3: ~ trt + age + celltype\n   loglik   Chisq Df P(>|Chi|)    \n1 -505.44                         \n2 -505.14  0.6162  1    0.4325    \n3 -492.43 25.4161  3 1.264e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nstep 함수를 이용, AIC 기반 최적 모형을 고를 수 있다. scope 옵션으로 빠지면 안 될 변수를 미리 정한다.\n\nstep(exp3, scope = list(lower = ~ 1))\n\nStart:  AIC=994.86\nSurv(time, status) ~ trt + age + celltype\n\n           Df     AIC\n- age       1  993.04\n- trt       1  993.65\n<none>         994.86\n- celltype  3 1014.27\n\nStep:  AIC=993.04\nSurv(time, status) ~ trt + celltype\n\n           Df     AIC\n- trt       1  992.05\n<none>         993.04\n- celltype  3 1012.89\n\nStep:  AIC=992.05\nSurv(time, status) ~ celltype\n\n           Df     AIC\n<none>         992.05\n- celltype  3 1010.90\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ celltype, data = veteran)\n\n                    coef exp(coef) se(coef)     z        p\ncelltypesmallcell 1.0013    2.7217   0.2535 3.950 7.83e-05\ncelltypeadeno     1.1477    3.1510   0.2929 3.919 8.90e-05\ncelltypelarge     0.2301    1.2588   0.2773 0.830    0.407\n\nLikelihood ratio test=24.85  on 3 df, p=1.661e-05\nn= 137, number of events= 128"
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "href": "posts/2020-10-29-survivalpractice/index.html#time-dependent-analysis",
    "title": "생존분석 실습",
    "section": "Time-dependent analysis",
    "text": "Time-dependent analysis\n자세한 내용은 https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf 를 참고하기 바란다.\n비례위험가정 깨졌을 때 (time-dependent coefficients)\n어떤 공변량이 비례위험가정을 만족하지 않을 경우, 먼저 survSplit 으로 time 을 쪼개 몇 개의 그룹으로 나눈다.\n\nvet2 <- survSplit(Surv(time, status) ~ ., data = veteran, cut=c(90, 180), episode = \"tgroup\", id = \"id\")\ndatatable(vet2, rownames = F, caption = \"Time split data\", options = list(scrollX = T))\n\n\n\n\n\n\n이제 공변량의 계수를 시간그룹 별로 따로 구한다.\n\nvfit2 <- coxph(Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), data=vet2)\nsummary(vfit2)\n\nCall:\ncoxph(formula = Surv(tstart, time, status) ~ trt + prior + karno:strata(tgroup), \n    data = vet2)\n\n  n= 225, number of events= 128 \n\n                                  coef exp(coef)  se(coef)      z Pr(>|z|)    \ntrt                          -0.011025  0.989035  0.189062 -0.058    0.953    \nprior                        -0.006107  0.993912  0.020355 -0.300    0.764    \nkarno:strata(tgroup)tgroup=1 -0.048755  0.952414  0.006222 -7.836 4.64e-15 ***\nkarno:strata(tgroup)tgroup=2  0.008050  1.008083  0.012823  0.628    0.530    \nkarno:strata(tgroup)tgroup=3 -0.008349  0.991686  0.014620 -0.571    0.568    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                             exp(coef) exp(-coef) lower .95 upper .95\ntrt                             0.9890      1.011    0.6828    1.4327\nprior                           0.9939      1.006    0.9550    1.0344\nkarno:strata(tgroup)tgroup=1    0.9524      1.050    0.9409    0.9641\nkarno:strata(tgroup)tgroup=2    1.0081      0.992    0.9831    1.0337\nkarno:strata(tgroup)tgroup=3    0.9917      1.008    0.9637    1.0205\n\nConcordance= 0.725  (se = 0.024 )\nLikelihood ratio test= 63.04  on 5 df,   p=3e-12\nWald test            = 63.7  on 5 df,   p=2e-12\nScore (logrank) test = 71.33  on 5 df,   p=5e-14\n\n\n반복측정 공변량이 있을 때\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6015946/pdf/atm-06-07-121.pdf 예제를 이용하였다.\n\nlibrary(survsim)\n\nLoading required package: eha\n\n\nLoading required package: statmod\n\nN=100 #number of patients\nset.seed(123)\ndf.tf<-simple.surv.sim(#baseline time fixed\n n=N, foltime=500,\n dist.ev=c('llogistic'),\n anc.ev=c(0.68), beta0.ev=c(5.8),\n anc.cens=1.2,\n beta0.cens=7.4,\n z=list(c(\"unif\", 0.8, 1.2)),\n beta=list(c(-0.4),c(0)),\n x=list(c(\"bern\", 0.5),\n c(\"normal\", 70, 13)))\n\nfor (v in 4:7){\n  df.tf[[v]] <- round(df.tf[[v]])\n}\n\nnames(df.tf)[c(1,4,6,7)]<-c(\"id\", \"time\", \"grp\",\"age\")\ndf.tf <- df.tf[, -3]\n\ndatatable(df.tf, rownames = F, caption = \"df.tf: Original data\", options = list(scrollX = T))\n\n\n\n\n\n nft<-sample(1:10,\n N,replace=T)#number of follow up time points\ncrp<-round(abs(rnorm(sum(nft)+N,\n mean=100,sd=40)),1)\ntime<-NA\nid<-NA\ni=0\nfor(n in nft){\ni=i+1\ntime.n<-sample(1:500,n)\ntime.n<-c(0,sort(time.n))\ntime<-c(time,time.n)\nid.n<-rep(i,n+1)\nid<-c(id,id.n)\n}\ndf.td <- cbind(data.frame(id,time)[-1,],crp)\ndatatable(df.td, rownames = F, caption = \"df.td: Time dependent CRP\", options = list(scrollX = T))\n\n\n\n\n\n\ndf.tf 는 기본정보가 담긴 데이터, df.td 는 time-dependent covariate 가 담긴 데이터이다. tmerge 함수를 2번 실행하면 두 정보를 합칠 수 있다. 먼저 df.tf 만 이용해서 tstart, tstop 변수를 만들자.\n\ndf <- tmerge(df.tf, df.tf, id = id, status1 = event(time, status))\n\ndatatable(df, rownames = F, caption = \"df: add tstart/tstop\", options = list(scrollX = T))\n\n\n\n\n\n\ntmerge 함수의 첫번째는 baseline data, 둘째는 time-dependent covariate 가 담긴 데이터가 들어가지만, tstart, tstop 를 만들기 위해 모두 df.tf 를 넣었다. status1 이라는 변수를 event(time, status) 로 지정함으로서 tstart, tstop 을 인식할 수 있다. status1 변수 자체는 status 와 동일하다. 이렇게 만든 df 에 time-dependent 정보가 담긴 df.td 를 결합하면 원하는 데이터를 얻을 수 있다. tmerge 의 자세한 내용은 https://ww2.amstat.org/meetings/sdss/2018/onlineprogram/ViewPresentation.cfm?file=304494.pdf 를 참고하기 바란다.\n\ndf2 <- tmerge(df, df.td, id = id, crp = tdc(time, crp))\n\ndatatable(df2, rownames = F, caption = \"df2: final\", options = list(scrollX = T))\n\n\n\n\n\n\ncrp 변수를 tdc(time, crp) 로 만들었다. 이제 cox model 을 실행할 수 있는데, 반복측정정보를 cluster 옵션에 넣는 것을 잊지 말자.\n\nmodel.td <- coxph(Surv(tstart, tstop, status1) ~ grp + age + crp, data = df2, cluster = id)\nsummary(model.td)\n\nCall:\ncoxph(formula = Surv(tstart, tstop, status1) ~ grp + age + crp, \n    data = df2, cluster = id)\n\n  n= 376, number of events= 67 \n\n         coef exp(coef)  se(coef) robust se     z Pr(>|z|)  \ngrp 0.5022750 1.6524764 0.2525914 0.2555150 1.966   0.0493 *\nage 0.0005535 1.0005536 0.0081077 0.0072342 0.077   0.9390  \ncrp 0.0007922 1.0007925 0.0027391 0.0023373 0.339   0.7347  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n    exp(coef) exp(-coef) lower .95 upper .95\ngrp     1.652     0.6052    1.0015     2.727\nage     1.001     0.9994    0.9865     1.015\ncrp     1.001     0.9992    0.9962     1.005\n\nConcordance= 0.554  (se = 0.04 )\nLikelihood ratio test= 4.21  on 3 df,   p=0.2\nWald test            = 4.34  on 3 df,   p=0.2\nScore (logrank) test = 4.18  on 3 df,   p=0.2,   Robust = 4.55  p=0.2\n\n  (Note: the likelihood ratio and score tests assume independence of\n     observations within a cluster, the Wald and robust score tests do not)."
  },
  {
    "objectID": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "href": "posts/2020-10-29-survivalpractice/index.html#모수적parametric-생존분석",
    "title": "생존분석 실습",
    "section": "모수적(parametric) 생존분석",
    "text": "모수적(parametric) 생존분석\nCox model 은 baseline hazard 없이도 HR 을 구할 수 있는 장점이 있다. 아래 식\n\\[h(t) = h_0(t) \\cdot \\exp(\\sum \\beta_i x_i)\\]\n에서 \\(h_0(t)\\) 를 몰라도 \\(\\beta\\) 들을 구할 수 있다는 뜻이고, cox model 이 준모수적(semi-parametric) 모형으로 불리는 이유이기도 하다. 그러나 Cox model 로 예측모형을 만들 때 이것은 단점이 된다. \\(t\\) 년 생존율을 구할 수 없기 때문이다. 생존함수 \\(S(t)\\) 는 아래처럼 계산하는데\n\\[S(t) = \\int_{0}^{t} h(u) \\,du\\]\nbaseline hazard 를 모르므로 \\(h(t)\\) 도 알 수 없고 따라서 \\(S(t)\\) 도 수식으로 표현할 수 없다. Cox model 로 예측모형을 만든 연구는 (1) 데이터에서 시간 \\(t\\) 마다 \\(S(t)\\) 의 값을 직접 구해 이용하거나, (2) 인구집단통계에서 \\(S(t)\\) 를 얻어온다.\n그러면 baseline hazard 가 어떤 형태라고 가정하면 어떨까? 이것이 모수적 생존분석이며 cox model 과 장단점을 비교하면 아래와 같다.\nCox model\n– distribution of survival time unkonwn\n– Less consistent with theoretical \\(S(t)\\) (typically step function)\n+ Does not rely on distributional assumptions\n+ Baseline hazard not necessary for estimation of hazard ratio\nParametric Survival Model\n+ Completely specified \\(h(t)\\) and \\(S(t)\\)\n+ More consistent with theoretical \\(S(t)\\)\n+ time-quantile prediction possible\n– Assumption on underlying distribution\n아래는 대표적인 분포들이며 본 글에서는 흔히 쓰는 weibull 을 다루려 한다.\n\n\n\n\n\n\n\n\n아까 비례위험가정 얘기할 때 weibull 모형은 log-log 그래프가 직선인지도 확인해야 한다고 했는데, 그 이유는 아래 식에 나와있듯이 \\(\\log(-\\log(S(t)))\\) 와 \\(\\log(t)\\) 가 정비례관계이기 때문이다.\n\\[\n\\begin{align}\nS(t) &= \\exp(-\\lambda t^p) \\\\\n-\\log(S(t)) &= \\lambda t^p \\\\\n\\log(-\\log(S(t))) &= \\log(\\lambda) + p\\log(t) \\\\\n\\log(-\\log(S(t))) &\\propto \\log(t)\n\\end{align}\n\\]\n\\(p\\) 를 scale parameter 라 하며 \\(p = 1\\) 이면 baseline hazard 가 시간에 따라 일정함을 의미하며, 자세한 내용은 https://stat.ethz.ch/education/semesters/ss2011/seminar/contents/handout_9.pdf 를 참고하자. R의 survreg 함수를 이용하며, 결과해석은 cox model 과 동일한데 scale parameter 값이 추가로 나온다(scale parameter를 미리 정할 수도 있다).\n\nmodel.weibull <- survreg(Surv(time, status) ~ trt, data = veteran)\nsummary(model.weibull)\n\n\nCall:\nsurvreg(formula = Surv(time, status) ~ trt, data = veteran)\n             Value Std. Error     z      p\n(Intercept) 4.7218     0.3275 14.42 <2e-16\ntrt         0.0478     0.2079  0.23  0.818\nLog(scale)  0.1585     0.0673  2.35  0.019\n\nScale= 1.17 \n\nWeibull distribution\nLoglik(model)= -748.1   Loglik(intercept only)= -748.1\n    Chisq= 0.05 on 1 degrees of freedom, p= 0.82 \nNumber of Newton-Raphson Iterations: 5 \nn= 137 \n\n\nScale = 1.17 임을 확인할 수 있고, trt 그룹별 \\(S(t)\\) 를 그려보면 아래와 같다.\n\npcut <- seq(0.01, 1, by = 0.01)  ## 1%-99%\nptime <- predict(model.weibull, newdata = data.frame(trt = 1), type = \"quantile\", p = pcut, se = T)\nmatplot(cbind(ptime$fit, ptime$fit + 1.96*ptime$se.fit, ptime$fit - 1.96*ptime$se.fit), 1 - pcut,\n        xlab = \"Days\", ylab = \"Survival\", type = 'l', lty = c(1, 2, 2), col=1)\n\n\n\n\n\n\n\n\\(S(t)\\) 를 구할 수 없는 cox model 의 그림과 비교해보자.\n\nmodel.cox <- exp\nkmfit.exp <- survfit(exp, newdata = data.frame(trt = 1))\nplot(kmfit.exp, lty = c(1, 2, 2), col=1, lwd=2, xlab = \"Days\", ylab = \"Survival\")\n\n\n\n\n\n\n\n지금까지 생존분석 때 고려할 내용을 다루었으며 처음의 요약을 반복하면 아래와 같다.\n\n자체 개발한 jskm 패키지로 kaplan-meier 그림을 그린다.\nLog-log plot, Observed-expected plot 으로 비례위험가정을 확인 후, cox.zph 함수로 p-value 를 구한다.\nanova 로 여러 모형의 log-likelohood 를 비교하고, step 으로 AIC 기반 최적모형을 고를 수 있다.\nTime-dependent analysis 는 (1) 비례위험가정이 깨졌을 때, (2) 반복측정 공변량이 있을 때 수행한다.\n모수적 생존분석은 생존함수 \\(S(t)\\) 를 구할 수 있어 예측모형을 만들 수 있다.\n\n자세한 내용은 중간중간 링크한 자료들을 참고하기 바란다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html",
    "href": "posts/2022-02-11-datatable/index.html",
    "title": "data.table 패키지 소개",
    "section": "",
    "text": "본 자료는 빠른 속도와 메모리 효율성에 강점이 있는 data.table 패키지에 관해 기본 개념과 문법, 함수들을 예제를 통해 다루어 볼 것이다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "href": "posts/2022-02-11-datatable/index.html#load-save-data-fread-fwrite",
    "title": "data.table 패키지 소개",
    "section": "Load & save data: fread & fwrite\n",
    "text": "Load & save data: fread & fwrite\n\nfread 함수로 빠르게 csv 파일을 읽어와서 data.table 자료로 만들 수 있고, fwrite 함수로 csv 파일을 쓸 수 있다. fread와 fwrite 는 이름답게 매우 빠르며 Base R의 함수보다 40배 더 빠른 속도를 자랑한다. 파일을 읽어와서 data.table 자료로 만들 때, 로컬 file path를 입력하거나 http:// 로 시작하는 url을 입력하는 방법을 사용한다. fread로 파일을 읽으면 그 class는 data.frame에 data.table이 추가되며 문법이 원래의 data.frame과 달라지는 점을 유의하자.\nfread를 통해 데이터를 불러와 data.table 형태로 만들어보자. 데이터는 09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료이며, 자세한 내용은 “data/2교시 테이블 세부 레이아웃 소개(최신자료).pdf” 를 참고하자.\nSetup\n\n## Setup\n\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\n\nlibrary(data.table)\nlibrary(curl)\n\nUsing libcurl 7.79.1 with LibreSSL/3.3.6\n\n\nLoad file\n\n# Load file\nurl <- \"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\"\ndf <- read.table(url,header=T)\ndt <- fread(url,header=T)\n# Class\nprint(class(df))\n\n[1] \"data.frame\"\n\nprint(class(dt))\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 볼 수 있다.\n\n\n# dt = data.table(df)\n# df = data.frame(dt)\n## See \ndt\n\n\n\n\n\n  \n\n\n\nSave file\n\n# Save file\nwrite.csv(dt, \"aa.csv\",row.names=F)\nfwrite(dt, \"aa.csv\")"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#row-operation",
    "href": "posts/2022-02-11-datatable/index.html#row-operation",
    "title": "data.table 패키지 소개",
    "section": "row operation",
    "text": "row operation\n행을 선택하는 slice는 data.table에서 DT[c(row1, row2, …),] 또는 DT[c(row1, row2, …)]의 형식으로 data.frame과 동일하게 쓰인다.\n\ndt[c(3,5)]\n\n\n\n\n\n  \n\n\n\n\n특정한 조건을 만족하는 행을 선택하는 filter는 data.table에서 DT[cond]의 형식으로 쓰인다.\n(이때, cond는 논리형 벡터이다.)\n\ndt[BMI>=30 & HGHT<150]\n\n\n\n\n\n  \n\n\n\n미리 key들을 지정하면 더 빠르게 검색할 수 있다. 자세한 내용은 뒤에서 다루도록 하겠다."
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#column-operation",
    "href": "posts/2022-02-11-datatable/index.html#column-operation",
    "title": "data.table 패키지 소개",
    "section": "column operation",
    "text": "column operation\n열을 이름 또는 순번으로 선택하는 select는 DT[, .(cols)] 또는 DT[, list(cols)]의 형식으로 data.frame과 비슷하나 몇 가지 차이점이 있다. 변수 이름으로 선택하는 경우 앞에 .()이나 list를 붙이지 않으면 결과로 벡터가 반환된다.\n\n순번으로 열 선택\n\n\ndt[, c(13, 14)]\n\n\n\n\n\n  \n\n\n\n\n\n이름으로 열 선택\n\n\n## same\n# dt[, list(HGHT, WGHT)]\ndt[, .(HGHT, WGHT)]\n\n\n\n\n\n  \n\n\n\n\n열을 선택할 때 DT[, .(new_col = col)] 형식을 사용하여 열 이름을 지정해서 출력할 수 있다.\n\n# rename\ndt[, .(Height = HGHT, Weight = WGHT)]\n\n\n\n\n\n  \n\n\n\n\n\n변수로 열 선택\n\n\n## same\n# vars <- c(13,14) \nvars <- c(\"HGHT\", \"WGHT\")\n\ndt[, ..vars]\n\n\n\n\n\n  \n\n\n\n변수로 열을 선택하는 경우, 변수 앞에 ..을 붙이지 않으면 오류가 발생하므로 주의하도록 한다. .. 대신 with = F 를 뒤에 붙이거나 .SD, .SDcols 옵션을 사용하기도 한다. .SD는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, vars, with = F]\n\n\n\n\n\n  \n\n\n\n\n\n열 제외\n\n필요없는 열을 제외할 때는 - 또는 ! 를 붙인다.\n\nicols = c(1:12)\n\n## same\n# dt[, -..icols]\ndt[, !..icols]\n\n\n\n\n\n  \n\n\n\ndata.table의 column operation에서는 열을 선택할뿐만 아니라 연산하는 식을 처리할 수 있다. 앞에서 배운 내용을 통해 특정 조건을 만족하는 행을 대상으로 mean 연산을 수행하여 보자.\n\ndt[HGHT >= 180 & WGHT <= 80, .(m_chol = mean(TOT_CHOL), m_TG = mean(TG))]\n\n     m_chol     m_TG\n1: 181.1143 144.8857"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#by-operation",
    "href": "posts/2022-02-11-datatable/index.html#by-operation",
    "title": "data.table 패키지 소개",
    "section": "by operation",
    "text": "by operation\nby 옵션을 이용하여 그룹별로 함수를 적용할 수 있다. by=.(그룹1, 그룹2, …)을 사용해 두 개 이상의 그룹별로 함수를 적용할 수 있다. 이때 괄호 앞에 있는 점(‘.’)은 list()를 의미하므로 꼭 포함시키도록 한다. by 대신 keyby를 사용할 경우, 그룹별 집계 결과가 정렬되어 나타난다.\n연도 변수인 EXMD_BZ_YYYY을 기준으로 집단을 분리한 후 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같다.\n\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\n\n기준으로 사용되지 않은 모든 열에 대해 평균을 구할때는 .SD를 사용한다. 이는 뒤에서 더 자세히 다루도록 하겠다.\n\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\n\n이번에는 두 개의 그룹 변수를 지정해 행의 개수를 출력해보자.\n\ndt[HGHT >= 175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n\n  \n\n\n\n정렬\n그룹별로 함수를 적용한 결과를 정렬하고자 할 때는 keyby를 사용하거나 마지막에 [order()]를 붙인다.\n\nkeyby\n\n\ndt[HGHT >= 175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n\n  \n\n\n\nby를 사용한 예제와는 달리 Q_SMK_YN에 대해서 정렬된 것을 볼 수 있다.\n\n\n[order()]\n\n\n# BMI에 대해 내림차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(BMI)]\n\n\n\n\n\n  \n\n\n\n\n# BMI에 대해 오름차순 정렬\ndt[, .(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by=EXMD_BZ_YYYY] [order(-BMI)]\n\n\n\n\n\n  \n\n\n\nExpressions in by\n\nby 옵션에는 변수뿐만 아니라 식을 지정할 수도 있다.\n약물 치료 여부에 따른 환자수를 확인하려는 경우, 다음과 같이 식을 지정한다.\n\ndt[, .N, by=.(Q_PHX_DX_STK > 0, Q_PHX_DX_HTDZ > 0)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "href": "posts/2022-02-11-datatable/index.html#key를-이용한-탐색-setkey",
    "title": "data.table 패키지 소개",
    "section": "key를 이용한 탐색 setkey()",
    "text": "key를 이용한 탐색 setkey()\nkey를 사용하면 데이터의 탐색 및 처리 속도가 매우 향상된다. setkey(DT, col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용한다. 설정되어 있는 키를 제거할 때는 setkey(DT, NULL)를 설정한다.\n\n키 설정\n\nsetkey를 활용해 데이터 테이블에 키를 설정하고, key 함수로 설정된 키를 확인할 수 있다.\n\n# 1 key\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\n\n# 2 keys\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\n\n키를 활용한 행 선택\n\ndt[.(a)], dt[J(a)], dt[list(a)], dt[col == a] 중에서 아무거나 사용하여 행을 선택할 수 있다.\n\n## same\n# dt[.(2011)]\n# dt[list(2011)]\n# dt[EXMD_BZ_YYYY==2011]\ndt[J(2011)]\n\n\n\n\n\n  \n\n\n\n\n## same\n# dt[.(2011, 2)]\n# dt[list(2011, 2)]\n# dt[EXMD_BZ_YYYY==2011 & Q_HBV_AG==2]\ndt[J(2011, 2)]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-병합-merge",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 병합 merge()",
    "text": "데이터 테이블 병합 merge()\ndata.table은 key를 사용하거나, on=을 활용하여 두 데이터 데이블을 병합할 수 있다.\n\n\n\n\nMerge in data.table\n\n\n\n\n기존 데이터를 가공하여 새로운 data.table인 dt1, dt2에 저장하고 연도에 따라 merge해 보자.\n\n# dt1\n(dt1 <- dt[c(1, 300, 500, 700, 1000)])\nsetkey(dt1, EXMD_BZ_YYYY)\n\n\n\n\n\n  \n\n\n\n\n# dt2\n(dt2 <- dt[c(400, 600, 800 ,1200, 1500)])\nsetkey(dt2, EXMD_BZ_YYYY)\n\n\n\n\n\n  \n\n\n\n1. inner join\n두 데이터에 모두 존재하는 경우 dt1[dt2, on=‘key’, nomatch=0] 또는 merge(dt1, dt2, by=‘key’, all=F) 형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY', nomatch=0]\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all = F)\n\n\n\n\n\n  \n\n\n\n2. left_outer_join\n첫 번째 데이터에 존재하는 경우 dt2[dt1, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.x=T) 형식 사용\n\n\n\n\n\n\n\n\n\ndt2[dt1, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.x=T)\n\n\n\n\n\n  \n\n\n\n3. right_outer_join\n두 번째 데이터에 존재하는 경우 dt1[dt2, on=‘key’] 또는 merge(dt1, dt2, by=‘key’, all.y=T)형식 사용\n\n\n\n\n\n\n\n\n\ndt1[dt2, on='EXMD_BZ_YYYY']\n\n\n# same\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all.y=T)\n\n\n\n\n\n  \n\n\n\n4. full_outer_join\n어느 한 쪽에 존재하는 경우 merge(dt1, dt2, by=‘key’, all=T) 형식 사용\n\n\n\n\n\n\n\n\n\nmerge(dt1, dt2, by='EXMD_BZ_YYYY', all=TRUE)"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "href": "posts/2022-02-11-datatable/index.html#데이터-테이블-수정-연산자",
    "title": "data.table 패키지 소개",
    "section": "데이터 테이블 수정 연산자 :=",
    "text": "데이터 테이블 수정 연산자 :=\n데이터 테이블에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용한다. 수정 또는 생성할 열이 하나인 경우, dt[ , newcol1 := ] 형식을 쓰며 열이 두 개 이상인 경우 dt[, ‘:=’ (col1=, col2=)]을 사용한다.\n다음의 예시를 통해서 자세히 알아보자.\n기존의 데이터 테이블에서 HDL - LDL을 구해서 diff라는 이름의 열을 새로 생성한다. 그리고, HGHT와 WGHT는 HGHT*0.9, WGHT+5로 수정한다.\n열 생성/수정\n\n\n\n위 코드를 실행시키면 갱신이 눈에 보이지 않는 상태로 실행되며, 만약 갱신 결과를 눈에 보이도록 출력하려면 제일 뒤에 [] 를 붙여주어야 한다.\n\n# 열 생성\ndt[, diff := HDL-LDL][]\n\n\n\n\n\n  \n\n\n\n\n# 열 수정\ndt[, ':=' (HGHT = HGHT*0.9, WGHT = WGHT+5)][]\n\n\n\n\n\n  \n\n\n\n열 삭제\n데이터 테이블에서 열을 삭제하려면 col := NULL 형식을 사용한다.\n\n# BMI 삭제\ndt[, BMI := NULL]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#특수-기호",
    "href": "posts/2022-02-11-datatable/index.html#특수-기호",
    "title": "data.table 패키지 소개",
    "section": "특수 기호",
    "text": "특수 기호\n.SD\n.SD 는 ’Subset of Data’의 약자로, by로 지정한 그룹 칼럼을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용한다.\n\n# 모든 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\n 또한 .SD 기호를 사용하여 연도별로 처음 두 개의 행을 추출할 수 있다.\n\ndt[, head(.SD, 2), by=EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\n.SDcols\n.SDcols는 연산 대상이 되는 특정 칼럼을 지정하는 특수 기호이다. 특정 열을 대상으로 연산을 할 때 by 다음에 .SDcols = c(“col1”, “col2”, …)로 연산 대상을 지정한다.\n\n# HGHT, WGHT 칼럼의 연도별 평균값\ndt[, lapply(.SD, mean), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]\n\n\n\n\n\n  \n\n\n\n.N\n.N는 부분 데이터의 행의 수를 나타내며, 요약 통계치를 구할 때 대상 데이터의 수를 간편하게 구할 수 있다.\n\n# 특정 조건을 만족하는 행의 수\ndt[LDL >= 150, .N]\n\n[1] 228\n\n\n\n# HGHT, WGHT 칼럼의 연도별 평균값과 행의 수\ndt[, c(.N, lapply(.SD, mean)), by=EXMD_BZ_YYYY, .SDcols=c(\"HGHT\", \"WGHT\")]"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#melt",
    "href": "posts/2022-02-11-datatable/index.html#melt",
    "title": "data.table 패키지 소개",
    "section": "melt",
    "text": "melt\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있다. melt(data, id.vars, measure.vars, variable.name, value.name) 형식으로 쓰이며, id.vars에는 고정 칼럼을 measure.vars는 stack 처리할 칼럼을 넣는다. melt함수를 써서 길게 재구조화한 후의 “variable”, “value” 변수 이름을 바꾸고 싶다면 variable.name=“new_var_name”, value.name=“new_val_name” 처럼 새로운 칼럼 이름을 부여하여 지정할 수 있다.\n\n# wide to long\ndt.long1 <- melt(dt,\n                id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HGHT\", \"WGHT\"),\n                measure.vars = c(\"TOT_CHOL\", \"HDL\", \"LDL\"),\n                variable.name = \"measure\",\n                value.name = \"val\")\ndt.long1\n\n\n\n\n\n  \n\n\n\nEnhanced melt\nmelt 함수는 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있다.\n\n\nlist에 복수의 칼럼 이름을 입력하는 방법\n\nmelt 함수에 measure=list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣는다. 이때 공통의 value.name을 지정할 수 있다.\n\ncol1 <- c(\"BP_SYS\", \"BP_DIA\")\ncol2 <- c(\"HDL\", \"LDL\")\ndt.long2 <- melt(dt, \n                 measure = list(col1, col2),\n                 value.name = c(\"BP\", \"Chol\"))\ndt.long2\n\n\n\n\n\n  \n\n\n\n\n\n특정 패턴을 정규 표현식으로 매칭하는 방법\n\nmelt 함수에 measure=patterns() 형식으로 특정 패턴을 따르는 복수의 칼럼을 정규 표현식을 통해 설정한다.\n\ndt.long3 <- melt(dt, \n            measure=patterns(\"^Q_PHX_DX\", \"^BP\"), \n            value.name=c(\"Q_PHX_DX\", \"BP\"))\ndt.long3"
  },
  {
    "objectID": "posts/2022-02-11-datatable/index.html#dcast",
    "href": "posts/2022-02-11-datatable/index.html#dcast",
    "title": "data.table 패키지 소개",
    "section": "dcast",
    "text": "dcast\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용한다. dcast(data, formula, value.var, fun.aggregate) 형식으로 쓰이며, formula의 LHS에는 id.vars, RHS에는 variable을 입력하고 value.name에는 “value”를 입력한다. 이때 “variable”과 “value” 변수 이름을 바꿨다면, 새로 지정한 칼럼명을 넣는다.\n\n# long to wide\ndt.wide1 <- dcast(dt.long1, EXMD_BZ_YYYY + RN_INDI + HGHT + WGHT ~ measure, value.var = \"val\")\ndt.wide1\n\n\n\n\n\n  \n\n\n\n\ndcast 함수에 집계 함수(fun.aggregate)를 사용하여 그룹별 요약 통계량을 계산한 결과를 재구조화하여 반환할 수 있다.\n\n# 연도별 TOT_CHOL, HDL, LDL의 평균값\ndt.wide2 <- dcast(dt.long1, EXMD_BZ_YYYY ~ measure, value.var = \"val\", fun.aggregate = mean, na.rm =T)\ndt.wide2\n\n\n\n\n\n  \n\n\n\nEnhanced dcast\ndcast 함수의 value.vars에 복수의 칼럼을 넣어 여러 개의 칼럼을 동시에 재구조화할 수 있다.\n\ndt.wide3 <- dcast(dt.long2,\n                  ... ~ variable,\n                  value.var = c(\"BP\", \"Chol\"))\ndt.wide3"
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html",
    "href": "posts/2019-01-03-rdatamanagement/index.html",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "",
    "text": "김진섭 대표는 1월 28일(월) 성균관의대 사회의학교실를 방문, tidyverse 생태계에서의 데이터 매니지먼트 방법을 강의할 예정입니다. 강의 내용을 미리 정리하였습니다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "href": "posts/2019-01-03-rdatamanagement/index.html#시작하기-전에",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "시작하기 전에",
    "text": "시작하기 전에\nR 데이터 매니지먼트 방법은 크게 3 종류가 있다.\n\n원래의 R 문법을 이용한 방법으로 과거 홈페이지1에 정리했었다.\ntidyverse는 직관적인 코드를 작성할 수 있는 점을 장점으로 원래의 R 문법을 빠르게 대체하고 있다.\ndata.table 패키지는 빠른 실행속도를 장점으로 tidyverse 의 득세 속에서 살아남았으며, 역시 과거 홈페이지2에 정리한 바 있다.\n\n본 강의는 이중 두 번째의 기초에 해당하며\n\nreadr 패키지의 read_csv 함수로 데이터를 빠르게 읽은 후\nmagrittr 패키지의 %>% 연산자와 dplyr 패키지의 select, mutate, filter, group_by, summarize 함수로 직관적인 코드를 작성하고\npurrr 패키지의 map 함수로 쉽게 반복문을 처리하는 것을 목표로 한다.\n\n각각의 패키지를 따로 설치할 수도 있고 install.packages(\"tidyverse\")로 tidyverse 생태계의 패키지를 모두 설치할 수도 있다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-읽기-readr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 읽기: readr\n",
    "text": "데이터 읽기: readr\n\nreadr 패키지에서 csv 파일을 읽는 함수는 read_csv이며, 구분자(ex: 공백, 탭)가 다를 때는 read_delim 함수를 이용하여 구분자를 설정할 수 있다. 예제 데이터를 읽어보자.\n\nlibrary(readr)\na <- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\")\n\nRows: 1000 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Sex, STRESS_EXIST\ndbl (12): Age, Height, Weight, BMI, DM, HTN, Smoking, MACCE, Death, MACCE_da...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n데이터를 읽으면 각 변수들을 어떤 형태(숫자형, 문자형)로 읽었는지 표현되는데, 바꾸고 싶은 것이 있으면 아래와 같이 col_types 옵션을 이용하면 된다.\n\n## Character: col_character() or \"c\"\na <- read_csv(\"https://raw.githubusercontent.com/jinseob2kim/R-skku-biohrs/master/data/smc_example1.csv\",col_types = cols(HTN = \"c\"))\n\n이제 데이터를 살펴보면 HTN 변수가 문자형이 된것을 볼 수 있다.\n\na\n\n\n\n\n\n  \n\n\n\n기본 함수인 read.csv 와 비교했을 때 아주 작은 데이터에서는 장점이 없으나, 그 크기가 커질수록 read_csv가 더 좋은 성능을 보임이 알려져 있다(Figure @ref(fig:readfunc)).\n\n\n\n\n파일 읽기 함수 비교3\n\n\n\n\nread_csv로 읽은 데이터는 tibble이라는 새로운 클래스로 저장된다. 직접 확인해보자.\n\nclass(a)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\n기존의 data.frame 외에도 tbl_df, tbl 와 같은 것들이 추가되어 있다. tibble은 data.frame보다 좀 더 날 것(?)의 정보를 보여주는데, 범주형 변수를 factor가 아닌 character 그대로 저장하고 변수명을 그대로 유지하는 것(data.frame에서 변수명의 특수문자나 공백은 .으로 바뀜)이 가장 큰 특징이다. tibble의 추가 내용은 jumpingrivers 블로그4를 참고하기 바란다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "href": "posts/2019-01-03-rdatamanagement/index.html#직관적인-코드-연산자",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "직관적인 코드: %>% 연산자",
    "text": "직관적인 코드: %>% 연산자\ntidyverse 생태계에서 가장 중요한 것을 하나만 고르라면 magrittr 패키지의 %>% 연산자를 선택하겠다. %>%은 Rstudio에서 단축키 Ctrl + Shift + M (OS X: Cmd + Shift + M)로 입력할 수 있는데, 이것을 이용하면 의식의 흐름대로 코딩을 할 수 있어 직관적인 코드를 작성할 수 있다. head함수를 통해 %>%의 장점을 알아보자.\n\nlibrary(magrittr)\n\n## head(a)\na %>% head\n\n\n\n\n\n  \n\n\n\n\na %>% head(n = 10)\n\n\n\n\n\n  \n\n\n\nhead(a)와 a %>% head는 같은 명령어 “a의 head를 보여줘”로, a %>% head가 생각의 흐름이 반영된 코드임을 알 수 있다. 일반적으로 %>% 연산자는 함수의 맨 처음 인자를 앞으로 빼오는 역할을 하고 f(x, y)는 x %>% f(y)로 바꿀 수 있다. 맨 처음 인자가 아닐 때에는 y %>% f(x, .)과 같이 앞으로 빼온 변수의 자리에 .를 넣으면 된다. 예를 들면 아래에 나오는 세 명령어는 모두 같다.\n\na %>% head(n = 10)\n10 %>% head(a, .)   \n10 %>% head(a, n = .)\n\n%>%의 진가는 여러 함수를 한 번에 사용할 때 나타나는데, head와 subset을 동시에 쓰는 경우를 예로 살펴보겠다.\n\n## head(subset(a, Sex == \"M\"))\na %>% subset(Sex == \"M\") %>% head\n\n\n\n\n\n  \n\n\n\n이 명령어는 a에서 남자만 뽑아서 head를 보여줘로 기존의 head(subset(a, Sex == \"M\"))보다 훨씬 직관적이며 함수가 3개, 4개로 늘어날수록 비교가 안될 정도의 가독성을 보여준다. 남자만 뽑아 회귀분석을 수행하고 그 계수와 p-value를 구하는 예를 살펴보자. 먼저 기존의 코딩 스타일대로 명령을 수행하면 아래와 같다.\n\nb <- subset(a, Sex == \"M\")\nmodel <- glm(DM ~ Age + Weight + BMI, data = b, family = binomial)\nsumm.model <- summary(model)\nsumm.model$coefficients\n\n\n\n\n\n\n\n\n다음은 %>% 를 이용한 코딩이다.\n\na %>% \n  subset(Sex == \"M\") %>% \n  glm(DM ~ Age + Weight + BMI, data = ., family = binomial) %>% \n  summary %>% \n  .$coefficients\n\n\n\n\n\n\n\n\n%>% 연산자로 쓴 코드는 읽기 쉬운 것은 물론 불필요한 중간변수(b, model, summ.model)가 없어 깔끔하고 메모리의 낭비도 없다. 딱 하나 주의할 점은 코드를 내려쓸 때 각 줄은 반드시 %>%로 끝나야 한다는 점이다. 예를 들어 아래의 코드를 실행하면 맨 윗줄만 실행되는 것을 확인할 수 있다.\n\na %>% subset(Sex == \"M\")\n  %>% head \n\n본 강의 후 다른 것은 까먹어도 %>% 만 익숙해지면 성공이라고 생각한다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#데이터-정리-dplyr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "데이터 정리: dplyr\n",
    "text": "데이터 정리: dplyr\n\ndplyr 는 데이터를 효과적으로 다룰 수 있는 일련의 함수들을 제공한다. 이중 group_by와 summarize는 쉽게 그룹 별 요약통계량을 보여줌으로서 기존 R 문법과 차별화된 가치를 제공하므로 꼭 익혀두자.\nfilter\nfilter는 subset 함수와 같은 기능으로 특정 조건으로 데이터를 필터링하는 데 이용된다. 아래는 데이터에서 남자만 추출하는 예시이다.\n\nlibrary(dplyr)\na %>% filter(Sex == \"M\") \n\n\n\n\n\n  \n\n\n\nfilter에서는 & 외에 ,으로도 AND 조건을 쓸 수 있어 가독성이 좋다. between 함수를 이용하면 연속변수의 특정 범위를 선택할 수도 있는데, 이것 역시 기존의 &를 활용하는 것보다 직관적이다. 50~60세 사이를 필터링하는 예시를 살펴보자.\n\n## Age between 50 and 60.\na %>% filter(between(Age, 50, 60))\n\n\n\n\n\n  \n\n\n\n아래의 &나 ,로 표현한 조건도 between을 이용한 것과 같은 결과를 보여준다.\n\na %>% filter(Age >= 50 & Age <= 60)\na %>% filter(Age >= 50, Age <= 60)\n\n\narrange: 정렬\narrange는 특정 순서에 따라 데이터를 정렬하는 함수로, 정렬 순서만 알려주는 order 함수와는 달리 정렬된 데이터를 보여주는 것이 특징이다.\n\n## a[order(a$Age), ]\na %>% arrange(Age)\n\n\n\n\n\n  \n\n\n\n정렬 조건이 2개 이상이면 ,로 같이 적을 수 있으며 내림차순 정렬은 desc 명령어를 이용한다. 아래는 Age에 대해 오름차순, BMI에 대해 내림차순 정렬을 수행하는 예시이다.\n\n## a[order(a$Age, -a$BMI), ]\na %>% arrange(Age, desc(BMI))\n\n\n\n\n\n  \n\n\n\n조건에 Age 대신 \"Age\"와 같이 문자열을 넣을 때는 언더바(_)가 붙은 arrange_ 함수를 이용하며, 이것은 나머지 함수들에서도 마찬가지이다.\n\na %>% arrange_(\"Age\")\n\n\nselect: 변수 선택\nselect는 데이터에서 특정 변수들을 선택하는 함수로 기본 R 에는 없는 유용한 기능들을 제공한다.\n\n## a[, c(\"Sex\", \"Age\")]\na %>% select(Sex, Age, Height)\n\n\n\n\n\n  \n\n\n\n변수명 대신에 열 번호를 넣어도 되며 Sex:Height을 이용해서 Sex와 Height 사이의 모든 변수를 선택할 수도 있다. arrange 함수와는 달리 \"Sex\"와 같은 문자열도 그대로 입력 가능하며, 아래의 방법들은 모두 a %>% select(Sex, Age, Height)와 같은 코드이다.\n\na %>% select(Sex:Height)\na %>% select(\"Sex\":\"Height\")\na %>% select(2, 3, 4)\na %>% select(c(2, 3, 4))\na %>% select(2:4)\n\n특정 변수들을 빼려면 -Sex나 -(Sex:Height)와 같이 적으면 된다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %>% select(-Sex, -Age, -Height)\n\n\n\n\n\n  \n\n\n\n아래의 코드도 a %>% select(-Sex, -Age, -Height)와 같은 결과를 준다.\n\n## a[, -c(\"Sex\", \"Age\", \"Height\")]\na %>% select(-2, -3, -4)\na %>% select(-(2:4))\na %>% select(-c(2, 3, 4))\n\na %>% select(-(Sex:Height))\na %>% select(-\"Sex\", -\"Age\", -\"Height\")\na %>% select(-(\"Sex\":\"Height\"))\n\n만약 MACCE_date, Death_date와 같이 _date로 끝나는 변수들을 선택하고 싶다면 end_with 함수를 이용하면 된다.\n\n## a[, grep(\"_date\", names(a))]\na %>% select(ends_with(\"date\"))\n\n\n\n\n\n  \n\n\n\n이외에 select와 함께 쓸 수 있는 유용한 함수들을 정리하면 아래와 같다.\n\nstart_with(\"abc\"): ’abc’로 시작하는 이름\nend_with(\"xyz\"): ’xyz’로 끝나는 이름\ncontains(\"ijk\"): ’ijk’를 포함하는 이름\none_of(c(\"a\", \"b\", \"c\")): 변수명이 a, b, c 중 하나\nmatches(\"(.)\\\\1\"): 정규표현식 조건\nnum_range(\"x\", 1:3): x1, x2, x3\n\nmutate: 변수 생성\nmutate는 새로운 변수를 만드는 함수이다. Age와 BMI 변수에서 고령과 비만을 뜻하는 Old, Overweight 변수를 만들어 보자.\n\n## a$old <- as.integer(a$Age >= 65); a$overweight <- as.integer(a$BMI >= 27)\na %>% mutate(Old = as.integer(Age >= 65),\n             Overweight = as.integer(BMI >= 27)\n             )\n\n\n\n\n\n  \n\n\n\n새로운 변수만 보여주려면 mutate 대신 transmute를 사용한다.\n\na %>% transmute(Old = as.integer(Age >= 65),\n             Overweight = as.integer(BMI >= 27)\n             )\n\n\n\n\n\n  \n\n\n\n\ngroup_by와 summarize\n\ngroup_by, summarize를 이용하여 원하는대로 그룹을 나누고 각 그룹의 요약통계량을 간단히 구할 수 있다. 기본 R에서는 aggregate함수가 같은 기능을 수행한다.\n\na %>% \n  group_by(Sex, Smoking) %>% \n  summarize(count = n(),\n            meanBMI = mean(BMI),\n            sdBMI = sd(BMI))\n\n\n\n\n\n  \n\n\n\ngroup_by에 \"Age\" 같은 문자열을 넣으려면 언더바(_)가 붙은 group_by_ 함수를 이용해야 한다.\n모든 변수에 같은 요약방법을 적용하려면 summarize_all 함수를 사용한다. 아래는 50세 이상을 대상으로 모든 변수에 그룹별 평균을 적용한 예시이다.\n\na %>% \n  filter(Age >= 50) %>% \n  group_by(Sex, Smoking) %>% \n  summarize_all(mean) \n\n\n\n\n\n  \n\n\n\n평균값을 계산할 수 없는 범주형 변수는 NA를, 그 외 변수들은 평균값을 보여줌을 확인할 수 있다. 여러 요약값을 동시에 보여주려면 funs 명령어로 여러 함수를 같이 적어주면 된다.\n\na %>% \n  filter(Age >= 50) %>% \n  select(-Patient_ID, -STRESS_EXIST) %>%       ## Except categorical variable\n  group_by(Sex, Smoking) %>% \n  summarize_all(funs(mean = mean, sd = sd)) \n\n\n\n\n\n  \n\n\n\n\n%>%와 기본함수만으로 똑같이 구현하기.\n마지막에 수행했던 작업을 %>%와 R 기본함수만으로 구현하면서 본 단원을 마무리하겠다. filter 대신 subset, select 대신 [], group_by와 summarize_all 대신에 aggregate를 이용하면 된다.\n\na %>% \n  subset(Age >= 50) %>%\n  .[, -c(1, 14)] %>% \n  aggregate(list(Sex = .$Sex, Smoking = .$Smoking), \n            FUN = function(x){c(mean = mean(x), sd = sd(x))})\n\n위와 같이 기본 함수로도 %>% 연산자를 이용하여 그럴듯하게 코드를 작성할 수 있으나, dplyr 와 비교했을 때 아무래도 가독성이 떨어진다는 점에서 dplyr가 유용한 패키지임을 확인할 수 있었다."
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "href": "posts/2019-01-03-rdatamanagement/index.html#반복문-처리-purrr",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "반복문 처리: purrr\n",
    "text": "반복문 처리: purrr\n\n본 내용에서는 for문이나 멀티코어의 사용은 언급하지 않는다. 해당 내용은 과거 강의5를 참고하기 바란다.\nR에서 반복문을 처리하는데 가장 많이 이용되는 함수는 lapply(또는 sapply)일 것이다. purrr의 대표 함수인 map은 이 lapply를 tidyverse 철학으로 구현한 것이다. 이제부터 차이점을 알아보자.\nmap\n데이터의 모든 변수들의 형태를 살펴보는 lapply 구문은 아래와 같다.\n\nlapply(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\n이것을 map으로 구현하면 lapply와 정확히 같은 형태가 된다.\n\nlibrary(purrr)\nmap(a, class)\n\n$Sex\n[1] \"character\"\n\n$Age\n[1] \"numeric\"\n\n$Height\n[1] \"numeric\"\n\n$Weight\n[1] \"numeric\"\n\n$BMI\n[1] \"numeric\"\n\n$DM\n[1] \"numeric\"\n\n$HTN\n[1] \"character\"\n\n$Smoking\n[1] \"numeric\"\n\n$MACCE\n[1] \"numeric\"\n\n$Death\n[1] \"numeric\"\n\n$MACCE_date\n[1] \"numeric\"\n\n$Death_date\n[1] \"numeric\"\n\n$STRESS_EXIST\n[1] \"character\"\n\n$Number_stent\n[1] \"numeric\"\n\n\nmap은 list 형태로 결과를 반환하며 특정 형태를 지정하려면 아래와 같은 함수들을 이용한다.\n\nmap: 리스트\nmap_lgl: 논리값(T, F)\nmap_int: 정수\nmap_dbl: 실수\nmap_chr: 문자열\nmap_dfr: rbind된 data.frame\nmap_dfc: cbind된 data.frame\n\n앞의 예를 map_chr을 이용해 다시 실행하자.\n\nmap_chr(a, class)\n\n         Sex          Age       Height       Weight          BMI           DM \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n \"character\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\"    \"numeric\" \nSTRESS_EXIST Number_stent \n \"character\"    \"numeric\" \n\n\n이것은 sapply함수의 tidyverse 버전으로 기억하면 좋다. 아래의 명령어들은 모두 같은 결과를 나타낸다.\n\nmap_chr(a, class)\na %>% map_chr(class)\nsapply(a, class)\nunlist(map(a, class))\nunlist(lapply(a, class))\n\n각 변수에서 첫 번째 값을 뽑는 아래의 코드들도 sapply와 map_*의 차이는 없다.\n\na %>% sapply(function(x){x[1]})\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %>% sapply(`[`, 1)\n\n         Sex          Age       Height       Weight          BMI           DM \n         \"M\"         \"52\"        \"160\"         \"63\"  \"24.609375\"          \"0\" \n         HTN      Smoking        MACCE        Death   MACCE_date   Death_date \n         \"1\"          \"1\"          \"0\"          \"0\"       \"1056\"       \"1056\" \nSTRESS_EXIST Number_stent \n   \"No test\"          \"3\" \n\na %>% map_chr(`[`, 1)\n\n          Sex           Age        Height        Weight           BMI \n          \"M\"   \"52.000000\"  \"160.000000\"   \"63.000000\"   \"24.609375\" \n           DM           HTN       Smoking         MACCE         Death \n   \"0.000000\"           \"1\"    \"1.000000\"    \"0.000000\"    \"0.000000\" \n   MACCE_date    Death_date  STRESS_EXIST  Number_stent \n\"1056.000000\" \"1056.000000\"     \"No test\"    \"3.000000\" \n\n\nclass나 mean, 그리고 `[` 등 간단한 함수들은 lapply를 쓰나 map을 쓰나 차이가 없다. map의 진가는 반복할 함수가 복잡할 때 드러난다. 성별로 같은 회귀분석을 반복하는 코드를 예로 들어 보자. 먼저 lapply를 이용한 코드는 아래와 같다.\n\na %>% \n  group_split(Sex) %>% \n  lapply(function(x){lm(Death ~ Age, data = x, family = binomial)})\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = x, family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\n위 예시에서 알 수 있듯이 lapply에서 복잡한 함수를 반복하려면 function(x) 문이 꼭 필요하고 가독성을 저해하는 원인이 된다. 그러나 map에서는 ~로 간단히 function(x)를 대체할 수 있다. 아래 코드를 살펴보자.\n\na %>% \n  group_split(Sex) %>% \n  map(~lm(Death ~ Age, data = ., family = binomial))\n\n[[1]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  0.0461858    0.0001931  \n\n\n[[2]]\n\nCall:\nlm(formula = Death ~ Age, data = ., family = binomial)\n\nCoefficients:\n(Intercept)          Age  \n  -0.208833     0.004355  \n\n\nmap함수를 이용하여 function(x)를 ~로, 성별 데이터에 해당하는 x를 .으로 바꾸니 훨씬 읽기가 쉬워졌다.\n이번엔 같은 회귀분석을 수행한 후 Age의 p-value만 뽑는다고 하자.\n\na %>% \n  group_split(Sex) %>% \n  sapply(function(x){\n    lm(Death ~ Age, data = x, family = binomial) %>% \n      summary %>% \n      .$coefficients %>% \n      .[8]     ## p-value: 8th value\n    }) \n\n[1] 9.016998e-01 2.094342e-07\n\n\n%>% 연산자를 이용해 나름대로 읽기 쉬운 코드가 되었다. 이것을 map_dbl로 다시 표현하면 아래와 같다.\n\na %>% \n  group_split(Sex) %>% \n  map_dbl(~lm(Death ~ Age, data = ., family = binomial) %>% \n        summary %>% \n        .$coefficients %>% \n        .[8]\n  )\n\n[1] 9.016998e-01 2.094342e-07\n\n\nfunction(x) 가 없어 더 잘 보이기는 하나 고작 이 정도의 장점이라면 map을 쓸 필요가 없을 것 같다. map을 좀 더 적극적으로 사용해 보자.\n\na %>% \n  group_split(Sex) %>% \n  map(~lm(Death ~ Age, data = ., family = binomial)) %>% \n  map(summary) %>% \n  map(\"coefficients\") %>% \n  map_dbl(8)\n\n[1] 9.016998e-01 2.094342e-07\n\n\n기존 코드는 .$coefficient와 .[8] 같은 부분이 거슬렸는데, map(\"coefficients\"), map_dbl(8)으로 바꾸니 보기에 훨씬 깔끔하다. 참고로 map(\"coefficients\") 은 map(`[[`, \"coefficients\")의, map_dbl(8)은 map_dbl(`[`, 8)의 축약형 표현이다. 사실 아까 다루었던 첫 번째 값 뽑기의 경우도 아래와 같이 더 간단하게 쓸 수 있다.\n\na %>% map_chr(`[`, 1)\na %>% map_chr(1) \n\nmap을 통해 함수의 모든 중간과정을 따로따로 구현한 것도 장점이다. 에러가 발생했을 때 드래그로 중간과정까지만 실행할 수 있어 함수의 어느 부분에서 에러가 발생했는지를 쉽게 알아낼 수 있다.\n\nmap2, pmap: 입력 변수 2개 이상\n2개 이상의 입력값에 대해 반복문을 수행하는 함수로는 mapply가 있다. 이것의 tidyverse 버전이 map2와 pmap인데 전자는 2개의 조건에, 후자는 리스트 형태로 입력값의 갯수에 상관없이 반복문을 구현할 수 있다. 본 강의에서는 간단한 예만 다루어 보겠다. 먼저 mapply를 이용해서 여러 입력값의 합을 구하는 코드를 살펴보자.\n\nmapply(sum, 1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %>% mapply(1:5, 1:5)\n\n[1]  2  4  6  8 10\n\nsum %>% mapply(1:5, 1:5, 1:5)\n\n[1]  3  6  9 12 15\n\n\nmapply는 첫번째 인수에 함수를, 그 다음부터는 입력값들을 2개, 3개… 계속 받을 수 있다. 반면 map2와 pmap은 입력값을 먼저 받는데, 이 때문에 pmap에서는 입력값들을 리스트 형태로 받는다.\n\nmap2(1:5, 1:5, sum)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 4\n\n[[3]]\n[1] 6\n\n[[4]]\n[1] 8\n\n[[5]]\n[1] 10\n\npmap(list(1:5, 1:5, 1:5), sum)\n\n[[1]]\n[1] 3\n\n[[2]]\n[1] 6\n\n[[3]]\n[1] 9\n\n[[4]]\n[1] 12\n\n[[5]]\n[1] 15\n\n\n리턴 형태를 지정하려면 map과 마찬가지로 map2_*나 pmap_* 꼴의 함수를 이용하면 되며 아래의 코드들은 같은 결과를 나타낸다.\n\npmap_int(list(1:5, 1:5, 1:5), sum)\n\n[1]  3  6  9 12 15\n\nlist(1:5, 1:5, 1:5) %>% pmap_int(sum)\n\n[1]  3  6  9 12 15\n\n\n마지막으로 paste함수로 문자열을 합치는 예를 살펴보겠다. 먼저 map2_chr로 두 문자열을 합쳐보자.\n\nname <- c(\"Alice\", \"Bob\")\nplace <- c(\"LA\", \"New york\")\nmap2_chr(name, place, ~paste(.x, \"was born at\", .y))\n\n[1] \"Alice was born at LA\"     \"Bob was born at New york\"\n\n\n첫 번째 입력값은 함수에서 .x로 두 번째 입력값은 .y로 표현할 수 있다. pmap 함수를 이용할 때는 ..1, ..2, ..3으로 바꿔 표현하면 된다.\n\nlife <- c(\"born\", \"died\")\nlist(name, life, place) %>% pmap_chr(~paste(..1, \"was\", ..2, \"at\", ..3))\n\n[1] \"Alice was born at LA\"     \"Bob was died at New york\""
  },
  {
    "objectID": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "href": "posts/2019-01-03-rdatamanagement/index.html#마치며",
    "title": "R 데이터 매니지먼트: tidyverse",
    "section": "마치며",
    "text": "마치며\n지금까지 tidyverse 생태계에서 몇 가지 패키지를 이용해 데이터를 다루는 방법을 살펴보았다. 앞서 말했듯이 이 생태계에서 가장 중요한 것은 %>% 연산자를 이용하여 의식의 흐름대로 코딩을 수행하는 것이며, 이후 나머지 내용을 하나씩 적용해나가면 어느 순간 tidyverse 없이 살 수 없는(?) 자신을 발견하게 될 것이다. 본 글에서 다루지 않은 내용인 long, short 포맷을 다루는 tidyr, 문자열을 다루는 stringr, factor를 다루는 forcats, 날짜를 다루는 lubridate 그리고 모델을 다루는 modelr과 broom 등은 R for Data Science[^8]와 Rstudio cheetsheet[^9]를 참고하기 바란다. 다음번에는 빠른 속도가 장점인 data.table를 다시 한번 정리해 볼 생각이다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html",
    "href": "posts/2018-11-24-basic-biostatistics/index.html",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "",
    "text": "김진섭 대표는 11월 28일(수) 중앙보훈병원 정신건강의학과를 방문, 의학 연구에 필요한 기술 통계(descriptive statistics)에 대해 강의하고 자체 제작한 웹 애플리케이션과 Rstudio Addins를 이용하여 실습을 진행하였습니다. 강의 내용을 공유합니다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#시작하기-전에",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "시작하기 전에",
    "text": "시작하기 전에\n의학연구에서 R 활용 능력을 대략 5단계로 구분할 수 있다.\n\n데이터 정리는 미리 excel로 완료, 통계분석만 R 이용.\nR로 데이터 정리와 통계분석을 모두 수행.\nR로 그림을 그린다.\n논문에 들어갈 Table과 figure를 모두 R로 만든다.\n글쓰기, 참고문헌 등 논문의 모든 작업을 R에서 직접 수행한다.\n\n본 강의는 1단계에 해당하는 연구자가 R을 쉽게 이용할 수 있도록 돕는 내용에 해당하며 R과 Rstudio의 설치과정은 생략한다. 혹시 설치를 못했다면 https://www.r-project.org 와 https://www.rstudio.com/products/rstudio/download 를 참조하여 설치하길 바란다. R의 전반적인 도움말은 help.start() 명령어를 활용하고 특정 함수를 보려면 help(which) 형태로 실행하면 된다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#의학-연구에서의-기술-통계",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "의학 연구에서의 기술 통계",
    "text": "의학 연구에서의 기술 통계\n기술 통계는 원래 평균(mean), 중위수(median), 분산(variance), 빈도표(frequency table)등의 데이터를 설명하는 숫자들이나 히스토그램(histogram), 상자그림(box-plot)같은 그래프를 의미한다. 그러나 대부분의 의학 연구에서는 단순한 기술 통계가 아닌 그것들의 그룹 비교(ex: 성별, 질환 유무)가 Table 1에 기술 통계란 제목으로 제시된다.\n\n\n\n\nTable 1 example(Balaji 2011)\n\n\n\n\n보통 연구의 흐름은 기술 통계로 데이터를 보여주고 단변량(univariate) 분석을 통해 가설 검정을 수행한 후, 다변량(multivariate) 혹은 소그룹(subgroup) 분석 을 이용하여 다른 변수들의 효과를 보정한 결과를 보여주는 것으로 이루어진다. 그러나 단변량 분석에서 끝나는 간단한 연구도 많고 이것은 본질적으로 기술 통계의 그룹 비교와 같으므로, Table 1에 필요한 통계를 알고 쉽게 구현할 수 있다면 그것만으로 간단한 의학 연구를 수행할 수 있다.\n본 강의에서는 Table 1의 그룹 비교에 필요한 통계 방법들을 알아보고 R을 이용해서 실제 분석을 수행할 것이다. 통계 방법을 선택하는 기준은 크게 (1) 연속 변수(continuous variable) vs 범주형 변수(categorical variable), (2) 비교할 그룹 수, (3) 샘플 수 혹은 정규분포 여부 의 3가지가 있으며 추가로 짝지은 그룹인 경우를 살펴보겠다.\n마지막으로 자체 개발한 웹 애플리케이션과 Rstudio Addins을 사용하여 간단히 Table 1을 만들어 볼 것이다(Figure @ref(fig:appgif), @ref(fig:addingif))."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#연속-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "연속 변수의 그룹 비교",
    "text": "연속 변수의 그룹 비교\n연속 변수의 그룹 비교는 2 그룹일 때는 t-test, 3 그룹 이상이면 ANOVA라고 생각하면 되며, 2 그룹일 때 ANOVA 결과는 t-test 결과와 거의(?) 같다. 따라서 연속 변수는 무조건 ANOVA라고 생각해도 대충 맞다.\nT-test\nT-test는 2 그룹의 평균값을 비교하는 통계 방법1으로 필요한 숫자는 각 그룹의 평균과 분산이다. 실제로 데이터 없이 두 그룹의 평균과 분산만 있어도 t-test를 수행할 수 있으며 https://www.evanmiller.org/ab-testing/t-test.html 를 통해 웹에서도 간단히 계산할 수 있다. 아래 남녀의 총 콜레스테롤 데이터를 이용해 t-test를 수행해 보자.\n\n\n\n\n\n\n\n이제 t.test 함수를 이용하여 남녀의 총 콜레스테롤 수치를 비교한다.\n\nnev.ttest <- t.test(tChol ~ sex, data = data.t, var.equal = F)\nnev.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  tChol by sex\nt = -0.47138, df = 23.437, p-value = 0.6417\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -23.36286  14.68429\nsample estimates:\nmean in group Female   mean in group Male \n            149.3750             153.7143 \n\n\n여자의 평균 콜레스테롤 값은 149.4, 남자는 153.7 이고 \\(p\\)-value는 0.642임을 확인할 수 있다.\n위의 t.test 함수의 옵션 중 var.equal = F는 등분산 가정 없이 분석하겠다는 뜻으로 옵션을 적지 않아도 기본적으로 F가 적용된다. 등분산 가정이란 두 그룹의 분산이 같다고 가정하는 것인데, 계산이 좀 더 쉽다는 이점이 있으나 아무 근거 없이 분산이 같다고 가정하는 것은 위험한 가정이다. 위의 분석에 var.equal = T를 적용해보자.\n\nev.ttest <- t.test(tChol ~ sex, data = data.t, var.equal = T)\nev.ttest\n\n\n    Two Sample t-test\n\ndata:  tChol by sex\nt = -0.48157, df = 28, p-value = 0.6339\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -22.79682  14.11825\nsample estimates:\nmean in group Female   mean in group Male \n            149.3750             153.7143 \n\n\n앞서는 Welch t-test였는데 이름이 바뀐 것을 확인할 수 있고 \\(p\\)-value도 0.634로 아까와 다르다. 논문 리뷰어가 요구하는 등의 특별한 경우가 아니고서야 위험한 등분산가정을 할 필요가 없고, 이제부터 var.equal 옵션은 신경쓰지 않아도 좋다.\n\nANOVA2\n\n3 그룹 이상일 때는 2그룹씩 짝을 지어서 t-test를 여러 번 수행할 수 있다. 그러나 Table 1에서는 대부분 하나의 \\(p\\)-value만 제시하고 이것은 전체적으로 튀는 것이 하나라도 있는가?를 테스트하는 ANOVA를 이용한다.3 ANOVA는 비교할 모든 그룹에서 분산이 같다는 등분산 가정 하에 분석을 수행하며, 실제로 2 그룹일 때 ANOVA를 수행하면 등분산 가정 하에 수행한 t-test와 동일한 결과를 얻는다. 위에도 언급했듯이 모든 그룹에서 분산이 같다는 것은 너무 위험한 가정이나, 3 그룹 이상인 경우 마땅한 대안이 없고 Table 1에서 엄밀한 통계를 요구하는 것도 아니기 때문에 그냥 ANOVA를 쓰는 것이 관행이다. 아래 세 그룹의 총 콜레스테롤 데이터를 활용해 분석을 수행해 보자.\n\n\n\n\n\n\n\n이제 aov함수를 이용하여 3 그룹의 평균 콜레스테롤 값을 한번에 비교한다.\n\nres.aov <- aov(tChol ~ group, data = data.aov)\nsummary(res.aov)\n\n            Df Sum Sq Mean Sq F value Pr(>F)\ngroup        2   3421  1710.3   2.103  0.142\nResiduals   27  21956   813.2               \n\n\n결과에서 나온 \\(p\\)-value인 0.142가 Table 1에 이용되며 의미는 “3 그룹에서 총콜레스테롤 값이 비슷하다(다른 것이 있다고 할 수 없다)” 이다.\n비모수 통계: 정규분포 아닐 때\nT-test나 ANOVA는 모두 변수가 정규분포를 이룬다고 가정하고 분석을 수행하는데, 언뜻 생각하기에 의학에서 정규분포가 아닌 변수는 없을 것 같지만4 일부 지표(ex: CRP, 자녀 수)들은 정규분포를 따르지 않는다고 알려져 있다. 이 때는 변수의 값 자체가 아닌 순위 정보만을 이용하는 비모수 검정을 이용한다. T-test에 대응되는 비모수 분석은 Wilcoxon rank-sum test 혹은 Mann–Whitney U test 로 불리며 앞서 이용한 남녀별 총 콜레스테롤 데이터로 분석을 수행하면 아래와 같다.\n\nres.wilcox <- wilcox.test(tChol ~ sex, data = data.t)\nres.wilcox\n\n\n    Wilcoxon rank sum exact test\n\ndata:  tChol by sex\nW = 114, p-value = 0.951\nalternative hypothesis: true location shift is not equal to 0\n\n\n위 결과에서 \\(p\\)-value는 0.951임을 확인할 수 있다. ANOVA에 대응되는 비모수 분석은 Kruskal–Wallis one-way ANOVA이며 역시 앞서 이용한 그룹별 총 콜레스테롤 데이터에 적용하면 아래와 같다.\n\nres.kruskal <- kruskal.test(tChol ~ group, data = data.aov)\nres.kruskal\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  tChol by group\nKruskal-Wallis chi-squared = 2.6013, df = 2, p-value = 0.2724\n\n\n마찬가지로 \\(p\\)-value는 0.272임을 확인할 수 있다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#범주형-변수의-그룹-비교",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "범주형 변수의 그룹 비교",
    "text": "범주형 변수의 그룹 비교\n범주형 변수의 그룹 비교는 그룹 수나 정규분포를 고려할 필요가 없어 연속 변수일 때보다 훨씬 간단하며 딱 하나, 샘플 수가 충분한지만 확인하면 된다.\n샘플 수 충분: Chi-square test\nChi-square test는 두 범주형 변수가 관계가 있는지 없는지를 파악하는 테스트로5 아래의 혈압약과 당뇨약 복용 여부를 조사한 데이터로 분석을 수행해 보겠다.\n\n\n\n\n\n\n\n두 약물 복용 여부를 테이블로 나타내면\n\ntb.chi <- table(data.chi)\ntb.chi\n\n        DM_medi\nHTN_medi  0  1\n       0 15 13\n       1 14  8\n\n\n이며 언뜻 봐서는 관계가 있는지 아닌지 잘 모르겠다. 이제 chisq.test함수를 이용해서 Chi-square test를 수행하자.\n\nres.chi <- chisq.test(tb.chi)\nres.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.chi\nX-squared = 0.18246, df = 1, p-value = 0.6693\n\n\n\\(p\\)-value는 0.669가 나오고 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 말할 수 있다.\n샘플 수 부족: Fisher’s exact test\n이번엔 다른 사람들의 혈압, 당뇨약 복용 데이터로 chi-square test를 수행해 보겠다.\n\n\n\n\n\n\n\n아까와 마찬가지로 테이블로 두 약물 복용상태를 비교하면 아래와 같다.\n\ntb.fisher <- table(data.fisher)\ntb.fisher\n\n        DM_medi\nHTN_medi  0  1\n       0 31  8\n       1  9  2\n\n\n혈압약과 당뇨약을 모두 복용한 사람이 2명으로 좀 작아보이지만 무시하고 chi-square test를 수행하면 결과는 나오나 Warning 메시지가 뜬다.\n\nchisq.test(tb.fisher)\n\nWarning in chisq.test(tb.fisher): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb.fisher\nX-squared = 4.5971e-31, df = 1, p-value = 1\n\n\n이는 두 약을 모두 복용한 사람이 2명뿐이라서 일어나는 문제로, 일반적으로 분석할 테이블에서 샘플 수가 너무 작은 항이 있으면 chi-square test의 계산이 부정확해진다. 이 때는 fisher’s exact test를 수행하며 아래와 같이 fisher.test함수를 이용하면 된다.\n\nres.fisher <- fisher.test(tb.fisher)\nres.fisher\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb.fisher\np-value = 1\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.07627205 5.55561549\nsample estimates:\nodds ratio \n 0.8636115 \n\n\n\\(p\\)-value는 1로 확인되고 마찬가지로 혈압약 복용과 당뇨약 복용은 유의한 관계가 없다고 할 수 있다.\n여기서 의문점이 들 수 있다. 무조건 fisher’s test만 하면 간단한데 도대체 chi-square test는 왜 하는 것일까? 샘플 수가 작을 때는 fisher’s test만 하는 것이 실제로 더 간단하고 방법론적으로도 아무 문제가 없다. 그러나 샘플 수나 그룹 수가 늘어날수록 fisher’s test는 필요한 계산량이 급격하게 증가하는 문제가 있어 chi-square test를 먼저 수행하는 것을 권유한다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#추가-짝지은-2-그룹",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "추가: 짝지은 2 그룹",
    "text": "추가: 짝지은 2 그룹\n각 사람의 혈압을 한 번은 사람이 직접, 한 번은 자동혈압계로 측정했다고 하자. 이 때 직접 잰 혈압과 자동혈압계의 측정값을 비교한다면 t-test로 충분할까? t-test는 혈압 재는 방법마다 평균을 먼저 구한 후 그것이 같은지를 테스트하므로 짝지은 정보를 활용하지 못한다. 이 때는 각 사람마다 두 혈압값의 차이를 먼저 구한 후 평균이 0인지를 테스트하면, 짝지은 정보를 활용하면서 계산도 더 간단한 방법이 된다.\n연속변수: Paired t-test\n위에 언급한 대로 각 사람마다 차이값을 먼저 구한 후 그 평균이 0인지를 테스트하는 방법이 paired t-test이다. 아래의 수축기 혈압 데이터를 통해 t-test와의 차이점을 알아보자.\n\ndata.pt <- data.frame(SBP_hand = round(rnorm(30, mean = 125, sd = 5)), SBP_machine = round(rnorm(30, mean = 125, sd = 5)))\nrownames(data.pt) <- paste(\"person\", 1:30)\ndatatable(data.pt, rownames = T, caption = \"data.pt: systolic blood pressure measured by hand & machine\")\n\n\n\n\n\n\n위 데이터는 30명의 사람이 앞서 말한 두 가지 방법으로 수축기 혈압을 측정한 데이터이다. 먼저 t-test를 수행하자.\n\npt.ttest <- t.test(data.pt$SBP_hand, data.pt$SBP_machine)\npt.ttest\n\n\n    Welch Two Sample t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.45768, df = 57.863, p-value = 0.6489\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -3.224307  2.024307\nsample estimates:\nmean of x mean of y \n    125.0     125.6 \n\n\n위 결과를 보면 각 방법의 평균을 먼저 구한 후 그것을 비교한 것을 확인할 수 있고 \\(p\\)-value는 0.649이다. 이제 paired t-test를 수행하자.\n\npt.ttest.pair <- t.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\npt.ttest.pair\n\n\n    Paired t-test\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nt = -0.46171, df = 29, p-value = 0.6477\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.257804  2.057804\nsample estimates:\nmean difference \n           -0.6 \n\n\n이번에는 사람마다 차이값을 먼저 구한 후 그것이 0인지 테스트 한 것을 확인할 수 있고 \\(p\\)-value는 0.648이다.\nPaired t-test의 비모수버전은 wilcoxon-signed rank test 이며 아래와 같이 실행한다.\n\npt.wilcox.pair <- wilcox.test(data.pt$SBP_hand, data.pt$SBP_machine, paired = TRUE)\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with ties\n\n\nWarning in wilcox.test.default(data.pt$SBP_hand, data.pt$SBP_machine, paired =\nTRUE): cannot compute exact p-value with zeroes\n\npt.wilcox.pair\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  data.pt$SBP_hand and data.pt$SBP_machine\nV = 214, p-value = 0.9482\nalternative hypothesis: true location shift is not equal to 0\n\n\n본 강의에서는 다루지 않겠지만 짝지은 3개 이상의 그룹은 repeated measure ANOVA6라는 방법을 이용한다.\n범주형 변수: Mcnemar test, Symmetry test for a paired contingency table\n이번에는 측정값이 0,1과 같은 범주형 변수인 경우를 살펴보자. 아래 데이터를 활용해 약 복용 전후로 복통증상 발생에 차이가 있는지 알아본다고 하자.\n\n\n\n\n\n\n\n이 데이터를 2 by 2 테이블로 정리하면 아래와 같다.\n\ntable.mc <- table(data.mc)\ntable.mc\n\n           Pain_after\nPain_before 0 1\n          0 8 8\n          1 9 5\n\n\n먼저 앞서 배운 Chi-sqaure test 를 이용한 결과를 보자.\n\nmc.chi <- chisq.test(table.mc)\nmc.chi\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table.mc\nX-squared = 0.17514, df = 1, p-value = 0.6756\n\n\n이것은 약 복용 전 복통 증상과 복용 후의 복통 증상이 얼마나 관계가 있는지 알아보는 테스트로 짝지은 정보를 활용하지 않는다. 이번엔 짝지은 정보를 활용하는 mcnemar test를 수행하자.\n\nmc.mcnemar <- mcnemar.test(table.mc)\nmc.mcnemar\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  table.mc\nMcNemar's chi-squared = 0, df = 1, p-value = 1\n\n\nMcnemar test는 약 복용후 증상발생이 달라진 사람 즉, discordant pair만 분석에 이용한다. 따라서 condordant pair 의 구성과 어떻더라도 통계결과는 동일하게 나온다.\n한편 측정값이 3개 이상일 때는 chi-square test는 그대로 이용할 수 있으나, mcnemar test는 그대로 쓰지 못하고 symmetry test for a paired contingency table7라는 일반화된 테스트를 사용한다. 본 강의에서는 간단한 실행법만 살펴볼 것이며 먼저 rcompanion R package를 설치하자.\n\n## For symmmetry test\n#install.packages(\"rcompanion\")\nlibrary(rcompanion)\n\n예제 테이블(3 \\(\\times\\) 3)을 아래와 같이 불러온 후\n\n## Example data\ndata(AndersonRainGarden) \nAndersonRainGarden       \n\n             Yes.after No.after Maybe.after\nYes.before           6        0           1\nNo.before            5        3           7\nMaybe.before        11        1          12\n\n\nnominalSymmetryTest 함수로 분석을 수행한다.\n\n## Symmetry test\nnominalSymmetryTest(AndersonRainGarden)\n\n$Global.test.for.symmetry\n  Dimensions  p.value\n1      3 x 3 0.000476\n\n$Pairwise.symmetry.tests\n                                       Comparison p.value p.adjust\n1       Yes.before/Yes.after : No.before/No.after  0.0736   0.0771\n2 Yes.before/Yes.after : Maybe.before/Maybe.after 0.00937   0.0281\n3   No.before/No.after : Maybe.before/Maybe.after  0.0771   0.0771\n\n$p.adjustment\n  Method\n1    fdr\n\n$statistical.method\n        Method\n1 McNemar test"
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#실습",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "실습",
    "text": "실습\n웹 애플리케이션\nAnpanman 에서 만든 기초통계 앱을 소개한다(Figure @ref(fig:appgif)). 5메가 이하의 excel, csv 형태 혹은 sas, spss 프로그램으로 만든 데이터를 업로드하면 Table 1과 회귀분석, 로지스틱 회귀분석을 간단하게 수행하고 결과를 excel로 바로 다운받을 수 있다.\n\n\n\n\nApplication mady by Anpanman\n\n\n\n\nRstudio Addins\n5메가보다 큰 데이터는 R에서 데이터를 읽은 후, 자체적으로 만든 jsmodule R package를 설치하여 앱을 이용할 수 있다.\n\n## For private package install \ninstall.packages(\"devtools\")   \ndevtools::install_github(c(\"jinseob2kim/jstable\", \"jinseob2kim/jsmodule\")) \n\n패키지를 설치한 후 Rstudio 프로그램의 Addins 탭을 누르면 Basic statistics 항목이 보일 것이다. 데이터를 읽고 그것의 이름을 드래그 한 상태로 Basic statistics 를 누르면 된다(Figure @ref(fig:addingif)).\n\n\n\n\nRstudo Addins made by Anpanman8\n\n\n\n\n직접 R에서 코드를 실행하고 싶은 유저는 tableone R package를 참고하기 바란다."
  },
  {
    "objectID": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "href": "posts/2018-11-24-basic-biostatistics/index.html#마치며",
    "title": "의학 연구에서의 기술 통계 with R",
    "section": "마치며",
    "text": "마치며\n지금까지 의학 연구에서 쓰이는 그룹 비교 통계들을 알아보고 웹 앱과 Rstudio Addins 을 이용하여 직접 Table 1을 만들어보았다. 앞으로 연구자들은 어려운 통계 프로그램을 이용할 필요 없이 Anpanman의 서비스를 활용, 빠르게 통계 분석을 수행하고 테이블과 그림을 바로 다운받을 수 있다."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html",
    "href": "posts/2019-01-09-doctorskku2019/index.html",
    "title": "진료실 밖 의사로서의 경험",
    "section": "",
    "text": "김진섭 대표는 2월 1일(금) 성균관대학교 의과대학 학부 강의인 의사의 길에서 진료실 밖 의사로서의 경험을 의대생들과 공유할 예정입니다. 발표 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "href": "posts/2019-01-09-doctorskku2019/index.html#요약",
    "title": "진료실 밖 의사로서의 경험",
    "section": "요약",
    "text": "요약\n\n수학만 하다가 얼떨결에 1학기 수시 합격.\n예방의학 전공하며 통계, 프로그래밍 공부.\n삼성전자 근무하며 디지털헬스와 창업을 알게 됨.\n통계 이론으로 박사논문 쓰려다 실패, 창업지원사업 선정.\n통계지원 법인 설립."
  },
  {
    "objectID": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "href": "posts/2019-01-09-doctorskku2019/index.html#slide",
    "title": "진료실 밖 의사로서의 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/doctorskku2019 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html",
    "href": "posts/2019-11-30-rseleniumtip/index.html",
    "title": "RSelenium 이용 팁",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 1월 Shinykorea 밋업에 참석, RSelenium 으로 웹크롤링을 하면서 얻은 팁을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "href": "posts/2019-11-30-rseleniumtip/index.html#요약",
    "title": "RSelenium 이용 팁",
    "section": "요약",
    "text": "요약\n웹에 로그인 후 클릭기반 데이터 다운받는 과정을 RSelenium 으로 자동화 하였다.\n\nSelenium docker image 를 이용, 복잡한 설치과정 없이 Selenium 을 실행하고 다운로드 경로를 설정하였다.\nfindElement 와 sendKeysToElement, clickElement 를 이용, 아이디와 비번을 입력하고 로그인버튼을 클릭하였다.\nclickElement 이 안될 때 mouseMoveToLocation 과 click 을 이용, 마우스로 클릭하였다.\n작업 팝업창을 바꾸는 switchToWindow 가 안될 때, queryRD 로 자체 함수를 만들어 작업하였다.\n50개 일별 데이터 다운로드에 성공하였다."
  },
  {
    "objectID": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "href": "posts/2019-11-30-rseleniumtip/index.html#slide",
    "title": "RSelenium 이용 팁",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/RSelenium 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html",
    "href": "posts/2021-07-05-channel.io-install-review/index.html",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "",
    "text": "얼마 전 회사 웹사이트에 채널톡 서비스를 추가했습니다. 우리 회사는 Hugo를 이용해 홈페이지를 운영합니다. Hugo는 Jekyll과 비슷한 서비스로 Static Site Generator의 한 종류입니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설치",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설치",
    "text": "설치\n우선 채널톡 서비스는 Javascript 코드를 추가하는 것만으로도 쉽게 설치할 수 있습니다. 회원가입을 하고 나면 페이지에 추가해야 할 Javascript 코드가 제공됩니다. 설정을 위해, 제공되는 Javascript 코드를 파일화 하여 홈페이지 프로젝트 폴더의 /public/js에 넣고 config.toml파일에 해당 Javascript 파일을 사용하겠다고 설정해주는 것으로 간단히 설치가 끝납니다.\n\n\n\nchannel.js는 제가 임의로 지정한 이름입니다. 원하시는 이름으로 바꾸셔도 무방합니다.\n\n\n설치 직후에는 두 번째 행이\n\ncustom_js = []\n\n와 같은 상태입니다. 원하는 Javascript 파일의 경로와 이름을 큰따옴표로 묶어 대괄호 속에 넣어주면 사이트에 해당 파일이 적용됩니다. CSS또한 동일한 방법으로 custom_css에 원하는 파일의 경로와 이름을 입력한 후 선택적으로 적용할 수 있습니다. 이후 Netlify를 통해 사이트를 빌드 후 Publish 하였습니다."
  },
  {
    "objectID": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "href": "posts/2021-07-05-channel.io-install-review/index.html#설정",
    "title": "채널톡(channel.io) 설치 후기",
    "section": "설정",
    "text": "설정\n \n처음 실행 후 먼저 회사의 로고와 설명을 추가하고, 채널톡 주소도 설정하였습니다.\nZarathu는 Medical Research Supporters입니다. 위와 같이 설정하고 나면\n\n위와 같이 고객님들께서 상담 버튼을 누르자마자 나오는 메시지들과 이미지가 변경됩니다.\n고객님들께서 상담을 시작하시면\n\n위에서 설정한 것과 같은 메시지가 표시됩니다. 이는 회사의 응대를 기다리는 동안 정보를 입력하게 하여 상담을 원활히 진행하게 하는 것에 큰 도움이 됩니다.\n템플릿 설정도 있어, 매크로를 통해 빠르게 응대할 수도 있었습니다. \n\n마지막으로 운영 시간을 설정하였습니다. 출근, 퇴근 시 따로 설정할 필요 없이 자동으로 온/오프라인 설정을 할 수도 있었고 수동으로 설정할 수도 있었습니다. 저희는 회사 특성상 수동으로 설정하였습니다.\n\n종합적인 테스트를 해 보았습니다. 웹으로 들어가 테스트라고 메시지를 남기니\n\n와 같은 정보를 물을 수 있습니다. 이후 담당자를 배정한 후 테스트 계정과 소통할 수 있었습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html",
    "href": "posts/2022-08-26-shinyforpython/index.html",
    "title": "Shiny for Python",
    "section": "",
    "text": "이제 파이썬에서도 Dash 이외에도 shiny를 사용하여 반응형 웹앱을 쉽게 구현할 수 있게 되었습니다.\n아직 초기 단계라 많은 것이 구현되지는 않았고 Alpha단계이므로 API가 변경되거나 아래의 설명과 다른 점이 생길 수 있습니다. 글의 내용은 Shiny for Python를 참고했습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#python과-shiny-설치하기",
    "title": "Shiny for Python",
    "section": "\n2.1 python과 Shiny 설치하기",
    "text": "2.1 python과 Shiny 설치하기\n우선 Shiny app을 저장할 디렉토리를 만듭니다.\n\n# make new directory for Shiny App\nmkdir myapp\n\n# change directory\ncd myapp\n\npython은 Python.org Anaconda 에서 설치할 수 있습니다.\npython이 설치되었다면 사용하는 디렉토리의 폴더에 python 가상 환경을 만들고 이를 활성화합니다.\n\n# Create a virtual environment in the .venv subdirectory\npython3 -m venv venv\n\n# Activate the virtual environment\nsource venv/bin/activate\n\nshiny 를 설치합니다.\n\n# install shiny\npip install shiny"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-server에서-이용하기",
    "title": "Shiny for Python",
    "section": "\n2.2 Shiny Server에서 이용하기",
    "text": "2.2 Shiny Server에서 이용하기\nShiny for python은 Shinyapps.io, Shiny server, shinylive와 같은 다양한 방법으로 배포될 수 있습니다. 그중 shiny server에 배포하는 방법을 소개합니다.\nShiny Server는 v1.5.19 이상이 필요합니다. 만약 이전 버전을 사용하고 있다면 업데이트 합니다.\n\n#shiny-server v1.4.19\nwget https://download3.rstudio.org/ubuntu-18.04/x86_64/shiny-server-1.5.19.995-amd64.deb\ngdebi shiny-server-1.5.19.995-amd64.deb\n\nshiny server에서 python 파일을 실행하기 위해 config file을 수정해야 합니다.\nconfig file 은 /etc/shiny-server/shiny-server.conf 에 위치해있습니다.\n\n# Edit the file /etc/shiny-server/shiny-server.conf\nsudo vim /etc/shiny-server/shiny-server.conf\n\nconfig file 을 열어 코드 상단에 python 경로를 추가합니다.\n예를 들어 /home/js/myapp/venv/bin/python3를 사용하고 싶다면 코드는 아래와 같습니다.\n\n# Use system python3 to run Shiny apps\npython /home/js/myapp/venv/bin/python3;\n\n# Instruct Shiny Server to run applications as the user \"shiny\"\nrun_as shiny;\n\n# Define a server that listens on port 3838\nserver {\n  listen 3838;\n\n  # Define a location at the base URL\n  location / {\n\n    # Host the directory of Shiny Apps stored in this directory\n    site_dir /srv/shiny-server;\n\n    # Log all Shiny output to files in this directory\n    log_dir /var/log/shiny-server;\n\n    # When a user visits the base URL rather than a particular application,\n    # an index of the applications available in this directory will be shown.\n    directory_index on;\n  }\n}"
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-app",
    "title": "Shiny for Python",
    "section": "\n3.1 Shiny App",
    "text": "3.1 Shiny App\n\n\npython\nR\n\n\n\n\n# import shiny\nfrom shiny import ui, render, App\n\n# ui\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"N\", 0, 100, 40),\n    ui.output_text_verbatim(\"txt\"),\n)\n\n# server\ndef server(input, output, session):\n    @output\n    @render.text\n    def txt():\n        return f\"n*2 is {input.n() * 2}\"\n\n# This is a shiny.App object. It must be named `app`.\napp = App(app_ui, server)\n\n\n\n\n\nlibrary(shiny)\n\n# ui\nui = fluidPage(\n    sliderInput(\"n\", \"N\", 0, 100, 40),\n    textOutput(\"txt\")\n  )\n\nserver = function(input, output,session) {\n     output$txt = renderText(\"n*2 is\",input$n * 2,\"}\")\n  }\n\nshinyApp(ui, server)\n\n\n\n\n위는 ui에서 입력 받은 값을 server에서 2를 곱해주어 계산하고 이를 ui부분에서 출력하여 보여주는 예시입니다.\n\n먼저 from shiny import *를 통해 필요한 shiny 모듈을 불러옵니다.\nui의 ui.input_slider() 함수를 통해 입력값을 받습니다. 이처럼 인풋에 해당하는 부분은 ui.input_*() 함수를 통해 만들 수 있습니다. “n”은 해당 input의 이름에 해당하는 부분이며, N은 label, 0,100,40은 각각 min, max,value에 해당하는 인자입니다.\n입력된 인풋값을 서버에 전송하면 이를 토대로 계산하여 값을 return하고 @render.text 라는 decorator를 통해 텍스트 형태로 렌더링합니다.\n다시 ui 부분에서는 ui.output_text_verbatim() 함수를 통해 텍스트를 출력합니다. 이처럼 ui부분에서 출력할 때는 ui.output_* 함수가 사용됩니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "href": "posts/2022-08-26-shinyforpython/index.html#shiny-for-python의-문법",
    "title": "Shiny for Python",
    "section": "\n3.2 Shiny for Python의 문법",
    "text": "3.2 Shiny for Python의 문법\n위의 예시를 이용하여 R과 Python의 문법을 간단하게 비교해보면 다음과 같습니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\n\nimport shiny\nfrom shiny import *\nlibrary(shiny)\n\n\nUI\napp_ui = ui.page_fluid(\nui = fluidPage(\n\n\ninput\n  ui.input_slider(“n”,“N”,0,100,40),   ui.output_text_verbatim(“txt”)   )\n  sliderInput(“n”,“N”,0,100,40)   textOutput(“txt”))\n\n\nServer\ndef server(in, out, session):\nserver=function(in,out,session){\n\n\ndecorator\n\n@output @render.text\n\n\n\noutput\n  def txt:     return input.x()\n output$txt = renderText (input $ x)   }\n\n\napp\napp = App(app_ui, server)\nshinyApp(ui, server)\n\n\n\n인풋에 해당하는 부분은 ui.input_*(), ui 부분에서 출력할 때는 ui.output_*()함수들을 사용합니다.\n서버 부분에서 def server(input, output, session): def txt(): 와 같이 파이썬의 함수 문법을 사용합니다.\n\n\n\n\n\n\n\n\npython\nR\n\n\ndecorater\n\n@render.text … @reactive.event() @reactive.Calc() …@output\nrenderText() …  reactive({}) output$id\n\n\n예시의 server부분에서 @render.text @output이 사용된 것을 볼 수 있습니다. 이것은 함수를 인자로 받아 함수를 출력하는 decorator 라는 함수입니다. 보통 python 에서 여러 함수들이 부분적으로 중복될때 코드의 재사용을 용이하게 하기 위해 사용됩니다. 여기서는 그냥 함수의 일종이라고 생각하면 될 것 같습니다.\nR에서는 렌더링을 위해 renderPlot, renderText 같은 함수를 사용하지만 Python에서는 @render.text 같은 decorator를 사용합니다. 위의 예시에서는 txt() 라는 텍스트를 반환하는 함수에 @render.text라는 decorator가 적용됩니다. 이는 텍스트를 반환하는 함수를 리턴하는 함수로 텍스트를 렌더링해 주는 함수라고 생각할 수 있습니다.\ndecorator는 output, module, reactivity, rendering 등 많은 부분에서 사용됩니다. 하나의 함수에 여러개가 적용될 수 있으며 @render.text 는 @output 보다 먼저 적용되어야 하는 것과 같이 순서나 parameter가 정해져 있습니다.\n\n\n\npython\nR\n\n\nHTML\nui.div(), ui.a()\ntags$div, tags$a\n\n\nHTML 태그의 경우는 ui.tags.*() 를 통해서 사용할 수 있습니다. 예를 들어 li 태그의 경우 ui.tags.li()로 사용 가능합니다.\n일반적으로 많이 사용되는 div, span, a 같은 태그들은 ui.div()와 같이 ui 서브모듈에서 직접 사용할 수 있습니다.\n\n\n\npython\nR\n\n\nmutability\nobjects can be modified\nobjects cannot be modified\n\n\npython으로 shiny앱을 작성할때 가변성 처리(Handling mutability)도 고려해야합니다. Python에서 문자,숫자열,튜플 같은 간단한 객체들은 변경할 수 없지만 딕셔너리,리스트 같은 대부분의 객체들은 수정할 수 있습니다.\n이로 인해 반응형 프로그래밍에서 문제가 발생할 수 있습니다. 즉, 프로그램의 한 부분에서 객체를 수정하면 프로그램의 다른 부분과 값이 다른 문제가 발생할 수 있습니다.\n이러한 문제를 해결하기 위한 몇가지 방법들이 있습니다.\n첫번째는 두 값이 객체를 copy해서 동일한 객체를 먼저 가리키는 것을 피하는 것입니다. a = [1,2], b = a.copy와 같이 사용하게 되면 a의 값이 바뀌어도 b는 바뀌기 이전의 값을 가리키게 됩니다. 두번째는 객체를 변경하는 연산자나 매서드를 사용하는 것입니다. 예를 들어 a = [1,2]를 a = [1,2,3]로 만들고 싶을 때 a = a+[3]보다는 a.append(3)을 사용하는것이 바람직합니다. 마지막은 변경 불가능한 객체를 사용하는 것입니다. 리스트 대신 튜플을 사용하거나 pyrsistent 패키지의 리스트나 딕셔너리를 사용할 수 있습니다."
  },
  {
    "objectID": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "href": "posts/2022-08-26-shinyforpython/index.html#shinylive-배포",
    "title": "Shiny for Python",
    "section": "\n4.1 shinylive 배포",
    "text": "4.1 shinylive 배포\nshinylive editor을 통해 간단하게 shinylive를 통해 배포해볼 수 있습니다 .\n이외에도 Netlify, GITHUB gist를 통해서도 가능하며 Sharing Shinylive applications를 참고할 수 있습니다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "",
    "text": "김진섭 대표는 5월 31일(금) 차라투(주)가 후원하는 Shinykorea 밋업에 참석, shiny와 R Markdown을 활용, 의학연구를 지원했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#요약",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "요약",
    "text": "요약\n\n\n의학연구자들에게 맞춤형 ShinyApps를 제공함.\n\n범용으로 쓰일만한 것들을 Shiny module로 만든 후, 웹과 RStudio Addins로 배포.\n\n\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 리포트를 제공 중(R Markdown 활용).\n심평원/보험공단 빅데이터 연구에서 R Markdown 리포트로 연구지원 중."
  },
  {
    "objectID": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "href": "posts/2019-05-10-shinymedicalresearch/index.html#slide",
    "title": "Shiny 활용 의학연구지원 경험",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/ 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html",
    "href": "posts/2019-08-25-shinymanager/index.html",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "",
    "text": "김진섭 대표는 Zarathu 가 후원하는 9월 Shinykorea 밋업에 참석, Shiny 의 로그인 기능 추가방법을 리뷰하고, useR! 2019 에서 소개된 shinymanager 패키지 사용법을 설명할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#요약",
    "href": "posts/2019-08-25-shinymanager/index.html#요약",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "요약",
    "text": "요약\n\nshinymanager 로 UI 종류에 상관없이, 간단하게 로그인기능을 추가한다.\nSQLite db 를 이용, 접속자와 그 log를 관리한다."
  },
  {
    "objectID": "posts/2019-08-25-shinymanager/index.html#slide",
    "href": "posts/2019-08-25-shinymanager/index.html#slide",
    "title": "ShinyApps 에 로그인 기능 넣기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/shinymanager 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html",
    "href": "posts/2021-09-28-shinyecrf2/index.html",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 삼성서울병원 심혈관중재실에 서비스중인 shiny 환자데이티 입력웹을 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "href": "posts/2021-09-28-shinyecrf2/index.html#요약",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "요약",
    "text": "요약\n4월 발표 개발완료 후 서비스 중\n\n수백개 변수 추가, 입력화면 디자인 개선(Thanks to 김진환)\nshinydashboard 적용, 환자수 대시보드 추가(Thanks to 고현준)\nshinymanager 로 로그인 모듈: 특정 ID만 삭제권한.\n\n타 연구 빠른적용 위해 모듈화 필요, 설문지 Builder 가능할까?"
  },
  {
    "objectID": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "href": "posts/2021-09-28-shinyecrf2/index.html#slide",
    "title": "Shiny 환자데이터 입력웹 개발(2)",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://zarathucorp.github.io/eCRF-SMCcath/shinykorea2 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "",
    "text": "김진섭 대표는 11월 14일 서울IT직업전문학교 빅데이터 사이언스 실무자 양성과정 에 강사로 초청, Shiny 기초학습을 위한 워크샵을 진행할 계획입니다. 강의 슬라이드를 포함한 실습 자료를 미리 공유합니다. daattalli 의 NBA 2018/2019 선수 스탯 데이터를 이용한 워크샵 을 그대로 활용했습니다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습-목표",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습 목표",
    "text": "실습 목표\n\nRStudio cloud 를 이용, 클라우드 환경에서 R을 쓸 수 있다.\napp.R 파일에 Shiny의 ui와 server 코드를 입력할 수 있다.\nfluidPage의 sidebarLayout 레이아웃을 이용, 왼쪽에는 UI 옵션, 오른쪽에는 해당되는 결과를 보여줄 수 있다.\nDT 패키지로 데이터를, ggplot2 로 히스토그램을 보여줄 수 있다.\nReactivity 를 이해한다.\nshinyapps.io 에 app 을 배포할 수 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-rstudio-cloud",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: RStudio cloud\n",
    "text": "실습환경 만들기: RStudio cloud\n\nStep 1: https://rstudio.cloud 회원 가입\nStep 2: https://rstudio.cloud/spaces/30306/join?access_code=s4hEiPXQF%2BjosPclQEzgTtR0mPWDuh7Dhr2O7wAg 들어가서 “Join Space” 클릭\nStep 3: 위쪽 “Projects” 클릭 후, “New Project” 를 눌러 “New Project from Git Repo” 선택. Repo 주소는 https://github.com/jinseob2kim/shiny-workshop-odsc2019\n\n\nStep 3\n\n\n모든 강의자료는 RStudio cloud 안에 있다."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#실습환경-만들기-개인-pc",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "실습환경 만들기: 개인 PC",
    "text": "실습환경 만들기: 개인 PC\nStep 1: 패키지 설치\n\ninstall.packages(c(\"shiny\", \"ggplot2\", \"dplyr\", \"DT\", \"colourpicker\", \"readr\")) \n\nStep 2: https://github.com/jinseob2kim/shiny-workshop-odsc2019 들어간 후\nStep 3: 녹색 “Clone or download” 클릭 후 “Download ZIP” 을 눌러 자료 다운."
  },
  {
    "objectID": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "href": "posts/2019-10-26-shinyworkshop2019/index.html#slide",
    "title": "Shiny 워크샵: 서울IT직업전문학교 국비교육",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/shiny-workshop-odsc2019 를 클릭하면 볼 수 있고, 전체 워크숍 자료는 https://github.com/jinseob2kim/shiny-workshop-odsc2019에서 다운받을 수 있다."
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 5주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "href": "posts/2022-02-08-traefik-reverseproxy/index.html#목차",
    "title": "Docker와 Traefik을 활용한 Reverse-Proxy 구현",
    "section": "목차",
    "text": "목차\n\nTraefik이란?\ndocker-compose.yml 파일 작성\nrules.yml 파일 작성\n실행결과\n결론\n\n\nTraefik이란?\n nginx와 같이 reverse프록시의 종류로서 별도의 제어 없이 실행중에 실시간으로 통신되는 요소끼리 찾아서 연결해주는 기능이 특징입니다. 또한 기본적으로 제공하는 대시보드 기능을 통하여 실시간으로 연결되어 있는 서비스들을 확인할 수 있고 또한 어떤 서버와 연결되어 있는지 파악이 가능합니다.\n해당 게시글은 Docker와 Traefik version2.2를 활용한 서비스에서 Reverse-Proxy를 구현하는 방법에 대해 설명 하도록 하겠습니다. 위 이미지에서 보이는 것처럼 Traefik은 들어오는 요청을 각각의 Docker Container에 배정해주는 역할을 수행합니다.\n아래 이미지는 Traefik 대시보드 페이지 입니다. \n\n\n\ndocker-compose.yml 파일 작성\n\ntraefik을 image로 가지고 있는 proxy 컨테이너와, whoami를 image로 가지고 있는 website 컨테이너를 생성하기 위한 docker-compose.yml에 작성된 코드입니다. 해당 코드를 활용하여 traefik으로 각 컨테이너를 어떻게 제어할 수 있는지에 대해 알아보도록 하겠습니다. 먼저 proxy 컨테이너 속 주요 코드를 살펴 보도록 하겠습니다.\n\n\nProxy 컨테이너\nimage: traefik:v2.2\ncommand:\n  - --entrypoints.web.address=:80\n  - --entrypoints.websecure.address=:443\n컨테이너의 이미지로서 traefik:v2.2를 사용하며 entrypoints로서 80번 포트로 들어오는 요청들은 web, 443번 포트로 들어오는 요청들은 websecure로 각각 명명하는 코드입니다\n\n- --certificatesresolvers.re.acme.email=*****@naver.com\n- --certificatesresolvers.re.acme.storage=./acme.json\n- --certificatesresolvers.re.acme.httpchallenge.entryPoint=websecure\n웹사이트에 Https를 적용하기 위해 Let’s Encrypt로부터 Certificate를 발급 받는 과정을 ACME protocol을 활용해서 자동으로 발급 받고 적용시킬 수 있도록 해주는 코드입니다.\n\n위 코드에서 ‘re’, ‘email 주소’, ‘acme.json’ 파일의 경로 혹은 파일명’은 사용자가 편하게 수정해도 괜찮습니다. 물론 entryPoint 값 또한 앞서 선언한 entrypoints 중에 본인이 희망하는 포트로 변경해도 괜찮습니다.\n\n해당 코드를 작성하기에 앞서 해당 경로에 acme.json파일을 생성해야 합니다. 이후 코드를 실행시키면 acme.json 파일에 certificate에 대한 내용이 담기게 됩니다.\n\n(아래 이미지는 acme.json 파일의 일부입니다.)\n\n\nports:\n  - 80:80\n  - 443:443\n  - 8080:8080\n80번 포트, 443번 포트, 8080포트로 들어오는 요청에 대한 포트 연결입니다.\n\n8080포트는 Traefik dashboard의 기본 포트입니다. 8080포트를 통하여 Traefik dashboard에 접속이 가능합니다.\n\nvolumes:\n  - /var/run/docker.sock:/var/run/docker.sock\n  - ./rules.yml:/etc/traefik/rules.yml:ro\n  - ./acme.json:/acme.json \ntraefik이 /var/run/docker.sock를 사용 가능하도록하여 docker container들의 정보를 사용이 가능합니다. 또한 앞서 작성한 acme.json 파일과 앞으로 작성할 rules.yml 파일 또한 사용이 가능하도록하는 코드입니다. rule.yml파일을 통하여 저희는 동작을 제어할것입니다.\n\n\n\nWebsite 컨테이너\nwebsite:\n  image: containous/whoami\n  labels:\n    - traefik.http.routers.website.rule=Host('food.dogdog.cf')\n    - traefik.http.routers.website.tls=true\n    - traefik.http.routers.website.tls.certresolver=re\n    - traefik.http.routers.website.entrypoints=websecure\nwebsite 컨테이너의 image를 containous/whoami로 설정합니다. 또한 http 서비스 제공시 routers 규칙을 traefik.http.routers.website.()를 통해 설정 합니다.’food.dogdog.cf’로 요청이 들어오면 ‘website’컨테이너로 라우팅이 되도록 설정합니다. 또한 https가 가능하도록 ’tls=true’로 설정하며, ’tls/certresolver=re’ 앞서 설정한 re 값을 certresolver값으로 설정합니다. 해당 컨테이너의 entrypoints는 앞서 설정한 :443포트로 들어오는 요청인 ’websecure’로 설정합니다. traefik 동작의 전반적인 이해를 돕고자 이미지를 첨부합니다.\n\n\n- traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https\n- traefik.http.routers.redirs.rule=hostregexp(`{host:.+}`)\n- traefik.http.routers.redirs.entrypoints=web\n- traefik.http.routers.redirs.middlewares=redirect-to-https\nhttp로 ’food.dogdog.cf’를 통해 들어오는 요청들에 대해서 https로 redirect가 가능하도록 도와주는 코드입니다.\n\n\n\n\nrules.yml 파일 작성\n앞서 작성한 docker-compose.yml 파일을 통해서 서비스를 제공하는 기본적인 컨테이너 구축은 완료했습니다. 이제는 특정 요청에 대해서는 외부 서버에 있는 서비스를 이용하도록 하는 rules.yml 파일을 작성하도록 하겠습니다. \n\nroute1: \n  entryPoints:\n    -websecure\n  rule: Host('food.dogdog.cf')&&PathPrefix('/toy')\n  service: reverse-proxy\n  tls: {}\nentrypoints가 websecure로 들어오는 요청(:443포트로 들어오는 요청)에 대해서 만약 ‘food.dogdog.cf/toy’ 요청이면 reverse-proxy라는 서비스로 넘기며 해당 서비스의 내용을 수행하며. tls(https 서비스)를 사용하겠다는 코드입니다. 해당 코드형식만 유지하면 변수명 및 요청명에 대해서는 수정하셔도 됩니다.\n\n\n\n실행결과\n\n\n\n\n\n\n결론\nTraefik을 활용한 reverse-proxy를 구현하는 방법에 대해 알아봤습니다. 실습을 진행하며 파악한 것과 같이 Traefik에서는 let’s Encrypt를 통해 자동으로 https업로드, 대시 보드 제공을 통한 router, service, Middleware 상태 확인등의 기능을 확인할 수 있었습니다. Traefik은 이번 실습에서 다룬 기능보다 많은 기능이 제공되는는 유용한 오픈소스입니다"
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html",
    "href": "posts/2019-05-13-rmedicalresearch/index.html",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "",
    "text": "김진섭 대표는 을지의과대학교 5월 EMBRI 세미나와 CRScube 6월 세미나에 참석, 의학연구를 지원하면서 다양하게 R을 활용했던 경험을 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#요약",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "요약",
    "text": "요약\n\nR로 통계분석 뿐 아니라 논문, 발표 슬라이드, 홈페이지, 블로그, 웹 어플리케이션을 만들 수 있다.\n의학연구자들에게 맞춤형 통계 웹을 제공하는 것을 업으로 삼고 있다.\n범용으로 쓰일만한 것들을 웹과 R 패키지로 배포한다.\n심혈관중재학회와 계약, 1년간 레지스트리 연구에 대한 통계지원을 맡고 있다.\n심평원/보험공단 빅데이터 연구도 개별적으로 지원중."
  },
  {
    "objectID": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "href": "posts/2019-05-13-rmedicalresearch/index.html#slide",
    "title": "R 활용 맞춤형 통계지원 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/CRScube 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html",
    "href": "posts/2020-02-11-tokyor81/index.html",
    "title": "TokyoR 81회 리뷰",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 3월 Shinykorea 밋업에 참석, 일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#요약",
    "href": "posts/2020-02-11-tokyor81/index.html#요약",
    "title": "TokyoR 81회 리뷰",
    "section": "요약",
    "text": "요약\n일본 R 밋업 중 하나인 TokyoR 중 81회 shiny 특집을 리뷰하였다.\n\n초심자세션: R과 shiny 기초를 다루었다.\n응용세션1: 비동기 프로그래밍, 30분만에 적당히 사용할 shiny 앱 만들기 를 다루었다.\n응용세션2: leaflet 이용 지도앱 만들기, shiny앱을 지탱하는 엔지니어링 패키지 를 다루었다.\nLT: Shiny로 만드는 사진편집앱, Shiny로 사내용 범용 통계 앱 만들기, 오픈소스 약물동태 시뮬레이션, shiymeta로 재현가능한 R code 생성하기 등을 다루었다."
  },
  {
    "objectID": "posts/2020-02-11-tokyor81/index.html#slide",
    "href": "posts/2020-02-11-tokyor81/index.html#slide",
    "title": "TokyoR 81회 리뷰",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/LectureRpackage/TokyoR81 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html",
    "href": "posts/2022-07-13-r-datatable2/index.html",
    "title": "data.table 패키지 기초",
    "section": "",
    "text": "data.table은 빠른 속도와 메모리 효율성에 가장 적합한 패키지입니다.\n대용량의 데이터를 분산처리 시스템 없이 처리할 수 있습니다.\n데이터 프레임(data.frame)을 대신하여 더 빠르고 편리하게 사용할 수 있는 데이터 타입입니다.\n\n장점으로는, 상대적으로 메모리 요구량이 적고, 속도가 매우 빠르다는 특징이 있습니다.\n단점으로는, 다소 난해한 문법으로 널리 사용되지 못하고 있다는 특징이 있습니다.\n\n본격적으로 data.table에 대해서 알아보기 전에, Setup 과정에 대해서 먼저 소개하려고 합니다. data.table은 R에서 기본적으로 제공되는 데이터 구조가 아니기 때문에, package 설치가 필요합니다.\n\n## Setup\n# install.packages(\"data.table\")\n# install.packages(\"curl\")\nlibrary(data.table)\nlibrary(curl)\n\n위의 과정을 통해 pacakge 설치 및 불러오기를 실행합니다.\ndata.table을 생성하는 데는 두 가지 방법이 있습니다.\n첫번째는, data.table() 함수를 통해 직접 생성하는 방식입니다. 다음의 예시로 살펴보겠습니다.\n\nEX=data.table(\n  ID=c(\"A\",\"B\",\"C\",\"D\",\"E\"),\n  MATH=c(100,96,94,88,92),\n  ENGLISH=c(96,86,97,92,93),\n  HISTORY=c(85,92,87,92,94))\n\n\n\n\n\nID\nMATH\nENGLISH\nHISTORY\n\n\n\nA\n100\n96\n85\n\n\nB\n96\n86\n92\n\n\nC\n94\n97\n87\n\n\nD\n88\n92\n92\n\n\nE\n92\n93\n94\n\n\n\n\n\nID, MATH, ENGLISH, HISTORY를 변수로 한, data.table이 형성된 것을 확인할 수 있습니다.\n두번째는, 기존의 데이터를 불러오는 방법이 있습니다.\nfread 함수는 대용량 파일을 빠르게 가져올 수 있는 함수입니다. 파일을 읽어와서 data.table 형식의 자료로 만들 때, 로컬 file path를 입력하거나, http://로 시작하는 URL을 입력하는 방법을 사용할 수 있습니다.\n\nlibrary(data.table) ; library(magrittr)\ndf <- read.csv(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\ndt <- fread(\"https://raw.githubusercontent.com/jinseob2kim/lecture-snuhlab/master/data/example_g1e.csv\")\n\n09-15년 공단 건강검진 데이터에서 실습용으로 32명을 뽑은 자료를 이용하여,\ndf에는 data.frame 형식으로 데이터를 불러왔고, dt에는 fread 함수를 이용하여 data.table의 형식으로 데이터를 불러온 것을 확인할 수 있습니다.\nfread 함수로 파일을 불러오면 그 class는 data.frame에 data.table이 추가되며, 문법이 원래의 data.frame과 달라지는 점을 유의해야 합니다.\nclass 함수를 통해 df와 dt의 속성을 확인해보겠습니다.\n\nprint(class(df)) ; print(class(dt))\n\n[1] \"data.frame\"\n\n\n[1] \"data.table\" \"data.frame\"\n\n\ndt의 class에 data.table이 추가된 것을 확인할 수 있습니다.\n지금까지 data.table을 생성하는 두 가지 방법에 대해서 알아보았습니다.\n다음으로 data.table이 data.frame과 다른 점은, 행(Row)의 이름을 받지 않는 것을 기본값으로 한다는 것입니다.\n예시로 알아보도록 하겠습니다.\nR에 기본적으로 저장되어 있는 mtcars 데이터를 이용하도록 하겠습니다.\n\n# mtcars\nEX1<-as.data.frame(mtcars)\nEX2<-as.data.table(mtcars)\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n실행하였을 때, EX1과 EX2의 행의 이름에서 차이점이 있음을 확인할 수 있습니다.\n만약, data.table에서도 행의 이름을 남겨 놓고 싶을 때는 다음과 같이 실행하면 됩니다.\n\nEX3<-as.data.table(mtcars,keep.rownames=T)\n\n\n\n\n\n  \n\n\n\ndata 값 뒤에 keep.rownames=T로 설정하였을 때,\n각 행의 이름이 rn 컬럼에 남아 있는 것을 확인할 수 있습니다.\n\ndata.table의 기본 문법은 DT[i, j, by] 형태입니다.\n\ni는 행(row)과 관련되어, 행에 대해서 subset 하는 역할을 수행합니다.\nj는 열(column)을 선택하거나, 열 또는 테이블 전체(.SD)에 함수를 적용합니다.\nby는 집단을 나눕니다. j에서 지정한 열과 함수에 대한 실행을 그룹 별로(by group) 수행합니다.\n맨 마지막에 [order]를 붙여 오름차순이나 내림차순으로 정렬할 수 있습니다.\n\ndata.table에서만 확인할 수 있는 특수기호들이 있습니다.\n각 특수기호의 자세한 기능과 사용법은 이하에서 설명하기로 하고, 여기에서는 간단히 개념정도만 다뤄보려고 합니다.\n\n.SD : Subset of Data(by=로 나눠진 부분 데이터). 특수 기호를 사용하여 그룹 칼럼(by grouping columns)을 제외한 모든 칼럼을 대상으로 연산을 수행할 때 사용합니다.\n.SDcols : 특수 기호를 사용하여 특정 다수의 칼럼을 지정하여 처리할 때 사용합니다.\n.N : 부분 데이터의 행의 수를 나타낼 때 사용합니다. 특정한 열을 잡아서 length() 함수를 이용해도 되지만 좀 더 간편하게 구할 수 있습니다.\n:= : DT[i, j, by]에서 칼럼 j를 추가/갱신/삭제할 때 특수기호 := 연산자를 사용하여 수행할 수 있습니다.\n\n이상에서 data.table을 이용하면서 가장 많이 쓰이는 특수기호들에 대해서 알아보았습니다. 각각의 특수기호들이 어떻게 실제로 쓰이는지에 대해서는 이하에서 등장할 때마다 자세하게 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "href": "posts/2022-07-13-r-datatable2/index.html#data.table에-접근하기",
    "title": "data.table 패키지 기초",
    "section": "2. data.table에 접근하기",
    "text": "2. data.table에 접근하기\n이하에서는 위에서 불러온 dt(=09-15년 공단 건강검진 데이터)와 EX3(=mtcars) 데이터를 이용해서 실습하려고 합니다.\n2-1. 행(Row)에 접근하기\ndata.table에서 행(Row)에 접근하는 방법은 DT[i, j, by]에서 i 자리에 값을 넣는 것입니다. 즉, DT[c(row1, row2, …)]의 방식입니다.\nmtcars 데이터로 예시를 들어보겠습니다.\n여러 개의 자동차 종류 중, Datsun 710과 Hornet Sprotabout에 대해서만 알아보고 싶을 때는 다음과 같이 작성하면 됩니다.\n\nEX3[c(3,5)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\n4\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\n\n\n\n만약 Mazda RX4 부터 Hornet Sportabout까지 알아보고 싶다면, 범위로 지정할 수도 있습니다.\n\nEX3[1:5]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrn\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n2\n\n\n\n\n\n여기서 중요한 것은 DT[i, j, by]라는 기본적인 형식에서 i의 자리에 지금 내용을 채워 넣는 것인데, i 자리에 내용을 작성한 후 꼭 콤마(,)를 찍지 않아도 된다는 것입니다. 꼭 콤마(,)를 찍지 않아도 뒤에 특정한 열 을 선택하지 않으면 모든 열에 대해서 알아서 필터링을 하기 때문입니다.\n다음으로는 특정 조건을 만족하는 행(row)을 선택하는 방법에 대해서 알아보려고 합니다.\nDT[조건]의 형식을 이용하면 됩니다.\nmtcars 데이터에서 cyl>=6이면서, carb==4인 조건을 만족하는 데이터를 찾고 싶은 경우, 다음과 같이 작성하면 됩니다.\n\nEX3[cyl>=6 & carb==4]\n\n\n\n\n\n  \n\n\n\nKEY를 미리 설정해놓으면 더 빠르게 검색할 수 있는데, 이 내용에 대해서는 뒤에서 자세하게 다루도록 하겠습니다.\n다음으로는 특정 행(row)을 제외하는 방법에 대해서 알아보려고 합니다. 제외하려는 행 혹은 행의 범위 앞에 - 나 ! 를 붙여주면 됩니다.\n\nEX3[!1:5]\nEX3[-2]\n\n\n\n\n\n  \n\n\n\n\n\n  \n\n\n\n위와 같이 실행했을 때, 제외하려는 데이터가 사라진 것을 확인할 수 있습니다.\n2-2. 열(Column)에 접근하기\n행(Row)을 선택할 때와 유사합니다. 기본적인 형식은 DT[i, j, by]의 j 자리에 넣는 것입니다.\nmtcars 데이터에서 ‘cyl’ 열(Column)을 가져오고 싶을 때는 다음과 같이 가져올 수 있습니다.\n\nEX3[,3] ; EX3[,.(cyl)] ; EX3[,\"cyl\"]\n\n\n\n\n\ncyl\n\n\n\n6\n\n\n6\n\n\n4\n\n\n6\n\n\n8\n\n\n6\n\n\n\n\n\n열(column)의 숫자로 불러와도 되고, 변수의 이름으로 불러오는 것도 가능합니다. 그런데, 여기서 중요하게 봐야할 점은 변수의 이름으로 가져올 때 앞에 .()의 형식을 이용했다는 점입니다.\n.()은 list()와 동일한 기능을 하는데, 조금 더 간편하게 쓸 수 있는 형식이라 생각하면 됩니다.\ndata.table에서는 변수의 이름만 넣었을 경우, 벡터의 형식으로 값을 불러옵니다. 그렇기 때문에 data.table의 형식을 유지하면서 데이터를 불러오려면 .() 혹은 list() 형식을 이용해야 합니다. 혹은 변수를 따옴표를 이용하여 작성하는 것도 동일한 결과를 가져옵니다.\n열(column)을 선택할 때, DT[,.(new_col_name=col)] 형식을 사용하여 새로운 열 이름을 지정해서 출력할 수도 있습니다.\n\nEX3[,.(MPG=mpg, CYL=cyl)]\n\n\n\n\n\n  \n\n\n\n위와 같이 mpg와 cyl에 대해서 변수 이름을 대문자로 바꿔준 것을 확인할 수 있습니다.\n다음으로는 변수로 열을 선택하는 방법에 대해서 알아보려고 합니다.\n예시를 위해 mpg, cyl, disp 세 변수를 묶는 VARS라는 새로운 변수를 임의로 설정하겠습니다.\n\nVARS<-c(\"mpg\",\"cyl\",\"disp\")\n\n\nEX3[,..VARS]\n\n\n\n\n\n  \n\n\n\nVARS 라는 변수를 넣었을 때, 위에서 설정한 것처럼 mpg, cyl, disp에 대한 값들만 추출한 것을 확인할 수 있습니다. 여기서 중요한 것은, 변수 앞에 .. 을 넣어줬다는 것입니다.\ndata.table의 약속이라고 보면 되는데, 같은 결과를 도출하는 다른 형식들에 대해서 소개하려고 합니다.\n우선은, with = F가 있습니다.\n\nEX3[,VARS,with=F]\n\n\n\n\n\n  \n\n\n\nVARS 앞에 ..을 붙이지 않아도, with=F를 추가한다면 같은 결과를 도출하는 것을 확인할 수 있습니다.\n다음으로는 앞서 배운 .SD 과 .SDcols를 이용하는 방법에 대해 알아보겠습니다.\n\nEX3[,.SD,.SDcols=VARS]\n\n\n\n\n\n  \n\n\n\n.SD를 통해 전체 변수를 대상으로 하되, .SDcols로 특정 변수만을 설정하는 메커니즘입니다.\n또한 특정조건을 만족하는 값들에 대해 VARS의 변수 값을 알고 싶으면 다음과 같이 실행하면 됩니다.\n\nEX3[hp>=130 & gear>=4, ..VARS]\n\n\n\n\n\n  \n\n\n\nhp가 130을 넘고, gear가 4를 넘는 값들 중 VARS(mpg,cyl,disp) 변수에서 해당하는 값들을 보여주는 것을 확인할 수 있습니다.\n다음으로는 열을 제거하는 방법에 대해서 알아보려고 합니다. 행(row)을 제거할 때와 유사하게 - , ! 을 통해서 실행하면 됩니다. 그리고 같은 결과를 도출하는 다른 형식들에 대해서도 소개하려고 합니다.\n\nEX3[,-..VARS] ; EX3[,!..VARS] ; EX3[,.SD,.SDcols=-VARS]\n\n\n\n\n\n  \n\n\n\n마지막으로 열(column)의 값에 대해서 함수들을 이용해 값들을 가공하는 방법입니다.\nmpg와 hp의 평균에 대해서 구해보겠습니다.\n\nEX3[,.(mean(mpg), mean(hp))]\n\n\n\n\n\n  \n\n\n\n값을 실행할 경우, V1, V2라는 변수 아래에 값이 도출되는 것을 확인할 수 있습니다.\n위에서 배웠던, 변수에 새로운 이름을 부여하는 방식을 이용해보겠습니다.\n\nEX3[,.(MEAN_mpg=mean(mpg), MEAN_hp=mean(hp))]\n\n\n\n\n\n  \n\n\n\n위와 동일한 값에 변수의 이름이 생긴 것을 확인할 수 있습니다.\n.SD, .SDcols를 이용해서도 도출할 수 있습니다.\n\nEX3[,lapply(.SD,mean), .SDcols=(c(\"mpg\", \"hp\"))]\n\n\n\n\n\nmpg\nhp\n\n\n20.09062\n146.6875\n\n\n\n\n또한 행(row)의 자리에 특정 조건을 입력하여, 특정 조건을 만족하는 변수들에 대해서만 특정 함수를 적용할 수도 있습니다.\n\nEX3[gear==4, lapply(.SD,mean), .SDcols=c(\"mpg\",\"hp\")]\n\n\n\n\n\nmpg\nhp\n\n\n24.53333\n89.5\n\n\n\n\n2-3. by에 접근하기\nby는 집단을 나눕니다. 정확히는 옵션을 이용하여 그룹별로 함수를 적용할 수 있습니다.\nby = (그룹1, 그룹2, …)의 형식으로 두 개 이상의 그룹별로 함수를 적용할 수도 있는데, 이 때 괄호 앞에 있는 점(.)은 list를 의미하므로 꼭 포함시켜야 합니다. (ex. by=.(EXMD_BZ_YYYY, Q_SMK_YN) 와 같이 두 개 이상의 그룹으로 묶을 때는 .()의 형식을 이용해야 합니다.)\n어떻게 쓰이는지 바로 알아보도록 하겠습니다.\n여기에서는 dt(=09-15 공단 건강검진 데이터)를 이용해서 실습해보려고 합니다. EXMD_BZ_YYYY을 기준으로 집단을 분리한 후, 각 집단의 HGHT와 WGHT, BMI 평균을 구하는 방법은 다음과 같습니다.\n\ndt[,.(HGHT=mean(HGHT), WGHT=mean(WGHT), BMI=mean(BMI)), by= EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY에 따라 각 연도를 기준으로, HGHT, WGHT, BMI가 정렬이 되었고, 그 값들의 평균을 그룹별로 구하여 나타낸 데이터 값입니다.\n만약 특정한 변수가 아닌, 모든 변수에 대해서 평균을 구하고 싶다면 .SD를 이용하면 됩니다.\n\ndt[,lapply(.SD,mean), by=EXMD_BZ_YYYY]\n\n\n\n\n\n  \n\n\n\n위의 값은 평균을 낼 수 없는 변수들에 대해서도 일괄적으로 평균을 돌렸기 때문에 NA 값이 도출되었습니다. 값에 집중하기보다, 전체에 대한 함수를 적용하는 방식에 대해서 알아두면 좋을 것 같습니다. 만일 전체가 아닌 특정 변수에만 함수를 적용하고 싶다면 .SDcols 을 이용하면 됩니다.\n다음으로는 두 개 이상의 그룹 변수를 지정해 행(row)의 개수를 구해보겠습니다.\n키가 175cm 이상인 사람들에 대해서, 연도(EXMD_BZ_YYYY)와 흡연 여부(Q_SMK_YN)로 구분해보려고 합니다.\n\ndt[HGHT>=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)]\n\n\n\n\n\n  \n\n\n\n위에서 잠깐 언급한 .N 을 이용하여 특정조건에 부합하며 각 변수값에 해당되는 행(row)의 수를 구해보았습니다. 그러나, 여기에서 도출된 결과값의 문제는 Q_SMK_YN의 값이 섞여 있다는 것입니다.\n조금 더 정렬된 결과값으로 나타내고 싶을 때는, by 대신에 keyby를 이용하면 됩니다. keyby는 기존의 by에 오름차순/내림차순 기능이 포함되었다고 생각하면 됩니다. 만약 by를 이용하면서 정렬을 시키고 싶다면 마지막에 [order(정렬기준)]를 붙이면 됩니다.\n\ndt[HGHT>=175, .N, keyby=.(EXMD_BZ_YYYY, Q_SMK_YN)]\ndt[HGHT>=175, .N, by=.(EXMD_BZ_YYYY, Q_SMK_YN)][order(EXMD_BZ_YYYY)]\n\n\n\n\n\n  \n\n\n\n연도를 기준으로 정렬을 하고 싶은 경우, 위와 같이 뒤에 [order(EXMD_BZ_YYYY)]를 붙여주면, 위의 값과 다르게 정렬된 것을 확인할 수 있습니다.\n다음으로, 특정 조건(HGHT>=175)를 만족시키면서, 하나의 기준을 더 추가하여 분류하고 싶을 때는 다음과 같은 방식을 이용하면 됩니다.\n\ndt[HGHT>=175, .N, keyby=.(Y2015 = ifelse(EXMD_BZ_YYYY>=2015, \">=2015\", \"<2015\"))]\n\n\n\n\n\nY2015\nN\n\n\n\n<2015\n206\n\n\n>=2015\n36\n\n\n\n\n\nHGHT가 175 이상인 사람들을 우선으로 뽑아놓고, 거기에서 Y=2015를 기준으로 행(row)의 갯수를 확인하였습니다."
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "href": "posts/2022-07-13-r-datatable2/index.html#다른-기능들",
    "title": "data.table 패키지 기초",
    "section": "3. 다른 기능들",
    "text": "3. 다른 기능들\n3-1. setkey()\n키를 설정합니다. 키를 활용하는 이유는 자료를 찾을 때, 그 탐색 및 처리 시간을 단축시키기 위함입니다.\nsetkey(DT,col)로 키를 설정하며 키가 문자열 벡터일 경우 setkeyv을 활용합니다.\n만일 설정된 키를 제거할 경우, setkey(DT, NULL)를 활용합니다.\ndt 데이터를 이용하여, key를 설정하고 활용해보겠습니다.\n\nsetkey(dt, EXMD_BZ_YYYY)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\"\n\n\ndt의 키로 EXMD_BZ_YYYY가 설정된 것을 확인할 수 있습니다. 다른 변수들도 setkey 함수에 추가로 입력하면, 그 변수들이 key로 저장된 것을 확인할 수 있습니다.\n다음으로는 키를 활용한 행(row) 선택에 대해서 알아보려고 합니다. dt[.(a)], dt[J(a)], dt[list(a)], dt[col==a] 중에서 아무거나 사용하여 행을 선택할 수 있습니다. 위에서 EXMD_BZ_YYYY를 key로 설정하였기 때문에, dt[J(a)]에서 a의 자리에 EXMD_BZ_YYYY에 포함되어 있는 값을 넣으면 그 값을 기준으로 데이터를 정리합니다.\n예시로,\n\ndt[J(2013)]\n\n\n\n\n\n  \n\n\n\n이 값의 경우, EXMD_BZ_YYYY가 2013인 값에 대하여 정리한 것을 확인할 수 있습니다.\n만약, key를 두 개 이상 설정해놓은 경우, a의 자리에 다른 조건을 연결하면 그 조건도 포함하고 있는 값이 도출됩니다.\n\nsetkey(dt, EXMD_BZ_YYYY, Q_HBV_AG)\nkey(dt)\n\n[1] \"EXMD_BZ_YYYY\" \"Q_HBV_AG\"    \n\n\n\ndt[J(2013,2)]\n\n\n\n\n\n  \n\n\n\n위의 과정은 key에 Q_HBV_AG를 추가한 뒤, 2013년도에 Q_HBV_AG가 2인 값들에 대해서 정리한 것입니다.\n3-2. Merge : data.table 병합\n다음으로는 두 개의 data.table에 대해서 공통된 column을 기준으로,\n하나의 data.table로 만드는 방법에 대해서 소개하려고 합니다.\ndt 파일의 설문조사에 관한 데이터(Q_)들을 하나의 변수 colvars로 편의를 위해 설정하였습니다.\n\ncolvars<-grep(\"Q_\", names(dt), value=T)\n\n다음으로는 dt 데이터를 임의로 분리하여 dt1, dt2를 설정하겠습니다.\n\ndt1<-dt[1:10, .SD, .SDcols=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\", colvars)]\ndt2<-dt[6:15, -..colvars]\n\n본격적인 분석을 하기에 앞서, dt1 과 dt2에 대해서 간단히 살펴보겠습니다.\n행(row)을 기준으로는 6:10행까지가 겹치고,\n열(column)을 기준으로는 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM” 이 공통입니다.\ndt1, dt2 데이터를 이용해 merge 함수에 대해서 알아보도록 하겠습니다.\nmerge 에는 inner_join, full_join, left_join, right_join, anti_join 등이 있습니다.\n하나하나씩 예시와 함께 알아보도록 하겠습니다.\n처음으로 알아볼 것은 inner_join 입니다. 집합의 교집합 개념과 유사하지만, 약간의 차이점은 존재합니다.\n\ninner_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=F)\n\n\n\n\n\n  \n\n\n\n우선 inner_join을 실행함에 있어 단순하게 inner_join 함수를 이용해도 되지만, merge 함수에서 공통 변수인 “EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”을 직접 merge의 매개체로 설정할 수도 있습니다. 그리고 inner_join의 경우, merge 함수의 뒤에 all=F가 들어간다는 것을 유의하시면 될 것 같습니다. (뒤에 full_join과 비교)\ninner_join을 실행하였습니다. dt1과 dt2의 공통 행(row)에 속하는 6:10행까지를 기준으로 정렬하되, 각 값들이 dt1, dt2에서 가지고 있던 변수 값들도 그대로 가져온 것을 확인할 수 있습니다.\n결과값의 RN_INDI가 714509인 값을 살펴보겠습니다.\n이 변수값은 원래 dt1에서는 HGHT와 WGHT 등의 값을 가지고 있지 않았습니다. 그러나, inner_join을 하면서 dt2의 값을 그대로 받아와, HGHT, WGHT 등의 값을 부여받은 것을 확인할 수 있습니다.\n다음으로는 full_join 입니다. 집합의 합집합 개념과 유사합니다.\n\nfull_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all=T)\n\n\n\n\n\n  \n\n\n\nfull_join을 실행하였습니다. dt1과 dt2의 모든 행이 나열된 것을 확인할 수 있습니다. (공통된 행(row)은 한번만 표시되었습니다. 또한 inner_join과 다르게 all=T 임을 확인할 수 있습니다.)\n여기서 유심히 봐야할 것은 1:5, 11:15행입니다.\n1:5행의 경우에는 dt에는 속해 있지만, dt2에는 속해있지 않습니다. 그렇기 때문에 1:5행은 HGHT, WGHT 등 dt2에만 있는 값들에 대해서는 받을 값이 존재하지 않아, NA로 표시된 것을 확인할 수 있습니다.\n반면, 11:15행의 경우에는 dt2에는 속해 있지만, dt1에는 속해있지 않습니다. 그렇기 때문에 Q_로 시작하는 변수값에 대해서 받을 수가 없어서 NA로 나온 것을 확인할 수 있습니다.\n다음으로는 left_join과 right_join 입니다.\n\nleft_join(dt1,dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.x=T)\ndt2[dt1, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n\n  \n\n\n\nleft_join을 실행하였습니다. 변수의 값이 dt1의 row를 기준으로 설정된 것을 확인할 수 있습니다. 그러나, dt2의 column이 추가되어, HGHT, WGHT 등 기존의 dt1에 없던 변수들이 생긴 것을 확인할 수 있습니다. inner_join과 유사하게, dt2에 있는 변수들에 대해서는 left_join을 했을 때도, 원래 dt1에는 없었던 HGHT, WGHT 등의 값이 새로 생긴 것을 확인할 수 있습니다.\nright_join으로도 직접 실습하여 차이를 확인하시기 바랍니다.\n하나의 차이가 있다면, left_join을 했을 때는 all.x = T 였지만, right_join의 경우에는 all.y =T 를 사용합니다.\n\nright_join(dt1, dt2)\nmerge(dt1, dt2, by=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"), all.y=T)\ndt1[dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n\n  \n\n\n\n마지막으로 anti_join이 있습니다. 예시부터 보이고 설명하도록 하겠습니다.\n\nanti_join(dt1,dt2)\ndt1[!dt2, on = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\")]\n\n\n\n\n\n  \n\n\n\n직관적으로 확인할 수 있듯, dt2와 겹치지 않는 dt1의 값들에 대해서만 나타낸 것을 확인할 수 있습니다. 또한 다른 join들이 dt2의 column(variable)을 가져왔던 것과 다르게, anti_join은 오직 dt1의 변수들로만 구성된 것을 확인할 수 있습니다.\n만약에 anti_join(dt2,dt1) (또는, dt2[!dt1, on = c(“EXMD_BZ_YYYY”, “RN_INDI”, “HME_YYYYMM”))은 이라고 작성한다면 위와 반대로 dt2를 기준으로 데이터의 값이 도출됩니다.\n3-3. 수정 연산자 :=\ndata.table에서 열 j를 추가하거나 갱신 또는 삭제할 때 특수 기호 := 연산자를 사용합니다.\n수정 또는 생성하는 column이 두 개 이상이라면, DT[,c(“cola”, “colb”) := list(valA,valB)] 또는, DT[, “:=”(cola, colb)]의 형식을 사용합니다.\n즉, := 는 새로운 data.table을 생성하지 않고 기존의 데이터 테이블에 덮어씌우거나(수정), 새로운 컬럼을 추가합니다.\n다음 예시로 알아보겠습니다.\nBMI2 라는 새로운 변수를 data.table에 추가하려고 합니다.\n(BMI2 = WGHT/(HGHT/100)^2 를 하고, 소수점 첫째자리까지 반영)\n\ndt[, BMI2 := round(WGHT/(HGHT/100)^2, 1)]\n\n\n\n\n\n  \n\n\n\n열(column)의 맨 마지막에 BMI2가 새롭게 생긴 것을 확인할 수 있습니다.\n다음으로는 특정 조건을 충족하는 값들에 대해서 새로운 변수를 만들어 확인하는 것에 대해서 알아보려고 합니다.\n두 가지 조건을 설정하겠습니다. 하나는, BP_SYS가 140이 넘는지 그리고 BMI가 25가 넘는지에 대해서, factor로 바꾸어 0과 1로 나타내는 column에 대해서 추가하려고 합니다.\n\ndt[, ':=' (BP_SYS140 = factor(as.integer(BP_SYS>=140)), BMI25 = factor(as.integer(BMI>=25)))]\n\n\n\n\n\n  \n\n\n\ndt의 column에 새롭게 BP_SYS140과 BMI25 컬럼이 생성되어, 0과 1로 TRUE/FALSE를 보여주고 있습니다.\n그리고 := 을 이용해서 column을 삭제할 수도 있습니다. 위에서 만들어본 BMI25 column에 대해서 제거해보려고 합니다.\n\ndt[,BMI25 := NULL]\n\n\n\n\n\n  \n\n\n\n3-4. 데이터 재구조화\n마지막으로, data의 형태를 바꾸는 melt(wide to long), dcast(long to wide) 함수에 대해서 알아보겠습니다.\n우선 melt 함수입니다.\nmelt 함수는 일부 고정 칼럼을 제외한 나머지 칼럼을 stack 처리할 수 있습니다. melt 함수의 기본 구조는 다음과 같습니다.\nmelt(data, id.vars, measure.vars, variable.name, value.name)의 구조를 가지고 있습니다.\n\nid.vars에는 data에서 그대로 유지할, 고정될 column에 대해 작성하면 됩니다.\nmeasure.vars에는 새로운 변수에 포함될 기존의 데이터 값들에 대해서 넣으면 됩니다.\nvariable.name에는 measure.vars에서 추출한 데이터들을 모은 변수에 대한 이름을 설정하면 됩니다.\nvalue.name에는 그 variable.name의 값들이 적히는 곳의 이름을 설정한다고 보면 됩니다.\n\n예시로 쉽게 설명해보겠습니다.\n\nmelt_EX<-melt(dt,\n              id.vars=c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n              measure.vars = c(\"TOT_CHOL\", \"TG\", \"HDL\", \"LDL\"),\n              variable.name = \"Lipie\",\n              value.name = \"Value\")\nmelt_EX\n\n\n\n\n\n  \n\n\n\n위의 예시에서 EXMD_BZ_YYYY, RN_INDI, HME_YYYYMM 세 변수는 고정되어 있는 것을 확인할 수 있습니다. 그리고 TOT_CHOL, TG, HDL, LDL 값들이 Lipid라는 새로운 변수에 묶여있고, 그것들의 값이 Value에 나타나는 것을 확인할 수 있습니다.\nmelt 함수는 또한 동시에 여러 개의 칼럼들을 묶어서 사용할 수도 있습니다. melt 함수에 meausre = list(col1, col2, …) 형식으로 여러 개의 칼럼 이름을 list() 형태로 넣습니다. 이 때 공통의 value.name을 지정할 수 있습니다.\n다음의 예시를 보겠습니다.\n\ncol1<-c(\"BP_SYS\", \"BP_DIA\")\ncol2<-c(\"VA_LT\", \"VA_RT\")\nmelt_EX2 <- melt(dt,\n                 id.vars = c(\"EXMD_BZ_YYYY\", \"RN_INDI\", \"HME_YYYYMM\"),\n                 measure.vars = list(col1, col2),\n                 value.name = c(\"BP\", \"VA\"))\nmelt_EX2\n\n\n\n\n\n  \n\n\n\n예시가 직관적이진 않지만 간단하게 설명을 하자면, col1(BP_SYS, BP_DIA)과 col2(VA_LT, VA_RT)의 내용이 매칭 되는 것입니다. 그래서 BP_SYS와 VA_LT일 때, variable에서 1로 나타나고, BP_DIA와 VA_RT일 때, 2로 나타나는 것입니다.\n(따라서, BP_SYS와 VA_RT의 값이 매칭되는 경우는 없습니다.)\n(또한 만약에 col1과 col2에 각각 새로운 변수가 하나씩 추가되었다면 그 값은 variable에 3으로 표시될 것입니다.)\n다음은 dcast 입니다.\ndcast 함수는 melt 함수를 통해 길게 쌓여진 칼럼을 각 항목별로 분리하기 위해 사용합니다. 쉽게 설명하면, 방금 melt에서 행한 작업을 정확히 반대로 수행한다고 보면 됩니다.\ndcast의 기본 구조는 다음과 같습니다.\ndcast(data, formula, value.var, fun.aggregate)\n조금 더 실용적으로 작성하면 dcast(data, ID1+ID2+ID3+…~ variable, value.var = “val”)의 구조입니다.\n구조에 대해서 설명하자면,\n\ndata에는 dcast 함수를 실행할 데이터를 의미하고\nID1+ID2+ID3+…는 기존의 data부터 dcast 이후에도 고정적으로 유지될 변수들을 의미합니다.\nvariable은 melt 함수로 모아진 변수들을 다시 long to wide 하게 하기 위해 해당 변수들을 작성하는 곳입니다.\nvalue.var = ‘val’ 은 dcast 함수로 long to wide 하게 될 변수 값을 작성하는 공간입니다.\n그리고 variable과 value.var = 칸에는 melt를 하면서 지정한 변수 이름을 넣어주면 됩니다.\n\n예시로 알아보도록 하겠습니다.\n\ndcast_EX <- dcast(melt_EX, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ Lipid, value.var=\"Value\")\ndcast_EX\n\n\n\n\n\n  \n\n\n\nLipid 변수에 하나로 묶어뒀던 TOT_CHOL, TG, HDL, LDL 변수가 다시 각각의 변수로 바뀐 것을 확인할 수 있습니다.\n다음은 dcast 함수를 조금 더 응용해서, 재구조화를 할 때 sum, mean 등의 집계함수를 이용해서 그룹별 요약 통계량을 나타내는 과정을 소개하겠습니다.\n\ndcast_EX2 <- dcast(melt_EX, EXMD_BZ_YYYY ~ Lipid, value.var = \"Value\", fun.aggregate = mean, na.rm = T)\ndcast_EX2\n\n\n\n\n\n  \n\n\n\nEXMD_BZ_YYYY, 즉 연도 변수를 기준으로 Lipid에 묶여있던 TOT_CHOL, TG, HDL, LDL 변수들의 평균에 대해서 (결측치를 제거하고) 나타낸 것을 확인할 수 있습니다.\n조금 더 구체적으로 fun.aggregate 뒤에는 기존에 존재하는 함수가 아닌, fun.aggregate = function(x){}의 형식으로 어떠한 함수도 이용할 수 있습니다.\n다음으로는 여러 개의 칼럼들을 묶어서 melt 한 data.table에 대해서도 dcast를 하는 것에 대해 설명하겠습니다. 기본적으로 dcast 구조와 동일하지만 약간의 차이가 있습니다. variable 칸에는 그대로 long to wide 하려는 변수 이름만을 작성하면 되지만, value.var = 칸에는 각각의 데이터 값의 이름을 다 적어야 한다는 차이점이 존재합니다.\n예시로 보이겠습니다.\n\ndcast_EX3 <- dcast(melt_EX2, EXMD_BZ_YYYY+RN_INDI+HME_YYYYMM ~ variable, value.var = c(\"BP\", \"VA\"))\ndcast_EX3"
  },
  {
    "objectID": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "href": "posts/2022-07-13-r-datatable2/index.html#마치며",
    "title": "data.table 패키지 기초",
    "section": "4. 마치며",
    "text": "4. 마치며\n이상에서 R에서 데이터를 쉽고 빠르게 가공할 수 있는 data.table에 대하여 알아보았습니다.\n배운 내용을 가볍게 정리하고 마무리하도록 하겠습니다.\n1) 생성하기 : data.table은 기본적으로 설치되어 있는 프로그램이 아니기 때문에, package 설치가 필요하다.\n2) 기본구조 : data.table의 기본 문법은 DT [ i, j, by ] 형태이며 각각의 쓰임새는 다음과 같다.\n\ni : 행(row)을 선택\nj : 열(column)을 선택, data.table 전반에 함수를 적용\nby: 그룹을 구성, j에서 행한 함수를 by 그룹 별로 수행시킬 수 있음.\n\n3) 특수기호: data.table 에서만 확인할 수 있는 특수기호들이 있다. (.SD, .SDcols, .N, :=)\n4) KEY를 설정하여, 데이터에 조금 더 빠르게 접근할 수 있다.\n5) merge, melt, dcast 등의 함수를 통해 데이터를 보기 쉽게 가공할 수 있다.\n감사합니다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html",
    "href": "posts/2022-01-20-dbbackup/index.html",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "",
    "text": "숭실대학교 인턴십 프로그램을 통하여 참여한 차라투에서 인턴으로 활동하며 3주차 동안 학습한 내용에 대해 공유합니다."
  },
  {
    "objectID": "posts/2022-01-20-dbbackup/index.html#목차",
    "href": "posts/2022-01-20-dbbackup/index.html#목차",
    "title": "인턴십 - DB 자동 백업을 위한 Docker 및 Github 활용",
    "section": "목차",
    "text": "목차\n\nSSH-key 생성 및 GitHub 등록\nDockerfile 작성\nCronFile 및 Shell File 작성\n\n\n\nSSH-Key 생성 및 GitHub 등록\nGitHub Repository에 SSH를 통한 접근 인증을 위해서는 SSH Public Key 생성 및 등록 과정을 걸쳐야 합니다.\n\nSSH-Key 생성\nssh-keygen -t rsa \n\n\n\n생성된 key는 인증키(private key)의 경우 ‘/.ssh/id_rsa’에 저장되어 있고 공개 키(public key)는’/.ssh/id_rsa.pub’로 저장되어 있습니다. 저희는 공개키를 사용하기에 공개키를 복사해주세요.\ncat ~/.ssh/id_rsa.pub\n위 명령어를 실행하면 공개키가 출력되는 모습을 확인하실 수 있습니다. 이제 GitHub로 이동하겠습니다.\n\n\n\nGitHub 등록\n\n\n\n\n\n\n\n\n본인 GitHub계정에 로그인한 이후 Settings → SSH and GPG keys → new SSH key에 복사한 SSH Key를 붙여주면 됩니다. Title은 임의로 정하셔도 상관없습니다. SSH-Key 등록 이후 본격적인 진행에 앞서 앞으로의 코드에 이해를 돕고자 현재 작업하고 있는 로컬 디렉토리의 계층도를 안내하겠습니다.\n\n\n\n\n\n\n\n\nDockerFile 작성\nDockerContainer를 DockerFile을 통해 만들도록 하겠습니다. 코드의 전문은 아래와 같습니다.\n\n\n\nFROM mysql:8.0.27\n생성되는 컨테이너는 MYSQL:8.0.27 이미지를 바탕으로 한다는 내용의 코드입니다\n\nRUN apt-get update\nRUN apt-get install cron -y\nRUN apt-get install git-all -y \n생성되는 컨테이너에 주기적인 백업을 위한 Cron, Git 명령어 수행을 위한 Git을 설치하는 코드입니다.\n\nRUN mkdir ~/.ssh \nCOPY /rsa/id_rsa /root/.ssh/id_rsa\nCOPY /rsa/id_rsa.pub /root/.ssh/id_rsa.pub\n앞서 생성한 SSH-key를 컨테이너에서 활용 가능하도록 컨테이너 생성시 .ssh폴더를 생성한 뒤 해당 키 값들을 복사하여 생성하는 코드입니다.\n\nCOPY cron /etc/cron.d/cron\nCOPY backup.sh /etc/cron.d/backup.sh \nCOPY cronstart.sh /etc/cron.d/cronstart.sh \n로컬에 있는 cron, backup.sh, cronstart.sh 파일들을 복사하여 컨테이너 생성시 해당 디렉토리에 넣는 코드입니다.해당 파일의 내용은 아래에서 살펴보도록 하겠습니다.\n\n\n\nCronFile 및 Shell File작성\n\n\nCronFile 작성\n\n\n\n앞서 컨테이너의 /etc/cron.d/cron에 복사한 cron파일입니다.\nCron을 작동시키는 시간을 맞추기 위해서 TZ = Asia/Seoul를 설정하여 현재 서울의 시간대와 일치시켰습니다.\n* * * * * /etc/cron.d/backup.sh >> /var/log/cron.log 2>&1\n매분 마다 /etc/cron.d/backup.sh 파일을 작동시키고, 해당 파일의 작동 결과를 /var/log/cron.log 파일에 기록할 수 있도록 설정하였습니다. Backup.sh의 내용은 아래에서 살펴보겠습니다.\n(Cron 설정에 관련해서는 임의로 설정하셔도 상관없습니다.)\n\n\n\nShell 파일 작성\n\n\n\n앞서 Cron 설정으로 일정주기 마다 실행되는 Backup.sh 파일입니다. mysqldump를 활용해서 test db를 백업한 내용을 “$FileDir/$YmdH”.sql에 저장하고 이후 Git으로 Push를 진행해준다.\n여기까지cron을 활용하여 GitHub에 주기적으로 백업을 진행하는 내용의 코드는 완성되었습니다. 이에 더하여 컨테이너를 실행하면 cron이 자동으로 시작되는 기능을 추가하겠습니다.\n앞선 DockerFile에서 다음과 같은 코드를 확인할 수 있습니다.\nENTRYPOINT [\"/etc/cron.d/cronstart.sh\"]\nENTRYPOINT는 컨테이너가 시작되었을때 스크립트 혹은 명령을 실행합니다. 위 코드에서는 /etc/cron.d/cronstart.sh 파일을 실행하는 것입니다. /etc/cron.d/cronstart.sh 파일의 내용은 아래에서 확인하겠습니다.\n\n\n\ncronstart.sh\n#!/bin/bash\nservice cron start \nbash /usr/local/bin/docker-entrypoint.sh mysqld\n컨테이너가 시작되면 service cron start로 cron을 시작합니다. 이후 bash /usr/local/bin/docker-entrypoint.sh 파일에 mysqld 인자를 넘기며 실행시켜 해당 컨테이너에 mysqld를 작동시킵니다.\n\ndockerfile을 활용하여 컨테이너를 구축하고 실제로 작동이 잘되는지 확인해보도록 하겠습니다.\ndocker build -t test .\ndocker run -i --name test test \n\n\n\n컨테이너 생성과 동시에 정상적으로 DB 백업이 이루어지는 것을 확인할 수 있습니다.\n\n\n\n\n결론\nDB의 내용을 GitHub에 주기적으로 백업하는 방식에 대해 알아보았습니다. DB의 내용을 주기적으로 자동으로 백업이 가능하다는 유의미한 결과를 보이기는 하지만 코드상 DB의 암호가 노출된다는 점 등을 미루어 보았을 때 취약점이 존재하는것 같습니다. 개선된 방향으로의 학습이 필요할것 같습니다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html",
    "href": "posts/2020-10-05-docker-rshiny/index.html",
    "title": "RStudio & Shiny Docker 소개",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 10월 Shinykorea 밋업에 참석, 자체 제작한 RStudio & Shiny-server Docker image 를 소개할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "href": "posts/2020-10-05-docker-rshiny/index.html#요약",
    "title": "RStudio & Shiny Docker 소개",
    "section": "요약",
    "text": "요약\n\nRStudio와 Shiny-server 가 포함된 Docker image 이용, 새로 서버 구축할 때마다 재설치하는 번거로움을 없앤다.\n공식 image 참고하여 자체개발. https://github.com/jinseob2kim/docker-rshiny"
  },
  {
    "objectID": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "href": "posts/2020-10-05-docker-rshiny/index.html#slide",
    "title": "RStudio & Shiny Docker 소개",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/docker-rshiny 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html",
    "href": "posts/2020-06-20-shinymed2020/index.html",
    "title": "2020년 만들었던 ShinyApps",
    "section": "",
    "text": "김진섭 대표는 차라투 가 후원하는 6월 Shinykorea 밋업에 참석, 올해 만들었던 ShinyApps 를 공유할 예정입니다. 정리한 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#요약",
    "href": "posts/2020-06-20-shinymed2020/index.html#요약",
    "title": "2020년 만들었던 ShinyApps",
    "section": "요약",
    "text": "요약\n\n대한심혈관중재학회 COBIS III 레지스트리 분석: 추가계약\n서울성모병원 COREA-AMI II 레지스트리 분석: 10개 연구 계약\n삼성서울병원 공통데이터모델(CDM) 분석: 심평원 코로나데이터 분석 중\n강동성심병원 위암 위험인자 분석: 공단표본데이터 분석 중\n경기도감염병관리지원단 코로나 대시보드 with Shinykorea: 최종보고\n삼성서울병원 이식외과 육종(sarcoma) 데이터 분석: 5개 연구 계약\n해운대백병원 정신질환 네트워크분석: 논문 4편 게재\n성균관의대 환경역학연구실 미세먼지 대시보드\nShiny로 연구용 환자정보 입력웹(Electronic Case Report Forms, eCRF) 만들어 분석모듈 앞에 붙이고 싶습니다."
  },
  {
    "objectID": "posts/2020-06-20-shinymed2020/index.html#slide",
    "href": "posts/2020-06-20-shinymed2020/index.html#slide",
    "title": "2020년 만들었던 ShinyApps",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/PresentationShinyMed/shinymed2020 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "contributors.html",
    "href": "contributors.html",
    "title": "Contributors",
    "section": "",
    "text": "Below is a list of contributors to this blog.\n\n\n\nName\nRole\n\n\n\n\nJinseob Kim\nFounder & Lead Supporter\n\n\nChangwoo Lim\nMarketer\n\n\nHyunjun Ko\nDeveloper\n\n\nhyunki Lee\nDeveloper\n\n\nSiyeol Jung\nIntern\n\n\nYujin Lee\nIntern\n\n\nYumin Kim\nData analyst\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  title = {Contributors},\n  url = {https://blog.zarathu.com/posts/2023-02-01-streamlit/contributors.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Contributors.” n.d. https://blog.zarathu.com/posts/2023-02-01-streamlit/contributors.html."
  },
  {
    "objectID": "README.html",
    "href": "README.html",
    "title": "Zarathu Blog",
    "section": "",
    "text": "Zarathu Blog\nZarathu Official Blog - https://blog.zarathu.com\n   \n\n\n\n\nCitationBibTeX citation:@online{untitled,\n  author = {},\n  url = {https://blog.zarathu.com/README.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nn.d. https://blog.zarathu.com/README.html."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html",
    "href": "posts/2022-10-17-medical-scientist/index.html",
    "title": "의료데이터분석가 성장기",
    "section": "",
    "text": "김진섭 대표는 10월 19일(토) 융합형 의사과학자 심포지움 에서 “의료데이터분석가 성장기” 를 발표 예정입니다. 슬라이드를 미리 공유합니다."
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#요약",
    "href": "posts/2022-10-17-medical-scientist/index.html#요약",
    "title": "의료데이터분석가 성장기",
    "section": "요약",
    "text": "요약\n\n수학올림피아드 + 의대 = 의학통계(예방의학)\n의학통계 + IT기업(삼성전자 무선사업부) = 창업(의학통계지원)\n연매출 1.5억 + 파트타임 job = 소상공인(투자없이생존)\n소상공인 + 정부지원(사업비, 사무실) = 스타트업\n의학연구지원 \\(\\rightarrow\\) 임상시험분석시장 목표\n천하3분지계: 법학, 의학, 종교\n사람을 살리고 널리 인간을 이롭게하는 홍익인간\n김옥균, 맹상군, 유비\n문제해결 \\(\\rightarrow\\) 문제정의 \\(\\rightarrow\\) 아름다움 \\(\\rightarrow\\) 신내림"
  },
  {
    "objectID": "posts/2022-10-17-medical-scientist/index.html#slide",
    "href": "posts/2022-10-17-medical-scientist/index.html#slide",
    "title": "의료데이터분석가 성장기",
    "section": "Slide",
    "text": "Slide\n아래 슬라이드를 보거나 https://jinseob2kim.github.io/lecture-general/medical-scientist 를 클릭하면 볼 수 있다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html",
    "href": "posts/2023-02-01-streamlit/index.html",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "",
    "text": "https://docs.streamlit.io/library/api-reference 를 바탕으로 정리한 글입니다.\n2023년 2월 기준) streamlit version 1.17.0 을 기준으로 작성하였습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit과-shiny-예제-비교",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "1.1 streamlit과 shiny 예제 비교",
    "text": "1.1 streamlit과 shiny 예제 비교\n\nstreamlit 활용 사례\n\nhttps://streamlit.io/gallery\n\nshiny 활용 사례\n\nhttps://shiny.rstudio.com/gallery/"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-설치",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.1 스트림릿 설치",
    "text": "2.1 스트림릿 설치\n파이썬, 가상환경 설치와 관한 내용은 다른 게시물을 참고하시기 바랍니다.\npip install streamlit \n파이썬 가상환경에 streamlit패키지를 설치합니다\napp.py를 생성한 후 다음 과 같이 수정하여 저장합니다.\n\nimport streamlit as st\n\nst.title('Hello Streamlit')\n\n이후 터미널에서\nstreamlit run app.py \nstreamlit run app.py 명렁어를 실행하면, 로컬서버로 페이지가 만들어지게 됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "href": "posts/2023-02-01-streamlit/index.html#스트림릿-기능-소개",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.2 스트림릿 기능 소개",
    "text": "2.2 스트림릿 기능 소개\n더 자세한 내용은 https://docs.streamlit.io/library/api-reference에서 확인하실 수 있습니다.\n\n2.2.1 강조 문구\n\ntitle\nheader\nsubheader\n\n\nimport streamlit as st\n\nst.title('this is title')\nst.header('this is header')\nst.subheader('this is subheader')\n\n\n제목과 헤더,서브헤더를 구현할 수 있다.\n\netc\n\n추가적으로 Markdown 문법을 st.markdown으로, caption, Latex Code block을 활용가능합니다\n\n\n\n2.2.2 layout 짜기\n페이지의 공간을 레이아웃을 통해 웹페이지를 분할할 수 있다.\n\ncolumn\n\n\nimport streamlit as st\n\ncol1,col2 = st.columns([2,3])\n# 공간을 2:3 으로 분할하여 col1과 col2라는 이름을 가진 컬럼을 생성합니다.  \n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# with 구문 말고 다르게 사용 가능 \ncol1.subheader(' i am column1  subheader !! ')\ncol2.checkbox('this is checkbox2 in col2 ') \n#=>위에 with col2: 안의 내용과 같은 기능을합니다\n\n\n결과물\n\n\ntab\n\n\nimport streamlit as st\n\n# 탭 생성 : 첫번째 탭의 이름은 Tab A 로, Tab B로 표시합니다. \ntab1, tab2= st.tabs(['Tab A' , 'Tab B'])\n\nwith tab1:\n  #tab A 를 누르면 표시될 내용\n  st.write('hello')\n    \nwith tab2:\n  #tab B를 누르면 표시될 내용 \n  st.write('hi')\n\n 다음을 실행하면 tab A 를 눌렀을 경우 hello, tab B를 눌렀을 경우 hi가 나오게 됩니다.\n탭의 특징으로는, 탭을 클릭과 동시에 데이터가 만들어지는 것이 아니라,탭에 표시될 데이터가 이미 만들어져 았는 것이 특징입니다. 장점이 될 수도 있고, 단점이 될 수도 있습니다.\n \n\nsidebar\n\n\nimport streamlit as st\n\n#st.sidebar는 \n\nst.sidebar.title('this is sidebar')\nst.sidebar.checkbox('체크박스에 표시될 문구')\n# 사이드바에 체크박스, 버튼등 추가할 수 있습니다! \n\n \n\netc\n\n추가적으로 Expander, Container ,Empty가 있습니다\n\n\n2.2.3 이미지 불러오기\n\nimport streamlit as st\nfrom PIL import Image\n\n#PIL 패키지에 이미지 모듈을 통해 이미지 열기 \n# Image.open('이미지 경로')\nzarathu_img = Image.open('zarathu.png')\n\ncol1,col2 = st.columns([2,3])\n\nwith col1 :\n  # column 1 에 담을 내용\n  st.title('here is column1')\nwith col2 :\n  # column 2 에 담을 내용\n  st.title('here is column2')\n  st.checkbox('this is checkbox1 in col2 ')\n\n\n# 컬럼2에 불러온 사진 표시하기\ncol2.image(zarathu_img)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "href": "posts/2023-02-01-streamlit/index.html#웹사용자로부터-input-받기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "2.3 웹사용자로부터 input 받기",
    "text": "2.3 웹사용자로부터 input 받기\n이제 UI적인 부분을 전반적으로 살펴봤기 때문에, 사용자로 하여금 input을 받아 interactive하게 데이터를 보여주는 페이지를 만들어보겠습니다\n데이터는 사이킷런의 아이리스 데이터를 가져와 사용하도록 하겠습니다. \n\nimport numpy as np\nimport pandas as pd \nfrom sklearn.datasets import load_iris \nimport matplotlib.pyplot as plt\nimport streamlit as st\n\n\niris_dataset = load_iris()\n\ndf= pd.DataFrame(data=iris_dataset.data,columns= iris_dataset.feature_names)\ndf.columns= [ col_name.split(' (cm)')[0] for col_name in df.columns] # 컬럼명을 뒤에 cm 제거하였습니다\ndf['species']= iris_dataset.target \n\n\nspecies_dict = {0 :'setosa', 1 :'versicolor', 2 :'virginica'} \n\ndef mapp_species(x):\n  return species_dict[x]\n\n\ndf['species'] = df['species'].apply(mapp_species)\nprint(df)\n\n     sepal length  sepal width  petal length  petal width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n145           6.7          3.0           5.2          2.3  virginica\n146           6.3          2.5           5.0          1.9  virginica\n147           6.5          3.0           5.2          2.0  virginica\n148           6.2          3.4           5.4          2.3  virginica\n149           5.9          3.0           5.1          1.8  virginica\n\n[150 rows x 5 columns]\n\n\n\nstreamlit 에서 데이터 프레임을 보여주는 방식은 table과 dataframe 두가지를 사용할 수 있습니다.\ndataframe의 head함수를 이용하여 첫 5행의 데이터에 대해 table과 dataframe으로 출력한 경우입니다.\n\nst.subheader('this is table')\nst.table(df.head())\n\nst.subheader('this is data frame')\nst.dataframe(df.head())\n\n\n이제 버튼을 동작시키는 방법을 배워보도록 하겠습니다.\n 1. Select Box\n\n# 사이드바에 select box를 활용하여 종을 선택한 다음 그에 해당하는 행만 추출하여 데이터프레임을 만들고자합니다.\nst.sidebar.title('Iris Species🌸')\n\n# select_species 변수에 사용자가 선택한 값이 지정됩니다\nselect_species = st.sidebar.selectbox(\n    '확인하고 싶은 종을 선택하세요',\n    ['setosa','versicolor','virginica']\n)\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species']== select_species]\n# 선택한 종의 맨 처음 5행을 보여줍니다 \nst.table(tmp_df.head())\n\n\n사용자가 sidebar에서 종을 바꿀 때마다 자동으로 해당하는 종의 테이블 정보가 불러와지게 됩니다.\n\n\n\nmulti select\n\n\n# 여러개 선택할 수 있을 때는 multiselect를 이용하실 수 있습니다 \n# return : list\nselect_multi_species = st.sidebar.multiselect(\n    '확인하고자 하는 종을 선택해 주세요. 복수선택가능',\n    ['setosa','versicolor','virginica']\n\n)\n\n# 원래 dataframe으로 부터 꽃의 종류가 선택한 종류들만 필터링 되어서 나오게 일시적인 dataframe을 생성합니다\ntmp_df = df[df['species'].isin(select_multi_species)]\n# 선택한 종들의 결과표를 나타냅니다.  \nst.table(tmp_df)\n\n\n\n\nRadio / Slider\n\n\n# 라디오에 선택한 내용을 radio select변수에 담습니다\nradio_select =st.sidebar.radio(\n    \"what is key column?\",\n    ['sepal length', 'sepal width', 'petal length','petal width'],\n    horizontal=True\n    )\n# 선택한 컬럼의 값의 범위를 지정할 수 있는 slider를 만듭니다. \nslider_range = st.sidebar.slider(\n    \"choose range of key column\",\n     0.0, #시작 값 \n     10.0, #끝 값  \n    (2.5, 7.5) # 기본값, 앞 뒤로 2개 설정 /  하나만 하는 경우 value=2.5 이런 식으로 설정가능\n)\n\n# 필터 적용버튼 생성 \nstart_button = st.sidebar.button(\n    \"filter apply 📊 \"#\"버튼에 표시될 내용\"\n)\n\n# button이 눌리는 경우 start_button의 값이 true로 바뀌게 된다.\n# 이를 이용해서 if문으로 버튼이 눌렸을 때를 구현 \nif start_button:\n    tmp_df = df[df['species'].isin(select_multi_species)]\n    #slider input으로 받은 값에 해당하는 값을 기준으로 데이터를 필터링합니다.\n    tmp_df= tmp_df[ (tmp_df[radio_select] >= slider_range[0]) & (tmp_df[radio_select] <= slider_range[1])]\n    st.table(tmp_df)\n    # 성공문구 + 풍선이 날리는 특수효과 \n    st.sidebar.success(\"Filter Applied!\")\n    st.balloons()\n\nslider_range : list 형식으로 2개의 값이 저장됩니다. 양쪽 앞뒤로 두개의 값을 저장합니다.\nslider_range[0] : 최솟값 \nslider_range[1] : 최댓값  \n\npetal_width 컬럼값이 0에서 1.38사이인 값들로 정보를 filtering한 결과물이 표시됩니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#plotly",
    "href": "posts/2023-02-01-streamlit/index.html#plotly",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.1 plotly",
    "text": "3.1 plotly\npython의 시각화로 자주 이용되는 패키지는 matplotlib,seaborn,bokeh,plotly 등이 있습니다.\n스트림릿에서는 bokeh, plotly, matplotlib 등의 패키지를 통해 생성한 그림(figure)를 streamlit을 통해 웹에서 표시하는 기능을 제공합니다. \n\n3.1.1 st.plotly_chart\n\n#st.plotly_chart(figure_or_data, use_container_width=False, theme=\"streamlit\", **kwargs)\n\n 인자로 줄 수 있는 옵션들에 대해서 하나씩 설명해드리도록 하겠습니다.\n\nfigure_or_data : 첫번째 인자로 plotly로 생성한 그림의 이름이 들어가는 위치입니다.\nuse_container_width : 레이아웃으로 지정한 사이즈에 그림이 해상도를 조절해서 들어갈 것인지 (->‘True’)  아니면 원래그림 크기대로 표시될 것인지 (-> ‘False’) 선택하는 옵션입니다.\ntheme : 스트림릿 웹에 어떻게 표시될지 테마를 설정합니다. 인자로는 “streamlit” 과 None(입력하지 않음)을 선택할 수 있습니다.\n\n\n\nimport plotly.express as px\n\ndf = px.data.gapminder()\n\nfig = px.scatter(\n    df.query(\"year==2007\"),\n    x=\"gdpPercap\",\n    y=\"lifeExp\",\n    size=\"pop\",\n    color=\"continent\",\n    hover_name=\"country\",\n    log_x=True,\n    size_max=60,\n)\nfig.show()\n\ntab1, tab2 = st.tabs([\"Streamlit theme (default)\", \"Plotly native theme\"])\nwith tab1:\n    # Use the Streamlit theme.\n    # This is the default. So you can also omit the theme argument.\n    st.plotly_chart(fig, theme=\"streamlit\", use_container_width=True)\nwith tab2:\n    # Use the native Plotly theme.\n    st.plotly_chart(fig, theme=None, use_container_width=True)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#지도표시",
    "href": "posts/2023-02-01-streamlit/index.html#지도표시",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "3.2 지도표시",
    "text": "3.2 지도표시\n\nimport numpy as np\nimport pandas as pd \n\n#지도 위에 표시될 점 좌표 값을 위도경도에 담습니다 .\nbase_position =  [37.5073423, 127.0572734]\n#중심점의 위도, 경도 좌표를 리스트에 담습니다.\n\n# base_position에, 랜덤으로 생성한 값을 더하여 5개의 좌표를 데이터 프레임으로 생성하였고,\n# 컬럼명은 위도 :lat  경도 lon으로 지정하였습니다. \n\n\nmap_data = pd.DataFrame(\n    np.random.randn(5, 1) / [20, 20] + base_position , \n    columns=['lat', 'lon'])\n# map data 생성 : 위치와 경도 \n\nprint(map_data)\n\n         lat         lon\n0  37.425135  126.975066\n1  37.525337  127.075268\n2  37.485133  127.035064\n3  37.486136  127.036067\n4  37.439659  126.989590\n\n\n이어서 이 위도 경도 데이터를 스트림릿 웹 페이지에 지도로 나타내는 과정은 다음과 같습니다. \n\nst.code('st.map(map_data)')\n# 웹사이트에 어떤 코드인지 표시해주기 \nst.subheader('Map of Data ')\n# 제목 생성 \nst.map(map_data)\n# 지도 생성 \n\n결과물"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-세션-스테이트란",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.1 st.session_state 세션 스테이트란?",
    "text": "4.1 st.session_state 세션 스테이트란?\n상태가 자꾸 변하는 것들을 세션스테이트에 관리해두면 바뀌는 값에 따라 내용이 바뀌는 것들을 기록할 수 있다.\n** 아주 중요한 역할을 하는 기능입니다. **"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "href": "posts/2023-02-01-streamlit/index.html#st.session_state-활용",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.2 st.session_state 활용",
    "text": "4.2 st.session_state 활용\n\n\n\nwhy we use sessionstate\n\n\n세션스테이트에 key값의 초기값이 없으면, 초기값을 생성하여 놓는 작업을 if문을 통해 진행합니다.\n세션스테이트에 사용자가 입력한 인풋에 따라서 dataframe이 재가공 되는데 이 값이 interactive하게 지정되게 하기 위해 st.session_state값으로 사용합니다.\n\n# 예시코드 \n# import streamlit as st\n\n\n\n# if 'final_dataframe' not in st.session_state:\n#   # session state 에 final 이라는 값이 없으면, \n#   st.session_state['final_dataframe']= df\n#   # 초기 값 설정 : session_state에 final_dataframe키 값에 초기값 데이터를 집어넣습니다 .\n\n# #아래 코드는 df의 테이블 값이 바뀌더라도 interactive하게 연동되서 바뀌지 않습니다\n# st.table(df)\n\n# #  아래 코드는  이제 dataframe가 조작될 때 마다 session_state객체 안에 final_dataframe값을 변경하면, \n# #  수정 될 때  계속 바뀌어서 보여줍니다. \n# st.table(st.session_state.final_dataframe)"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "href": "posts/2023-02-01-streamlit/index.html#cache-란-무엇인가",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.3 cache 란 무엇인가?",
    "text": "4.3 cache 란 무엇인가?\n\n\n\n캐싱기능의 필요성\n\n\n캐싱에 관한 간단한 개념은 주문이 들어왔을 때 우리가 만들기 시작하면 코드가 결과물을 만들어내는데 시간이 오래 걸리는 경우 유저가 결과물을 오랜시간 기달려야하는 경우가 발생합니다.\n따라서 만들어내는데 오래걸리는 결과물을 미리 만들어두고, 보이지 않는 곳에 캐싱하여 필요할때 찾아 꺼내는 것을 cache기능이라고 간단히 설명하도록 하겠습니다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "href": "posts/2023-02-01-streamlit/index.html#streamlit-에서-cache-기능-사용하기",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.4 streamlit 에서 cache 기능 사용하기",
    "text": "4.4 streamlit 에서 cache 기능 사용하기\nstreamlit에서는 시간이 오래걸리는 작업: 데이터 로드 등을 할때 위에 (st.cache를?) 추가하여 캐싱할 수 있게 할 수 있다.\n\nimport streamlit as st\nimport pandas as pd \n\nfile_path = '~~~filepath'\n@st.cache\ndef load_data():\n  data = pd.read_csv(file_path)\n  return data\n\n큰 데이터를 로드하거나, 실행이 오래걸리는 복잡한 연산을 해야할 때 cache기능을 이용하면 용이하다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "href": "posts/2023-02-01-streamlit/index.html#로딩상태-구현",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "4.5 로딩상태 구현",
    "text": "4.5 로딩상태 구현\n\n4.5.1 st.progress\n\nimport time \n\n# 방법 1 progress bar \nlatest_iteration = st.empty()\nbar = st.progress(0)\n\nfor i in range(100):\n  # Update the progress bar with each iteration.\n  latest_iteration.text(f'Iteration {i+1}')\n  bar.progress(i + 1)\n  time.sleep(0.05)\n  # 0.05 초 마다 1씩증가\n\nst.balloons()\n# 시간 다 되면 풍선 이펙트 보여주기 \n\n코드 수행 소요시간이 긴 경우, progress bar와 streamlit의 spinner를 통해서 로딩페이지를 만들 수 있다!\n \n\n\n4.5.2 st. spinner 사용\n\n#방법2  st.spinner 사용 \nimport time \n\nwith st.spinner('Wait for it...'):\n  time.sleep(5)\n  st.success('Done!')"
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "href": "posts/2023-02-01-streamlit/index.html#근본적-한계",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.1 근본적 한계",
    "text": "5.1 근본적 한계\nstreamlit 에서 제공하는 UI들이 대부분 예쁘지만, 디테일적으로 맘에 들지 않을 수가 있다.\n\n예를 들어 4.87로 표시된 부분의 폰트가 맘에들지 않는다고 하자. 그렇다면 streamlit를 markdown을 통해서 style을 직접 수정해주어야한다."
  },
  {
    "objectID": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "href": "posts/2023-02-01-streamlit/index.html#극복방안-css-hack",
    "title": "Python Streamlit 패키지를 이용한 대시보드 만들기",
    "section": "5.2 극복방안 : CSS hack",
    "text": "5.2 극복방안 : CSS hack\n이 작업을 streamlit CSS hack이라고 한다\n\n브라우저에 검사기능을 이용하여 css해당하는 부분을 마크다운으로 직접 style tag를 넣어서 수정하면 된다.\n아래 유튜브를 통해서 더욱 자세한 내용을 배울 수 있습니다.\nhttps://www.youtube.com/watch?v=gr_KyGfO_eU"
  }
]